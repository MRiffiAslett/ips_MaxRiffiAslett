Used config:
{'B': 16,
 'B_seq': 16,
 'D': 128,
 'D_inner': 512,
 'D_k': 16,
 'D_v': 16,
 'H': 8,
 'I': 100,
 'M': 100,
 'N': 3600,
 'attn_dropout': 0.1,
 'data_dir': 'data/megapixel_mnist/dsets/megapixel_mnist_1500',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.001,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 1,
 'n_class': 10,
 'n_epoch': 100,
 'n_epoch_warmup': 10,
 'n_res_blocks': 2,
 'n_token': 4,
 'n_worker': 2,
 'patch_size': [50, 50],
 'patch_stride': [50, 50],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': False,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'majority'},
           'task1': {'act_fn': 'softmax',
                     'id': 1,
                     'metric': 'accuracy',
                     'name': 'max'},
           'task2': {'act_fn': 'softmax',
                     'id': 2,
                     'metric': 'accuracy',
                     'name': 'top'},
           'task3': {'act_fn': 'sigmoid',
                     'id': 3,
                     'metric': 'multilabel_accuracy',
                     'name': 'multi'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': True,
 'wd': 0.1}
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: majority, mean loss: 2.36780, accuracy: 0.11250, task: max, mean loss: 2.14768, accuracy: 0.23300, task: top, mean loss: 2.35016, accuracy: 0.10000, task: multi, mean loss: 0.67046, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.88402, lr: 0.0001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 1 
task: majority, mean loss: 2.31706, accuracy: 0.09000, task: max, mean loss: 1.89259, accuracy: 0.27400, task: top, mean loss: 2.31390, accuracy: 0.09800, task: multi, mean loss: 0.60413, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78192
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 2 
task: majority, mean loss: 2.32486, accuracy: 0.09200, task: max, mean loss: 1.86214, accuracy: 0.23800, task: top, mean loss: 2.32864, accuracy: 0.10000, task: multi, mean loss: 0.60572, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78034, lr: 0.0002
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 2 
task: majority, mean loss: 2.32673, accuracy: 0.08900, task: max, mean loss: 1.88143, accuracy: 0.27400, task: top, mean loss: 2.32621, accuracy: 0.10000, task: multi, mean loss: 0.60147, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78396
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 3 
task: majority, mean loss: 2.33168, accuracy: 0.10450, task: max, mean loss: 1.85040, accuracy: 0.24600, task: top, mean loss: 2.32634, accuracy: 0.11900, task: multi, mean loss: 0.60636, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77869, lr: 0.0003
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 3 
task: majority, mean loss: 2.32206, accuracy: 0.09000, task: max, mean loss: 1.86175, accuracy: 0.27400, task: top, mean loss: 2.33448, accuracy: 0.10100, task: multi, mean loss: 0.60117, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77987
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 4 
task: majority, mean loss: 2.33189, accuracy: 0.09600, task: max, mean loss: 1.84975, accuracy: 0.25300, task: top, mean loss: 2.33080, accuracy: 0.10550, task: multi, mean loss: 0.60597, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77960, lr: 0.0004
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 4 
task: majority, mean loss: 2.35040, accuracy: 0.09000, task: max, mean loss: 1.86223, accuracy: 0.27400, task: top, mean loss: 2.31765, accuracy: 0.09700, task: multi, mean loss: 0.60333, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78340
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 5 
task: majority, mean loss: 2.33324, accuracy: 0.09750, task: max, mean loss: 1.84885, accuracy: 0.25250, task: top, mean loss: 2.33130, accuracy: 0.10200, task: multi, mean loss: 0.60556, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77974, lr: 0.0005
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 5 
task: majority, mean loss: 2.32726, accuracy: 0.09400, task: max, mean loss: 1.86265, accuracy: 0.21400, task: top, mean loss: 2.32546, accuracy: 0.10000, task: multi, mean loss: 0.60067, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77901
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 6 
task: majority, mean loss: 2.33295, accuracy: 0.10550, task: max, mean loss: 1.84258, accuracy: 0.25000, task: top, mean loss: 2.32723, accuracy: 0.10950, task: multi, mean loss: 0.60617, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77723, lr: 0.0006
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 6 
task: majority, mean loss: 2.31186, accuracy: 0.10600, task: max, mean loss: 1.86474, accuracy: 0.25300, task: top, mean loss: 2.30786, accuracy: 0.11400, task: multi, mean loss: 0.60123, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77142
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 7 
task: majority, mean loss: 2.32746, accuracy: 0.10350, task: max, mean loss: 1.84287, accuracy: 0.26950, task: top, mean loss: 2.32884, accuracy: 0.10950, task: multi, mean loss: 0.60440, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77589, lr: 0.0007
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 7 
task: majority, mean loss: 2.31637, accuracy: 0.09400, task: max, mean loss: 1.90886, accuracy: 0.21300, task: top, mean loss: 2.31527, accuracy: 0.10000, task: multi, mean loss: 0.60531, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78645
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 8 
task: majority, mean loss: 2.32406, accuracy: 0.10850, task: max, mean loss: 1.84430, accuracy: 0.25650, task: top, mean loss: 2.32172, accuracy: 0.10650, task: multi, mean loss: 0.60559, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77392, lr: 0.0008
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 8 
task: majority, mean loss: 2.33504, accuracy: 0.10100, task: max, mean loss: 1.86766, accuracy: 0.27400, task: top, mean loss: 2.32163, accuracy: 0.10200, task: multi, mean loss: 0.60210, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78161
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 9 
task: majority, mean loss: 2.32697, accuracy: 0.09200, task: max, mean loss: 1.84252, accuracy: 0.25400, task: top, mean loss: 2.32280, accuracy: 0.09800, task: multi, mean loss: 0.60537, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77441, lr: 0.0009
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 9 
task: majority, mean loss: 2.30923, accuracy: 0.11100, task: max, mean loss: 1.85543, accuracy: 0.27400, task: top, mean loss: 2.31012, accuracy: 0.09700, task: multi, mean loss: 0.60059, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76884
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 10 
task: majority, mean loss: 2.32775, accuracy: 0.09700, task: max, mean loss: 1.84317, accuracy: 0.25200, task: top, mean loss: 2.32307, accuracy: 0.10400, task: multi, mean loss: 0.60554, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77488, lr: 0.001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 10 
task: majority, mean loss: 2.34055, accuracy: 0.10600, task: max, mean loss: 1.86362, accuracy: 0.23500, task: top, mean loss: 2.33370, accuracy: 0.09900, task: multi, mean loss: 0.60262, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78512
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 11 
task: majority, mean loss: 2.29139, accuracy: 0.11850, task: max, mean loss: 1.83728, accuracy: 0.25000, task: top, mean loss: 2.30107, accuracy: 0.11950, task: multi, mean loss: 0.60270, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.75811, lr: 0.0009996957180960382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 11 
task: majority, mean loss: 2.37768, accuracy: 0.11500, task: max, mean loss: 1.85119, accuracy: 0.27900, task: top, mean loss: 2.31428, accuracy: 0.10500, task: multi, mean loss: 0.60185, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78625
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 12 
task: majority, mean loss: 2.23289, accuracy: 0.15050, task: max, mean loss: 1.82728, accuracy: 0.26400, task: top, mean loss: 2.27030, accuracy: 0.14750, task: multi, mean loss: 0.59363, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.73103, lr: 0.0009987832431047822
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 12 
task: majority, mean loss: 2.34562, accuracy: 0.12700, task: max, mean loss: 1.87843, accuracy: 0.26100, task: top, mean loss: 2.35546, accuracy: 0.12100, task: multi, mean loss: 0.59737, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.79422
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 13 
task: majority, mean loss: 2.18351, accuracy: 0.15900, task: max, mean loss: 1.82036, accuracy: 0.26200, task: top, mean loss: 2.23538, accuracy: 0.14650, task: multi, mean loss: 0.58691, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.70654, lr: 0.0009972636867364526
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 13 
task: majority, mean loss: 2.20957, accuracy: 0.16200, task: max, mean loss: 1.83602, accuracy: 0.27500, task: top, mean loss: 2.25226, accuracy: 0.10700, task: multi, mean loss: 0.58014, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.71950
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 14 
task: majority, mean loss: 2.14875, accuracy: 0.18250, task: max, mean loss: 1.80990, accuracy: 0.27550, task: top, mean loss: 2.19932, accuracy: 0.17450, task: multi, mean loss: 0.58433, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.68558, lr: 0.0009951389003364144
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 14 
task: majority, mean loss: 2.32261, accuracy: 0.15400, task: max, mean loss: 1.85653, accuracy: 0.21300, task: top, mean loss: 2.30617, accuracy: 0.12300, task: multi, mean loss: 0.59880, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77103
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 15 
task: majority, mean loss: 2.11089, accuracy: 0.21750, task: max, mean loss: 1.80701, accuracy: 0.28250, task: top, mean loss: 2.14953, accuracy: 0.19800, task: multi, mean loss: 0.57659, multilabel_accuracy: 0.00050, avg. loss over tasks: 1.66100, lr: 0.000992411472629598
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 15 
task: majority, mean loss: 2.09578, accuracy: 0.20800, task: max, mean loss: 1.82292, accuracy: 0.27300, task: top, mean loss: 2.15493, accuracy: 0.19600, task: multi, mean loss: 0.57009, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.66093
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 16 
task: majority, mean loss: 2.10900, accuracy: 0.21900, task: max, mean loss: 1.79921, accuracy: 0.25950, task: top, mean loss: 2.14154, accuracy: 0.20500, task: multi, mean loss: 0.58119, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.65773, lr: 0.000989084726566536
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 16 
task: majority, mean loss: 2.16727, accuracy: 0.19700, task: max, mean loss: 1.83756, accuracy: 0.24800, task: top, mean loss: 2.26794, accuracy: 0.14000, task: multi, mean loss: 0.57655, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.71233
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 17 
task: majority, mean loss: 2.07650, accuracy: 0.23450, task: max, mean loss: 1.79848, accuracy: 0.26600, task: top, mean loss: 2.09742, accuracy: 0.21750, task: multi, mean loss: 0.57384, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.63656, lr: 0.00098516271527486
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 17 
task: majority, mean loss: 2.25845, accuracy: 0.15500, task: max, mean loss: 1.87006, accuracy: 0.25900, task: top, mean loss: 2.24934, accuracy: 0.16700, task: multi, mean loss: 0.58002, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.73947
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 18 
task: majority, mean loss: 2.07135, accuracy: 0.22500, task: max, mean loss: 1.78650, accuracy: 0.26200, task: top, mean loss: 2.08713, accuracy: 0.22100, task: multi, mean loss: 0.57132, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.62908, lr: 0.0009806502171211902
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 18 
task: majority, mean loss: 2.03480, accuracy: 0.26100, task: max, mean loss: 1.82070, accuracy: 0.26900, task: top, mean loss: 2.11953, accuracy: 0.20900, task: multi, mean loss: 0.56502, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.63501
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 19 
task: majority, mean loss: 2.00522, accuracy: 0.25150, task: max, mean loss: 1.78414, accuracy: 0.26900, task: top, mean loss: 2.02887, accuracy: 0.25000, task: multi, mean loss: 0.56614, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.59609, lr: 0.0009755527298894293
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 19 
task: majority, mean loss: 2.04270, accuracy: 0.27400, task: max, mean loss: 1.78663, accuracy: 0.26500, task: top, mean loss: 2.10894, accuracy: 0.23700, task: multi, mean loss: 0.56271, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.62524
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 20 
task: majority, mean loss: 1.94124, accuracy: 0.29400, task: max, mean loss: 1.76534, accuracy: 0.27800, task: top, mean loss: 1.95674, accuracy: 0.28700, task: multi, mean loss: 0.56277, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.55652, lr: 0.0009698764640825613
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 20 
task: majority, mean loss: 1.95040, accuracy: 0.29700, task: max, mean loss: 1.79712, accuracy: 0.28500, task: top, mean loss: 2.05838, accuracy: 0.23000, task: multi, mean loss: 0.56077, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.59167
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 21 
task: majority, mean loss: 1.90456, accuracy: 0.33150, task: max, mean loss: 1.75941, accuracy: 0.28800, task: top, mean loss: 1.91457, accuracy: 0.30550, task: multi, mean loss: 0.55953, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.53452, lr: 0.0009636283353561103
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 21 
task: majority, mean loss: 1.99637, accuracy: 0.28600, task: max, mean loss: 1.81903, accuracy: 0.25500, task: top, mean loss: 2.05174, accuracy: 0.24500, task: multi, mean loss: 0.57264, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.60994
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 22 
task: majority, mean loss: 1.81910, accuracy: 0.34350, task: max, mean loss: 1.74376, accuracy: 0.29600, task: top, mean loss: 1.84114, accuracy: 0.33000, task: multi, mean loss: 0.55354, multilabel_accuracy: 0.00050, avg. loss over tasks: 1.48938, lr: 0.0009568159560924791
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 22 
task: majority, mean loss: 1.96655, accuracy: 0.34400, task: max, mean loss: 1.81497, accuracy: 0.25000, task: top, mean loss: 1.96511, accuracy: 0.30200, task: multi, mean loss: 0.54010, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.57168
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 23 
task: majority, mean loss: 1.73458, accuracy: 0.38000, task: max, mean loss: 1.69569, accuracy: 0.32400, task: top, mean loss: 1.77182, accuracy: 0.35800, task: multi, mean loss: 0.53283, multilabel_accuracy: 0.00600, avg. loss over tasks: 1.43373, lr: 0.000949447626126434
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 23 
task: majority, mean loss: 1.98473, accuracy: 0.29700, task: max, mean loss: 1.69550, accuracy: 0.34600, task: top, mean loss: 2.05280, accuracy: 0.27900, task: multi, mean loss: 0.52440, multilabel_accuracy: 0.00300, avg. loss over tasks: 1.56436
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 24 
task: majority, mean loss: 1.65810, accuracy: 0.40300, task: max, mean loss: 1.64204, accuracy: 0.34300, task: top, mean loss: 1.71175, accuracy: 0.39450, task: multi, mean loss: 0.52231, multilabel_accuracy: 0.00500, avg. loss over tasks: 1.38355, lr: 0.000941532322633034
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 24 
task: majority, mean loss: 1.79132, accuracy: 0.35000, task: max, mean loss: 1.67966, accuracy: 0.35600, task: top, mean loss: 1.87959, accuracy: 0.33500, task: multi, mean loss: 0.52368, multilabel_accuracy: 0.00400, avg. loss over tasks: 1.46856
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 25 
task: majority, mean loss: 1.54548, accuracy: 0.42700, task: max, mean loss: 1.57430, accuracy: 0.38800, task: top, mean loss: 1.63276, accuracy: 0.42200, task: multi, mean loss: 0.50784, multilabel_accuracy: 0.00800, avg. loss over tasks: 1.31510, lr: 0.0009330796891903273
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 25 
task: majority, mean loss: 2.41054, accuracy: 0.23700, task: max, mean loss: 1.79836, accuracy: 0.29800, task: top, mean loss: 2.20560, accuracy: 0.23000, task: multi, mean loss: 0.58046, multilabel_accuracy: 0.00900, avg. loss over tasks: 1.74874
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 26 
task: majority, mean loss: 1.47732, accuracy: 0.46300, task: max, mean loss: 1.49596, accuracy: 0.42200, task: top, mean loss: 1.54121, accuracy: 0.45700, task: multi, mean loss: 0.50237, multilabel_accuracy: 0.01400, avg. loss over tasks: 1.25421, lr: 0.0009241000240301347
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 26 
task: majority, mean loss: 1.84062, accuracy: 0.32500, task: max, mean loss: 1.53939, accuracy: 0.38700, task: top, mean loss: 1.74701, accuracy: 0.37900, task: multi, mean loss: 0.50462, multilabel_accuracy: 0.02300, avg. loss over tasks: 1.40791
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 27 
task: majority, mean loss: 1.44126, accuracy: 0.47950, task: max, mean loss: 1.38247, accuracy: 0.47150, task: top, mean loss: 1.51909, accuracy: 0.46650, task: multi, mean loss: 0.49659, multilabel_accuracy: 0.01200, avg. loss over tasks: 1.20985, lr: 0.0009146042674912433
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 27 
task: majority, mean loss: 1.54557, accuracy: 0.45100, task: max, mean loss: 1.38920, accuracy: 0.48900, task: top, mean loss: 1.56013, accuracy: 0.47100, task: multi, mean loss: 0.48291, multilabel_accuracy: 0.01800, avg. loss over tasks: 1.24445
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 28 
task: majority, mean loss: 1.24831, accuracy: 0.56400, task: max, mean loss: 1.17614, accuracy: 0.57950, task: top, mean loss: 1.33578, accuracy: 0.55750, task: multi, mean loss: 0.48089, multilabel_accuracy: 0.01900, avg. loss over tasks: 1.06028, lr: 0.0009046039886902862
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 28 
task: majority, mean loss: 1.73333, accuracy: 0.43600, task: max, mean loss: 1.38535, accuracy: 0.52100, task: top, mean loss: 1.57881, accuracy: 0.46200, task: multi, mean loss: 0.48939, multilabel_accuracy: 0.02400, avg. loss over tasks: 1.29672
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 29 
task: majority, mean loss: 1.07587, accuracy: 0.63750, task: max, mean loss: 1.01309, accuracy: 0.64650, task: top, mean loss: 1.15432, accuracy: 0.62350, task: multi, mean loss: 0.45432, multilabel_accuracy: 0.03800, avg. loss over tasks: 0.92440, lr: 0.0008941113714265576
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 29 
task: majority, mean loss: 1.10992, accuracy: 0.62300, task: max, mean loss: 0.98701, accuracy: 0.66200, task: top, mean loss: 1.26245, accuracy: 0.59800, task: multi, mean loss: 0.43649, multilabel_accuracy: 0.05900, avg. loss over tasks: 0.94897
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 30 
task: majority, mean loss: 0.95349, accuracy: 0.67750, task: max, mean loss: 0.90783, accuracy: 0.69350, task: top, mean loss: 1.06966, accuracy: 0.64850, task: multi, mean loss: 0.43002, multilabel_accuracy: 0.06000, avg. loss over tasks: 0.84025, lr: 0.0008831391993379295
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 30 
task: majority, mean loss: 1.12703, accuracy: 0.62100, task: max, mean loss: 0.99494, accuracy: 0.67000, task: top, mean loss: 1.22878, accuracy: 0.61600, task: multi, mean loss: 0.41679, multilabel_accuracy: 0.06700, avg. loss over tasks: 0.94189
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 31 
task: majority, mean loss: 0.78823, accuracy: 0.73750, task: max, mean loss: 0.82210, accuracy: 0.71500, task: top, mean loss: 0.90884, accuracy: 0.71050, task: multi, mean loss: 0.39512, multilabel_accuracy: 0.08500, avg. loss over tasks: 0.72857, lr: 0.0008717008403259585
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 31 
task: majority, mean loss: 0.89512, accuracy: 0.68300, task: max, mean loss: 0.83057, accuracy: 0.71400, task: top, mean loss: 1.14378, accuracy: 0.66300, task: multi, mean loss: 0.37296, multilabel_accuracy: 0.11900, avg. loss over tasks: 0.81061
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 32 
task: majority, mean loss: 0.66232, accuracy: 0.77350, task: max, mean loss: 0.72503, accuracy: 0.75800, task: top, mean loss: 0.85932, accuracy: 0.73000, task: multi, mean loss: 0.36750, multilabel_accuracy: 0.13100, avg. loss over tasks: 0.65354, lr: 0.0008598102302691562
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 32 
task: majority, mean loss: 1.04618, accuracy: 0.63000, task: max, mean loss: 0.86487, accuracy: 0.70500, task: top, mean loss: 1.09234, accuracy: 0.67800, task: multi, mean loss: 0.36437, multilabel_accuracy: 0.15700, avg. loss over tasks: 0.84194
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 33 
task: majority, mean loss: 0.54229, accuracy: 0.81200, task: max, mean loss: 0.64845, accuracy: 0.78300, task: top, mean loss: 0.72907, accuracy: 0.77400, task: multi, mean loss: 0.33215, multilabel_accuracy: 0.18350, avg. loss over tasks: 0.56299, lr: 0.0008474818560442692
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 33 
task: majority, mean loss: 1.15238, accuracy: 0.60300, task: max, mean loss: 0.87173, accuracy: 0.70900, task: top, mean loss: 1.19090, accuracy: 0.64900, task: multi, mean loss: 0.35843, multilabel_accuracy: 0.14700, avg. loss over tasks: 0.89336
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 34 
task: majority, mean loss: 0.52552, accuracy: 0.82250, task: max, mean loss: 0.61554, accuracy: 0.78400, task: top, mean loss: 0.68823, accuracy: 0.78550, task: multi, mean loss: 0.31311, multilabel_accuracy: 0.21300, avg. loss over tasks: 0.53560, lr: 0.0008347307378762497
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 34 
task: majority, mean loss: 0.92008, accuracy: 0.68700, task: max, mean loss: 0.74797, accuracy: 0.75500, task: top, mean loss: 1.05646, accuracy: 0.69300, task: multi, mean loss: 0.31633, multilabel_accuracy: 0.21100, avg. loss over tasks: 0.76021
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 35 
task: majority, mean loss: 0.43932, accuracy: 0.85100, task: max, mean loss: 0.55139, accuracy: 0.81350, task: top, mean loss: 0.59893, accuracy: 0.81200, task: multi, mean loss: 0.29484, multilabel_accuracy: 0.24950, avg. loss over tasks: 0.47112, lr: 0.0008215724110384265
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 35 
task: majority, mean loss: 0.70767, accuracy: 0.74300, task: max, mean loss: 0.72630, accuracy: 0.76700, task: top, mean loss: 1.06894, accuracy: 0.67700, task: multi, mean loss: 0.29723, multilabel_accuracy: 0.24800, avg. loss over tasks: 0.70004
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 36 
task: majority, mean loss: 0.38893, accuracy: 0.86050, task: max, mean loss: 0.49648, accuracy: 0.83900, task: top, mean loss: 0.53201, accuracy: 0.83500, task: multi, mean loss: 0.27770, multilabel_accuracy: 0.25950, avg. loss over tasks: 0.42378, lr: 0.0008080229069251663
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 36 
task: majority, mean loss: 0.61737, accuracy: 0.80300, task: max, mean loss: 0.82732, accuracy: 0.71100, task: top, mean loss: 0.97788, accuracy: 0.72800, task: multi, mean loss: 0.29055, multilabel_accuracy: 0.27500, avg. loss over tasks: 0.67828
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 37 
task: majority, mean loss: 0.37682, accuracy: 0.87500, task: max, mean loss: 0.44354, accuracy: 0.85200, task: top, mean loss: 0.49211, accuracy: 0.84100, task: multi, mean loss: 0.26975, multilabel_accuracy: 0.29100, avg. loss over tasks: 0.39556, lr: 0.0007940987335200903
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 37 
task: majority, mean loss: 0.78789, accuracy: 0.75000, task: max, mean loss: 0.79460, accuracy: 0.76100, task: top, mean loss: 0.96333, accuracy: 0.71900, task: multi, mean loss: 0.27629, multilabel_accuracy: 0.30500, avg. loss over tasks: 0.70553
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 38 
task: majority, mean loss: 0.34963, accuracy: 0.87100, task: max, mean loss: 0.42382, accuracy: 0.85850, task: top, mean loss: 0.43148, accuracy: 0.86700, task: multi, mean loss: 0.25560, multilabel_accuracy: 0.31500, avg. loss over tasks: 0.36513, lr: 0.0007798168552836382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 38 
task: majority, mean loss: 0.87279, accuracy: 0.70700, task: max, mean loss: 0.83938, accuracy: 0.74100, task: top, mean loss: 1.05584, accuracy: 0.72100, task: multi, mean loss: 0.29966, multilabel_accuracy: 0.28400, avg. loss over tasks: 0.76692
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 39 
task: majority, mean loss: 0.26856, accuracy: 0.91100, task: max, mean loss: 0.38516, accuracy: 0.87450, task: top, mean loss: 0.37682, accuracy: 0.88950, task: multi, mean loss: 0.24478, multilabel_accuracy: 0.33800, avg. loss over tasks: 0.31883, lr: 0.0007651946724844859
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 39 
task: majority, mean loss: 0.73043, accuracy: 0.76700, task: max, mean loss: 0.70443, accuracy: 0.77700, task: top, mean loss: 0.91730, accuracy: 0.75500, task: multi, mean loss: 0.25023, multilabel_accuracy: 0.36300, avg. loss over tasks: 0.65060
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 40 
task: majority, mean loss: 0.26577, accuracy: 0.90000, task: max, mean loss: 0.33442, accuracy: 0.88650, task: top, mean loss: 0.36234, accuracy: 0.88500, task: multi, mean loss: 0.23564, multilabel_accuracy: 0.36850, avg. loss over tasks: 0.29954, lr: 0.00075025
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 40 
task: majority, mean loss: 0.62309, accuracy: 0.78900, task: max, mean loss: 0.78503, accuracy: 0.75500, task: top, mean loss: 0.99681, accuracy: 0.73900, task: multi, mean loss: 0.26362, multilabel_accuracy: 0.34300, avg. loss over tasks: 0.66714
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 41 
task: majority, mean loss: 0.19710, accuracy: 0.92900, task: max, mean loss: 0.29997, accuracy: 0.90900, task: top, mean loss: 0.29615, accuracy: 0.91000, task: multi, mean loss: 0.22235, multilabel_accuracy: 0.38900, avg. loss over tasks: 0.25389, lr: 0.0007350010456115524
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 41 
task: majority, mean loss: 0.57511, accuracy: 0.81700, task: max, mean loss: 0.84124, accuracy: 0.74800, task: top, mean loss: 1.04010, accuracy: 0.73000, task: multi, mean loss: 0.24674, multilabel_accuracy: 0.35200, avg. loss over tasks: 0.67580
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 42 
task: majority, mean loss: 0.21812, accuracy: 0.92450, task: max, mean loss: 0.31429, accuracy: 0.89350, task: top, mean loss: 0.29262, accuracy: 0.90800, task: multi, mean loss: 0.21932, multilabel_accuracy: 0.38500, avg. loss over tasks: 0.26109, lr: 0.0007194663878211441
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 42 
task: majority, mean loss: 0.80683, accuracy: 0.75200, task: max, mean loss: 0.80185, accuracy: 0.76800, task: top, mean loss: 0.92395, accuracy: 0.75400, task: multi, mean loss: 0.27143, multilabel_accuracy: 0.30600, avg. loss over tasks: 0.70101
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 43 
task: majority, mean loss: 0.19118, accuracy: 0.93800, task: max, mean loss: 0.28796, accuracy: 0.90450, task: top, mean loss: 0.25293, accuracy: 0.92350, task: multi, mean loss: 0.20912, multilabel_accuracy: 0.40800, avg. loss over tasks: 0.23530, lr: 0.0007036649532163622
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 43 
task: majority, mean loss: 0.37619, accuracy: 0.87000, task: max, mean loss: 0.65364, accuracy: 0.80400, task: top, mean loss: 0.77513, accuracy: 0.79300, task: multi, mean loss: 0.21684, multilabel_accuracy: 0.42100, avg. loss over tasks: 0.50545
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 44 
task: majority, mean loss: 0.18118, accuracy: 0.93450, task: max, mean loss: 0.23192, accuracy: 0.92300, task: top, mean loss: 0.21846, accuracy: 0.92850, task: multi, mean loss: 0.20471, multilabel_accuracy: 0.42550, avg. loss over tasks: 0.20907, lr: 0.0006876159934112482
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 44 
task: majority, mean loss: 0.56254, accuracy: 0.81600, task: max, mean loss: 0.78934, accuracy: 0.77300, task: top, mean loss: 0.89266, accuracy: 0.76500, task: multi, mean loss: 0.24060, multilabel_accuracy: 0.38300, avg. loss over tasks: 0.62129
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 45 
task: majority, mean loss: 0.17535, accuracy: 0.94250, task: max, mean loss: 0.23217, accuracy: 0.92350, task: top, mean loss: 0.21304, accuracy: 0.93500, task: multi, mean loss: 0.20178, multilabel_accuracy: 0.42150, avg. loss over tasks: 0.20559, lr: 0.0006713390615911716
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 45 
task: majority, mean loss: 0.58901, accuracy: 0.81000, task: max, mean loss: 0.68316, accuracy: 0.79500, task: top, mean loss: 0.87098, accuracy: 0.77800, task: multi, mean loss: 0.21948, multilabel_accuracy: 0.43700, avg. loss over tasks: 0.59066
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 46 
task: majority, mean loss: 0.15535, accuracy: 0.94900, task: max, mean loss: 0.20011, accuracy: 0.93800, task: top, mean loss: 0.19576, accuracy: 0.93450, task: multi, mean loss: 0.19086, multilabel_accuracy: 0.45850, avg. loss over tasks: 0.18552, lr: 0.0006548539886902863
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 46 
task: majority, mean loss: 0.35452, accuracy: 0.88500, task: max, mean loss: 0.70957, accuracy: 0.80800, task: top, mean loss: 0.89787, accuracy: 0.78700, task: multi, mean loss: 0.21501, multilabel_accuracy: 0.45200, avg. loss over tasks: 0.54424
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 47 
task: majority, mean loss: 0.13608, accuracy: 0.95750, task: max, mean loss: 0.16718, accuracy: 0.94200, task: top, mean loss: 0.17476, accuracy: 0.95000, task: multi, mean loss: 0.18560, multilabel_accuracy: 0.46900, avg. loss over tasks: 0.16590, lr: 0.0006381808592305911
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 47 
task: majority, mean loss: 0.47617, accuracy: 0.85800, task: max, mean loss: 0.71576, accuracy: 0.80800, task: top, mean loss: 0.93733, accuracy: 0.79200, task: multi, mean loss: 0.21900, multilabel_accuracy: 0.43100, avg. loss over tasks: 0.58706
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 48 
task: majority, mean loss: 0.14679, accuracy: 0.94600, task: max, mean loss: 0.17513, accuracy: 0.94400, task: top, mean loss: 0.17314, accuracy: 0.94200, task: multi, mean loss: 0.18789, multilabel_accuracy: 0.45550, avg. loss over tasks: 0.17074, lr: 0.0006213399868520341
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 48 
task: majority, mean loss: 0.65958, accuracy: 0.79900, task: max, mean loss: 1.16723, accuracy: 0.74200, task: top, mean loss: 1.07627, accuracy: 0.76200, task: multi, mean loss: 0.27760, multilabel_accuracy: 0.35300, avg. loss over tasks: 0.79517
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 49 
task: majority, mean loss: 0.17988, accuracy: 0.93800, task: max, mean loss: 0.21084, accuracy: 0.92350, task: top, mean loss: 0.20479, accuracy: 0.93350, task: multi, mean loss: 0.19540, multilabel_accuracy: 0.44750, avg. loss over tasks: 0.19773, lr: 0.0006043518895634708
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 49 
task: majority, mean loss: 0.39071, accuracy: 0.87100, task: max, mean loss: 0.72923, accuracy: 0.80900, task: top, mean loss: 0.87847, accuracy: 0.78800, task: multi, mean loss: 0.20698, multilabel_accuracy: 0.47700, avg. loss over tasks: 0.55135
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 50 
task: majority, mean loss: 0.10691, accuracy: 0.96650, task: max, mean loss: 0.12011, accuracy: 0.96500, task: top, mean loss: 0.14794, accuracy: 0.95600, task: multi, mean loss: 0.16821, multilabel_accuracy: 0.50800, avg. loss over tasks: 0.13579, lr: 0.0005872372647446318
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 50 
task: majority, mean loss: 0.62623, accuracy: 0.82300, task: max, mean loss: 0.90339, accuracy: 0.77300, task: top, mean loss: 1.07583, accuracy: 0.75500, task: multi, mean loss: 0.24865, multilabel_accuracy: 0.38500, avg. loss over tasks: 0.71353
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 51 
task: majority, mean loss: 0.11810, accuracy: 0.95800, task: max, mean loss: 0.13794, accuracy: 0.95650, task: top, mean loss: 0.12312, accuracy: 0.96750, task: multi, mean loss: 0.17084, multilabel_accuracy: 0.50250, avg. loss over tasks: 0.13750, lr: 0.0005700169639295527
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 51 
task: majority, mean loss: 0.56581, accuracy: 0.82400, task: max, mean loss: 0.70313, accuracy: 0.82700, task: top, mean loss: 0.86727, accuracy: 0.78400, task: multi, mean loss: 0.22509, multilabel_accuracy: 0.41400, avg. loss over tasks: 0.59032
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 52 
task: majority, mean loss: 0.12315, accuracy: 0.95600, task: max, mean loss: 0.13336, accuracy: 0.95350, task: top, mean loss: 0.11845, accuracy: 0.96300, task: multi, mean loss: 0.16819, multilabel_accuracy: 0.50250, avg. loss over tasks: 0.13579, lr: 0.0005527119674021931
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 52 
task: majority, mean loss: 0.36724, accuracy: 0.88200, task: max, mean loss: 0.79767, accuracy: 0.80400, task: top, mean loss: 0.82996, accuracy: 0.79200, task: multi, mean loss: 0.20746, multilabel_accuracy: 0.46600, avg. loss over tasks: 0.55058
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 53 
task: majority, mean loss: 0.10600, accuracy: 0.96300, task: max, mean loss: 0.11523, accuracy: 0.95550, task: top, mean loss: 0.09288, accuracy: 0.97250, task: multi, mean loss: 0.16161, multilabel_accuracy: 0.52700, avg. loss over tasks: 0.11893, lr: 0.0005353433586351906
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 53 
task: majority, mean loss: 0.47752, accuracy: 0.85500, task: max, mean loss: 0.72350, accuracy: 0.83200, task: top, mean loss: 0.90543, accuracy: 0.78700, task: multi, mean loss: 0.20430, multilabel_accuracy: 0.46200, avg. loss over tasks: 0.57769
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 54 
task: majority, mean loss: 0.07715, accuracy: 0.97350, task: max, mean loss: 0.08431, accuracy: 0.97450, task: top, mean loss: 0.10123, accuracy: 0.97150, task: multi, mean loss: 0.15296, multilabel_accuracy: 0.54350, avg. loss over tasks: 0.10391, lr: 0.0005179322986028993
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 54 
task: majority, mean loss: 0.40069, accuracy: 0.87000, task: max, mean loss: 0.74260, accuracy: 0.81700, task: top, mean loss: 0.88850, accuracy: 0.80000, task: multi, mean loss: 0.20931, multilabel_accuracy: 0.46800, avg. loss over tasks: 0.56028
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 55 
task: majority, mean loss: 0.10353, accuracy: 0.96300, task: max, mean loss: 0.10695, accuracy: 0.96650, task: top, mean loss: 0.09250, accuracy: 0.97150, task: multi, mean loss: 0.15650, multilabel_accuracy: 0.53500, avg. loss over tasks: 0.11487, lr: 0.0005004999999999999
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 55 
task: majority, mean loss: 0.43858, accuracy: 0.86600, task: max, mean loss: 0.82518, accuracy: 0.79900, task: top, mean loss: 0.90472, accuracy: 0.79100, task: multi, mean loss: 0.22107, multilabel_accuracy: 0.45400, avg. loss over tasks: 0.59739
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 56 
task: majority, mean loss: 0.06586, accuracy: 0.98150, task: max, mean loss: 0.11588, accuracy: 0.96200, task: top, mean loss: 0.08264, accuracy: 0.97600, task: multi, mean loss: 0.15257, multilabel_accuracy: 0.54350, avg. loss over tasks: 0.10424, lr: 0.00048306770139710083
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 56 
task: majority, mean loss: 0.42822, accuracy: 0.87300, task: max, mean loss: 0.81842, accuracy: 0.80300, task: top, mean loss: 0.99810, accuracy: 0.77800, task: multi, mean loss: 0.20612, multilabel_accuracy: 0.45300, avg. loss over tasks: 0.61271
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 57 
task: majority, mean loss: 0.08527, accuracy: 0.97250, task: max, mean loss: 0.08329, accuracy: 0.97550, task: top, mean loss: 0.06368, accuracy: 0.98100, task: multi, mean loss: 0.14712, multilabel_accuracy: 0.55750, avg. loss over tasks: 0.09484, lr: 0.0004656566413648094
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 57 
task: majority, mean loss: 0.40550, accuracy: 0.87300, task: max, mean loss: 0.79024, accuracy: 0.82000, task: top, mean loss: 0.88370, accuracy: 0.81600, task: multi, mean loss: 0.19431, multilabel_accuracy: 0.49500, avg. loss over tasks: 0.56843
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 58 
task: majority, mean loss: 0.05900, accuracy: 0.98450, task: max, mean loss: 0.09581, accuracy: 0.96750, task: top, mean loss: 0.07421, accuracy: 0.98000, task: multi, mean loss: 0.14709, multilabel_accuracy: 0.56550, avg. loss over tasks: 0.09403, lr: 0.0004482880325978072
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 58 
task: majority, mean loss: 0.31190, accuracy: 0.89900, task: max, mean loss: 0.91896, accuracy: 0.78900, task: top, mean loss: 0.84630, accuracy: 0.80600, task: multi, mean loss: 0.19833, multilabel_accuracy: 0.47800, avg. loss over tasks: 0.56887
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 59 
task: majority, mean loss: 0.07073, accuracy: 0.97900, task: max, mean loss: 0.08761, accuracy: 0.97150, task: top, mean loss: 0.06672, accuracy: 0.97900, task: multi, mean loss: 0.14401, multilabel_accuracy: 0.56950, avg. loss over tasks: 0.09227, lr: 0.0004309830360704473
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 59 
task: majority, mean loss: 0.43217, accuracy: 0.87000, task: max, mean loss: 0.81171, accuracy: 0.81500, task: top, mean loss: 0.88421, accuracy: 0.80800, task: multi, mean loss: 0.20680, multilabel_accuracy: 0.48200, avg. loss over tasks: 0.58373
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 60 
task: majority, mean loss: 0.04761, accuracy: 0.98600, task: max, mean loss: 0.08536, accuracy: 0.97300, task: top, mean loss: 0.04615, accuracy: 0.98950, task: multi, mean loss: 0.13297, multilabel_accuracy: 0.60050, avg. loss over tasks: 0.07802, lr: 0.00041376273525536844
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 60 
task: majority, mean loss: 0.31396, accuracy: 0.91100, task: max, mean loss: 0.79264, accuracy: 0.82000, task: top, mean loss: 0.89288, accuracy: 0.81000, task: multi, mean loss: 0.18871, multilabel_accuracy: 0.51900, avg. loss over tasks: 0.54704
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 61 
task: majority, mean loss: 0.06609, accuracy: 0.97650, task: max, mean loss: 0.08692, accuracy: 0.97100, task: top, mean loss: 0.06827, accuracy: 0.98150, task: multi, mean loss: 0.13552, multilabel_accuracy: 0.59350, avg. loss over tasks: 0.08920, lr: 0.00039664811043652916
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 61 
task: majority, mean loss: 0.44313, accuracy: 0.87500, task: max, mean loss: 0.87931, accuracy: 0.80700, task: top, mean loss: 0.88923, accuracy: 0.80500, task: multi, mean loss: 0.19561, multilabel_accuracy: 0.51100, avg. loss over tasks: 0.60182
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 62 
task: majority, mean loss: 0.05688, accuracy: 0.98050, task: max, mean loss: 0.06438, accuracy: 0.98400, task: top, mean loss: 0.03621, accuracy: 0.99050, task: multi, mean loss: 0.12813, multilabel_accuracy: 0.61450, avg. loss over tasks: 0.07140, lr: 0.00037966001314796593
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 62 
task: majority, mean loss: 0.39141, accuracy: 0.88100, task: max, mean loss: 0.77283, accuracy: 0.82600, task: top, mean loss: 0.96567, accuracy: 0.79900, task: multi, mean loss: 0.19398, multilabel_accuracy: 0.49600, avg. loss over tasks: 0.58097
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 63 
task: majority, mean loss: 0.04916, accuracy: 0.98400, task: max, mean loss: 0.05741, accuracy: 0.98050, task: top, mean loss: 0.04984, accuracy: 0.98550, task: multi, mean loss: 0.12915, multilabel_accuracy: 0.61200, avg. loss over tasks: 0.07139, lr: 0.00036281914076940894
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 63 
task: majority, mean loss: 0.27706, accuracy: 0.91600, task: max, mean loss: 0.74311, accuracy: 0.82400, task: top, mean loss: 0.88906, accuracy: 0.81200, task: multi, mean loss: 0.17750, multilabel_accuracy: 0.54100, avg. loss over tasks: 0.52168
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 64 
task: majority, mean loss: 0.04129, accuracy: 0.98750, task: max, mean loss: 0.05154, accuracy: 0.98350, task: top, mean loss: 0.03455, accuracy: 0.98900, task: multi, mean loss: 0.12199, multilabel_accuracy: 0.63350, avg. loss over tasks: 0.06234, lr: 0.00034614601130971383
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 64 
task: majority, mean loss: 0.31426, accuracy: 0.91000, task: max, mean loss: 0.75874, accuracy: 0.84500, task: top, mean loss: 0.85042, accuracy: 0.82800, task: multi, mean loss: 0.18187, multilabel_accuracy: 0.53600, avg. loss over tasks: 0.52632
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 65 
task: majority, mean loss: 0.04695, accuracy: 0.98750, task: max, mean loss: 0.06228, accuracy: 0.97900, task: top, mean loss: 0.04605, accuracy: 0.98650, task: multi, mean loss: 0.12062, multilabel_accuracy: 0.63100, avg. loss over tasks: 0.06898, lr: 0.0003296609384088285
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 65 
task: majority, mean loss: 0.35029, accuracy: 0.88500, task: max, mean loss: 0.74509, accuracy: 0.83000, task: top, mean loss: 0.85853, accuracy: 0.82600, task: multi, mean loss: 0.18498, multilabel_accuracy: 0.53000, avg. loss over tasks: 0.53472
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 66 
task: majority, mean loss: 0.03933, accuracy: 0.98800, task: max, mean loss: 0.06996, accuracy: 0.97600, task: top, mean loss: 0.05383, accuracy: 0.98350, task: multi, mean loss: 0.12087, multilabel_accuracy: 0.63300, avg. loss over tasks: 0.07100, lr: 0.00031338400658875205
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 66 
task: majority, mean loss: 0.42219, accuracy: 0.87400, task: max, mean loss: 0.90347, accuracy: 0.81600, task: top, mean loss: 0.96140, accuracy: 0.80500, task: multi, mean loss: 0.21005, multilabel_accuracy: 0.50500, avg. loss over tasks: 0.62428
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 67 
task: majority, mean loss: 0.04124, accuracy: 0.98900, task: max, mean loss: 0.05778, accuracy: 0.98200, task: top, mean loss: 0.04524, accuracy: 0.98750, task: multi, mean loss: 0.12037, multilabel_accuracy: 0.64000, avg. loss over tasks: 0.06615, lr: 0.00029733504678363786
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 67 
task: majority, mean loss: 0.37501, accuracy: 0.88200, task: max, mean loss: 0.79542, accuracy: 0.83300, task: top, mean loss: 1.00694, accuracy: 0.79400, task: multi, mean loss: 0.18598, multilabel_accuracy: 0.51800, avg. loss over tasks: 0.59084
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 68 
task: majority, mean loss: 0.03330, accuracy: 0.99100, task: max, mean loss: 0.04684, accuracy: 0.98700, task: top, mean loss: 0.03888, accuracy: 0.99000, task: multi, mean loss: 0.10783, multilabel_accuracy: 0.65900, avg. loss over tasks: 0.05671, lr: 0.0002815336121788558
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 68 
task: majority, mean loss: 0.26974, accuracy: 0.92100, task: max, mean loss: 0.80075, accuracy: 0.82500, task: top, mean loss: 0.89004, accuracy: 0.82600, task: multi, mean loss: 0.18464, multilabel_accuracy: 0.52100, avg. loss over tasks: 0.53629
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 69 
task: majority, mean loss: 0.03175, accuracy: 0.98850, task: max, mean loss: 0.03693, accuracy: 0.99100, task: top, mean loss: 0.02170, accuracy: 0.99500, task: multi, mean loss: 0.10716, multilabel_accuracy: 0.66850, avg. loss over tasks: 0.04938, lr: 0.0002659989543884475
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 69 
task: majority, mean loss: 0.28547, accuracy: 0.91100, task: max, mean loss: 0.74784, accuracy: 0.84900, task: top, mean loss: 0.87799, accuracy: 0.83200, task: multi, mean loss: 0.17800, multilabel_accuracy: 0.54700, avg. loss over tasks: 0.52233
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 70 
task: majority, mean loss: 0.04326, accuracy: 0.98750, task: max, mean loss: 0.03709, accuracy: 0.98700, task: top, mean loss: 0.04420, accuracy: 0.98900, task: multi, mean loss: 0.10581, multilabel_accuracy: 0.67850, avg. loss over tasks: 0.05759, lr: 0.0002507500000000001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 70 
task: majority, mean loss: 0.29292, accuracy: 0.90500, task: max, mean loss: 0.78675, accuracy: 0.84700, task: top, mean loss: 0.87916, accuracy: 0.82400, task: multi, mean loss: 0.18885, multilabel_accuracy: 0.53200, avg. loss over tasks: 0.53692
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 71 
task: majority, mean loss: 0.03343, accuracy: 0.99050, task: max, mean loss: 0.03279, accuracy: 0.99100, task: top, mean loss: 0.01936, accuracy: 0.99550, task: multi, mean loss: 0.10335, multilabel_accuracy: 0.67950, avg. loss over tasks: 0.04723, lr: 0.0002358053275155142
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 71 
task: majority, mean loss: 0.33418, accuracy: 0.89800, task: max, mean loss: 0.83311, accuracy: 0.83500, task: top, mean loss: 0.91837, accuracy: 0.81600, task: multi, mean loss: 0.18934, multilabel_accuracy: 0.53400, avg. loss over tasks: 0.56875
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 72 
task: majority, mean loss: 0.03203, accuracy: 0.99000, task: max, mean loss: 0.03753, accuracy: 0.98700, task: top, mean loss: 0.03317, accuracy: 0.99050, task: multi, mean loss: 0.10243, multilabel_accuracy: 0.68400, avg. loss over tasks: 0.05129, lr: 0.00022118314471636204
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 72 
task: majority, mean loss: 0.34062, accuracy: 0.89000, task: max, mean loss: 0.84209, accuracy: 0.82600, task: top, mean loss: 0.90555, accuracy: 0.80400, task: multi, mean loss: 0.18279, multilabel_accuracy: 0.53600, avg. loss over tasks: 0.56776
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 73 
task: majority, mean loss: 0.03228, accuracy: 0.98950, task: max, mean loss: 0.03915, accuracy: 0.98600, task: top, mean loss: 0.02195, accuracy: 0.99500, task: multi, mean loss: 0.09911, multilabel_accuracy: 0.68600, avg. loss over tasks: 0.04812, lr: 0.00020690126647990973
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 73 
task: majority, mean loss: 0.27495, accuracy: 0.91900, task: max, mean loss: 0.80339, accuracy: 0.84000, task: top, mean loss: 0.86640, accuracy: 0.83400, task: multi, mean loss: 0.17935, multilabel_accuracy: 0.53800, avg. loss over tasks: 0.53102
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 74 
task: majority, mean loss: 0.02406, accuracy: 0.99400, task: max, mean loss: 0.03038, accuracy: 0.99100, task: top, mean loss: 0.02065, accuracy: 0.99400, task: multi, mean loss: 0.09447, multilabel_accuracy: 0.70950, avg. loss over tasks: 0.04239, lr: 0.00019297709307483367
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 74 
task: majority, mean loss: 0.24816, accuracy: 0.92000, task: max, mean loss: 0.86104, accuracy: 0.82700, task: top, mean loss: 0.89214, accuracy: 0.83000, task: multi, mean loss: 0.18268, multilabel_accuracy: 0.54500, avg. loss over tasks: 0.54601
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 75 
task: majority, mean loss: 0.01729, accuracy: 0.99600, task: max, mean loss: 0.02021, accuracy: 0.99550, task: top, mean loss: 0.01314, accuracy: 0.99700, task: multi, mean loss: 0.09208, multilabel_accuracy: 0.71950, avg. loss over tasks: 0.03568, lr: 0.0001794275889615736
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 75 
task: majority, mean loss: 0.26105, accuracy: 0.91800, task: max, mean loss: 0.77729, accuracy: 0.84800, task: top, mean loss: 0.86598, accuracy: 0.83000, task: multi, mean loss: 0.17875, multilabel_accuracy: 0.54800, avg. loss over tasks: 0.52077
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 76 
task: majority, mean loss: 0.01764, accuracy: 0.99550, task: max, mean loss: 0.02202, accuracy: 0.99200, task: top, mean loss: 0.01051, accuracy: 0.99850, task: multi, mean loss: 0.09302, multilabel_accuracy: 0.72600, avg. loss over tasks: 0.03580, lr: 0.0001662692621237503
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 76 
task: majority, mean loss: 0.26409, accuracy: 0.92000, task: max, mean loss: 0.77823, accuracy: 0.85500, task: top, mean loss: 0.84682, accuracy: 0.83400, task: multi, mean loss: 0.17769, multilabel_accuracy: 0.55900, avg. loss over tasks: 0.51671
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 77 
task: majority, mean loss: 0.01342, accuracy: 0.99850, task: max, mean loss: 0.02424, accuracy: 0.99300, task: top, mean loss: 0.00909, accuracy: 0.99800, task: multi, mean loss: 0.08789, multilabel_accuracy: 0.73100, avg. loss over tasks: 0.03366, lr: 0.00015351814395573083
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 77 
task: majority, mean loss: 0.24488, accuracy: 0.93200, task: max, mean loss: 0.80212, accuracy: 0.84900, task: top, mean loss: 0.85384, accuracy: 0.82900, task: multi, mean loss: 0.17285, multilabel_accuracy: 0.56700, avg. loss over tasks: 0.51842
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 78 
task: majority, mean loss: 0.01634, accuracy: 0.99600, task: max, mean loss: 0.02014, accuracy: 0.99250, task: top, mean loss: 0.01013, accuracy: 0.99750, task: multi, mean loss: 0.08833, multilabel_accuracy: 0.71900, avg. loss over tasks: 0.03374, lr: 0.00014118976973084385
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 78 
task: majority, mean loss: 0.23677, accuracy: 0.92700, task: max, mean loss: 0.83687, accuracy: 0.83100, task: top, mean loss: 0.84267, accuracy: 0.82700, task: multi, mean loss: 0.17851, multilabel_accuracy: 0.54800, avg. loss over tasks: 0.52371
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 79 
task: majority, mean loss: 0.02190, accuracy: 0.99450, task: max, mean loss: 0.01858, accuracy: 0.99550, task: top, mean loss: 0.01219, accuracy: 0.99800, task: multi, mean loss: 0.08873, multilabel_accuracy: 0.72900, avg. loss over tasks: 0.03535, lr: 0.0001292991596740417
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 79 
task: majority, mean loss: 0.25128, accuracy: 0.92500, task: max, mean loss: 0.80123, accuracy: 0.84600, task: top, mean loss: 0.84888, accuracy: 0.83800, task: multi, mean loss: 0.17530, multilabel_accuracy: 0.56200, avg. loss over tasks: 0.51917
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 80 
task: majority, mean loss: 0.01425, accuracy: 0.99650, task: max, mean loss: 0.01287, accuracy: 0.99700, task: top, mean loss: 0.01770, accuracy: 0.99600, task: multi, mean loss: 0.08561, multilabel_accuracy: 0.74450, avg. loss over tasks: 0.03261, lr: 0.00011786080066207054
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 80 
task: majority, mean loss: 0.23924, accuracy: 0.92500, task: max, mean loss: 0.84368, accuracy: 0.84900, task: top, mean loss: 0.83651, accuracy: 0.83500, task: multi, mean loss: 0.17627, multilabel_accuracy: 0.56200, avg. loss over tasks: 0.52392
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 81 
task: majority, mean loss: 0.02193, accuracy: 0.99500, task: max, mean loss: 0.01440, accuracy: 0.99650, task: top, mean loss: 0.01219, accuracy: 0.99750, task: multi, mean loss: 0.08322, multilabel_accuracy: 0.73550, avg. loss over tasks: 0.03294, lr: 0.00010688862857344241
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 81 
task: majority, mean loss: 0.23233, accuracy: 0.93000, task: max, mean loss: 0.81944, accuracy: 0.84400, task: top, mean loss: 0.84984, accuracy: 0.83700, task: multi, mean loss: 0.17744, multilabel_accuracy: 0.55800, avg. loss over tasks: 0.51976
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 82 
task: majority, mean loss: 0.01528, accuracy: 0.99750, task: max, mean loss: 0.01799, accuracy: 0.99450, task: top, mean loss: 0.01228, accuracy: 0.99700, task: multi, mean loss: 0.07971, multilabel_accuracy: 0.76400, avg. loss over tasks: 0.03131, lr: 9.63960113097138e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 82 
task: majority, mean loss: 0.23688, accuracy: 0.92700, task: max, mean loss: 0.84326, accuracy: 0.84200, task: top, mean loss: 0.85226, accuracy: 0.84200, task: multi, mean loss: 0.17666, multilabel_accuracy: 0.56900, avg. loss over tasks: 0.52727
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 83 
task: majority, mean loss: 0.01405, accuracy: 0.99650, task: max, mean loss: 0.01713, accuracy: 0.99450, task: top, mean loss: 0.00736, accuracy: 0.99950, task: multi, mean loss: 0.08061, multilabel_accuracy: 0.74950, avg. loss over tasks: 0.02979, lr: 8.639573250875671e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 83 
task: majority, mean loss: 0.22756, accuracy: 0.92700, task: max, mean loss: 0.83372, accuracy: 0.84500, task: top, mean loss: 0.82146, accuracy: 0.84700, task: multi, mean loss: 0.17517, multilabel_accuracy: 0.56600, avg. loss over tasks: 0.51448
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 84 
task: majority, mean loss: 0.01369, accuracy: 0.99650, task: max, mean loss: 0.01580, accuracy: 0.99450, task: top, mean loss: 0.01253, accuracy: 0.99750, task: multi, mean loss: 0.08041, multilabel_accuracy: 0.75000, avg. loss over tasks: 0.03061, lr: 7.689997596986524e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 84 
task: majority, mean loss: 0.23151, accuracy: 0.93200, task: max, mean loss: 0.88955, accuracy: 0.83900, task: top, mean loss: 0.87910, accuracy: 0.83600, task: multi, mean loss: 0.17652, multilabel_accuracy: 0.55800, avg. loss over tasks: 0.54417
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 85 
task: majority, mean loss: 0.01554, accuracy: 0.99650, task: max, mean loss: 0.01028, accuracy: 0.99850, task: top, mean loss: 0.01333, accuracy: 0.99700, task: multi, mean loss: 0.07452, multilabel_accuracy: 0.76650, avg. loss over tasks: 0.02842, lr: 6.792031080967287e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 85 
task: majority, mean loss: 0.23062, accuracy: 0.92900, task: max, mean loss: 0.83375, accuracy: 0.85100, task: top, mean loss: 0.84688, accuracy: 0.83900, task: multi, mean loss: 0.17658, multilabel_accuracy: 0.55900, avg. loss over tasks: 0.52196
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 86 
task: majority, mean loss: 0.01057, accuracy: 0.99800, task: max, mean loss: 0.01023, accuracy: 0.99700, task: top, mean loss: 0.00811, accuracy: 0.99900, task: multi, mean loss: 0.07690, multilabel_accuracy: 0.76300, avg. loss over tasks: 0.02645, lr: 5.946767736696608e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 86 
task: majority, mean loss: 0.22007, accuracy: 0.92900, task: max, mean loss: 0.84157, accuracy: 0.84400, task: top, mean loss: 0.83260, accuracy: 0.84200, task: multi, mean loss: 0.17398, multilabel_accuracy: 0.56200, avg. loss over tasks: 0.51706
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 87 
task: majority, mean loss: 0.01576, accuracy: 0.99550, task: max, mean loss: 0.01353, accuracy: 0.99600, task: top, mean loss: 0.01220, accuracy: 0.99700, task: multi, mean loss: 0.07557, multilabel_accuracy: 0.76700, avg. loss over tasks: 0.02927, lr: 5.155237387356618e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 87 
task: majority, mean loss: 0.24668, accuracy: 0.92500, task: max, mean loss: 0.84910, accuracy: 0.84800, task: top, mean loss: 0.85378, accuracy: 0.83600, task: multi, mean loss: 0.17533, multilabel_accuracy: 0.56700, avg. loss over tasks: 0.53122
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 88 
task: majority, mean loss: 0.01156, accuracy: 0.99900, task: max, mean loss: 0.01205, accuracy: 0.99650, task: top, mean loss: 0.01033, accuracy: 0.99700, task: multi, mean loss: 0.07132, multilabel_accuracy: 0.79300, avg. loss over tasks: 0.02632, lr: 4.418404390752081e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 88 
task: majority, mean loss: 0.23031, accuracy: 0.92300, task: max, mean loss: 0.86569, accuracy: 0.84900, task: top, mean loss: 0.85295, accuracy: 0.84100, task: multi, mean loss: 0.17414, multilabel_accuracy: 0.56400, avg. loss over tasks: 0.53077
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 89 
task: majority, mean loss: 0.01005, accuracy: 0.99750, task: max, mean loss: 0.00917, accuracy: 0.99800, task: top, mean loss: 0.00582, accuracy: 0.99950, task: multi, mean loss: 0.07215, multilabel_accuracy: 0.77750, avg. loss over tasks: 0.02430, lr: 3.7371664643889735e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 89 
task: majority, mean loss: 0.22378, accuracy: 0.92900, task: max, mean loss: 0.86226, accuracy: 0.84000, task: top, mean loss: 0.84706, accuracy: 0.84100, task: multi, mean loss: 0.17340, multilabel_accuracy: 0.56200, avg. loss over tasks: 0.52662
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 90 
task: majority, mean loss: 0.00941, accuracy: 0.99850, task: max, mean loss: 0.01567, accuracy: 0.99550, task: top, mean loss: 0.01368, accuracy: 0.99700, task: multi, mean loss: 0.07003, multilabel_accuracy: 0.78750, avg. loss over tasks: 0.02720, lr: 3.11235359174388e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 90 
task: majority, mean loss: 0.22693, accuracy: 0.92900, task: max, mean loss: 0.84801, accuracy: 0.84600, task: top, mean loss: 0.83800, accuracy: 0.83400, task: multi, mean loss: 0.17400, multilabel_accuracy: 0.56400, avg. loss over tasks: 0.52174
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 91 
task: majority, mean loss: 0.00881, accuracy: 0.99900, task: max, mean loss: 0.01206, accuracy: 0.99650, task: top, mean loss: 0.00645, accuracy: 0.99900, task: multi, mean loss: 0.06983, multilabel_accuracy: 0.78600, avg. loss over tasks: 0.02429, lr: 2.544727011057081e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 91 
task: majority, mean loss: 0.23204, accuracy: 0.92800, task: max, mean loss: 0.85202, accuracy: 0.85200, task: top, mean loss: 0.84379, accuracy: 0.84000, task: multi, mean loss: 0.17451, multilabel_accuracy: 0.56800, avg. loss over tasks: 0.52559
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 92 
task: majority, mean loss: 0.00909, accuracy: 0.99900, task: max, mean loss: 0.01362, accuracy: 0.99550, task: top, mean loss: 0.00786, accuracy: 0.99850, task: multi, mean loss: 0.07004, multilabel_accuracy: 0.78600, avg. loss over tasks: 0.02515, lr: 2.0349782878809714e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 92 
task: majority, mean loss: 0.22597, accuracy: 0.92600, task: max, mean loss: 0.85079, accuracy: 0.85400, task: top, mean loss: 0.84625, accuracy: 0.84500, task: multi, mean loss: 0.17313, multilabel_accuracy: 0.57400, avg. loss over tasks: 0.52403
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 93 
task: majority, mean loss: 0.00932, accuracy: 0.99850, task: max, mean loss: 0.01792, accuracy: 0.99600, task: top, mean loss: 0.00723, accuracy: 0.99900, task: multi, mean loss: 0.07018, multilabel_accuracy: 0.79350, avg. loss over tasks: 0.02616, lr: 1.583728472513976e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 93 
task: majority, mean loss: 0.22791, accuracy: 0.92300, task: max, mean loss: 0.84649, accuracy: 0.85300, task: top, mean loss: 0.83876, accuracy: 0.84000, task: multi, mean loss: 0.17246, multilabel_accuracy: 0.57100, avg. loss over tasks: 0.52140
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 94 
task: majority, mean loss: 0.00832, accuracy: 0.99950, task: max, mean loss: 0.01124, accuracy: 0.99700, task: top, mean loss: 0.00838, accuracy: 0.99800, task: multi, mean loss: 0.06893, multilabel_accuracy: 0.79450, avg. loss over tasks: 0.02422, lr: 1.1915273433464114e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 94 
task: majority, mean loss: 0.21591, accuracy: 0.92700, task: max, mean loss: 0.85095, accuracy: 0.85100, task: top, mean loss: 0.83733, accuracy: 0.84400, task: multi, mean loss: 0.17252, multilabel_accuracy: 0.56600, avg. loss over tasks: 0.51918
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 95 
task: majority, mean loss: 0.00987, accuracy: 0.99850, task: max, mean loss: 0.01051, accuracy: 0.99550, task: top, mean loss: 0.00723, accuracy: 0.99900, task: multi, mean loss: 0.06834, multilabel_accuracy: 0.79300, avg. loss over tasks: 0.02399, lr: 8.588527370402095e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 95 
task: majority, mean loss: 0.22414, accuracy: 0.92700, task: max, mean loss: 0.86038, accuracy: 0.84900, task: top, mean loss: 0.83586, accuracy: 0.84000, task: multi, mean loss: 0.17402, multilabel_accuracy: 0.56900, avg. loss over tasks: 0.52360
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 96 
task: majority, mean loss: 0.00875, accuracy: 0.99850, task: max, mean loss: 0.00885, accuracy: 0.99750, task: top, mean loss: 0.00781, accuracy: 0.99850, task: multi, mean loss: 0.07018, multilabel_accuracy: 0.79600, avg. loss over tasks: 0.02390, lr: 5.86109966358566e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 96 
task: majority, mean loss: 0.22199, accuracy: 0.92700, task: max, mean loss: 0.85824, accuracy: 0.85200, task: top, mean loss: 0.84045, accuracy: 0.84200, task: multi, mean loss: 0.17264, multilabel_accuracy: 0.56700, avg. loss over tasks: 0.52333
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 97 
task: majority, mean loss: 0.01142, accuracy: 0.99800, task: max, mean loss: 0.01326, accuracy: 0.99550, task: top, mean loss: 0.00771, accuracy: 0.99900, task: multi, mean loss: 0.06909, multilabel_accuracy: 0.79850, avg. loss over tasks: 0.02537, lr: 3.7363132635474912e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 97 
task: majority, mean loss: 0.21865, accuracy: 0.93200, task: max, mean loss: 0.86147, accuracy: 0.85100, task: top, mean loss: 0.84511, accuracy: 0.84000, task: multi, mean loss: 0.17381, multilabel_accuracy: 0.56300, avg. loss over tasks: 0.52476
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 98 
task: majority, mean loss: 0.00883, accuracy: 0.99800, task: max, mean loss: 0.01070, accuracy: 0.99750, task: top, mean loss: 0.00917, accuracy: 0.99900, task: multi, mean loss: 0.06996, multilabel_accuracy: 0.78350, avg. loss over tasks: 0.02467, lr: 2.2167568952178134e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 98 
task: majority, mean loss: 0.21890, accuracy: 0.93300, task: max, mean loss: 0.86499, accuracy: 0.85300, task: top, mean loss: 0.84513, accuracy: 0.83900, task: multi, mean loss: 0.17311, multilabel_accuracy: 0.56200, avg. loss over tasks: 0.52553
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 99 
task: majority, mean loss: 0.00800, accuracy: 0.99850, task: max, mean loss: 0.01710, accuracy: 0.99650, task: top, mean loss: 0.00813, accuracy: 0.99900, task: multi, mean loss: 0.06861, multilabel_accuracy: 0.79750, avg. loss over tasks: 0.02546, lr: 1.3042819039616668e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 99 
task: majority, mean loss: 0.22416, accuracy: 0.92300, task: max, mean loss: 0.86613, accuracy: 0.85000, task: top, mean loss: 0.84399, accuracy: 0.84200, task: multi, mean loss: 0.17357, multilabel_accuracy: 0.57100, avg. loss over tasks: 0.52696
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 100 
task: majority, mean loss: 0.01029, accuracy: 0.99850, task: max, mean loss: 0.01066, accuracy: 0.99700, task: top, mean loss: 0.00904, accuracy: 0.99750, task: multi, mean loss: 0.06857, multilabel_accuracy: 0.79300, avg. loss over tasks: 0.02464, lr: 1e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 100 
task: majority, mean loss: 0.21892, accuracy: 0.93000, task: max, mean loss: 0.86256, accuracy: 0.85000, task: top, mean loss: 0.85019, accuracy: 0.84000, task: multi, mean loss: 0.17367, multilabel_accuracy: 0.56600, avg. loss over tasks: 0.52633
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

