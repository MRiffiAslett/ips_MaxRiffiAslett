Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09383, accuracy: 0.63587, avg. loss over tasks: 1.09383, lr: 3e-05
Diversity Loss - Mean: -0.01055, Variance: 0.01049
Semantic Loss - Mean: 1.43134, Variance: 0.07245

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17491, accuracy: 0.66272, avg. loss over tasks: 1.17491
Diversity Loss - Mean: -0.03145, Variance: 0.01251
Semantic Loss - Mean: 1.16197, Variance: 0.05314

Train Epoch: 2 
task: sign, mean loss: 0.95784, accuracy: 0.67935, avg. loss over tasks: 0.95784, lr: 6e-05
Diversity Loss - Mean: -0.02302, Variance: 0.01045
Semantic Loss - Mean: 0.98286, Variance: 0.03915

Test Epoch: 2 
task: sign, mean loss: 1.11625, accuracy: 0.65680, avg. loss over tasks: 1.11625
Diversity Loss - Mean: -0.03860, Variance: 0.01219
Semantic Loss - Mean: 1.14341, Variance: 0.03223

Train Epoch: 3 
task: sign, mean loss: 0.79762, accuracy: 0.69565, avg. loss over tasks: 0.79762, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.04654, Variance: 0.01031
Semantic Loss - Mean: 0.99420, Variance: 0.02715

Test Epoch: 3 
task: sign, mean loss: 1.28665, accuracy: 0.53846, avg. loss over tasks: 1.28665
Diversity Loss - Mean: -0.06872, Variance: 0.01168
Semantic Loss - Mean: 1.11090, Variance: 0.03050

Train Epoch: 4 
task: sign, mean loss: 0.74009, accuracy: 0.71739, avg. loss over tasks: 0.74009, lr: 0.00012
Diversity Loss - Mean: -0.07258, Variance: 0.01007
Semantic Loss - Mean: 0.88843, Variance: 0.02098

Test Epoch: 4 
task: sign, mean loss: 1.80175, accuracy: 0.37278, avg. loss over tasks: 1.80175
Diversity Loss - Mean: -0.06564, Variance: 0.01115
Semantic Loss - Mean: 1.14149, Variance: 0.02539

Train Epoch: 5 
task: sign, mean loss: 0.71425, accuracy: 0.73370, avg. loss over tasks: 0.71425, lr: 0.00015
Diversity Loss - Mean: -0.07004, Variance: 0.00981
Semantic Loss - Mean: 0.77332, Variance: 0.01729

Test Epoch: 5 
task: sign, mean loss: 2.29453, accuracy: 0.39053, avg. loss over tasks: 2.29453
Diversity Loss - Mean: -0.06408, Variance: 0.01064
Semantic Loss - Mean: 1.31786, Variance: 0.02418

Train Epoch: 6 
task: sign, mean loss: 0.68541, accuracy: 0.75543, avg. loss over tasks: 0.68541, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.06345, Variance: 0.00974
Semantic Loss - Mean: 0.71204, Variance: 0.01490

Test Epoch: 6 
task: sign, mean loss: 2.11515, accuracy: 0.52071, avg. loss over tasks: 2.11515
Diversity Loss - Mean: -0.04529, Variance: 0.01075
Semantic Loss - Mean: 1.49518, Variance: 0.02354

Train Epoch: 7 
task: sign, mean loss: 0.67119, accuracy: 0.74457, avg. loss over tasks: 0.67119, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.07072, Variance: 0.00993
Semantic Loss - Mean: 0.70669, Variance: 0.01320

Test Epoch: 7 
task: sign, mean loss: 2.53632, accuracy: 0.34911, avg. loss over tasks: 2.53632
Diversity Loss - Mean: -0.04030, Variance: 0.01070
Semantic Loss - Mean: 1.95966, Variance: 0.02556

Train Epoch: 8 
task: sign, mean loss: 0.56620, accuracy: 0.80435, avg. loss over tasks: 0.56620, lr: 0.00024
Diversity Loss - Mean: -0.04240, Variance: 0.00992
Semantic Loss - Mean: 0.58642, Variance: 0.01188

Test Epoch: 8 
task: sign, mean loss: 2.35254, accuracy: 0.39645, avg. loss over tasks: 2.35254
Diversity Loss - Mean: -0.02301, Variance: 0.01104
Semantic Loss - Mean: 1.73367, Variance: 0.02655

Train Epoch: 9 
task: sign, mean loss: 0.72870, accuracy: 0.78261, avg. loss over tasks: 0.72870, lr: 0.00027
Diversity Loss - Mean: -0.05265, Variance: 0.00992
Semantic Loss - Mean: 0.64028, Variance: 0.01108

Test Epoch: 9 
task: sign, mean loss: 2.49839, accuracy: 0.53254, avg. loss over tasks: 2.49839
Diversity Loss - Mean: -0.04440, Variance: 0.01148
Semantic Loss - Mean: 1.80232, Variance: 0.03105

Train Epoch: 10 
task: sign, mean loss: 0.73206, accuracy: 0.75543, avg. loss over tasks: 0.73206, lr: 0.0003
Diversity Loss - Mean: -0.05841, Variance: 0.00994
Semantic Loss - Mean: 0.70695, Variance: 0.01041

Test Epoch: 10 
task: sign, mean loss: 3.33273, accuracy: 0.33728, avg. loss over tasks: 3.33273
Diversity Loss - Mean: -0.00510, Variance: 0.01139
Semantic Loss - Mean: 2.52230, Variance: 0.03234

Train Epoch: 11 
task: sign, mean loss: 0.63452, accuracy: 0.77174, avg. loss over tasks: 0.63452, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.04627, Variance: 0.00984
Semantic Loss - Mean: 0.63794, Variance: 0.00982

Test Epoch: 11 
task: sign, mean loss: 1.93446, accuracy: 0.59172, avg. loss over tasks: 1.93446
Diversity Loss - Mean: -0.02228, Variance: 0.01147
Semantic Loss - Mean: 1.65136, Variance: 0.03144

Train Epoch: 12 
task: sign, mean loss: 0.49648, accuracy: 0.82609, avg. loss over tasks: 0.49648, lr: 0.000299849111021216
Diversity Loss - Mean: -0.03069, Variance: 0.00978
Semantic Loss - Mean: 0.50790, Variance: 0.00919

Test Epoch: 12 
task: sign, mean loss: 2.22224, accuracy: 0.47929, avg. loss over tasks: 2.22224
Diversity Loss - Mean: -0.03178, Variance: 0.01155
Semantic Loss - Mean: 1.86273, Variance: 0.03143

Train Epoch: 13 
task: sign, mean loss: 0.68365, accuracy: 0.80978, avg. loss over tasks: 0.68365, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.04478, Variance: 0.00980
Semantic Loss - Mean: 0.68860, Variance: 0.00878

Test Epoch: 13 
task: sign, mean loss: 1.98849, accuracy: 0.65680, avg. loss over tasks: 1.98849
Diversity Loss - Mean: -0.06814, Variance: 0.01189
Semantic Loss - Mean: 1.72862, Variance: 0.03016

Train Epoch: 14 
task: sign, mean loss: 0.60839, accuracy: 0.78804, avg. loss over tasks: 0.60839, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.06838, Variance: 0.00990
Semantic Loss - Mean: 0.64112, Variance: 0.00852

Test Epoch: 14 
task: sign, mean loss: 1.21274, accuracy: 0.66272, avg. loss over tasks: 1.21274
Diversity Loss - Mean: -0.09638, Variance: 0.01224
Semantic Loss - Mean: 1.19669, Variance: 0.02889

Train Epoch: 15 
task: sign, mean loss: 0.36448, accuracy: 0.86957, avg. loss over tasks: 0.36448, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.07607, Variance: 0.01000
Semantic Loss - Mean: 0.39229, Variance: 0.00803

Test Epoch: 15 
task: sign, mean loss: 2.65952, accuracy: 0.34320, avg. loss over tasks: 2.65952
Diversity Loss - Mean: -0.03281, Variance: 0.01235
Semantic Loss - Mean: 2.16133, Variance: 0.02825

Train Epoch: 16 
task: sign, mean loss: 0.27448, accuracy: 0.90217, avg. loss over tasks: 0.27448, lr: 0.000298643821800925
Diversity Loss - Mean: -0.06229, Variance: 0.01001
Semantic Loss - Mean: 0.32584, Variance: 0.00778

Test Epoch: 16 
task: sign, mean loss: 2.90790, accuracy: 0.34911, avg. loss over tasks: 2.90790
Diversity Loss - Mean: -0.04368, Variance: 0.01235
Semantic Loss - Mean: 2.25252, Variance: 0.02799

Train Epoch: 17 
task: sign, mean loss: 0.31401, accuracy: 0.90217, avg. loss over tasks: 0.31401, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.05732, Variance: 0.01003
Semantic Loss - Mean: 0.34014, Variance: 0.00760

Test Epoch: 17 
task: sign, mean loss: 3.96663, accuracy: 0.35503, avg. loss over tasks: 3.96663
Diversity Loss - Mean: -0.01507, Variance: 0.01242
Semantic Loss - Mean: 3.14191, Variance: 0.02909

Train Epoch: 18 
task: sign, mean loss: 0.46877, accuracy: 0.85870, avg. loss over tasks: 0.46877, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.07694, Variance: 0.01014
Semantic Loss - Mean: 0.49373, Variance: 0.00760

Test Epoch: 18 
task: sign, mean loss: 1.64166, accuracy: 0.66272, avg. loss over tasks: 1.64166
Diversity Loss - Mean: -0.10935, Variance: 0.01262
Semantic Loss - Mean: 1.52350, Variance: 0.02843

Train Epoch: 19 
task: sign, mean loss: 0.54155, accuracy: 0.77717, avg. loss over tasks: 0.54155, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.08915, Variance: 0.01028
Semantic Loss - Mean: 0.57755, Variance: 0.00744

Test Epoch: 19 
task: sign, mean loss: 2.00692, accuracy: 0.23669, avg. loss over tasks: 2.00692
Diversity Loss - Mean: -0.06356, Variance: 0.01296
Semantic Loss - Mean: 1.59033, Variance: 0.02846

Train Epoch: 20 
task: sign, mean loss: 0.42585, accuracy: 0.80435, avg. loss over tasks: 0.42585, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.09454, Variance: 0.01043
Semantic Loss - Mean: 0.47920, Variance: 0.00718

Test Epoch: 20 
task: sign, mean loss: 3.33183, accuracy: 0.20710, avg. loss over tasks: 3.33183
Diversity Loss - Mean: -0.03302, Variance: 0.01298
Semantic Loss - Mean: 2.50631, Variance: 0.02928

Train Epoch: 21 
task: sign, mean loss: 0.34775, accuracy: 0.88043, avg. loss over tasks: 0.34775, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.07812, Variance: 0.01050
Semantic Loss - Mean: 0.37835, Variance: 0.00701

Test Epoch: 21 
task: sign, mean loss: 2.75789, accuracy: 0.44970, avg. loss over tasks: 2.75789
Diversity Loss - Mean: -0.06958, Variance: 0.01299
Semantic Loss - Mean: 2.25447, Variance: 0.02871

Train Epoch: 22 
task: sign, mean loss: 0.27170, accuracy: 0.90217, avg. loss over tasks: 0.27170, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.06928, Variance: 0.01051
Semantic Loss - Mean: 0.30589, Variance: 0.00695

Test Epoch: 22 
task: sign, mean loss: 1.76256, accuracy: 0.62722, avg. loss over tasks: 1.76256
Diversity Loss - Mean: -0.09054, Variance: 0.01301
Semantic Loss - Mean: 1.60241, Variance: 0.02849

Train Epoch: 23 
task: sign, mean loss: 0.29128, accuracy: 0.90217, avg. loss over tasks: 0.29128, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.06883, Variance: 0.01052
Semantic Loss - Mean: 0.31592, Variance: 0.00680

Test Epoch: 23 
task: sign, mean loss: 1.75615, accuracy: 0.40828, avg. loss over tasks: 1.75615
Diversity Loss - Mean: -0.07016, Variance: 0.01311
Semantic Loss - Mean: 1.60843, Variance: 0.02811

Train Epoch: 24 
task: sign, mean loss: 0.48006, accuracy: 0.83152, avg. loss over tasks: 0.48006, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.08672, Variance: 0.01063
Semantic Loss - Mean: 0.47622, Variance: 0.00679

Test Epoch: 24 
task: sign, mean loss: 3.26504, accuracy: 0.15976, avg. loss over tasks: 3.26504
Diversity Loss - Mean: -0.02299, Variance: 0.01321
Semantic Loss - Mean: 2.31602, Variance: 0.03055

Train Epoch: 25 
task: sign, mean loss: 0.50791, accuracy: 0.82065, avg. loss over tasks: 0.50791, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.08891, Variance: 0.01081
Semantic Loss - Mean: 0.54099, Variance: 0.00689

Test Epoch: 25 
task: sign, mean loss: 1.31803, accuracy: 0.52071, avg. loss over tasks: 1.31803
Diversity Loss - Mean: -0.04756, Variance: 0.01336
Semantic Loss - Mean: 1.12166, Variance: 0.02990

Train Epoch: 26 
task: sign, mean loss: 0.45296, accuracy: 0.80978, avg. loss over tasks: 0.45296, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.09183, Variance: 0.01101
Semantic Loss - Mean: 0.55053, Variance: 0.00727

Test Epoch: 26 
task: sign, mean loss: 3.58166, accuracy: 0.30769, avg. loss over tasks: 3.58166
Diversity Loss - Mean: -0.01057, Variance: 0.01349
Semantic Loss - Mean: 2.71150, Variance: 0.03380

Train Epoch: 27 
task: sign, mean loss: 0.52513, accuracy: 0.86413, avg. loss over tasks: 0.52513, lr: 0.000289228031029578
Diversity Loss - Mean: -0.09451, Variance: 0.01120
Semantic Loss - Mean: 0.57988, Variance: 0.00780

Test Epoch: 27 
task: sign, mean loss: 1.16662, accuracy: 0.44379, avg. loss over tasks: 1.16662
Diversity Loss - Mean: -0.07256, Variance: 0.01360
Semantic Loss - Mean: 0.92226, Variance: 0.03371

Train Epoch: 28 
task: sign, mean loss: 0.24509, accuracy: 0.93478, avg. loss over tasks: 0.24509, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.09397, Variance: 0.01136
Semantic Loss - Mean: 0.31244, Variance: 0.00762

Test Epoch: 28 
task: sign, mean loss: 0.89310, accuracy: 0.73964, avg. loss over tasks: 0.89310
Diversity Loss - Mean: -0.09126, Variance: 0.01375
Semantic Loss - Mean: 0.86762, Variance: 0.03286

Train Epoch: 29 
task: sign, mean loss: 0.22655, accuracy: 0.92935, avg. loss over tasks: 0.22655, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.08652, Variance: 0.01149
Semantic Loss - Mean: 0.29657, Variance: 0.00776

Test Epoch: 29 
task: sign, mean loss: 0.75871, accuracy: 0.74556, avg. loss over tasks: 0.75871
Diversity Loss - Mean: -0.07758, Variance: 0.01392
Semantic Loss - Mean: 0.72920, Variance: 0.03248

Train Epoch: 30 
task: sign, mean loss: 0.31533, accuracy: 0.90217, avg. loss over tasks: 0.31533, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.07427, Variance: 0.01156
Semantic Loss - Mean: 0.36938, Variance: 0.00782

Test Epoch: 30 
task: sign, mean loss: 2.04848, accuracy: 0.34320, avg. loss over tasks: 2.04848
Diversity Loss - Mean: -0.06713, Variance: 0.01408
Semantic Loss - Mean: 2.07613, Variance: 0.03320

Train Epoch: 31 
task: sign, mean loss: 0.34081, accuracy: 0.86413, avg. loss over tasks: 0.34081, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.08976, Variance: 0.01167
Semantic Loss - Mean: 0.38861, Variance: 0.00785

Test Epoch: 31 
task: sign, mean loss: 1.86808, accuracy: 0.43787, avg. loss over tasks: 1.86808
Diversity Loss - Mean: -0.03836, Variance: 0.01406
Semantic Loss - Mean: 1.47334, Variance: 0.03409

Train Epoch: 32 
task: sign, mean loss: 0.21341, accuracy: 0.91848, avg. loss over tasks: 0.21341, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.09366, Variance: 0.01176
Semantic Loss - Mean: 0.30151, Variance: 0.00810

Test Epoch: 32 
task: sign, mean loss: 1.81786, accuracy: 0.47929, avg. loss over tasks: 1.81786
Diversity Loss - Mean: -0.06692, Variance: 0.01406
Semantic Loss - Mean: 1.47859, Variance: 0.03407

Train Epoch: 33 
task: sign, mean loss: 0.14257, accuracy: 0.94565, avg. loss over tasks: 0.14257, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.09309, Variance: 0.01181
Semantic Loss - Mean: 0.22054, Variance: 0.00821

Test Epoch: 33 
task: sign, mean loss: 1.97106, accuracy: 0.37870, avg. loss over tasks: 1.97106
Diversity Loss - Mean: -0.06370, Variance: 0.01403
Semantic Loss - Mean: 1.66935, Variance: 0.03390

Train Epoch: 34 
task: sign, mean loss: 0.34748, accuracy: 0.91304, avg. loss over tasks: 0.34748, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.09012, Variance: 0.01184
Semantic Loss - Mean: 0.35349, Variance: 0.00842

Test Epoch: 34 
task: sign, mean loss: 1.94291, accuracy: 0.48521, avg. loss over tasks: 1.94291
Diversity Loss - Mean: -0.06324, Variance: 0.01401
Semantic Loss - Mean: 1.46720, Variance: 0.03410

Train Epoch: 35 
task: sign, mean loss: 0.27991, accuracy: 0.90761, avg. loss over tasks: 0.27991, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.09717, Variance: 0.01190
Semantic Loss - Mean: 0.34060, Variance: 0.00863

Test Epoch: 35 
task: sign, mean loss: 0.69018, accuracy: 0.79882, avg. loss over tasks: 0.69018
Diversity Loss - Mean: -0.07670, Variance: 0.01405
Semantic Loss - Mean: 0.82675, Variance: 0.03415

Train Epoch: 36 
task: sign, mean loss: 0.13618, accuracy: 0.95109, avg. loss over tasks: 0.13618, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.09317, Variance: 0.01193
Semantic Loss - Mean: 0.18940, Variance: 0.00847

Test Epoch: 36 
task: sign, mean loss: 0.62712, accuracy: 0.82840, avg. loss over tasks: 0.62712
Diversity Loss - Mean: -0.09249, Variance: 0.01409
Semantic Loss - Mean: 0.60007, Variance: 0.03356

Train Epoch: 37 
task: sign, mean loss: 0.16567, accuracy: 0.94022, avg. loss over tasks: 0.16567, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.09139, Variance: 0.01196
Semantic Loss - Mean: 0.21401, Variance: 0.00844

Test Epoch: 37 
task: sign, mean loss: 0.99154, accuracy: 0.75148, avg. loss over tasks: 0.99154
Diversity Loss - Mean: -0.09120, Variance: 0.01419
Semantic Loss - Mean: 0.93176, Variance: 0.03408

Train Epoch: 38 
task: sign, mean loss: 0.14521, accuracy: 0.95109, avg. loss over tasks: 0.14521, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.08991, Variance: 0.01198
Semantic Loss - Mean: 0.18167, Variance: 0.00837

Test Epoch: 38 
task: sign, mean loss: 1.17453, accuracy: 0.57396, avg. loss over tasks: 1.17453
Diversity Loss - Mean: -0.05967, Variance: 0.01417
Semantic Loss - Mean: 1.11212, Variance: 0.03566

Train Epoch: 39 
task: sign, mean loss: 0.10019, accuracy: 0.95652, avg. loss over tasks: 0.10019, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.08352, Variance: 0.01198
Semantic Loss - Mean: 0.16423, Variance: 0.00829

Test Epoch: 39 
task: sign, mean loss: 0.77591, accuracy: 0.71006, avg. loss over tasks: 0.77591
Diversity Loss - Mean: -0.05187, Variance: 0.01414
Semantic Loss - Mean: 0.77103, Variance: 0.03577

Train Epoch: 40 
task: sign, mean loss: 0.22974, accuracy: 0.92391, avg. loss over tasks: 0.22974, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.08969, Variance: 0.01201
Semantic Loss - Mean: 0.27208, Variance: 0.00844

Test Epoch: 40 
task: sign, mean loss: 1.97327, accuracy: 0.49704, avg. loss over tasks: 1.97327
Diversity Loss - Mean: -0.04673, Variance: 0.01406
Semantic Loss - Mean: 1.80900, Variance: 0.03840

Train Epoch: 41 
task: sign, mean loss: 0.11577, accuracy: 0.95652, avg. loss over tasks: 0.11577, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.09147, Variance: 0.01203
Semantic Loss - Mean: 0.20173, Variance: 0.00858

Test Epoch: 41 
task: sign, mean loss: 1.55722, accuracy: 0.68047, avg. loss over tasks: 1.55722
Diversity Loss - Mean: -0.07291, Variance: 0.01400
Semantic Loss - Mean: 1.29918, Variance: 0.03872

Train Epoch: 42 
task: sign, mean loss: 0.75555, accuracy: 0.79348, avg. loss over tasks: 0.75555, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.10008, Variance: 0.01210
Semantic Loss - Mean: 0.74569, Variance: 0.00878

Test Epoch: 42 
task: sign, mean loss: 3.35977, accuracy: 0.20118, avg. loss over tasks: 3.35977
Diversity Loss - Mean: -0.06333, Variance: 0.01411
Semantic Loss - Mean: 2.76900, Variance: 0.04025

Train Epoch: 43 
task: sign, mean loss: 0.80786, accuracy: 0.70109, avg. loss over tasks: 0.80786, lr: 0.000260757131773478
Diversity Loss - Mean: -0.10241, Variance: 0.01219
Semantic Loss - Mean: 0.83076, Variance: 0.00891

Test Epoch: 43 
task: sign, mean loss: 1.65023, accuracy: 0.37870, avg. loss over tasks: 1.65023
Diversity Loss - Mean: -0.08449, Variance: 0.01425
Semantic Loss - Mean: 1.36745, Variance: 0.03998

Train Epoch: 44 
task: sign, mean loss: 0.63793, accuracy: 0.79348, avg. loss over tasks: 0.63793, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.11482, Variance: 0.01235
Semantic Loss - Mean: 0.66683, Variance: 0.00890

Test Epoch: 44 
task: sign, mean loss: 1.72465, accuracy: 0.15385, avg. loss over tasks: 1.72465
Diversity Loss - Mean: -0.07640, Variance: 0.01430
Semantic Loss - Mean: 1.41495, Variance: 0.03986

Train Epoch: 45 
task: sign, mean loss: 0.31542, accuracy: 0.90217, avg. loss over tasks: 0.31542, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.11043, Variance: 0.01246
Semantic Loss - Mean: 0.39290, Variance: 0.00880

Test Epoch: 45 
task: sign, mean loss: 0.51381, accuracy: 0.80473, avg. loss over tasks: 0.51381
Diversity Loss - Mean: -0.08318, Variance: 0.01434
Semantic Loss - Mean: 0.53710, Variance: 0.03908

Train Epoch: 46 
task: sign, mean loss: 0.23723, accuracy: 0.90761, avg. loss over tasks: 0.23723, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.10324, Variance: 0.01254
Semantic Loss - Mean: 0.27944, Variance: 0.00877

Test Epoch: 46 
task: sign, mean loss: 0.87798, accuracy: 0.75740, avg. loss over tasks: 0.87798
Diversity Loss - Mean: -0.09054, Variance: 0.01438
Semantic Loss - Mean: 0.78542, Variance: 0.03836

Train Epoch: 47 
task: sign, mean loss: 0.19383, accuracy: 0.90217, avg. loss over tasks: 0.19383, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.09790, Variance: 0.01260
Semantic Loss - Mean: 0.23387, Variance: 0.00880

Test Epoch: 47 
task: sign, mean loss: 0.52174, accuracy: 0.82840, avg. loss over tasks: 0.52174
Diversity Loss - Mean: -0.09023, Variance: 0.01444
Semantic Loss - Mean: 0.50714, Variance: 0.03766

Train Epoch: 48 
task: sign, mean loss: 0.12265, accuracy: 0.95652, avg. loss over tasks: 0.12265, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.09835, Variance: 0.01266
Semantic Loss - Mean: 0.17519, Variance: 0.00885

Test Epoch: 48 
task: sign, mean loss: 0.52223, accuracy: 0.82249, avg. loss over tasks: 0.52223
Diversity Loss - Mean: -0.08513, Variance: 0.01446
Semantic Loss - Mean: 0.49110, Variance: 0.03694

Train Epoch: 49 
task: sign, mean loss: 0.09526, accuracy: 0.98370, avg. loss over tasks: 0.09526, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.09845, Variance: 0.01269
Semantic Loss - Mean: 0.15362, Variance: 0.00904

Test Epoch: 49 
task: sign, mean loss: 0.68285, accuracy: 0.77515, avg. loss over tasks: 0.68285
Diversity Loss - Mean: -0.07317, Variance: 0.01444
Semantic Loss - Mean: 0.65742, Variance: 0.03647

Train Epoch: 50 
task: sign, mean loss: 0.11307, accuracy: 0.96739, avg. loss over tasks: 0.11307, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.09635, Variance: 0.01270
Semantic Loss - Mean: 0.18379, Variance: 0.00905

Test Epoch: 50 
task: sign, mean loss: 0.52347, accuracy: 0.84615, avg. loss over tasks: 0.52347
Diversity Loss - Mean: -0.07822, Variance: 0.01445
Semantic Loss - Mean: 0.55117, Variance: 0.03605

Train Epoch: 51 
task: sign, mean loss: 0.08328, accuracy: 0.96739, avg. loss over tasks: 0.08328, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.09742, Variance: 0.01272
Semantic Loss - Mean: 0.16379, Variance: 0.00909

Test Epoch: 51 
task: sign, mean loss: 0.67540, accuracy: 0.84615, avg. loss over tasks: 0.67540
Diversity Loss - Mean: -0.09262, Variance: 0.01447
Semantic Loss - Mean: 0.55855, Variance: 0.03547

Train Epoch: 52 
task: sign, mean loss: 0.05366, accuracy: 0.97283, avg. loss over tasks: 0.05366, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.10144, Variance: 0.01273
Semantic Loss - Mean: 0.12838, Variance: 0.00918

Test Epoch: 52 
task: sign, mean loss: 0.61072, accuracy: 0.86391, avg. loss over tasks: 0.61072
Diversity Loss - Mean: -0.09678, Variance: 0.01451
Semantic Loss - Mean: 0.64314, Variance: 0.03529

Train Epoch: 53 
task: sign, mean loss: 0.13987, accuracy: 0.94565, avg. loss over tasks: 0.13987, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.10501, Variance: 0.01277
Semantic Loss - Mean: 0.23738, Variance: 0.00944

Test Epoch: 53 
task: sign, mean loss: 1.08997, accuracy: 0.81065, avg. loss over tasks: 1.08997
Diversity Loss - Mean: -0.07902, Variance: 0.01449
Semantic Loss - Mean: 0.99568, Variance: 0.03504

Train Epoch: 54 
task: sign, mean loss: 0.12572, accuracy: 0.96196, avg. loss over tasks: 0.12572, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.10556, Variance: 0.01280
Semantic Loss - Mean: 0.19977, Variance: 0.00947

Test Epoch: 54 
task: sign, mean loss: 1.18923, accuracy: 0.74556, avg. loss over tasks: 1.18923
Diversity Loss - Mean: -0.08789, Variance: 0.01451
Semantic Loss - Mean: 1.13651, Variance: 0.03480

Train Epoch: 55 
task: sign, mean loss: 0.09489, accuracy: 0.96739, avg. loss over tasks: 0.09489, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.10683, Variance: 0.01285
Semantic Loss - Mean: 0.15085, Variance: 0.00948

Test Epoch: 55 
task: sign, mean loss: 1.62145, accuracy: 0.52663, avg. loss over tasks: 1.62145
Diversity Loss - Mean: -0.08685, Variance: 0.01452
Semantic Loss - Mean: 1.42796, Variance: 0.03516

Train Epoch: 56 
task: sign, mean loss: 0.07793, accuracy: 0.97283, avg. loss over tasks: 0.07793, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.10974, Variance: 0.01289
Semantic Loss - Mean: 0.14883, Variance: 0.00955

Test Epoch: 56 
task: sign, mean loss: 1.39435, accuracy: 0.56213, avg. loss over tasks: 1.39435
Diversity Loss - Mean: -0.09291, Variance: 0.01454
Semantic Loss - Mean: 1.17684, Variance: 0.03578

Train Epoch: 57 
task: sign, mean loss: 0.07151, accuracy: 0.97283, avg. loss over tasks: 0.07151, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.10887, Variance: 0.01293
Semantic Loss - Mean: 0.13925, Variance: 0.00964

Test Epoch: 57 
task: sign, mean loss: 1.60952, accuracy: 0.65680, avg. loss over tasks: 1.60952
Diversity Loss - Mean: -0.09998, Variance: 0.01458
Semantic Loss - Mean: 1.49148, Variance: 0.03587

Train Epoch: 58 
task: sign, mean loss: 0.03620, accuracy: 0.99457, avg. loss over tasks: 0.03620, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.10941, Variance: 0.01295
Semantic Loss - Mean: 0.08678, Variance: 0.00963

Test Epoch: 58 
task: sign, mean loss: 1.43419, accuracy: 0.74556, avg. loss over tasks: 1.43419
Diversity Loss - Mean: -0.09842, Variance: 0.01460
Semantic Loss - Mean: 1.22436, Variance: 0.03587

Train Epoch: 59 
task: sign, mean loss: 0.04224, accuracy: 0.97826, avg. loss over tasks: 0.04224, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.10834, Variance: 0.01296
Semantic Loss - Mean: 0.11448, Variance: 0.00973

Test Epoch: 59 
task: sign, mean loss: 1.02196, accuracy: 0.79882, avg. loss over tasks: 1.02196
Diversity Loss - Mean: -0.10354, Variance: 0.01461
Semantic Loss - Mean: 0.91983, Variance: 0.03577

Train Epoch: 60 
task: sign, mean loss: 0.04750, accuracy: 0.97283, avg. loss over tasks: 0.04750, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.10787, Variance: 0.01297
Semantic Loss - Mean: 0.10926, Variance: 0.00972

Test Epoch: 60 
task: sign, mean loss: 1.02648, accuracy: 0.75740, avg. loss over tasks: 1.02648
Diversity Loss - Mean: -0.09758, Variance: 0.01462
Semantic Loss - Mean: 1.01782, Variance: 0.03565

Train Epoch: 61 
task: sign, mean loss: 0.05268, accuracy: 0.98913, avg. loss over tasks: 0.05268, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.11181, Variance: 0.01299
Semantic Loss - Mean: 0.13060, Variance: 0.00985

Test Epoch: 61 
task: sign, mean loss: 0.86333, accuracy: 0.75740, avg. loss over tasks: 0.86333
Diversity Loss - Mean: -0.10135, Variance: 0.01466
Semantic Loss - Mean: 0.93764, Variance: 0.03594

Train Epoch: 62 
task: sign, mean loss: 0.05262, accuracy: 0.99457, avg. loss over tasks: 0.05262, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.10992, Variance: 0.01300
Semantic Loss - Mean: 0.11174, Variance: 0.00991

Test Epoch: 62 
task: sign, mean loss: 0.66689, accuracy: 0.86391, avg. loss over tasks: 0.66689
Diversity Loss - Mean: -0.10576, Variance: 0.01470
Semantic Loss - Mean: 0.69855, Variance: 0.03572

Train Epoch: 63 
task: sign, mean loss: 0.07130, accuracy: 0.98913, avg. loss over tasks: 0.07130, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.11048, Variance: 0.01301
Semantic Loss - Mean: 0.11473, Variance: 0.00993

Test Epoch: 63 
task: sign, mean loss: 0.83129, accuracy: 0.85799, avg. loss over tasks: 0.83129
Diversity Loss - Mean: -0.10870, Variance: 0.01473
Semantic Loss - Mean: 0.86243, Variance: 0.03535

Train Epoch: 64 
task: sign, mean loss: 0.07001, accuracy: 0.97283, avg. loss over tasks: 0.07001, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.11063, Variance: 0.01302
Semantic Loss - Mean: 0.11523, Variance: 0.01005

Test Epoch: 64 
task: sign, mean loss: 1.28265, accuracy: 0.80473, avg. loss over tasks: 1.28265
Diversity Loss - Mean: -0.09946, Variance: 0.01473
Semantic Loss - Mean: 1.45409, Variance: 0.03656

Train Epoch: 65 
task: sign, mean loss: 0.06427, accuracy: 0.98370, avg. loss over tasks: 0.06427, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.11129, Variance: 0.01302
Semantic Loss - Mean: 0.11552, Variance: 0.01011

Test Epoch: 65 
task: sign, mean loss: 0.62800, accuracy: 0.86391, avg. loss over tasks: 0.62800
Diversity Loss - Mean: -0.10648, Variance: 0.01474
Semantic Loss - Mean: 0.69824, Variance: 0.03629

Train Epoch: 66 
task: sign, mean loss: 0.07846, accuracy: 0.97283, avg. loss over tasks: 0.07846, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.11112, Variance: 0.01302
Semantic Loss - Mean: 0.14380, Variance: 0.01011

Test Epoch: 66 
task: sign, mean loss: 1.30708, accuracy: 0.78698, avg. loss over tasks: 1.30708
Diversity Loss - Mean: -0.08996, Variance: 0.01470
Semantic Loss - Mean: 1.11171, Variance: 0.03627

Train Epoch: 67 
task: sign, mean loss: 0.13305, accuracy: 0.95652, avg. loss over tasks: 0.13305, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.11400, Variance: 0.01303
Semantic Loss - Mean: 0.18968, Variance: 0.01008

Test Epoch: 67 
task: sign, mean loss: 2.81035, accuracy: 0.42604, avg. loss over tasks: 2.81035
Diversity Loss - Mean: -0.08783, Variance: 0.01466
Semantic Loss - Mean: 1.98959, Variance: 0.03736

Train Epoch: 68 
task: sign, mean loss: 0.11224, accuracy: 0.97283, avg. loss over tasks: 0.11224, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.11262, Variance: 0.01305
Semantic Loss - Mean: 0.21070, Variance: 0.01021

Test Epoch: 68 
task: sign, mean loss: 0.83124, accuracy: 0.82840, avg. loss over tasks: 0.83124
Diversity Loss - Mean: -0.11645, Variance: 0.01465
Semantic Loss - Mean: 0.75736, Variance: 0.03707

Train Epoch: 69 
task: sign, mean loss: 0.06011, accuracy: 0.97826, avg. loss over tasks: 0.06011, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.11462, Variance: 0.01306
Semantic Loss - Mean: 0.14760, Variance: 0.01023

Test Epoch: 69 
task: sign, mean loss: 0.73675, accuracy: 0.81065, avg. loss over tasks: 0.73675
Diversity Loss - Mean: -0.11409, Variance: 0.01467
Semantic Loss - Mean: 0.82054, Variance: 0.03698

Train Epoch: 70 
task: sign, mean loss: 0.07504, accuracy: 0.97283, avg. loss over tasks: 0.07504, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.11383, Variance: 0.01307
Semantic Loss - Mean: 0.11365, Variance: 0.01016

Test Epoch: 70 
task: sign, mean loss: 0.52918, accuracy: 0.83432, avg. loss over tasks: 0.52918
Diversity Loss - Mean: -0.10150, Variance: 0.01473
Semantic Loss - Mean: 0.55221, Variance: 0.03664

Train Epoch: 71 
task: sign, mean loss: 0.06987, accuracy: 0.97283, avg. loss over tasks: 0.06987, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.11309, Variance: 0.01307
Semantic Loss - Mean: 0.11687, Variance: 0.01012

Test Epoch: 71 
task: sign, mean loss: 0.69270, accuracy: 0.85207, avg. loss over tasks: 0.69270
Diversity Loss - Mean: -0.11384, Variance: 0.01477
Semantic Loss - Mean: 0.72519, Variance: 0.03655

Train Epoch: 72 
task: sign, mean loss: 0.02442, accuracy: 1.00000, avg. loss over tasks: 0.02442, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.11238, Variance: 0.01307
Semantic Loss - Mean: 0.08164, Variance: 0.01009

Test Epoch: 72 
task: sign, mean loss: 0.86021, accuracy: 0.82840, avg. loss over tasks: 0.86021
Diversity Loss - Mean: -0.10642, Variance: 0.01476
Semantic Loss - Mean: 0.84552, Variance: 0.03634

Train Epoch: 73 
task: sign, mean loss: 0.01257, accuracy: 1.00000, avg. loss over tasks: 0.01257, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.11259, Variance: 0.01307
Semantic Loss - Mean: 0.06850, Variance: 0.01012

Test Epoch: 73 
task: sign, mean loss: 0.82315, accuracy: 0.85799, avg. loss over tasks: 0.82315
Diversity Loss - Mean: -0.11400, Variance: 0.01477
Semantic Loss - Mean: 0.72929, Variance: 0.03603

Train Epoch: 74 
task: sign, mean loss: 0.01117, accuracy: 1.00000, avg. loss over tasks: 0.01117, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.11360, Variance: 0.01307
Semantic Loss - Mean: 0.06611, Variance: 0.01015

Test Epoch: 74 
task: sign, mean loss: 0.85215, accuracy: 0.84024, avg. loss over tasks: 0.85215
Diversity Loss - Mean: -0.10789, Variance: 0.01477
Semantic Loss - Mean: 0.85582, Variance: 0.03599

Train Epoch: 75 
task: sign, mean loss: 0.01487, accuracy: 0.99457, avg. loss over tasks: 0.01487, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.11316, Variance: 0.01306
Semantic Loss - Mean: 0.07831, Variance: 0.01017

Test Epoch: 75 
task: sign, mean loss: 0.95868, accuracy: 0.73964, avg. loss over tasks: 0.95868
Diversity Loss - Mean: -0.09711, Variance: 0.01474
Semantic Loss - Mean: 1.06989, Variance: 0.03664

Train Epoch: 76 
task: sign, mean loss: 0.01150, accuracy: 1.00000, avg. loss over tasks: 0.01150, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.11500, Variance: 0.01306
Semantic Loss - Mean: 0.08418, Variance: 0.01032

Test Epoch: 76 
task: sign, mean loss: 1.06945, accuracy: 0.71598, avg. loss over tasks: 1.06945
Diversity Loss - Mean: -0.10364, Variance: 0.01472
Semantic Loss - Mean: 1.17080, Variance: 0.03717

Train Epoch: 77 
task: sign, mean loss: 0.00966, accuracy: 1.00000, avg. loss over tasks: 0.00966, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.11457, Variance: 0.01305
Semantic Loss - Mean: 0.06728, Variance: 0.01030

Test Epoch: 77 
task: sign, mean loss: 1.16901, accuracy: 0.70414, avg. loss over tasks: 1.16901
Diversity Loss - Mean: -0.10687, Variance: 0.01471
Semantic Loss - Mean: 1.40047, Variance: 0.03911

Train Epoch: 78 
task: sign, mean loss: 0.00367, accuracy: 1.00000, avg. loss over tasks: 0.00367, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.11611, Variance: 0.01304
Semantic Loss - Mean: 0.05231, Variance: 0.01031

Test Epoch: 78 
task: sign, mean loss: 1.21322, accuracy: 0.71598, avg. loss over tasks: 1.21322
Diversity Loss - Mean: -0.11015, Variance: 0.01471
Semantic Loss - Mean: 1.34772, Variance: 0.04030

Train Epoch: 79 
task: sign, mean loss: 0.02349, accuracy: 0.99457, avg. loss over tasks: 0.02349, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.11630, Variance: 0.01303
Semantic Loss - Mean: 0.07139, Variance: 0.01026

Test Epoch: 79 
task: sign, mean loss: 1.17382, accuracy: 0.72781, avg. loss over tasks: 1.17382
Diversity Loss - Mean: -0.11058, Variance: 0.01471
Semantic Loss - Mean: 1.20477, Variance: 0.04070

Train Epoch: 80 
task: sign, mean loss: 0.03362, accuracy: 0.98370, avg. loss over tasks: 0.03362, lr: 0.00015015
Diversity Loss - Mean: -0.11796, Variance: 0.01302
Semantic Loss - Mean: 0.06673, Variance: 0.01022

Test Epoch: 80 
task: sign, mean loss: 0.83666, accuracy: 0.85799, avg. loss over tasks: 0.83666
Diversity Loss - Mean: -0.11816, Variance: 0.01473
Semantic Loss - Mean: 0.78735, Variance: 0.04044

Train Epoch: 81 
task: sign, mean loss: 0.13322, accuracy: 0.97826, avg. loss over tasks: 0.13322, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.11897, Variance: 0.01301
Semantic Loss - Mean: 0.16989, Variance: 0.01033

Test Epoch: 81 
task: sign, mean loss: 0.94490, accuracy: 0.85207, avg. loss over tasks: 0.94490
Diversity Loss - Mean: -0.11841, Variance: 0.01474
Semantic Loss - Mean: 0.86630, Variance: 0.04046

Train Epoch: 82 
task: sign, mean loss: 0.04411, accuracy: 0.97826, avg. loss over tasks: 0.04411, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.12057, Variance: 0.01300
Semantic Loss - Mean: 0.12256, Variance: 0.01041

Test Epoch: 82 
task: sign, mean loss: 1.00489, accuracy: 0.81657, avg. loss over tasks: 1.00489
Diversity Loss - Mean: -0.11890, Variance: 0.01476
Semantic Loss - Mean: 0.93000, Variance: 0.04040

Train Epoch: 83 
task: sign, mean loss: 0.05555, accuracy: 0.98370, avg. loss over tasks: 0.05555, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12127, Variance: 0.01300
Semantic Loss - Mean: 0.10981, Variance: 0.01055

Test Epoch: 83 
task: sign, mean loss: 0.86047, accuracy: 0.81065, avg. loss over tasks: 0.86047
Diversity Loss - Mean: -0.11176, Variance: 0.01478
Semantic Loss - Mean: 1.08169, Variance: 0.04053

Train Epoch: 84 
task: sign, mean loss: 0.03744, accuracy: 0.98913, avg. loss over tasks: 0.03744, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.11832, Variance: 0.01299
Semantic Loss - Mean: 0.10937, Variance: 0.01065

Test Epoch: 84 
task: sign, mean loss: 1.03259, accuracy: 0.78107, avg. loss over tasks: 1.03259
Diversity Loss - Mean: -0.11057, Variance: 0.01477
Semantic Loss - Mean: 1.10878, Variance: 0.04062

Train Epoch: 85 
task: sign, mean loss: 0.05618, accuracy: 0.97826, avg. loss over tasks: 0.05618, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.11994, Variance: 0.01299
Semantic Loss - Mean: 0.09148, Variance: 0.01059

Test Epoch: 85 
task: sign, mean loss: 1.20318, accuracy: 0.78698, avg. loss over tasks: 1.20318
Diversity Loss - Mean: -0.11095, Variance: 0.01477
Semantic Loss - Mean: 1.30466, Variance: 0.04066

Train Epoch: 86 
task: sign, mean loss: 0.05364, accuracy: 0.97826, avg. loss over tasks: 0.05364, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.11991, Variance: 0.01299
Semantic Loss - Mean: 0.11582, Variance: 0.01064

Test Epoch: 86 
task: sign, mean loss: 1.03710, accuracy: 0.76331, avg. loss over tasks: 1.03710
Diversity Loss - Mean: -0.11194, Variance: 0.01477
Semantic Loss - Mean: 1.14230, Variance: 0.04097

Train Epoch: 87 
task: sign, mean loss: 0.03079, accuracy: 0.99457, avg. loss over tasks: 0.03079, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.11842, Variance: 0.01299
Semantic Loss - Mean: 0.09889, Variance: 0.01064

Test Epoch: 87 
task: sign, mean loss: 1.02080, accuracy: 0.76331, avg. loss over tasks: 1.02080
Diversity Loss - Mean: -0.11180, Variance: 0.01477
Semantic Loss - Mean: 1.13745, Variance: 0.04114

Train Epoch: 88 
task: sign, mean loss: 0.00573, accuracy: 1.00000, avg. loss over tasks: 0.00573, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.12130, Variance: 0.01299
Semantic Loss - Mean: 0.05290, Variance: 0.01061

Test Epoch: 88 
task: sign, mean loss: 1.01748, accuracy: 0.76331, avg. loss over tasks: 1.01748
Diversity Loss - Mean: -0.11148, Variance: 0.01478
Semantic Loss - Mean: 1.19205, Variance: 0.04144

Train Epoch: 89 
task: sign, mean loss: 0.00765, accuracy: 1.00000, avg. loss over tasks: 0.00765, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.12023, Variance: 0.01298
Semantic Loss - Mean: 0.05625, Variance: 0.01056

Test Epoch: 89 
task: sign, mean loss: 1.00723, accuracy: 0.75740, avg. loss over tasks: 1.00723
Diversity Loss - Mean: -0.11260, Variance: 0.01478
Semantic Loss - Mean: 1.16980, Variance: 0.04187

Train Epoch: 90 
task: sign, mean loss: 0.01880, accuracy: 0.99457, avg. loss over tasks: 0.01880, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.12187, Variance: 0.01299
Semantic Loss - Mean: 0.05861, Variance: 0.01055

Test Epoch: 90 
task: sign, mean loss: 0.86624, accuracy: 0.83432, avg. loss over tasks: 0.86624
Diversity Loss - Mean: -0.11545, Variance: 0.01478
Semantic Loss - Mean: 0.92258, Variance: 0.04249

Train Epoch: 91 
task: sign, mean loss: 0.01837, accuracy: 0.98913, avg. loss over tasks: 0.01837, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.12171, Variance: 0.01299
Semantic Loss - Mean: 0.06162, Variance: 0.01054

Test Epoch: 91 
task: sign, mean loss: 0.87352, accuracy: 0.84615, avg. loss over tasks: 0.87352
Diversity Loss - Mean: -0.11601, Variance: 0.01478
Semantic Loss - Mean: 0.88552, Variance: 0.04246

Train Epoch: 92 
task: sign, mean loss: 0.01023, accuracy: 0.99457, avg. loss over tasks: 0.01023, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.12194, Variance: 0.01298
Semantic Loss - Mean: 0.08066, Variance: 0.01061

Test Epoch: 92 
task: sign, mean loss: 0.87574, accuracy: 0.84024, avg. loss over tasks: 0.87574
Diversity Loss - Mean: -0.11496, Variance: 0.01478
Semantic Loss - Mean: 0.94776, Variance: 0.04226

Train Epoch: 93 
task: sign, mean loss: 0.00348, accuracy: 1.00000, avg. loss over tasks: 0.00348, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.12276, Variance: 0.01298
Semantic Loss - Mean: 0.03683, Variance: 0.01059

Test Epoch: 93 
task: sign, mean loss: 0.82978, accuracy: 0.86391, avg. loss over tasks: 0.82978
Diversity Loss - Mean: -0.11611, Variance: 0.01479
Semantic Loss - Mean: 0.83593, Variance: 0.04214

Train Epoch: 94 
task: sign, mean loss: 0.00268, accuracy: 1.00000, avg. loss over tasks: 0.00268, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.12281, Variance: 0.01298
Semantic Loss - Mean: 0.03164, Variance: 0.01052

Test Epoch: 94 
task: sign, mean loss: 0.82858, accuracy: 0.85799, avg. loss over tasks: 0.82858
Diversity Loss - Mean: -0.11570, Variance: 0.01479
Semantic Loss - Mean: 0.89204, Variance: 0.04216

Train Epoch: 95 
task: sign, mean loss: 0.00478, accuracy: 1.00000, avg. loss over tasks: 0.00478, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.12254, Variance: 0.01298
Semantic Loss - Mean: 0.05582, Variance: 0.01050

Test Epoch: 95 
task: sign, mean loss: 0.79308, accuracy: 0.86982, avg. loss over tasks: 0.79308
Diversity Loss - Mean: -0.11515, Variance: 0.01479
Semantic Loss - Mean: 0.99180, Variance: 0.04220

Train Epoch: 96 
task: sign, mean loss: 0.00914, accuracy: 1.00000, avg. loss over tasks: 0.00914, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.12283, Variance: 0.01297
Semantic Loss - Mean: 0.05233, Variance: 0.01044

Test Epoch: 96 
task: sign, mean loss: 0.67959, accuracy: 0.90533, avg. loss over tasks: 0.67959
Diversity Loss - Mean: -0.11615, Variance: 0.01479
Semantic Loss - Mean: 0.88250, Variance: 0.04242

Train Epoch: 97 
task: sign, mean loss: 0.01575, accuracy: 0.99457, avg. loss over tasks: 0.01575, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.12325, Variance: 0.01296
Semantic Loss - Mean: 0.06261, Variance: 0.01042

Test Epoch: 97 
task: sign, mean loss: 0.68259, accuracy: 0.88757, avg. loss over tasks: 0.68259
Diversity Loss - Mean: -0.11870, Variance: 0.01480
Semantic Loss - Mean: 0.73817, Variance: 0.04217

Train Epoch: 98 
task: sign, mean loss: 0.00247, accuracy: 1.00000, avg. loss over tasks: 0.00247, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.12496, Variance: 0.01296
Semantic Loss - Mean: 0.03448, Variance: 0.01037

Test Epoch: 98 
task: sign, mean loss: 0.63544, accuracy: 0.91124, avg. loss over tasks: 0.63544
Diversity Loss - Mean: -0.11937, Variance: 0.01481
Semantic Loss - Mean: 0.62897, Variance: 0.04183

Train Epoch: 99 
task: sign, mean loss: 0.01582, accuracy: 0.99457, avg. loss over tasks: 0.01582, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.12474, Variance: 0.01296
Semantic Loss - Mean: 0.04586, Variance: 0.01034

Test Epoch: 99 
task: sign, mean loss: 0.69908, accuracy: 0.88757, avg. loss over tasks: 0.69908
Diversity Loss - Mean: -0.11847, Variance: 0.01482
Semantic Loss - Mean: 0.78070, Variance: 0.04158

Train Epoch: 100 
task: sign, mean loss: 0.02126, accuracy: 0.99457, avg. loss over tasks: 0.02126, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.12462, Variance: 0.01296
Semantic Loss - Mean: 0.06925, Variance: 0.01035

Test Epoch: 100 
task: sign, mean loss: 0.81411, accuracy: 0.86982, avg. loss over tasks: 0.81411
Diversity Loss - Mean: -0.11870, Variance: 0.01483
Semantic Loss - Mean: 0.83664, Variance: 0.04127

Train Epoch: 101 
task: sign, mean loss: 0.01239, accuracy: 0.99457, avg. loss over tasks: 0.01239, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.12480, Variance: 0.01295
Semantic Loss - Mean: 0.05608, Variance: 0.01033

Test Epoch: 101 
task: sign, mean loss: 0.84362, accuracy: 0.86391, avg. loss over tasks: 0.84362
Diversity Loss - Mean: -0.11691, Variance: 0.01483
Semantic Loss - Mean: 1.06343, Variance: 0.04134

Train Epoch: 102 
task: sign, mean loss: 0.00535, accuracy: 1.00000, avg. loss over tasks: 0.00535, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.12423, Variance: 0.01295
Semantic Loss - Mean: 0.04458, Variance: 0.01034

Test Epoch: 102 
task: sign, mean loss: 0.85492, accuracy: 0.85799, avg. loss over tasks: 0.85492
Diversity Loss - Mean: -0.11881, Variance: 0.01484
Semantic Loss - Mean: 1.04991, Variance: 0.04176

Train Epoch: 103 
task: sign, mean loss: 0.00720, accuracy: 1.00000, avg. loss over tasks: 0.00720, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.12493, Variance: 0.01295
Semantic Loss - Mean: 0.04198, Variance: 0.01028

Test Epoch: 103 
task: sign, mean loss: 0.91097, accuracy: 0.84024, avg. loss over tasks: 0.91097
Diversity Loss - Mean: -0.11984, Variance: 0.01484
Semantic Loss - Mean: 1.05840, Variance: 0.04174

Train Epoch: 104 
task: sign, mean loss: 0.00292, accuracy: 1.00000, avg. loss over tasks: 0.00292, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.12516, Variance: 0.01294
Semantic Loss - Mean: 0.03109, Variance: 0.01022

Test Epoch: 104 
task: sign, mean loss: 0.79218, accuracy: 0.88166, avg. loss over tasks: 0.79218
Diversity Loss - Mean: -0.12071, Variance: 0.01484
Semantic Loss - Mean: 0.88226, Variance: 0.04168

Train Epoch: 105 
task: sign, mean loss: 0.00464, accuracy: 0.99457, avg. loss over tasks: 0.00464, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.12549, Variance: 0.01293
Semantic Loss - Mean: 0.02277, Variance: 0.01014

Test Epoch: 105 
task: sign, mean loss: 0.87792, accuracy: 0.84024, avg. loss over tasks: 0.87792
Diversity Loss - Mean: -0.11986, Variance: 0.01485
Semantic Loss - Mean: 1.04875, Variance: 0.04163

Train Epoch: 106 
task: sign, mean loss: 0.02185, accuracy: 0.99457, avg. loss over tasks: 0.02185, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.12612, Variance: 0.01293
Semantic Loss - Mean: 0.04758, Variance: 0.01007

Test Epoch: 106 
task: sign, mean loss: 0.88932, accuracy: 0.82249, avg. loss over tasks: 0.88932
Diversity Loss - Mean: -0.11928, Variance: 0.01485
Semantic Loss - Mean: 1.09019, Variance: 0.04150

Train Epoch: 107 
task: sign, mean loss: 0.00261, accuracy: 1.00000, avg. loss over tasks: 0.00261, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.12581, Variance: 0.01292
Semantic Loss - Mean: 0.02495, Variance: 0.01000

Test Epoch: 107 
task: sign, mean loss: 0.91976, accuracy: 0.82840, avg. loss over tasks: 0.91976
Diversity Loss - Mean: -0.12023, Variance: 0.01485
Semantic Loss - Mean: 1.06623, Variance: 0.04153

Train Epoch: 108 
task: sign, mean loss: 0.00303, accuracy: 1.00000, avg. loss over tasks: 0.00303, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.12615, Variance: 0.01292
Semantic Loss - Mean: 0.02738, Variance: 0.00994

Test Epoch: 108 
task: sign, mean loss: 0.84926, accuracy: 0.85207, avg. loss over tasks: 0.84926
Diversity Loss - Mean: -0.12089, Variance: 0.01486
Semantic Loss - Mean: 0.95397, Variance: 0.04147

Train Epoch: 109 
task: sign, mean loss: 0.00188, accuracy: 1.00000, avg. loss over tasks: 0.00188, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.12687, Variance: 0.01291
Semantic Loss - Mean: 0.03230, Variance: 0.00989

Test Epoch: 109 
task: sign, mean loss: 0.85503, accuracy: 0.85799, avg. loss over tasks: 0.85503
Diversity Loss - Mean: -0.12101, Variance: 0.01487
Semantic Loss - Mean: 0.97149, Variance: 0.04133

Train Epoch: 110 
task: sign, mean loss: 0.00704, accuracy: 1.00000, avg. loss over tasks: 0.00704, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.12687, Variance: 0.01291
Semantic Loss - Mean: 0.02453, Variance: 0.00983

Test Epoch: 110 
task: sign, mean loss: 0.84224, accuracy: 0.84024, avg. loss over tasks: 0.84224
Diversity Loss - Mean: -0.12060, Variance: 0.01487
Semantic Loss - Mean: 0.95630, Variance: 0.04119

Train Epoch: 111 
task: sign, mean loss: 0.00364, accuracy: 1.00000, avg. loss over tasks: 0.00364, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.12626, Variance: 0.01291
Semantic Loss - Mean: 0.05197, Variance: 0.00985

Test Epoch: 111 
task: sign, mean loss: 0.70081, accuracy: 0.88166, avg. loss over tasks: 0.70081
Diversity Loss - Mean: -0.12236, Variance: 0.01488
Semantic Loss - Mean: 0.70020, Variance: 0.04095

Train Epoch: 112 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.12751, Variance: 0.01290
Semantic Loss - Mean: 0.02497, Variance: 0.00979

Test Epoch: 112 
task: sign, mean loss: 0.71364, accuracy: 0.88166, avg. loss over tasks: 0.71364
Diversity Loss - Mean: -0.12213, Variance: 0.01488
Semantic Loss - Mean: 0.72526, Variance: 0.04076

Train Epoch: 113 
task: sign, mean loss: 0.00249, accuracy: 1.00000, avg. loss over tasks: 0.00249, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.12817, Variance: 0.01289
Semantic Loss - Mean: 0.02337, Variance: 0.00972

Test Epoch: 113 
task: sign, mean loss: 0.70093, accuracy: 0.88166, avg. loss over tasks: 0.70093
Diversity Loss - Mean: -0.12201, Variance: 0.01488
Semantic Loss - Mean: 0.73694, Variance: 0.04063

Train Epoch: 114 
task: sign, mean loss: 0.00176, accuracy: 1.00000, avg. loss over tasks: 0.00176, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.12668, Variance: 0.01289
Semantic Loss - Mean: 0.04114, Variance: 0.00971

Test Epoch: 114 
task: sign, mean loss: 0.74177, accuracy: 0.85799, avg. loss over tasks: 0.74177
Diversity Loss - Mean: -0.12154, Variance: 0.01488
Semantic Loss - Mean: 0.82163, Variance: 0.04049

Train Epoch: 115 
task: sign, mean loss: 0.00372, accuracy: 1.00000, avg. loss over tasks: 0.00372, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.12699, Variance: 0.01288
Semantic Loss - Mean: 0.02926, Variance: 0.00966

Test Epoch: 115 
task: sign, mean loss: 0.83555, accuracy: 0.85799, avg. loss over tasks: 0.83555
Diversity Loss - Mean: -0.12174, Variance: 0.01489
Semantic Loss - Mean: 0.94533, Variance: 0.04038

Train Epoch: 116 
task: sign, mean loss: 0.00232, accuracy: 1.00000, avg. loss over tasks: 0.00232, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.12641, Variance: 0.01288
Semantic Loss - Mean: 0.02423, Variance: 0.00958

Test Epoch: 116 
task: sign, mean loss: 0.69101, accuracy: 0.87574, avg. loss over tasks: 0.69101
Diversity Loss - Mean: -0.12209, Variance: 0.01489
Semantic Loss - Mean: 0.75577, Variance: 0.04014

Train Epoch: 117 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.12798, Variance: 0.01287
Semantic Loss - Mean: 0.02095, Variance: 0.00953

Test Epoch: 117 
task: sign, mean loss: 0.75430, accuracy: 0.87574, avg. loss over tasks: 0.75430
Diversity Loss - Mean: -0.12259, Variance: 0.01489
Semantic Loss - Mean: 0.82093, Variance: 0.03990

Train Epoch: 118 
task: sign, mean loss: 0.00740, accuracy: 0.99457, avg. loss over tasks: 0.00740, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.12804, Variance: 0.01287
Semantic Loss - Mean: 0.03195, Variance: 0.00948

Test Epoch: 118 
task: sign, mean loss: 0.76473, accuracy: 0.86391, avg. loss over tasks: 0.76473
Diversity Loss - Mean: -0.12233, Variance: 0.01490
Semantic Loss - Mean: 0.87786, Variance: 0.03970

Train Epoch: 119 
task: sign, mean loss: 0.00143, accuracy: 1.00000, avg. loss over tasks: 0.00143, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.12836, Variance: 0.01286
Semantic Loss - Mean: 0.02952, Variance: 0.00945

Test Epoch: 119 
task: sign, mean loss: 0.71975, accuracy: 0.88166, avg. loss over tasks: 0.71975
Diversity Loss - Mean: -0.12185, Variance: 0.01490
Semantic Loss - Mean: 0.82347, Variance: 0.03956

Train Epoch: 120 
task: sign, mean loss: 0.00574, accuracy: 1.00000, avg. loss over tasks: 0.00574, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.12759, Variance: 0.01286
Semantic Loss - Mean: 0.04763, Variance: 0.00943

Test Epoch: 120 
task: sign, mean loss: 0.77406, accuracy: 0.86982, avg. loss over tasks: 0.77406
Diversity Loss - Mean: -0.12253, Variance: 0.01490
Semantic Loss - Mean: 0.85613, Variance: 0.03936

Train Epoch: 121 
task: sign, mean loss: 0.00358, accuracy: 1.00000, avg. loss over tasks: 0.00358, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.12853, Variance: 0.01285
Semantic Loss - Mean: 0.05469, Variance: 0.00940

Test Epoch: 121 
task: sign, mean loss: 0.80911, accuracy: 0.85799, avg. loss over tasks: 0.80911
Diversity Loss - Mean: -0.12183, Variance: 0.01491
Semantic Loss - Mean: 0.94067, Variance: 0.03925

Train Epoch: 122 
task: sign, mean loss: 0.00298, accuracy: 1.00000, avg. loss over tasks: 0.00298, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.12863, Variance: 0.01285
Semantic Loss - Mean: 0.03407, Variance: 0.00937

Test Epoch: 122 
task: sign, mean loss: 0.80343, accuracy: 0.86982, avg. loss over tasks: 0.80343
Diversity Loss - Mean: -0.12244, Variance: 0.01491
Semantic Loss - Mean: 0.91879, Variance: 0.03914

Train Epoch: 123 
task: sign, mean loss: 0.00083, accuracy: 1.00000, avg. loss over tasks: 0.00083, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.12900, Variance: 0.01285
Semantic Loss - Mean: 0.01371, Variance: 0.00930

Test Epoch: 123 
task: sign, mean loss: 0.81971, accuracy: 0.85799, avg. loss over tasks: 0.81971
Diversity Loss - Mean: -0.12254, Variance: 0.01491
Semantic Loss - Mean: 0.94005, Variance: 0.03903

Train Epoch: 124 
task: sign, mean loss: 0.00228, accuracy: 1.00000, avg. loss over tasks: 0.00228, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.12935, Variance: 0.01285
Semantic Loss - Mean: 0.01950, Variance: 0.00925

Test Epoch: 124 
task: sign, mean loss: 0.74392, accuracy: 0.88166, avg. loss over tasks: 0.74392
Diversity Loss - Mean: -0.12230, Variance: 0.01491
Semantic Loss - Mean: 0.82236, Variance: 0.03884

Train Epoch: 125 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.12954, Variance: 0.01284
Semantic Loss - Mean: 0.01603, Variance: 0.00918

Test Epoch: 125 
task: sign, mean loss: 0.78446, accuracy: 0.86391, avg. loss over tasks: 0.78446
Diversity Loss - Mean: -0.12256, Variance: 0.01491
Semantic Loss - Mean: 0.92549, Variance: 0.03875

Train Epoch: 126 
task: sign, mean loss: 0.01859, accuracy: 0.99457, avg. loss over tasks: 0.01859, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.12910, Variance: 0.01284
Semantic Loss - Mean: 0.03729, Variance: 0.00915

Test Epoch: 126 
task: sign, mean loss: 0.65529, accuracy: 0.88757, avg. loss over tasks: 0.65529
Diversity Loss - Mean: -0.12257, Variance: 0.01492
Semantic Loss - Mean: 0.76661, Variance: 0.03863

Train Epoch: 127 
task: sign, mean loss: 0.00576, accuracy: 0.99457, avg. loss over tasks: 0.00576, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.12849, Variance: 0.01284
Semantic Loss - Mean: 0.01606, Variance: 0.00908

Test Epoch: 127 
task: sign, mean loss: 0.69807, accuracy: 0.87574, avg. loss over tasks: 0.69807
Diversity Loss - Mean: -0.12266, Variance: 0.01492
Semantic Loss - Mean: 0.81285, Variance: 0.03852

Train Epoch: 128 
task: sign, mean loss: 0.00130, accuracy: 1.00000, avg. loss over tasks: 0.00130, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.12897, Variance: 0.01284
Semantic Loss - Mean: 0.01650, Variance: 0.00902

Test Epoch: 128 
task: sign, mean loss: 0.78928, accuracy: 0.85799, avg. loss over tasks: 0.78928
Diversity Loss - Mean: -0.12165, Variance: 0.01492
Semantic Loss - Mean: 0.95263, Variance: 0.03850

Train Epoch: 129 
task: sign, mean loss: 0.00253, accuracy: 1.00000, avg. loss over tasks: 0.00253, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.12893, Variance: 0.01283
Semantic Loss - Mean: 0.02196, Variance: 0.00896

Test Epoch: 129 
task: sign, mean loss: 0.70176, accuracy: 0.87574, avg. loss over tasks: 0.70176
Diversity Loss - Mean: -0.12244, Variance: 0.01492
Semantic Loss - Mean: 0.85121, Variance: 0.03842

Train Epoch: 130 
task: sign, mean loss: 0.00174, accuracy: 1.00000, avg. loss over tasks: 0.00174, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.12933, Variance: 0.01283
Semantic Loss - Mean: 0.01722, Variance: 0.00891

Test Epoch: 130 
task: sign, mean loss: 0.72941, accuracy: 0.86391, avg. loss over tasks: 0.72941
Diversity Loss - Mean: -0.12189, Variance: 0.01493
Semantic Loss - Mean: 0.92063, Variance: 0.03840

Train Epoch: 131 
task: sign, mean loss: 0.00771, accuracy: 0.99457, avg. loss over tasks: 0.00771, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.12893, Variance: 0.01283
Semantic Loss - Mean: 0.03135, Variance: 0.00887

Test Epoch: 131 
task: sign, mean loss: 0.80169, accuracy: 0.85799, avg. loss over tasks: 0.80169
Diversity Loss - Mean: -0.12157, Variance: 0.01492
Semantic Loss - Mean: 0.99203, Variance: 0.03837

Train Epoch: 132 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.12950, Variance: 0.01282
Semantic Loss - Mean: 0.01620, Variance: 0.00882

Test Epoch: 132 
task: sign, mean loss: 0.76226, accuracy: 0.86982, avg. loss over tasks: 0.76226
Diversity Loss - Mean: -0.12201, Variance: 0.01493
Semantic Loss - Mean: 0.94701, Variance: 0.03830

Train Epoch: 133 
task: sign, mean loss: 0.00307, accuracy: 1.00000, avg. loss over tasks: 0.00307, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.12978, Variance: 0.01282
Semantic Loss - Mean: 0.02529, Variance: 0.00877

Test Epoch: 133 
task: sign, mean loss: 0.78130, accuracy: 0.86391, avg. loss over tasks: 0.78130
Diversity Loss - Mean: -0.12161, Variance: 0.01493
Semantic Loss - Mean: 0.97193, Variance: 0.03823

Train Epoch: 134 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.12982, Variance: 0.01282
Semantic Loss - Mean: 0.00905, Variance: 0.00871

Test Epoch: 134 
task: sign, mean loss: 0.74163, accuracy: 0.88757, avg. loss over tasks: 0.74163
Diversity Loss - Mean: -0.12195, Variance: 0.01493
Semantic Loss - Mean: 0.88453, Variance: 0.03811

Train Epoch: 135 
task: sign, mean loss: 0.00253, accuracy: 1.00000, avg. loss over tasks: 0.00253, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.12919, Variance: 0.01281
Semantic Loss - Mean: 0.02497, Variance: 0.00866

Test Epoch: 135 
task: sign, mean loss: 0.75888, accuracy: 0.88166, avg. loss over tasks: 0.75888
Diversity Loss - Mean: -0.12264, Variance: 0.01494
Semantic Loss - Mean: 0.87969, Variance: 0.03798

Train Epoch: 136 
task: sign, mean loss: 0.00062, accuracy: 1.00000, avg. loss over tasks: 0.00062, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.12946, Variance: 0.01281
Semantic Loss - Mean: 0.02370, Variance: 0.00864

Test Epoch: 136 
task: sign, mean loss: 0.72177, accuracy: 0.87574, avg. loss over tasks: 0.72177
Diversity Loss - Mean: -0.12282, Variance: 0.01494
Semantic Loss - Mean: 0.84474, Variance: 0.03785

Train Epoch: 137 
task: sign, mean loss: 0.00112, accuracy: 1.00000, avg. loss over tasks: 0.00112, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.12908, Variance: 0.01280
Semantic Loss - Mean: 0.02308, Variance: 0.00860

Test Epoch: 137 
task: sign, mean loss: 0.75741, accuracy: 0.88166, avg. loss over tasks: 0.75741
Diversity Loss - Mean: -0.12242, Variance: 0.01494
Semantic Loss - Mean: 0.92547, Variance: 0.03775

Train Epoch: 138 
task: sign, mean loss: 0.00237, accuracy: 1.00000, avg. loss over tasks: 0.00237, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.12871, Variance: 0.01280
Semantic Loss - Mean: 0.03127, Variance: 0.00856

Test Epoch: 138 
task: sign, mean loss: 0.78128, accuracy: 0.86982, avg. loss over tasks: 0.78128
Diversity Loss - Mean: -0.12218, Variance: 0.01494
Semantic Loss - Mean: 0.98270, Variance: 0.03769

Train Epoch: 139 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.12958, Variance: 0.01280
Semantic Loss - Mean: 0.02393, Variance: 0.00852

Test Epoch: 139 
task: sign, mean loss: 0.76280, accuracy: 0.86982, avg. loss over tasks: 0.76280
Diversity Loss - Mean: -0.12275, Variance: 0.01494
Semantic Loss - Mean: 0.94621, Variance: 0.03761

Train Epoch: 140 
task: sign, mean loss: 0.06840, accuracy: 0.98913, avg. loss over tasks: 0.06840, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.12953, Variance: 0.01279
Semantic Loss - Mean: 0.07286, Variance: 0.00851

Test Epoch: 140 
task: sign, mean loss: 0.74532, accuracy: 0.88166, avg. loss over tasks: 0.74532
Diversity Loss - Mean: -0.12244, Variance: 0.01495
Semantic Loss - Mean: 0.90921, Variance: 0.03752

Train Epoch: 141 
task: sign, mean loss: 0.00274, accuracy: 1.00000, avg. loss over tasks: 0.00274, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.12942, Variance: 0.01279
Semantic Loss - Mean: 0.02938, Variance: 0.00848

Test Epoch: 141 
task: sign, mean loss: 0.73291, accuracy: 0.88166, avg. loss over tasks: 0.73291
Diversity Loss - Mean: -0.12242, Variance: 0.01495
Semantic Loss - Mean: 0.88527, Variance: 0.03742

Train Epoch: 142 
task: sign, mean loss: 0.00180, accuracy: 1.00000, avg. loss over tasks: 0.00180, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.12805, Variance: 0.01279
Semantic Loss - Mean: 0.02255, Variance: 0.00844

Test Epoch: 142 
task: sign, mean loss: 0.72833, accuracy: 0.88166, avg. loss over tasks: 0.72833
Diversity Loss - Mean: -0.12264, Variance: 0.01495
Semantic Loss - Mean: 0.86466, Variance: 0.03730

Train Epoch: 143 
task: sign, mean loss: 0.00108, accuracy: 1.00000, avg. loss over tasks: 0.00108, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13009, Variance: 0.01278
Semantic Loss - Mean: 0.01235, Variance: 0.00839

Test Epoch: 143 
task: sign, mean loss: 0.74635, accuracy: 0.87574, avg. loss over tasks: 0.74635
Diversity Loss - Mean: -0.12259, Variance: 0.01496
Semantic Loss - Mean: 0.92491, Variance: 0.03721

Train Epoch: 144 
task: sign, mean loss: 0.00114, accuracy: 1.00000, avg. loss over tasks: 0.00114, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.12975, Variance: 0.01278
Semantic Loss - Mean: 0.01735, Variance: 0.00834

Test Epoch: 144 
task: sign, mean loss: 0.79831, accuracy: 0.86391, avg. loss over tasks: 0.79831
Diversity Loss - Mean: -0.12243, Variance: 0.01496
Semantic Loss - Mean: 1.01663, Variance: 0.03717

Train Epoch: 145 
task: sign, mean loss: 0.00107, accuracy: 1.00000, avg. loss over tasks: 0.00107, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.12991, Variance: 0.01277
Semantic Loss - Mean: 0.02035, Variance: 0.00830

Test Epoch: 145 
task: sign, mean loss: 0.75768, accuracy: 0.88166, avg. loss over tasks: 0.75768
Diversity Loss - Mean: -0.12294, Variance: 0.01496
Semantic Loss - Mean: 0.92069, Variance: 0.03707

Train Epoch: 146 
task: sign, mean loss: 0.00142, accuracy: 1.00000, avg. loss over tasks: 0.00142, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.12952, Variance: 0.01277
Semantic Loss - Mean: 0.03068, Variance: 0.00827

Test Epoch: 146 
task: sign, mean loss: 0.87690, accuracy: 0.86391, avg. loss over tasks: 0.87690
Diversity Loss - Mean: -0.12200, Variance: 0.01496
Semantic Loss - Mean: 1.12059, Variance: 0.03707

Train Epoch: 147 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.12982, Variance: 0.01277
Semantic Loss - Mean: 0.02218, Variance: 0.00823

Test Epoch: 147 
task: sign, mean loss: 0.81817, accuracy: 0.86391, avg. loss over tasks: 0.81817
Diversity Loss - Mean: -0.12225, Variance: 0.01496
Semantic Loss - Mean: 1.04844, Variance: 0.03704

Train Epoch: 148 
task: sign, mean loss: 0.00481, accuracy: 0.99457, avg. loss over tasks: 0.00481, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.12857, Variance: 0.01276
Semantic Loss - Mean: 0.03090, Variance: 0.00821

Test Epoch: 148 
task: sign, mean loss: 0.82326, accuracy: 0.86982, avg. loss over tasks: 0.82326
Diversity Loss - Mean: -0.12304, Variance: 0.01496
Semantic Loss - Mean: 0.99070, Variance: 0.03697

Train Epoch: 149 
task: sign, mean loss: 0.00119, accuracy: 1.00000, avg. loss over tasks: 0.00119, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.12885, Variance: 0.01276
Semantic Loss - Mean: 0.03670, Variance: 0.00819

Test Epoch: 149 
task: sign, mean loss: 0.78634, accuracy: 0.88166, avg. loss over tasks: 0.78634
Diversity Loss - Mean: -0.12283, Variance: 0.01496
Semantic Loss - Mean: 0.92456, Variance: 0.03687

Train Epoch: 150 
task: sign, mean loss: 0.00069, accuracy: 1.00000, avg. loss over tasks: 0.00069, lr: 3e-07
Diversity Loss - Mean: -0.12939, Variance: 0.01276
Semantic Loss - Mean: 0.02197, Variance: 0.00817

Test Epoch: 150 
task: sign, mean loss: 0.76672, accuracy: 0.88166, avg. loss over tasks: 0.76672
Diversity Loss - Mean: -0.12310, Variance: 0.01496
Semantic Loss - Mean: 0.91351, Variance: 0.03677

