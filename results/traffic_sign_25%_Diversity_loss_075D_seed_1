Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09208, accuracy: 0.63587, avg. loss over tasks: 1.09208, lr: 3e-05
Diversity Loss - Mean: -0.01161, Variance: 0.01053
Semantic Loss - Mean: 1.43161, Variance: 0.07230

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17587, accuracy: 0.66272, avg. loss over tasks: 1.17587
Diversity Loss - Mean: -0.03388, Variance: 0.01257
Semantic Loss - Mean: 1.16164, Variance: 0.05333

Train Epoch: 2 
task: sign, mean loss: 0.96480, accuracy: 0.67935, avg. loss over tasks: 0.96480, lr: 6e-05
Diversity Loss - Mean: -0.02929, Variance: 0.01050
Semantic Loss - Mean: 0.98182, Variance: 0.03898

Test Epoch: 2 
task: sign, mean loss: 1.10550, accuracy: 0.66272, avg. loss over tasks: 1.10550
Diversity Loss - Mean: -0.04847, Variance: 0.01232
Semantic Loss - Mean: 1.14230, Variance: 0.03257

Train Epoch: 3 
task: sign, mean loss: 0.80084, accuracy: 0.69565, avg. loss over tasks: 0.80084, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.06182, Variance: 0.01043
Semantic Loss - Mean: 0.98950, Variance: 0.02706

Test Epoch: 3 
task: sign, mean loss: 1.28866, accuracy: 0.60355, avg. loss over tasks: 1.28866
Diversity Loss - Mean: -0.08239, Variance: 0.01172
Semantic Loss - Mean: 1.10653, Variance: 0.02929

Train Epoch: 4 
task: sign, mean loss: 0.74521, accuracy: 0.69022, avg. loss over tasks: 0.74521, lr: 0.00012
Diversity Loss - Mean: -0.08757, Variance: 0.01024
Semantic Loss - Mean: 0.88484, Variance: 0.02093

Test Epoch: 4 
task: sign, mean loss: 1.46374, accuracy: 0.60947, avg. loss over tasks: 1.46374
Diversity Loss - Mean: -0.09041, Variance: 0.01114
Semantic Loss - Mean: 1.08298, Variance: 0.02377

Train Epoch: 5 
task: sign, mean loss: 0.70379, accuracy: 0.72826, avg. loss over tasks: 0.70379, lr: 0.00015
Diversity Loss - Mean: -0.08039, Variance: 0.00994
Semantic Loss - Mean: 0.77908, Variance: 0.01704

Test Epoch: 5 
task: sign, mean loss: 1.93519, accuracy: 0.60947, avg. loss over tasks: 1.93519
Diversity Loss - Mean: -0.07742, Variance: 0.01120
Semantic Loss - Mean: 1.28480, Variance: 0.02151

Train Epoch: 6 
task: sign, mean loss: 0.68742, accuracy: 0.78261, avg. loss over tasks: 0.68742, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.07577, Variance: 0.00982
Semantic Loss - Mean: 0.71731, Variance: 0.01480

Test Epoch: 6 
task: sign, mean loss: 2.05596, accuracy: 0.65680, avg. loss over tasks: 2.05596
Diversity Loss - Mean: -0.06260, Variance: 0.01215
Semantic Loss - Mean: 1.51144, Variance: 0.02075

Train Epoch: 7 
task: sign, mean loss: 0.59586, accuracy: 0.74457, avg. loss over tasks: 0.59586, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.08175, Variance: 0.00999
Semantic Loss - Mean: 0.68274, Variance: 0.01308

Test Epoch: 7 
task: sign, mean loss: 2.24507, accuracy: 0.39645, avg. loss over tasks: 2.24507
Diversity Loss - Mean: -0.04585, Variance: 0.01207
Semantic Loss - Mean: 1.75371, Variance: 0.02174

Train Epoch: 8 
task: sign, mean loss: 0.55442, accuracy: 0.80978, avg. loss over tasks: 0.55442, lr: 0.00024
Diversity Loss - Mean: -0.05628, Variance: 0.00994
Semantic Loss - Mean: 0.59353, Variance: 0.01204

Test Epoch: 8 
task: sign, mean loss: 2.35934, accuracy: 0.64497, avg. loss over tasks: 2.35934
Diversity Loss - Mean: -0.04450, Variance: 0.01214
Semantic Loss - Mean: 1.72877, Variance: 0.02272

Train Epoch: 9 
task: sign, mean loss: 0.61522, accuracy: 0.79348, avg. loss over tasks: 0.61522, lr: 0.00027
Diversity Loss - Mean: -0.06677, Variance: 0.00990
Semantic Loss - Mean: 0.57387, Variance: 0.01144

Test Epoch: 9 
task: sign, mean loss: 2.05912, accuracy: 0.27219, avg. loss over tasks: 2.05912
Diversity Loss - Mean: -0.01614, Variance: 0.01204
Semantic Loss - Mean: 1.75521, Variance: 0.02864

Train Epoch: 10 
task: sign, mean loss: 0.57938, accuracy: 0.83152, avg. loss over tasks: 0.57938, lr: 0.0003
Diversity Loss - Mean: -0.05852, Variance: 0.00984
Semantic Loss - Mean: 0.56117, Variance: 0.01121

Test Epoch: 10 
task: sign, mean loss: 1.88538, accuracy: 0.46154, avg. loss over tasks: 1.88538
Diversity Loss - Mean: -0.01014, Variance: 0.01206
Semantic Loss - Mean: 1.34335, Variance: 0.02985

Train Epoch: 11 
task: sign, mean loss: 0.46933, accuracy: 0.80435, avg. loss over tasks: 0.46933, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.03262, Variance: 0.00981
Semantic Loss - Mean: 0.48870, Variance: 0.01084

Test Epoch: 11 
task: sign, mean loss: 2.01374, accuracy: 0.46746, avg. loss over tasks: 2.01374
Diversity Loss - Mean: -0.00332, Variance: 0.01200
Semantic Loss - Mean: 1.35380, Variance: 0.04130

Train Epoch: 12 
task: sign, mean loss: 0.46964, accuracy: 0.84783, avg. loss over tasks: 0.46964, lr: 0.000299849111021216
Diversity Loss - Mean: -0.04196, Variance: 0.00984
Semantic Loss - Mean: 0.50550, Variance: 0.01064

Test Epoch: 12 
task: sign, mean loss: 1.04376, accuracy: 0.72189, avg. loss over tasks: 1.04376
Diversity Loss - Mean: -0.04246, Variance: 0.01207
Semantic Loss - Mean: 0.99295, Variance: 0.04325

Train Epoch: 13 
task: sign, mean loss: 0.27966, accuracy: 0.88043, avg. loss over tasks: 0.27966, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.05504, Variance: 0.00992
Semantic Loss - Mean: 0.42853, Variance: 0.01102

Test Epoch: 13 
task: sign, mean loss: 0.65176, accuracy: 0.78107, avg. loss over tasks: 0.65176
Diversity Loss - Mean: -0.08401, Variance: 0.01224
Semantic Loss - Mean: 0.71461, Variance: 0.04447

Train Epoch: 14 
task: sign, mean loss: 0.26408, accuracy: 0.89130, avg. loss over tasks: 0.26408, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.05819, Variance: 0.01001
Semantic Loss - Mean: 0.33273, Variance: 0.01106

Test Epoch: 14 
task: sign, mean loss: 2.07842, accuracy: 0.28994, avg. loss over tasks: 2.07842
Diversity Loss - Mean: 0.00769, Variance: 0.01217
Semantic Loss - Mean: 1.73184, Variance: 0.05338

Train Epoch: 15 
task: sign, mean loss: 0.26788, accuracy: 0.88587, avg. loss over tasks: 0.26788, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.06766, Variance: 0.01020
Semantic Loss - Mean: 0.32245, Variance: 0.01118

Test Epoch: 15 
task: sign, mean loss: 0.79411, accuracy: 0.79290, avg. loss over tasks: 0.79411
Diversity Loss - Mean: -0.05608, Variance: 0.01224
Semantic Loss - Mean: 0.75300, Variance: 0.05244

Train Epoch: 16 
task: sign, mean loss: 0.12851, accuracy: 0.95652, avg. loss over tasks: 0.12851, lr: 0.000298643821800925
Diversity Loss - Mean: -0.06846, Variance: 0.01034
Semantic Loss - Mean: 0.19954, Variance: 0.01089

Test Epoch: 16 
task: sign, mean loss: 1.07502, accuracy: 0.75148, avg. loss over tasks: 1.07502
Diversity Loss - Mean: -0.08975, Variance: 0.01247
Semantic Loss - Mean: 0.87889, Variance: 0.05050

Train Epoch: 17 
task: sign, mean loss: 0.08899, accuracy: 0.96739, avg. loss over tasks: 0.08899, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.07099, Variance: 0.01050
Semantic Loss - Mean: 0.19328, Variance: 0.01108

Test Epoch: 17 
task: sign, mean loss: 0.88216, accuracy: 0.75148, avg. loss over tasks: 0.88216
Diversity Loss - Mean: -0.07741, Variance: 0.01267
Semantic Loss - Mean: 0.76849, Variance: 0.05009

Train Epoch: 18 
task: sign, mean loss: 0.08924, accuracy: 0.97826, avg. loss over tasks: 0.08924, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.06981, Variance: 0.01067
Semantic Loss - Mean: 0.15281, Variance: 0.01071

Test Epoch: 18 
task: sign, mean loss: 1.60855, accuracy: 0.74556, avg. loss over tasks: 1.60855
Diversity Loss - Mean: -0.07982, Variance: 0.01280
Semantic Loss - Mean: 1.21666, Variance: 0.04989

Train Epoch: 19 
task: sign, mean loss: 0.10356, accuracy: 0.95652, avg. loss over tasks: 0.10356, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.07690, Variance: 0.01083
Semantic Loss - Mean: 0.16546, Variance: 0.01084

Test Epoch: 19 
task: sign, mean loss: 1.53164, accuracy: 0.55030, avg. loss over tasks: 1.53164
Diversity Loss - Mean: -0.06027, Variance: 0.01285
Semantic Loss - Mean: 1.16410, Variance: 0.05292

Train Epoch: 20 
task: sign, mean loss: 0.13023, accuracy: 0.96196, avg. loss over tasks: 0.13023, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.07812, Variance: 0.01093
Semantic Loss - Mean: 0.19444, Variance: 0.01083

Test Epoch: 20 
task: sign, mean loss: 2.46402, accuracy: 0.43195, avg. loss over tasks: 2.46402
Diversity Loss - Mean: -0.05011, Variance: 0.01296
Semantic Loss - Mean: 2.36166, Variance: 0.05676

Train Epoch: 21 
task: sign, mean loss: 0.13569, accuracy: 0.94565, avg. loss over tasks: 0.13569, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.07839, Variance: 0.01101
Semantic Loss - Mean: 0.22848, Variance: 0.01084

Test Epoch: 21 
task: sign, mean loss: 0.94254, accuracy: 0.71598, avg. loss over tasks: 0.94254
Diversity Loss - Mean: -0.06865, Variance: 0.01301
Semantic Loss - Mean: 0.65484, Variance: 0.05707

Train Epoch: 22 
task: sign, mean loss: 0.26543, accuracy: 0.90761, avg. loss over tasks: 0.26543, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.08589, Variance: 0.01110
Semantic Loss - Mean: 0.37123, Variance: 0.01204

Test Epoch: 22 
task: sign, mean loss: 1.09860, accuracy: 0.75740, avg. loss over tasks: 1.09860
Diversity Loss - Mean: -0.08679, Variance: 0.01304
Semantic Loss - Mean: 0.96930, Variance: 0.05576

Train Epoch: 23 
task: sign, mean loss: 0.24416, accuracy: 0.92391, avg. loss over tasks: 0.24416, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.08717, Variance: 0.01119
Semantic Loss - Mean: 0.30311, Variance: 0.01218

Test Epoch: 23 
task: sign, mean loss: 1.57361, accuracy: 0.79290, avg. loss over tasks: 1.57361
Diversity Loss - Mean: -0.08364, Variance: 0.01301
Semantic Loss - Mean: 1.35843, Variance: 0.05618

Train Epoch: 24 
task: sign, mean loss: 0.14331, accuracy: 0.92935, avg. loss over tasks: 0.14331, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.09976, Variance: 0.01135
Semantic Loss - Mean: 0.22777, Variance: 0.01208

Test Epoch: 24 
task: sign, mean loss: 1.83185, accuracy: 0.79290, avg. loss over tasks: 1.83185
Diversity Loss - Mean: -0.05720, Variance: 0.01293
Semantic Loss - Mean: 1.96222, Variance: 0.06288

Train Epoch: 25 
task: sign, mean loss: 0.08849, accuracy: 0.96739, avg. loss over tasks: 0.08849, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.09368, Variance: 0.01147
Semantic Loss - Mean: 0.18444, Variance: 0.01196

Test Epoch: 25 
task: sign, mean loss: 1.01263, accuracy: 0.78698, avg. loss over tasks: 1.01263
Diversity Loss - Mean: -0.07686, Variance: 0.01292
Semantic Loss - Mean: 0.98151, Variance: 0.06217

Train Epoch: 26 
task: sign, mean loss: 0.09676, accuracy: 0.95109, avg. loss over tasks: 0.09676, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.09527, Variance: 0.01158
Semantic Loss - Mean: 0.19328, Variance: 0.01198

Test Epoch: 26 
task: sign, mean loss: 0.53840, accuracy: 0.82249, avg. loss over tasks: 0.53840
Diversity Loss - Mean: -0.06808, Variance: 0.01294
Semantic Loss - Mean: 0.65178, Variance: 0.06419

Train Epoch: 27 
task: sign, mean loss: 0.09816, accuracy: 0.97283, avg. loss over tasks: 0.09816, lr: 0.000289228031029578
Diversity Loss - Mean: -0.09351, Variance: 0.01167
Semantic Loss - Mean: 0.18087, Variance: 0.01232

Test Epoch: 27 
task: sign, mean loss: 1.34926, accuracy: 0.79882, avg. loss over tasks: 1.34926
Diversity Loss - Mean: -0.08712, Variance: 0.01300
Semantic Loss - Mean: 1.29281, Variance: 0.06513

Train Epoch: 28 
task: sign, mean loss: 0.09347, accuracy: 0.97283, avg. loss over tasks: 0.09347, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.09742, Variance: 0.01176
Semantic Loss - Mean: 0.15503, Variance: 0.01227

Test Epoch: 28 
task: sign, mean loss: 0.88875, accuracy: 0.80473, avg. loss over tasks: 0.88875
Diversity Loss - Mean: -0.08719, Variance: 0.01300
Semantic Loss - Mean: 0.87939, Variance: 0.06535

Train Epoch: 29 
task: sign, mean loss: 0.18181, accuracy: 0.93478, avg. loss over tasks: 0.18181, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.10103, Variance: 0.01183
Semantic Loss - Mean: 0.25745, Variance: 0.01240

Test Epoch: 29 
task: sign, mean loss: 1.04436, accuracy: 0.81065, avg. loss over tasks: 1.04436
Diversity Loss - Mean: -0.09231, Variance: 0.01297
Semantic Loss - Mean: 0.77692, Variance: 0.06432

Train Epoch: 30 
task: sign, mean loss: 0.18455, accuracy: 0.92935, avg. loss over tasks: 0.18455, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.10473, Variance: 0.01188
Semantic Loss - Mean: 0.24936, Variance: 0.01227

Test Epoch: 30 
task: sign, mean loss: 0.76755, accuracy: 0.66864, avg. loss over tasks: 0.76755
Diversity Loss - Mean: -0.09864, Variance: 0.01298
Semantic Loss - Mean: 0.83334, Variance: 0.06339

Train Epoch: 31 
task: sign, mean loss: 0.19830, accuracy: 0.92935, avg. loss over tasks: 0.19830, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.10985, Variance: 0.01196
Semantic Loss - Mean: 0.25941, Variance: 0.01226

Test Epoch: 31 
task: sign, mean loss: 0.54349, accuracy: 0.85207, avg. loss over tasks: 0.54349
Diversity Loss - Mean: -0.09944, Variance: 0.01300
Semantic Loss - Mean: 0.50198, Variance: 0.06218

Train Epoch: 32 
task: sign, mean loss: 0.37379, accuracy: 0.86413, avg. loss over tasks: 0.37379, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.10729, Variance: 0.01201
Semantic Loss - Mean: 0.47149, Variance: 0.01235

Test Epoch: 32 
task: sign, mean loss: 4.22244, accuracy: 0.20710, avg. loss over tasks: 4.22244
Diversity Loss - Mean: -0.05292, Variance: 0.01286
Semantic Loss - Mean: 3.08106, Variance: 0.06770

Train Epoch: 33 
task: sign, mean loss: 0.13780, accuracy: 0.95652, avg. loss over tasks: 0.13780, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.10597, Variance: 0.01203
Semantic Loss - Mean: 0.22416, Variance: 0.01253

Test Epoch: 33 
task: sign, mean loss: 0.54270, accuracy: 0.83432, avg. loss over tasks: 0.54270
Diversity Loss - Mean: -0.09713, Variance: 0.01287
Semantic Loss - Mean: 0.53144, Variance: 0.06631

Train Epoch: 34 
task: sign, mean loss: 0.34600, accuracy: 0.91304, avg. loss over tasks: 0.34600, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.10282, Variance: 0.01204
Semantic Loss - Mean: 0.37794, Variance: 0.01279

Test Epoch: 34 
task: sign, mean loss: 0.57892, accuracy: 0.82840, avg. loss over tasks: 0.57892
Diversity Loss - Mean: -0.10586, Variance: 0.01299
Semantic Loss - Mean: 0.57356, Variance: 0.06485

Train Epoch: 35 
task: sign, mean loss: 0.20688, accuracy: 0.92935, avg. loss over tasks: 0.20688, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.10796, Variance: 0.01208
Semantic Loss - Mean: 0.28589, Variance: 0.01285

Test Epoch: 35 
task: sign, mean loss: 0.97328, accuracy: 0.77515, avg. loss over tasks: 0.97328
Diversity Loss - Mean: -0.10605, Variance: 0.01306
Semantic Loss - Mean: 0.96427, Variance: 0.06408

Train Epoch: 36 
task: sign, mean loss: 0.10990, accuracy: 0.94022, avg. loss over tasks: 0.10990, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.10883, Variance: 0.01211
Semantic Loss - Mean: 0.16623, Variance: 0.01270

Test Epoch: 36 
task: sign, mean loss: 1.36298, accuracy: 0.76923, avg. loss over tasks: 1.36298
Diversity Loss - Mean: -0.10510, Variance: 0.01308
Semantic Loss - Mean: 1.20248, Variance: 0.06354

Train Epoch: 37 
task: sign, mean loss: 0.23386, accuracy: 0.90761, avg. loss over tasks: 0.23386, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.10949, Variance: 0.01215
Semantic Loss - Mean: 0.29380, Variance: 0.01275

Test Epoch: 37 
task: sign, mean loss: 0.45316, accuracy: 0.84615, avg. loss over tasks: 0.45316
Diversity Loss - Mean: -0.11381, Variance: 0.01328
Semantic Loss - Mean: 0.43204, Variance: 0.06216

Train Epoch: 38 
task: sign, mean loss: 0.14581, accuracy: 0.95652, avg. loss over tasks: 0.14581, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.11168, Variance: 0.01217
Semantic Loss - Mean: 0.22255, Variance: 0.01263

Test Epoch: 38 
task: sign, mean loss: 0.58797, accuracy: 0.81065, avg. loss over tasks: 0.58797
Diversity Loss - Mean: -0.10476, Variance: 0.01333
Semantic Loss - Mean: 0.59141, Variance: 0.06101

Train Epoch: 39 
task: sign, mean loss: 0.08772, accuracy: 0.96196, avg. loss over tasks: 0.08772, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.11005, Variance: 0.01220
Semantic Loss - Mean: 0.15818, Variance: 0.01260

Test Epoch: 39 
task: sign, mean loss: 1.41506, accuracy: 0.77515, avg. loss over tasks: 1.41506
Diversity Loss - Mean: -0.11080, Variance: 0.01333
Semantic Loss - Mean: 1.21243, Variance: 0.06036

Train Epoch: 40 
task: sign, mean loss: 0.23204, accuracy: 0.92935, avg. loss over tasks: 0.23204, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.11387, Variance: 0.01224
Semantic Loss - Mean: 0.30859, Variance: 0.01259

Test Epoch: 40 
task: sign, mean loss: 0.69962, accuracy: 0.72189, avg. loss over tasks: 0.69962
Diversity Loss - Mean: -0.09974, Variance: 0.01334
Semantic Loss - Mean: 0.78654, Variance: 0.06077

Train Epoch: 41 
task: sign, mean loss: 0.13734, accuracy: 0.94565, avg. loss over tasks: 0.13734, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.11197, Variance: 0.01226
Semantic Loss - Mean: 0.22654, Variance: 0.01256

Test Epoch: 41 
task: sign, mean loss: 0.80287, accuracy: 0.78698, avg. loss over tasks: 0.80287
Diversity Loss - Mean: -0.11435, Variance: 0.01335
Semantic Loss - Mean: 0.82061, Variance: 0.06029

Train Epoch: 42 
task: sign, mean loss: 0.20672, accuracy: 0.95652, avg. loss over tasks: 0.20672, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.11656, Variance: 0.01230
Semantic Loss - Mean: 0.28032, Variance: 0.01278

Test Epoch: 42 
task: sign, mean loss: 0.98923, accuracy: 0.79882, avg. loss over tasks: 0.98923
Diversity Loss - Mean: -0.12242, Variance: 0.01335
Semantic Loss - Mean: 0.83602, Variance: 0.05923

Train Epoch: 43 
task: sign, mean loss: 0.11914, accuracy: 0.95109, avg. loss over tasks: 0.11914, lr: 0.000260757131773478
Diversity Loss - Mean: -0.11687, Variance: 0.01233
Semantic Loss - Mean: 0.20242, Variance: 0.01269

Test Epoch: 43 
task: sign, mean loss: 0.67590, accuracy: 0.81065, avg. loss over tasks: 0.67590
Diversity Loss - Mean: -0.11488, Variance: 0.01336
Semantic Loss - Mean: 0.65887, Variance: 0.05884

Train Epoch: 44 
task: sign, mean loss: 0.05275, accuracy: 0.99457, avg. loss over tasks: 0.05275, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.11559, Variance: 0.01234
Semantic Loss - Mean: 0.13955, Variance: 0.01279

Test Epoch: 44 
task: sign, mean loss: 0.49122, accuracy: 0.87574, avg. loss over tasks: 0.49122
Diversity Loss - Mean: -0.11502, Variance: 0.01337
Semantic Loss - Mean: 0.45649, Variance: 0.05774

Train Epoch: 45 
task: sign, mean loss: 0.04685, accuracy: 0.99457, avg. loss over tasks: 0.04685, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.11244, Variance: 0.01234
Semantic Loss - Mean: 0.10596, Variance: 0.01266

Test Epoch: 45 
task: sign, mean loss: 0.55326, accuracy: 0.84024, avg. loss over tasks: 0.55326
Diversity Loss - Mean: -0.11020, Variance: 0.01342
Semantic Loss - Mean: 0.54672, Variance: 0.05682

Train Epoch: 46 
task: sign, mean loss: 0.04112, accuracy: 0.98370, avg. loss over tasks: 0.04112, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.11424, Variance: 0.01234
Semantic Loss - Mean: 0.09572, Variance: 0.01257

Test Epoch: 46 
task: sign, mean loss: 0.51585, accuracy: 0.83432, avg. loss over tasks: 0.51585
Diversity Loss - Mean: -0.11362, Variance: 0.01347
Semantic Loss - Mean: 0.48790, Variance: 0.05588

Train Epoch: 47 
task: sign, mean loss: 0.04086, accuracy: 0.98913, avg. loss over tasks: 0.04086, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.11524, Variance: 0.01235
Semantic Loss - Mean: 0.12451, Variance: 0.01267

Test Epoch: 47 
task: sign, mean loss: 0.43797, accuracy: 0.88166, avg. loss over tasks: 0.43797
Diversity Loss - Mean: -0.11837, Variance: 0.01356
Semantic Loss - Mean: 0.44127, Variance: 0.05491

Train Epoch: 48 
task: sign, mean loss: 0.05930, accuracy: 0.97826, avg. loss over tasks: 0.05930, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.11749, Variance: 0.01236
Semantic Loss - Mean: 0.11872, Variance: 0.01261

Test Epoch: 48 
task: sign, mean loss: 0.49890, accuracy: 0.84615, avg. loss over tasks: 0.49890
Diversity Loss - Mean: -0.11822, Variance: 0.01362
Semantic Loss - Mean: 0.52033, Variance: 0.05422

Train Epoch: 49 
task: sign, mean loss: 0.03511, accuracy: 0.98913, avg. loss over tasks: 0.03511, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.11865, Variance: 0.01238
Semantic Loss - Mean: 0.12482, Variance: 0.01279

Test Epoch: 49 
task: sign, mean loss: 0.52625, accuracy: 0.87574, avg. loss over tasks: 0.52625
Diversity Loss - Mean: -0.12320, Variance: 0.01367
Semantic Loss - Mean: 0.50263, Variance: 0.05326

Train Epoch: 50 
task: sign, mean loss: 0.04431, accuracy: 0.99457, avg. loss over tasks: 0.04431, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.11992, Variance: 0.01239
Semantic Loss - Mean: 0.10667, Variance: 0.01271

Test Epoch: 50 
task: sign, mean loss: 0.86402, accuracy: 0.82840, avg. loss over tasks: 0.86402
Diversity Loss - Mean: -0.12049, Variance: 0.01371
Semantic Loss - Mean: 0.83252, Variance: 0.05264

Train Epoch: 51 
task: sign, mean loss: 0.03854, accuracy: 0.98913, avg. loss over tasks: 0.03854, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.12075, Variance: 0.01239
Semantic Loss - Mean: 0.09563, Variance: 0.01266

Test Epoch: 51 
task: sign, mean loss: 0.61338, accuracy: 0.85799, avg. loss over tasks: 0.61338
Diversity Loss - Mean: -0.11916, Variance: 0.01374
Semantic Loss - Mean: 0.47753, Variance: 0.05194

Train Epoch: 52 
task: sign, mean loss: 0.01682, accuracy: 0.99457, avg. loss over tasks: 0.01682, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.12061, Variance: 0.01239
Semantic Loss - Mean: 0.08405, Variance: 0.01256

Test Epoch: 52 
task: sign, mean loss: 0.53941, accuracy: 0.84615, avg. loss over tasks: 0.53941
Diversity Loss - Mean: -0.12390, Variance: 0.01382
Semantic Loss - Mean: 0.50189, Variance: 0.05142

Train Epoch: 53 
task: sign, mean loss: 0.04920, accuracy: 0.97826, avg. loss over tasks: 0.04920, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.12125, Variance: 0.01239
Semantic Loss - Mean: 0.09787, Variance: 0.01250

Test Epoch: 53 
task: sign, mean loss: 0.67661, accuracy: 0.86391, avg. loss over tasks: 0.67661
Diversity Loss - Mean: -0.12403, Variance: 0.01386
Semantic Loss - Mean: 0.67606, Variance: 0.05062

Train Epoch: 54 
task: sign, mean loss: 0.03136, accuracy: 0.98370, avg. loss over tasks: 0.03136, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.12289, Variance: 0.01239
Semantic Loss - Mean: 0.09543, Variance: 0.01242

Test Epoch: 54 
task: sign, mean loss: 0.73572, accuracy: 0.77515, avg. loss over tasks: 0.73572
Diversity Loss - Mean: -0.11864, Variance: 0.01388
Semantic Loss - Mean: 0.71982, Variance: 0.05034

Train Epoch: 55 
task: sign, mean loss: 0.03329, accuracy: 0.98913, avg. loss over tasks: 0.03329, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.12280, Variance: 0.01239
Semantic Loss - Mean: 0.09298, Variance: 0.01234

Test Epoch: 55 
task: sign, mean loss: 0.39477, accuracy: 0.88166, avg. loss over tasks: 0.39477
Diversity Loss - Mean: -0.12078, Variance: 0.01392
Semantic Loss - Mean: 0.36782, Variance: 0.04959

Train Epoch: 56 
task: sign, mean loss: 0.04788, accuracy: 0.97826, avg. loss over tasks: 0.04788, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.12329, Variance: 0.01240
Semantic Loss - Mean: 0.12595, Variance: 0.01238

Test Epoch: 56 
task: sign, mean loss: 1.28952, accuracy: 0.79290, avg. loss over tasks: 1.28952
Diversity Loss - Mean: -0.12241, Variance: 0.01392
Semantic Loss - Mean: 1.24916, Variance: 0.05017

Train Epoch: 57 
task: sign, mean loss: 0.08018, accuracy: 0.96196, avg. loss over tasks: 0.08018, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.12396, Variance: 0.01240
Semantic Loss - Mean: 0.11627, Variance: 0.01229

Test Epoch: 57 
task: sign, mean loss: 1.49016, accuracy: 0.69822, avg. loss over tasks: 1.49016
Diversity Loss - Mean: -0.12333, Variance: 0.01390
Semantic Loss - Mean: 1.06684, Variance: 0.05063

Train Epoch: 58 
task: sign, mean loss: 0.03210, accuracy: 0.98370, avg. loss over tasks: 0.03210, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.12400, Variance: 0.01241
Semantic Loss - Mean: 0.09811, Variance: 0.01233

Test Epoch: 58 
task: sign, mean loss: 0.85431, accuracy: 0.79882, avg. loss over tasks: 0.85431
Diversity Loss - Mean: -0.12420, Variance: 0.01391
Semantic Loss - Mean: 0.61775, Variance: 0.05077

Train Epoch: 59 
task: sign, mean loss: 0.05809, accuracy: 0.98370, avg. loss over tasks: 0.05809, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.12282, Variance: 0.01241
Semantic Loss - Mean: 0.08887, Variance: 0.01227

Test Epoch: 59 
task: sign, mean loss: 1.58741, accuracy: 0.73373, avg. loss over tasks: 1.58741
Diversity Loss - Mean: -0.12223, Variance: 0.01391
Semantic Loss - Mean: 1.53821, Variance: 0.05141

Train Epoch: 60 
task: sign, mean loss: 0.02294, accuracy: 0.99457, avg. loss over tasks: 0.02294, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.12144, Variance: 0.01241
Semantic Loss - Mean: 0.08086, Variance: 0.01218

Test Epoch: 60 
task: sign, mean loss: 1.05664, accuracy: 0.78698, avg. loss over tasks: 1.05664
Diversity Loss - Mean: -0.12094, Variance: 0.01389
Semantic Loss - Mean: 1.01104, Variance: 0.05154

Train Epoch: 61 
task: sign, mean loss: 0.03869, accuracy: 0.98370, avg. loss over tasks: 0.03869, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.12383, Variance: 0.01241
Semantic Loss - Mean: 0.11423, Variance: 0.01232

Test Epoch: 61 
task: sign, mean loss: 0.82800, accuracy: 0.79290, avg. loss over tasks: 0.82800
Diversity Loss - Mean: -0.12100, Variance: 0.01388
Semantic Loss - Mean: 0.70242, Variance: 0.05135

Train Epoch: 62 
task: sign, mean loss: 0.02271, accuracy: 0.98370, avg. loss over tasks: 0.02271, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.12411, Variance: 0.01242
Semantic Loss - Mean: 0.07757, Variance: 0.01230

Test Epoch: 62 
task: sign, mean loss: 0.81807, accuracy: 0.83432, avg. loss over tasks: 0.81807
Diversity Loss - Mean: -0.12481, Variance: 0.01388
Semantic Loss - Mean: 0.80819, Variance: 0.05146

Train Epoch: 63 
task: sign, mean loss: 0.04412, accuracy: 0.98913, avg. loss over tasks: 0.04412, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.12511, Variance: 0.01242
Semantic Loss - Mean: 0.11464, Variance: 0.01220

Test Epoch: 63 
task: sign, mean loss: 1.07739, accuracy: 0.82840, avg. loss over tasks: 1.07739
Diversity Loss - Mean: -0.12335, Variance: 0.01387
Semantic Loss - Mean: 1.16591, Variance: 0.05183

Train Epoch: 64 
task: sign, mean loss: 0.05414, accuracy: 0.98370, avg. loss over tasks: 0.05414, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.12619, Variance: 0.01242
Semantic Loss - Mean: 0.08212, Variance: 0.01220

Test Epoch: 64 
task: sign, mean loss: 1.04467, accuracy: 0.79882, avg. loss over tasks: 1.04467
Diversity Loss - Mean: -0.12544, Variance: 0.01384
Semantic Loss - Mean: 1.05038, Variance: 0.05198

Train Epoch: 65 
task: sign, mean loss: 0.03294, accuracy: 0.98370, avg. loss over tasks: 0.03294, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.12649, Variance: 0.01243
Semantic Loss - Mean: 0.10705, Variance: 0.01218

Test Epoch: 65 
task: sign, mean loss: 0.91841, accuracy: 0.81065, avg. loss over tasks: 0.91841
Diversity Loss - Mean: -0.12850, Variance: 0.01387
Semantic Loss - Mean: 0.91361, Variance: 0.05230

Train Epoch: 66 
task: sign, mean loss: 0.02533, accuracy: 0.98913, avg. loss over tasks: 0.02533, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.12776, Variance: 0.01243
Semantic Loss - Mean: 0.07880, Variance: 0.01224

Test Epoch: 66 
task: sign, mean loss: 0.65514, accuracy: 0.85207, avg. loss over tasks: 0.65514
Diversity Loss - Mean: -0.13104, Variance: 0.01392
Semantic Loss - Mean: 0.54430, Variance: 0.05182

Train Epoch: 67 
task: sign, mean loss: 0.02489, accuracy: 0.99457, avg. loss over tasks: 0.02489, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.12796, Variance: 0.01245
Semantic Loss - Mean: 0.06403, Variance: 0.01214

Test Epoch: 67 
task: sign, mean loss: 0.47232, accuracy: 0.88166, avg. loss over tasks: 0.47232
Diversity Loss - Mean: -0.13001, Variance: 0.01397
Semantic Loss - Mean: 0.40430, Variance: 0.05126

Train Epoch: 68 
task: sign, mean loss: 0.15366, accuracy: 0.97283, avg. loss over tasks: 0.15366, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.12788, Variance: 0.01245
Semantic Loss - Mean: 0.18650, Variance: 0.01226

Test Epoch: 68 
task: sign, mean loss: 1.26027, accuracy: 0.79290, avg. loss over tasks: 1.26027
Diversity Loss - Mean: -0.12644, Variance: 0.01397
Semantic Loss - Mean: 1.24128, Variance: 0.05186

Train Epoch: 69 
task: sign, mean loss: 0.04324, accuracy: 0.98370, avg. loss over tasks: 0.04324, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.13045, Variance: 0.01245
Semantic Loss - Mean: 0.12309, Variance: 0.01230

Test Epoch: 69 
task: sign, mean loss: 0.56053, accuracy: 0.86391, avg. loss over tasks: 0.56053
Diversity Loss - Mean: -0.13181, Variance: 0.01401
Semantic Loss - Mean: 0.52966, Variance: 0.05161

Train Epoch: 70 
task: sign, mean loss: 0.01125, accuracy: 1.00000, avg. loss over tasks: 0.01125, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.13054, Variance: 0.01245
Semantic Loss - Mean: 0.05982, Variance: 0.01219

Test Epoch: 70 
task: sign, mean loss: 0.57651, accuracy: 0.86982, avg. loss over tasks: 0.57651
Diversity Loss - Mean: -0.13265, Variance: 0.01405
Semantic Loss - Mean: 0.50040, Variance: 0.05141

Train Epoch: 71 
task: sign, mean loss: 0.00986, accuracy: 0.99457, avg. loss over tasks: 0.00986, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.13063, Variance: 0.01246
Semantic Loss - Mean: 0.06187, Variance: 0.01220

Test Epoch: 71 
task: sign, mean loss: 0.55804, accuracy: 0.89349, avg. loss over tasks: 0.55804
Diversity Loss - Mean: -0.13301, Variance: 0.01409
Semantic Loss - Mean: 0.52502, Variance: 0.05108

Train Epoch: 72 
task: sign, mean loss: 0.00822, accuracy: 1.00000, avg. loss over tasks: 0.00822, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.13143, Variance: 0.01248
Semantic Loss - Mean: 0.05155, Variance: 0.01212

Test Epoch: 72 
task: sign, mean loss: 0.52052, accuracy: 0.86982, avg. loss over tasks: 0.52052
Diversity Loss - Mean: -0.13257, Variance: 0.01412
Semantic Loss - Mean: 0.52295, Variance: 0.05112

Train Epoch: 73 
task: sign, mean loss: 0.02269, accuracy: 0.99457, avg. loss over tasks: 0.02269, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.13086, Variance: 0.01250
Semantic Loss - Mean: 0.06417, Variance: 0.01210

Test Epoch: 73 
task: sign, mean loss: 0.57742, accuracy: 0.85799, avg. loss over tasks: 0.57742
Diversity Loss - Mean: -0.13364, Variance: 0.01415
Semantic Loss - Mean: 0.56900, Variance: 0.05109

Train Epoch: 74 
task: sign, mean loss: 0.00520, accuracy: 1.00000, avg. loss over tasks: 0.00520, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.13157, Variance: 0.01252
Semantic Loss - Mean: 0.04449, Variance: 0.01201

Test Epoch: 74 
task: sign, mean loss: 1.28177, accuracy: 0.80473, avg. loss over tasks: 1.28177
Diversity Loss - Mean: -0.13463, Variance: 0.01419
Semantic Loss - Mean: 1.11756, Variance: 0.05124

Train Epoch: 75 
task: sign, mean loss: 0.00514, accuracy: 1.00000, avg. loss over tasks: 0.00514, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.13100, Variance: 0.01253
Semantic Loss - Mean: 0.06169, Variance: 0.01198

Test Epoch: 75 
task: sign, mean loss: 0.93227, accuracy: 0.84024, avg. loss over tasks: 0.93227
Diversity Loss - Mean: -0.13519, Variance: 0.01423
Semantic Loss - Mean: 0.80369, Variance: 0.05132

Train Epoch: 76 
task: sign, mean loss: 0.00507, accuracy: 1.00000, avg. loss over tasks: 0.00507, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.13206, Variance: 0.01255
Semantic Loss - Mean: 0.04057, Variance: 0.01190

Test Epoch: 76 
task: sign, mean loss: 0.72947, accuracy: 0.86391, avg. loss over tasks: 0.72947
Diversity Loss - Mean: -0.13553, Variance: 0.01427
Semantic Loss - Mean: 0.65256, Variance: 0.05154

Train Epoch: 77 
task: sign, mean loss: 0.01163, accuracy: 0.99457, avg. loss over tasks: 0.01163, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.13196, Variance: 0.01257
Semantic Loss - Mean: 0.05313, Variance: 0.01182

Test Epoch: 77 
task: sign, mean loss: 0.74777, accuracy: 0.82840, avg. loss over tasks: 0.74777
Diversity Loss - Mean: -0.13506, Variance: 0.01431
Semantic Loss - Mean: 0.71691, Variance: 0.05172

Train Epoch: 78 
task: sign, mean loss: 0.05436, accuracy: 0.98913, avg. loss over tasks: 0.05436, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.13228, Variance: 0.01259
Semantic Loss - Mean: 0.05652, Variance: 0.01174

Test Epoch: 78 
task: sign, mean loss: 0.59493, accuracy: 0.82840, avg. loss over tasks: 0.59493
Diversity Loss - Mean: -0.13373, Variance: 0.01436
Semantic Loss - Mean: 0.57534, Variance: 0.05146

Train Epoch: 79 
task: sign, mean loss: 0.00806, accuracy: 1.00000, avg. loss over tasks: 0.00806, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.13205, Variance: 0.01261
Semantic Loss - Mean: 0.03441, Variance: 0.01163

Test Epoch: 79 
task: sign, mean loss: 0.73320, accuracy: 0.82840, avg. loss over tasks: 0.73320
Diversity Loss - Mean: -0.13119, Variance: 0.01437
Semantic Loss - Mean: 0.84141, Variance: 0.05126

Train Epoch: 80 
task: sign, mean loss: 0.03218, accuracy: 0.99457, avg. loss over tasks: 0.03218, lr: 0.00015015
Diversity Loss - Mean: -0.13248, Variance: 0.01263
Semantic Loss - Mean: 0.04375, Variance: 0.01150

Test Epoch: 80 
task: sign, mean loss: 0.43211, accuracy: 0.85799, avg. loss over tasks: 0.43211
Diversity Loss - Mean: -0.13163, Variance: 0.01438
Semantic Loss - Mean: 0.42255, Variance: 0.05080

Train Epoch: 81 
task: sign, mean loss: 0.04540, accuracy: 0.99457, avg. loss over tasks: 0.04540, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.13204, Variance: 0.01266
Semantic Loss - Mean: 0.07881, Variance: 0.01142

Test Epoch: 81 
task: sign, mean loss: 0.37196, accuracy: 0.88757, avg. loss over tasks: 0.37196
Diversity Loss - Mean: -0.13209, Variance: 0.01439
Semantic Loss - Mean: 0.38137, Variance: 0.05041

Train Epoch: 82 
task: sign, mean loss: 0.05365, accuracy: 0.98370, avg. loss over tasks: 0.05365, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.13204, Variance: 0.01269
Semantic Loss - Mean: 0.12164, Variance: 0.01149

Test Epoch: 82 
task: sign, mean loss: 0.72931, accuracy: 0.84024, avg. loss over tasks: 0.72931
Diversity Loss - Mean: -0.13074, Variance: 0.01440
Semantic Loss - Mean: 0.79005, Variance: 0.05071

Train Epoch: 83 
task: sign, mean loss: 0.03291, accuracy: 0.98370, avg. loss over tasks: 0.03291, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.13167, Variance: 0.01271
Semantic Loss - Mean: 0.09517, Variance: 0.01148

Test Epoch: 83 
task: sign, mean loss: 0.97247, accuracy: 0.76923, avg. loss over tasks: 0.97247
Diversity Loss - Mean: -0.12499, Variance: 0.01441
Semantic Loss - Mean: 1.76099, Variance: 0.05602

Train Epoch: 84 
task: sign, mean loss: 0.07419, accuracy: 0.96739, avg. loss over tasks: 0.07419, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.12977, Variance: 0.01272
Semantic Loss - Mean: 0.15556, Variance: 0.01148

Test Epoch: 84 
task: sign, mean loss: 1.11630, accuracy: 0.77515, avg. loss over tasks: 1.11630
Diversity Loss - Mean: -0.12673, Variance: 0.01440
Semantic Loss - Mean: 1.38708, Variance: 0.05768

Train Epoch: 85 
task: sign, mean loss: 0.00961, accuracy: 1.00000, avg. loss over tasks: 0.00961, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.13179, Variance: 0.01273
Semantic Loss - Mean: 0.04715, Variance: 0.01140

Test Epoch: 85 
task: sign, mean loss: 1.16785, accuracy: 0.78698, avg. loss over tasks: 1.16785
Diversity Loss - Mean: -0.12944, Variance: 0.01440
Semantic Loss - Mean: 1.28606, Variance: 0.05781

Train Epoch: 86 
task: sign, mean loss: 0.00700, accuracy: 1.00000, avg. loss over tasks: 0.00700, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.13138, Variance: 0.01274
Semantic Loss - Mean: 0.05637, Variance: 0.01140

Test Epoch: 86 
task: sign, mean loss: 0.74394, accuracy: 0.81657, avg. loss over tasks: 0.74394
Diversity Loss - Mean: -0.13266, Variance: 0.01441
Semantic Loss - Mean: 0.71915, Variance: 0.05788

Train Epoch: 87 
task: sign, mean loss: 0.00356, accuracy: 1.00000, avg. loss over tasks: 0.00356, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.13124, Variance: 0.01275
Semantic Loss - Mean: 0.02606, Variance: 0.01129

Test Epoch: 87 
task: sign, mean loss: 0.66465, accuracy: 0.82840, avg. loss over tasks: 0.66465
Diversity Loss - Mean: -0.13337, Variance: 0.01442
Semantic Loss - Mean: 0.59448, Variance: 0.05804

Train Epoch: 88 
task: sign, mean loss: 0.00462, accuracy: 1.00000, avg. loss over tasks: 0.00462, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.13181, Variance: 0.01277
Semantic Loss - Mean: 0.04016, Variance: 0.01121

Test Epoch: 88 
task: sign, mean loss: 0.78746, accuracy: 0.81657, avg. loss over tasks: 0.78746
Diversity Loss - Mean: -0.13292, Variance: 0.01442
Semantic Loss - Mean: 0.76365, Variance: 0.05843

Train Epoch: 89 
task: sign, mean loss: 0.00531, accuracy: 1.00000, avg. loss over tasks: 0.00531, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.13202, Variance: 0.01279
Semantic Loss - Mean: 0.03344, Variance: 0.01111

Test Epoch: 89 
task: sign, mean loss: 0.81766, accuracy: 0.81065, avg. loss over tasks: 0.81766
Diversity Loss - Mean: -0.13279, Variance: 0.01442
Semantic Loss - Mean: 0.87574, Variance: 0.05917

Train Epoch: 90 
task: sign, mean loss: 0.00240, accuracy: 1.00000, avg. loss over tasks: 0.00240, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.13282, Variance: 0.01280
Semantic Loss - Mean: 0.01910, Variance: 0.01100

Test Epoch: 90 
task: sign, mean loss: 0.78916, accuracy: 0.85207, avg. loss over tasks: 0.78916
Diversity Loss - Mean: -0.13312, Variance: 0.01442
Semantic Loss - Mean: 0.82051, Variance: 0.05924

Train Epoch: 91 
task: sign, mean loss: 0.02152, accuracy: 0.99457, avg. loss over tasks: 0.02152, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.13240, Variance: 0.01282
Semantic Loss - Mean: 0.04120, Variance: 0.01093

Test Epoch: 91 
task: sign, mean loss: 0.53278, accuracy: 0.86982, avg. loss over tasks: 0.53278
Diversity Loss - Mean: -0.13437, Variance: 0.01443
Semantic Loss - Mean: 0.54281, Variance: 0.05889

Train Epoch: 92 
task: sign, mean loss: 0.00274, accuracy: 1.00000, avg. loss over tasks: 0.00274, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.13259, Variance: 0.01282
Semantic Loss - Mean: 0.02225, Variance: 0.01084

Test Epoch: 92 
task: sign, mean loss: 0.63009, accuracy: 0.85799, avg. loss over tasks: 0.63009
Diversity Loss - Mean: -0.13418, Variance: 0.01444
Semantic Loss - Mean: 0.65369, Variance: 0.05878

Train Epoch: 93 
task: sign, mean loss: 0.00192, accuracy: 1.00000, avg. loss over tasks: 0.00192, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.13263, Variance: 0.01283
Semantic Loss - Mean: 0.01712, Variance: 0.01073

Test Epoch: 93 
task: sign, mean loss: 0.55569, accuracy: 0.87574, avg. loss over tasks: 0.55569
Diversity Loss - Mean: -0.13511, Variance: 0.01445
Semantic Loss - Mean: 0.57263, Variance: 0.05865

Train Epoch: 94 
task: sign, mean loss: 0.00095, accuracy: 1.00000, avg. loss over tasks: 0.00095, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.13336, Variance: 0.01285
Semantic Loss - Mean: 0.01250, Variance: 0.01063

Test Epoch: 94 
task: sign, mean loss: 0.61553, accuracy: 0.85799, avg. loss over tasks: 0.61553
Diversity Loss - Mean: -0.13532, Variance: 0.01446
Semantic Loss - Mean: 0.64112, Variance: 0.05863

Train Epoch: 95 
task: sign, mean loss: 0.00082, accuracy: 1.00000, avg. loss over tasks: 0.00082, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.13364, Variance: 0.01286
Semantic Loss - Mean: 0.01617, Variance: 0.01053

Test Epoch: 95 
task: sign, mean loss: 0.68621, accuracy: 0.85207, avg. loss over tasks: 0.68621
Diversity Loss - Mean: -0.13493, Variance: 0.01446
Semantic Loss - Mean: 0.73864, Variance: 0.05860

Train Epoch: 96 
task: sign, mean loss: 0.00694, accuracy: 0.99457, avg. loss over tasks: 0.00694, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.13385, Variance: 0.01287
Semantic Loss - Mean: 0.02819, Variance: 0.01046

Test Epoch: 96 
task: sign, mean loss: 0.66205, accuracy: 0.84615, avg. loss over tasks: 0.66205
Diversity Loss - Mean: -0.13476, Variance: 0.01447
Semantic Loss - Mean: 0.73446, Variance: 0.05827

Train Epoch: 97 
task: sign, mean loss: 0.00293, accuracy: 1.00000, avg. loss over tasks: 0.00293, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.13372, Variance: 0.01288
Semantic Loss - Mean: 0.02347, Variance: 0.01037

Test Epoch: 97 
task: sign, mean loss: 0.79028, accuracy: 0.82249, avg. loss over tasks: 0.79028
Diversity Loss - Mean: -0.13568, Variance: 0.01448
Semantic Loss - Mean: 0.80947, Variance: 0.05823

Train Epoch: 98 
task: sign, mean loss: 0.00342, accuracy: 1.00000, avg. loss over tasks: 0.00342, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.13460, Variance: 0.01290
Semantic Loss - Mean: 0.02065, Variance: 0.01029

Test Epoch: 98 
task: sign, mean loss: 0.72906, accuracy: 0.82840, avg. loss over tasks: 0.72906
Diversity Loss - Mean: -0.13641, Variance: 0.01448
Semantic Loss - Mean: 0.71528, Variance: 0.05808

Train Epoch: 99 
task: sign, mean loss: 0.00306, accuracy: 1.00000, avg. loss over tasks: 0.00306, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.13447, Variance: 0.01292
Semantic Loss - Mean: 0.02121, Variance: 0.01020

Test Epoch: 99 
task: sign, mean loss: 0.61909, accuracy: 0.86391, avg. loss over tasks: 0.61909
Diversity Loss - Mean: -0.13638, Variance: 0.01449
Semantic Loss - Mean: 0.62967, Variance: 0.05784

Train Epoch: 100 
task: sign, mean loss: 0.02501, accuracy: 0.98913, avg. loss over tasks: 0.02501, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.13428, Variance: 0.01293
Semantic Loss - Mean: 0.04705, Variance: 0.01017

Test Epoch: 100 
task: sign, mean loss: 0.54900, accuracy: 0.88166, avg. loss over tasks: 0.54900
Diversity Loss - Mean: -0.13638, Variance: 0.01450
Semantic Loss - Mean: 0.54545, Variance: 0.05768

Train Epoch: 101 
task: sign, mean loss: 0.00650, accuracy: 0.99457, avg. loss over tasks: 0.00650, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.13513, Variance: 0.01294
Semantic Loss - Mean: 0.02881, Variance: 0.01013

Test Epoch: 101 
task: sign, mean loss: 0.52065, accuracy: 0.89941, avg. loss over tasks: 0.52065
Diversity Loss - Mean: -0.13685, Variance: 0.01451
Semantic Loss - Mean: 0.51766, Variance: 0.05742

Train Epoch: 102 
task: sign, mean loss: 0.00641, accuracy: 0.99457, avg. loss over tasks: 0.00641, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.13456, Variance: 0.01295
Semantic Loss - Mean: 0.03720, Variance: 0.01012

Test Epoch: 102 
task: sign, mean loss: 0.86678, accuracy: 0.80473, avg. loss over tasks: 0.86678
Diversity Loss - Mean: -0.13602, Variance: 0.01450
Semantic Loss - Mean: 0.89830, Variance: 0.05736

Train Epoch: 103 
task: sign, mean loss: 0.00217, accuracy: 1.00000, avg. loss over tasks: 0.00217, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.13494, Variance: 0.01296
Semantic Loss - Mean: 0.01882, Variance: 0.01003

Test Epoch: 103 
task: sign, mean loss: 0.92477, accuracy: 0.80473, avg. loss over tasks: 0.92477
Diversity Loss - Mean: -0.13626, Variance: 0.01450
Semantic Loss - Mean: 0.92383, Variance: 0.05735

Train Epoch: 104 
task: sign, mean loss: 0.00163, accuracy: 1.00000, avg. loss over tasks: 0.00163, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.13528, Variance: 0.01296
Semantic Loss - Mean: 0.02405, Variance: 0.00996

Test Epoch: 104 
task: sign, mean loss: 0.73267, accuracy: 0.83432, avg. loss over tasks: 0.73267
Diversity Loss - Mean: -0.13673, Variance: 0.01451
Semantic Loss - Mean: 0.70951, Variance: 0.05704

Train Epoch: 105 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.13494, Variance: 0.01297
Semantic Loss - Mean: 0.01487, Variance: 0.00988

Test Epoch: 105 
task: sign, mean loss: 0.75814, accuracy: 0.83432, avg. loss over tasks: 0.75814
Diversity Loss - Mean: -0.13736, Variance: 0.01452
Semantic Loss - Mean: 0.69267, Variance: 0.05668

Train Epoch: 106 
task: sign, mean loss: 0.00090, accuracy: 1.00000, avg. loss over tasks: 0.00090, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.13573, Variance: 0.01299
Semantic Loss - Mean: 0.01656, Variance: 0.00980

Test Epoch: 106 
task: sign, mean loss: 0.70466, accuracy: 0.83432, avg. loss over tasks: 0.70466
Diversity Loss - Mean: -0.13682, Variance: 0.01452
Semantic Loss - Mean: 0.70575, Variance: 0.05629

Train Epoch: 107 
task: sign, mean loss: 0.00173, accuracy: 1.00000, avg. loss over tasks: 0.00173, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.13562, Variance: 0.01300
Semantic Loss - Mean: 0.01020, Variance: 0.00972

Test Epoch: 107 
task: sign, mean loss: 0.78276, accuracy: 0.81657, avg. loss over tasks: 0.78276
Diversity Loss - Mean: -0.13693, Variance: 0.01452
Semantic Loss - Mean: 0.77052, Variance: 0.05588

Train Epoch: 108 
task: sign, mean loss: 0.00124, accuracy: 1.00000, avg. loss over tasks: 0.00124, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.13553, Variance: 0.01301
Semantic Loss - Mean: 0.01640, Variance: 0.00964

Test Epoch: 108 
task: sign, mean loss: 0.70651, accuracy: 0.83432, avg. loss over tasks: 0.70651
Diversity Loss - Mean: -0.13729, Variance: 0.01453
Semantic Loss - Mean: 0.68888, Variance: 0.05546

Train Epoch: 109 
task: sign, mean loss: 0.00864, accuracy: 0.99457, avg. loss over tasks: 0.00864, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.13513, Variance: 0.01302
Semantic Loss - Mean: 0.03020, Variance: 0.00959

Test Epoch: 109 
task: sign, mean loss: 0.69246, accuracy: 0.82249, avg. loss over tasks: 0.69246
Diversity Loss - Mean: -0.13721, Variance: 0.01453
Semantic Loss - Mean: 0.68816, Variance: 0.05504

Train Epoch: 110 
task: sign, mean loss: 0.00125, accuracy: 1.00000, avg. loss over tasks: 0.00125, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.13612, Variance: 0.01303
Semantic Loss - Mean: 0.00981, Variance: 0.00951

Test Epoch: 110 
task: sign, mean loss: 0.64686, accuracy: 0.83432, avg. loss over tasks: 0.64686
Diversity Loss - Mean: -0.13705, Variance: 0.01454
Semantic Loss - Mean: 0.67190, Variance: 0.05468

Train Epoch: 111 
task: sign, mean loss: 0.00412, accuracy: 1.00000, avg. loss over tasks: 0.00412, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.13565, Variance: 0.01305
Semantic Loss - Mean: 0.02993, Variance: 0.00944

Test Epoch: 111 
task: sign, mean loss: 0.61922, accuracy: 0.85207, avg. loss over tasks: 0.61922
Diversity Loss - Mean: -0.13742, Variance: 0.01455
Semantic Loss - Mean: 0.63234, Variance: 0.05438

Train Epoch: 112 
task: sign, mean loss: 0.00133, accuracy: 1.00000, avg. loss over tasks: 0.00133, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.13581, Variance: 0.01306
Semantic Loss - Mean: 0.02328, Variance: 0.00938

Test Epoch: 112 
task: sign, mean loss: 0.62067, accuracy: 0.85207, avg. loss over tasks: 0.62067
Diversity Loss - Mean: -0.13761, Variance: 0.01455
Semantic Loss - Mean: 0.62461, Variance: 0.05415

Train Epoch: 113 
task: sign, mean loss: 0.00435, accuracy: 1.00000, avg. loss over tasks: 0.00435, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.13646, Variance: 0.01307
Semantic Loss - Mean: 0.01906, Variance: 0.00931

Test Epoch: 113 
task: sign, mean loss: 0.55443, accuracy: 0.86391, avg. loss over tasks: 0.55443
Diversity Loss - Mean: -0.13783, Variance: 0.01457
Semantic Loss - Mean: 0.55722, Variance: 0.05392

Train Epoch: 114 
task: sign, mean loss: 0.00067, accuracy: 1.00000, avg. loss over tasks: 0.00067, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.13550, Variance: 0.01308
Semantic Loss - Mean: 0.02068, Variance: 0.00926

Test Epoch: 114 
task: sign, mean loss: 0.55991, accuracy: 0.88166, avg. loss over tasks: 0.55991
Diversity Loss - Mean: -0.13790, Variance: 0.01458
Semantic Loss - Mean: 0.54878, Variance: 0.05367

Train Epoch: 115 
task: sign, mean loss: 0.00074, accuracy: 1.00000, avg. loss over tasks: 0.00074, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.13558, Variance: 0.01309
Semantic Loss - Mean: 0.02013, Variance: 0.00920

Test Epoch: 115 
task: sign, mean loss: 0.66479, accuracy: 0.85799, avg. loss over tasks: 0.66479
Diversity Loss - Mean: -0.13777, Variance: 0.01459
Semantic Loss - Mean: 0.65613, Variance: 0.05350

Train Epoch: 116 
task: sign, mean loss: 0.00088, accuracy: 1.00000, avg. loss over tasks: 0.00088, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.13574, Variance: 0.01311
Semantic Loss - Mean: 0.01275, Variance: 0.00912

Test Epoch: 116 
task: sign, mean loss: 0.52181, accuracy: 0.88166, avg. loss over tasks: 0.52181
Diversity Loss - Mean: -0.13783, Variance: 0.01460
Semantic Loss - Mean: 0.53060, Variance: 0.05324

Train Epoch: 117 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.13596, Variance: 0.01312
Semantic Loss - Mean: 0.01514, Variance: 0.00905

Test Epoch: 117 
task: sign, mean loss: 0.53220, accuracy: 0.88166, avg. loss over tasks: 0.53220
Diversity Loss - Mean: -0.13793, Variance: 0.01461
Semantic Loss - Mean: 0.53279, Variance: 0.05298

Train Epoch: 118 
task: sign, mean loss: 0.00307, accuracy: 1.00000, avg. loss over tasks: 0.00307, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.13623, Variance: 0.01313
Semantic Loss - Mean: 0.01692, Variance: 0.00899

Test Epoch: 118 
task: sign, mean loss: 0.54320, accuracy: 0.88166, avg. loss over tasks: 0.54320
Diversity Loss - Mean: -0.13791, Variance: 0.01461
Semantic Loss - Mean: 0.54488, Variance: 0.05270

Train Epoch: 119 
task: sign, mean loss: 0.00120, accuracy: 1.00000, avg. loss over tasks: 0.00120, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.13633, Variance: 0.01314
Semantic Loss - Mean: 0.01867, Variance: 0.00893

Test Epoch: 119 
task: sign, mean loss: 0.51440, accuracy: 0.88166, avg. loss over tasks: 0.51440
Diversity Loss - Mean: -0.13812, Variance: 0.01462
Semantic Loss - Mean: 0.49914, Variance: 0.05241

Train Epoch: 120 
task: sign, mean loss: 0.00086, accuracy: 1.00000, avg. loss over tasks: 0.00086, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.13583, Variance: 0.01315
Semantic Loss - Mean: 0.01578, Variance: 0.00887

Test Epoch: 120 
task: sign, mean loss: 0.48774, accuracy: 0.89941, avg. loss over tasks: 0.48774
Diversity Loss - Mean: -0.13809, Variance: 0.01463
Semantic Loss - Mean: 0.48904, Variance: 0.05210

Train Epoch: 121 
task: sign, mean loss: 0.00127, accuracy: 1.00000, avg. loss over tasks: 0.00127, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.13665, Variance: 0.01316
Semantic Loss - Mean: 0.01554, Variance: 0.00880

Test Epoch: 121 
task: sign, mean loss: 0.50316, accuracy: 0.88757, avg. loss over tasks: 0.50316
Diversity Loss - Mean: -0.13781, Variance: 0.01464
Semantic Loss - Mean: 0.51254, Variance: 0.05179

Train Epoch: 122 
task: sign, mean loss: 0.00190, accuracy: 1.00000, avg. loss over tasks: 0.00190, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.13658, Variance: 0.01318
Semantic Loss - Mean: 0.02252, Variance: 0.00875

Test Epoch: 122 
task: sign, mean loss: 0.51826, accuracy: 0.88757, avg. loss over tasks: 0.51826
Diversity Loss - Mean: -0.13797, Variance: 0.01465
Semantic Loss - Mean: 0.53369, Variance: 0.05150

Train Epoch: 123 
task: sign, mean loss: 0.00068, accuracy: 1.00000, avg. loss over tasks: 0.00068, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.13683, Variance: 0.01319
Semantic Loss - Mean: 0.01428, Variance: 0.00869

Test Epoch: 123 
task: sign, mean loss: 0.55434, accuracy: 0.87574, avg. loss over tasks: 0.55434
Diversity Loss - Mean: -0.13770, Variance: 0.01466
Semantic Loss - Mean: 0.57346, Variance: 0.05125

Train Epoch: 124 
task: sign, mean loss: 0.00376, accuracy: 1.00000, avg. loss over tasks: 0.00376, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.13663, Variance: 0.01320
Semantic Loss - Mean: 0.01170, Variance: 0.00862

Test Epoch: 124 
task: sign, mean loss: 0.46885, accuracy: 0.89941, avg. loss over tasks: 0.46885
Diversity Loss - Mean: -0.13786, Variance: 0.01467
Semantic Loss - Mean: 0.48214, Variance: 0.05093

Train Epoch: 125 
task: sign, mean loss: 0.00069, accuracy: 1.00000, avg. loss over tasks: 0.00069, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.13669, Variance: 0.01322
Semantic Loss - Mean: 0.01061, Variance: 0.00856

Test Epoch: 125 
task: sign, mean loss: 0.52661, accuracy: 0.88757, avg. loss over tasks: 0.52661
Diversity Loss - Mean: -0.13788, Variance: 0.01467
Semantic Loss - Mean: 0.53158, Variance: 0.05064

Train Epoch: 126 
task: sign, mean loss: 0.00150, accuracy: 1.00000, avg. loss over tasks: 0.00150, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.13704, Variance: 0.01323
Semantic Loss - Mean: 0.01834, Variance: 0.00852

Test Epoch: 126 
task: sign, mean loss: 0.48116, accuracy: 0.89349, avg. loss over tasks: 0.48116
Diversity Loss - Mean: -0.13823, Variance: 0.01469
Semantic Loss - Mean: 0.45816, Variance: 0.05032

Train Epoch: 127 
task: sign, mean loss: 0.01655, accuracy: 0.99457, avg. loss over tasks: 0.01655, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.13720, Variance: 0.01323
Semantic Loss - Mean: 0.02497, Variance: 0.00846

Test Epoch: 127 
task: sign, mean loss: 0.56852, accuracy: 0.87574, avg. loss over tasks: 0.56852
Diversity Loss - Mean: -0.13802, Variance: 0.01469
Semantic Loss - Mean: 0.54207, Variance: 0.05004

Train Epoch: 128 
task: sign, mean loss: 0.01220, accuracy: 0.99457, avg. loss over tasks: 0.01220, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.13713, Variance: 0.01325
Semantic Loss - Mean: 0.02436, Variance: 0.00840

Test Epoch: 128 
task: sign, mean loss: 0.85393, accuracy: 0.83432, avg. loss over tasks: 0.85393
Diversity Loss - Mean: -0.13742, Variance: 0.01470
Semantic Loss - Mean: 0.83366, Variance: 0.04979

Train Epoch: 129 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.13719, Variance: 0.01326
Semantic Loss - Mean: 0.00970, Variance: 0.00834

Test Epoch: 129 
task: sign, mean loss: 0.70492, accuracy: 0.84024, avg. loss over tasks: 0.70492
Diversity Loss - Mean: -0.13771, Variance: 0.01471
Semantic Loss - Mean: 0.69831, Variance: 0.04956

Train Epoch: 130 
task: sign, mean loss: 0.00139, accuracy: 1.00000, avg. loss over tasks: 0.00139, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.13716, Variance: 0.01327
Semantic Loss - Mean: 0.01608, Variance: 0.00829

Test Epoch: 130 
task: sign, mean loss: 0.71088, accuracy: 0.83432, avg. loss over tasks: 0.71088
Diversity Loss - Mean: -0.13787, Variance: 0.01471
Semantic Loss - Mean: 0.68563, Variance: 0.04933

Train Epoch: 131 
task: sign, mean loss: 0.00467, accuracy: 1.00000, avg. loss over tasks: 0.00467, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.13685, Variance: 0.01328
Semantic Loss - Mean: 0.02259, Variance: 0.00824

Test Epoch: 131 
task: sign, mean loss: 0.78880, accuracy: 0.83432, avg. loss over tasks: 0.78880
Diversity Loss - Mean: -0.13784, Variance: 0.01472
Semantic Loss - Mean: 0.74422, Variance: 0.04910

Train Epoch: 132 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.13727, Variance: 0.01329
Semantic Loss - Mean: 0.01102, Variance: 0.00818

Test Epoch: 132 
task: sign, mean loss: 0.67276, accuracy: 0.84615, avg. loss over tasks: 0.67276
Diversity Loss - Mean: -0.13794, Variance: 0.01472
Semantic Loss - Mean: 0.63517, Variance: 0.04885

Train Epoch: 133 
task: sign, mean loss: 0.00059, accuracy: 1.00000, avg. loss over tasks: 0.00059, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.13703, Variance: 0.01331
Semantic Loss - Mean: 0.01412, Variance: 0.00813

Test Epoch: 133 
task: sign, mean loss: 0.68936, accuracy: 0.84024, avg. loss over tasks: 0.68936
Diversity Loss - Mean: -0.13805, Variance: 0.01473
Semantic Loss - Mean: 0.63751, Variance: 0.04859

Train Epoch: 134 
task: sign, mean loss: 0.00049, accuracy: 1.00000, avg. loss over tasks: 0.00049, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13764, Variance: 0.01332
Semantic Loss - Mean: 0.00511, Variance: 0.00807

Test Epoch: 134 
task: sign, mean loss: 0.66009, accuracy: 0.84615, avg. loss over tasks: 0.66009
Diversity Loss - Mean: -0.13795, Variance: 0.01473
Semantic Loss - Mean: 0.62129, Variance: 0.04834

Train Epoch: 135 
task: sign, mean loss: 0.00054, accuracy: 1.00000, avg. loss over tasks: 0.00054, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.13697, Variance: 0.01333
Semantic Loss - Mean: 0.01394, Variance: 0.00803

Test Epoch: 135 
task: sign, mean loss: 0.70564, accuracy: 0.84024, avg. loss over tasks: 0.70564
Diversity Loss - Mean: -0.13813, Variance: 0.01473
Semantic Loss - Mean: 0.65200, Variance: 0.04810

Train Epoch: 136 
task: sign, mean loss: 0.00033, accuracy: 1.00000, avg. loss over tasks: 0.00033, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13695, Variance: 0.01334
Semantic Loss - Mean: 0.01120, Variance: 0.00798

Test Epoch: 136 
task: sign, mean loss: 0.64588, accuracy: 0.85207, avg. loss over tasks: 0.64588
Diversity Loss - Mean: -0.13820, Variance: 0.01474
Semantic Loss - Mean: 0.59689, Variance: 0.04786

Train Epoch: 137 
task: sign, mean loss: 0.00196, accuracy: 1.00000, avg. loss over tasks: 0.00196, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13675, Variance: 0.01335
Semantic Loss - Mean: 0.02568, Variance: 0.00794

Test Epoch: 137 
task: sign, mean loss: 0.62924, accuracy: 0.85799, avg. loss over tasks: 0.62924
Diversity Loss - Mean: -0.13813, Variance: 0.01475
Semantic Loss - Mean: 0.58534, Variance: 0.04763

Train Epoch: 138 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13689, Variance: 0.01336
Semantic Loss - Mean: 0.01570, Variance: 0.00791

Test Epoch: 138 
task: sign, mean loss: 0.64583, accuracy: 0.84615, avg. loss over tasks: 0.64583
Diversity Loss - Mean: -0.13803, Variance: 0.01475
Semantic Loss - Mean: 0.61175, Variance: 0.04740

Train Epoch: 139 
task: sign, mean loss: 0.00051, accuracy: 1.00000, avg. loss over tasks: 0.00051, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13700, Variance: 0.01337
Semantic Loss - Mean: 0.01923, Variance: 0.00786

Test Epoch: 139 
task: sign, mean loss: 0.65053, accuracy: 0.85799, avg. loss over tasks: 0.65053
Diversity Loss - Mean: -0.13823, Variance: 0.01476
Semantic Loss - Mean: 0.60016, Variance: 0.04717

Train Epoch: 140 
task: sign, mean loss: 0.00201, accuracy: 1.00000, avg. loss over tasks: 0.00201, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.13712, Variance: 0.01338
Semantic Loss - Mean: 0.02792, Variance: 0.00783

Test Epoch: 140 
task: sign, mean loss: 0.62692, accuracy: 0.85799, avg. loss over tasks: 0.62692
Diversity Loss - Mean: -0.13838, Variance: 0.01477
Semantic Loss - Mean: 0.57577, Variance: 0.04693

Train Epoch: 141 
task: sign, mean loss: 0.00074, accuracy: 1.00000, avg. loss over tasks: 0.00074, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.13683, Variance: 0.01339
Semantic Loss - Mean: 0.01560, Variance: 0.00779

Test Epoch: 141 
task: sign, mean loss: 0.58860, accuracy: 0.87574, avg. loss over tasks: 0.58860
Diversity Loss - Mean: -0.13824, Variance: 0.01477
Semantic Loss - Mean: 0.54277, Variance: 0.04668

Train Epoch: 142 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.13527, Variance: 0.01340
Semantic Loss - Mean: 0.01734, Variance: 0.00774

Test Epoch: 142 
task: sign, mean loss: 0.58076, accuracy: 0.86982, avg. loss over tasks: 0.58076
Diversity Loss - Mean: -0.13829, Variance: 0.01478
Semantic Loss - Mean: 0.54063, Variance: 0.04644

Train Epoch: 143 
task: sign, mean loss: 0.00636, accuracy: 1.00000, avg. loss over tasks: 0.00636, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13764, Variance: 0.01341
Semantic Loss - Mean: 0.02754, Variance: 0.00770

Test Epoch: 143 
task: sign, mean loss: 0.61353, accuracy: 0.85799, avg. loss over tasks: 0.61353
Diversity Loss - Mean: -0.13815, Variance: 0.01479
Semantic Loss - Mean: 0.58860, Variance: 0.04623

Train Epoch: 144 
task: sign, mean loss: 0.00041, accuracy: 1.00000, avg. loss over tasks: 0.00041, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.13722, Variance: 0.01342
Semantic Loss - Mean: 0.01001, Variance: 0.00765

Test Epoch: 144 
task: sign, mean loss: 0.60385, accuracy: 0.85799, avg. loss over tasks: 0.60385
Diversity Loss - Mean: -0.13823, Variance: 0.01480
Semantic Loss - Mean: 0.56456, Variance: 0.04601

Train Epoch: 145 
task: sign, mean loss: 0.00074, accuracy: 1.00000, avg. loss over tasks: 0.00074, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13680, Variance: 0.01343
Semantic Loss - Mean: 0.01921, Variance: 0.00762

Test Epoch: 145 
task: sign, mean loss: 0.64178, accuracy: 0.85799, avg. loss over tasks: 0.64178
Diversity Loss - Mean: -0.13818, Variance: 0.01480
Semantic Loss - Mean: 0.61377, Variance: 0.04580

Train Epoch: 146 
task: sign, mean loss: 0.00078, accuracy: 1.00000, avg. loss over tasks: 0.00078, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.13713, Variance: 0.01344
Semantic Loss - Mean: 0.01461, Variance: 0.00757

Test Epoch: 146 
task: sign, mean loss: 0.77854, accuracy: 0.82840, avg. loss over tasks: 0.77854
Diversity Loss - Mean: -0.13810, Variance: 0.01481
Semantic Loss - Mean: 0.73596, Variance: 0.04563

Train Epoch: 147 
task: sign, mean loss: 0.00052, accuracy: 1.00000, avg. loss over tasks: 0.00052, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13704, Variance: 0.01345
Semantic Loss - Mean: 0.01447, Variance: 0.00753

Test Epoch: 147 
task: sign, mean loss: 0.67144, accuracy: 0.84615, avg. loss over tasks: 0.67144
Diversity Loss - Mean: -0.13802, Variance: 0.01481
Semantic Loss - Mean: 0.65208, Variance: 0.04544

Train Epoch: 148 
task: sign, mean loss: 0.00062, accuracy: 1.00000, avg. loss over tasks: 0.00062, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.13692, Variance: 0.01346
Semantic Loss - Mean: 0.00651, Variance: 0.00748

Test Epoch: 148 
task: sign, mean loss: 0.68735, accuracy: 0.83432, avg. loss over tasks: 0.68735
Diversity Loss - Mean: -0.13799, Variance: 0.01481
Semantic Loss - Mean: 0.66379, Variance: 0.04526

Train Epoch: 149 
task: sign, mean loss: 0.00046, accuracy: 1.00000, avg. loss over tasks: 0.00046, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.13606, Variance: 0.01347
Semantic Loss - Mean: 0.02336, Variance: 0.00745

Test Epoch: 149 
task: sign, mean loss: 0.61828, accuracy: 0.85799, avg. loss over tasks: 0.61828
Diversity Loss - Mean: -0.13820, Variance: 0.01482
Semantic Loss - Mean: 0.58603, Variance: 0.04505

Train Epoch: 150 
task: sign, mean loss: 0.00045, accuracy: 1.00000, avg. loss over tasks: 0.00045, lr: 3e-07
Diversity Loss - Mean: -0.13749, Variance: 0.01348
Semantic Loss - Mean: 0.00898, Variance: 0.00741

Test Epoch: 150 
task: sign, mean loss: 0.64618, accuracy: 0.86982, avg. loss over tasks: 0.64618
Diversity Loss - Mean: -0.13817, Variance: 0.01482
Semantic Loss - Mean: 0.60256, Variance: 0.04484

