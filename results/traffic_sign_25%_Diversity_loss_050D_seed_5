Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09356, accuracy: 0.63587, avg. loss over tasks: 1.09356, lr: 3e-05
Diversity Loss - Mean: -0.01049, Variance: 0.01052
Semantic Loss - Mean: 1.43123, Variance: 0.07243

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17692, accuracy: 0.66272, avg. loss over tasks: 1.17692
Diversity Loss - Mean: -0.03187, Variance: 0.01262
Semantic Loss - Mean: 1.16224, Variance: 0.05309

Train Epoch: 2 
task: sign, mean loss: 0.97081, accuracy: 0.65761, avg. loss over tasks: 0.97081, lr: 6e-05
Diversity Loss - Mean: -0.02259, Variance: 0.01052
Semantic Loss - Mean: 0.98286, Variance: 0.03919

Test Epoch: 2 
task: sign, mean loss: 1.12945, accuracy: 0.65680, avg. loss over tasks: 1.12945
Diversity Loss - Mean: -0.03752, Variance: 0.01228
Semantic Loss - Mean: 1.14910, Variance: 0.03268

Train Epoch: 3 
task: sign, mean loss: 0.80426, accuracy: 0.70109, avg. loss over tasks: 0.80426, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.04737, Variance: 0.01042
Semantic Loss - Mean: 0.99636, Variance: 0.02713

Test Epoch: 3 
task: sign, mean loss: 1.27790, accuracy: 0.52663, avg. loss over tasks: 1.27790
Diversity Loss - Mean: -0.06672, Variance: 0.01163
Semantic Loss - Mean: 1.10945, Variance: 0.02936

Train Epoch: 4 
task: sign, mean loss: 0.74296, accuracy: 0.70652, avg. loss over tasks: 0.74296, lr: 0.00012
Diversity Loss - Mean: -0.07426, Variance: 0.01026
Semantic Loss - Mean: 0.89246, Variance: 0.02097

Test Epoch: 4 
task: sign, mean loss: 1.59547, accuracy: 0.40828, avg. loss over tasks: 1.59547
Diversity Loss - Mean: -0.07209, Variance: 0.01109
Semantic Loss - Mean: 1.10795, Variance: 0.02478

Train Epoch: 5 
task: sign, mean loss: 0.73171, accuracy: 0.69022, avg. loss over tasks: 0.73171, lr: 0.00015
Diversity Loss - Mean: -0.06879, Variance: 0.00999
Semantic Loss - Mean: 0.78297, Variance: 0.01720

Test Epoch: 5 
task: sign, mean loss: 1.98248, accuracy: 0.39053, avg. loss over tasks: 1.98248
Diversity Loss - Mean: -0.07244, Variance: 0.01065
Semantic Loss - Mean: 1.27407, Variance: 0.02351

Train Epoch: 6 
task: sign, mean loss: 0.66354, accuracy: 0.77717, avg. loss over tasks: 0.66354, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.06404, Variance: 0.00988
Semantic Loss - Mean: 0.70576, Variance: 0.01475

Test Epoch: 6 
task: sign, mean loss: 2.37360, accuracy: 0.66272, avg. loss over tasks: 2.37360
Diversity Loss - Mean: -0.03011, Variance: 0.01117
Semantic Loss - Mean: 1.57739, Variance: 0.02336

Train Epoch: 7 
task: sign, mean loss: 0.62843, accuracy: 0.74457, avg. loss over tasks: 0.62843, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.07672, Variance: 0.01009
Semantic Loss - Mean: 0.66494, Variance: 0.01297

Test Epoch: 7 
task: sign, mean loss: 1.68418, accuracy: 0.67456, avg. loss over tasks: 1.68418
Diversity Loss - Mean: -0.06313, Variance: 0.01154
Semantic Loss - Mean: 1.23405, Variance: 0.02288

Train Epoch: 8 
task: sign, mean loss: 0.48160, accuracy: 0.82065, avg. loss over tasks: 0.48160, lr: 0.00024
Diversity Loss - Mean: -0.04294, Variance: 0.01004
Semantic Loss - Mean: 0.57439, Variance: 0.01187

Test Epoch: 8 
task: sign, mean loss: 1.97812, accuracy: 0.44970, avg. loss over tasks: 1.97812
Diversity Loss - Mean: -0.04363, Variance: 0.01154
Semantic Loss - Mean: 1.36524, Variance: 0.02344

Train Epoch: 9 
task: sign, mean loss: 0.96264, accuracy: 0.73913, avg. loss over tasks: 0.96264, lr: 0.00027
Diversity Loss - Mean: -0.03915, Variance: 0.01005
Semantic Loss - Mean: 0.77916, Variance: 0.01128

Test Epoch: 9 
task: sign, mean loss: 2.16007, accuracy: 0.25444, avg. loss over tasks: 2.16007
Diversity Loss - Mean: -0.00294, Variance: 0.01199
Semantic Loss - Mean: 1.76540, Variance: 0.02447

Train Epoch: 10 
task: sign, mean loss: 0.78272, accuracy: 0.70109, avg. loss over tasks: 0.78272, lr: 0.0003
Diversity Loss - Mean: -0.06419, Variance: 0.01016
Semantic Loss - Mean: 0.74930, Variance: 0.01059

Test Epoch: 10 
task: sign, mean loss: 2.68899, accuracy: 0.41420, avg. loss over tasks: 2.68899
Diversity Loss - Mean: -0.00961, Variance: 0.01193
Semantic Loss - Mean: 2.21873, Variance: 0.02603

Train Epoch: 11 
task: sign, mean loss: 0.58960, accuracy: 0.76087, avg. loss over tasks: 0.58960, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.04965, Variance: 0.01017
Semantic Loss - Mean: 0.59726, Variance: 0.00986

Test Epoch: 11 
task: sign, mean loss: 1.97088, accuracy: 0.52663, avg. loss over tasks: 1.97088
Diversity Loss - Mean: -0.02458, Variance: 0.01197
Semantic Loss - Mean: 1.59263, Variance: 0.02561

Train Epoch: 12 
task: sign, mean loss: 0.39147, accuracy: 0.85326, avg. loss over tasks: 0.39147, lr: 0.000299849111021216
Diversity Loss - Mean: -0.04227, Variance: 0.01018
Semantic Loss - Mean: 0.44342, Variance: 0.00946

Test Epoch: 12 
task: sign, mean loss: 3.29736, accuracy: 0.40237, avg. loss over tasks: 3.29736
Diversity Loss - Mean: 0.01998, Variance: 0.01189
Semantic Loss - Mean: 2.37662, Variance: 0.02825

Train Epoch: 13 
task: sign, mean loss: 0.51676, accuracy: 0.86413, avg. loss over tasks: 0.51676, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.04018, Variance: 0.01024
Semantic Loss - Mean: 0.54168, Variance: 0.00923

Test Epoch: 13 
task: sign, mean loss: 2.37139, accuracy: 0.66272, avg. loss over tasks: 2.37139
Diversity Loss - Mean: -0.05063, Variance: 0.01219
Semantic Loss - Mean: 1.96038, Variance: 0.02755

Train Epoch: 14 
task: sign, mean loss: 0.55078, accuracy: 0.80978, avg. loss over tasks: 0.55078, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.06953, Variance: 0.01042
Semantic Loss - Mean: 0.61016, Variance: 0.00898

Test Epoch: 14 
task: sign, mean loss: 2.57149, accuracy: 0.15976, avg. loss over tasks: 2.57149
Diversity Loss - Mean: -0.02879, Variance: 0.01224
Semantic Loss - Mean: 1.89686, Variance: 0.02978

Train Epoch: 15 
task: sign, mean loss: 0.43311, accuracy: 0.84783, avg. loss over tasks: 0.43311, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.07344, Variance: 0.01058
Semantic Loss - Mean: 0.46089, Variance: 0.00860

Test Epoch: 15 
task: sign, mean loss: 2.48985, accuracy: 0.37870, avg. loss over tasks: 2.48985
Diversity Loss - Mean: -0.02264, Variance: 0.01228
Semantic Loss - Mean: 1.98198, Variance: 0.03167

Train Epoch: 16 
task: sign, mean loss: 0.20416, accuracy: 0.92935, avg. loss over tasks: 0.20416, lr: 0.000298643821800925
Diversity Loss - Mean: -0.05761, Variance: 0.01062
Semantic Loss - Mean: 0.26757, Variance: 0.00828

Test Epoch: 16 
task: sign, mean loss: 1.32505, accuracy: 0.69822, avg. loss over tasks: 1.32505
Diversity Loss - Mean: -0.05684, Variance: 0.01240
Semantic Loss - Mean: 1.24539, Variance: 0.03038

Train Epoch: 17 
task: sign, mean loss: 0.16309, accuracy: 0.94565, avg. loss over tasks: 0.16309, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.04200, Variance: 0.01060
Semantic Loss - Mean: 0.22148, Variance: 0.00814

Test Epoch: 17 
task: sign, mean loss: 1.24923, accuracy: 0.74556, avg. loss over tasks: 1.24923
Diversity Loss - Mean: -0.05114, Variance: 0.01250
Semantic Loss - Mean: 1.05128, Variance: 0.02985

Train Epoch: 18 
task: sign, mean loss: 0.35494, accuracy: 0.89674, avg. loss over tasks: 0.35494, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.05385, Variance: 0.01061
Semantic Loss - Mean: 0.38014, Variance: 0.00826

Test Epoch: 18 
task: sign, mean loss: 2.19670, accuracy: 0.56805, avg. loss over tasks: 2.19670
Diversity Loss - Mean: -0.06339, Variance: 0.01255
Semantic Loss - Mean: 1.91253, Variance: 0.03049

Train Epoch: 19 
task: sign, mean loss: 0.23232, accuracy: 0.91848, avg. loss over tasks: 0.23232, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.06741, Variance: 0.01067
Semantic Loss - Mean: 0.27148, Variance: 0.00809

Test Epoch: 19 
task: sign, mean loss: 2.19072, accuracy: 0.42604, avg. loss over tasks: 2.19072
Diversity Loss - Mean: -0.03967, Variance: 0.01253
Semantic Loss - Mean: 1.73992, Variance: 0.03202

Train Epoch: 20 
task: sign, mean loss: 0.16461, accuracy: 0.93478, avg. loss over tasks: 0.16461, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.06227, Variance: 0.01069
Semantic Loss - Mean: 0.21715, Variance: 0.00807

Test Epoch: 20 
task: sign, mean loss: 2.07579, accuracy: 0.44379, avg. loss over tasks: 2.07579
Diversity Loss - Mean: -0.01915, Variance: 0.01246
Semantic Loss - Mean: 1.88028, Variance: 0.03368

Train Epoch: 21 
task: sign, mean loss: 0.10828, accuracy: 0.96739, avg. loss over tasks: 0.10828, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.05498, Variance: 0.01069
Semantic Loss - Mean: 0.15141, Variance: 0.00787

Test Epoch: 21 
task: sign, mean loss: 0.68364, accuracy: 0.76923, avg. loss over tasks: 0.68364
Diversity Loss - Mean: -0.04349, Variance: 0.01258
Semantic Loss - Mean: 0.61598, Variance: 0.03299

Train Epoch: 22 
task: sign, mean loss: 0.14709, accuracy: 0.94022, avg. loss over tasks: 0.14709, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.05533, Variance: 0.01067
Semantic Loss - Mean: 0.24123, Variance: 0.00828

Test Epoch: 22 
task: sign, mean loss: 0.73788, accuracy: 0.78107, avg. loss over tasks: 0.73788
Diversity Loss - Mean: -0.05902, Variance: 0.01270
Semantic Loss - Mean: 0.68547, Variance: 0.03307

Train Epoch: 23 
task: sign, mean loss: 0.13309, accuracy: 0.94565, avg. loss over tasks: 0.13309, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.06539, Variance: 0.01065
Semantic Loss - Mean: 0.20135, Variance: 0.00833

Test Epoch: 23 
task: sign, mean loss: 2.34398, accuracy: 0.58580, avg. loss over tasks: 2.34398
Diversity Loss - Mean: -0.04518, Variance: 0.01259
Semantic Loss - Mean: 2.04141, Variance: 0.03473

Train Epoch: 24 
task: sign, mean loss: 0.40939, accuracy: 0.84783, avg. loss over tasks: 0.40939, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.07346, Variance: 0.01065
Semantic Loss - Mean: 0.43602, Variance: 0.00852

Test Epoch: 24 
task: sign, mean loss: 2.50554, accuracy: 0.38462, avg. loss over tasks: 2.50554
Diversity Loss - Mean: -0.06615, Variance: 0.01258
Semantic Loss - Mean: 2.13824, Variance: 0.03573

Train Epoch: 25 
task: sign, mean loss: 0.42233, accuracy: 0.85326, avg. loss over tasks: 0.42233, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.08513, Variance: 0.01071
Semantic Loss - Mean: 0.44178, Variance: 0.00846

Test Epoch: 25 
task: sign, mean loss: 1.77130, accuracy: 0.56213, avg. loss over tasks: 1.77130
Diversity Loss - Mean: -0.06514, Variance: 0.01262
Semantic Loss - Mean: 1.52995, Variance: 0.03580

Train Epoch: 26 
task: sign, mean loss: 0.30589, accuracy: 0.86413, avg. loss over tasks: 0.30589, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.09048, Variance: 0.01078
Semantic Loss - Mean: 0.34954, Variance: 0.00826

Test Epoch: 26 
task: sign, mean loss: 1.44003, accuracy: 0.52071, avg. loss over tasks: 1.44003
Diversity Loss - Mean: -0.06870, Variance: 0.01272
Semantic Loss - Mean: 1.60012, Variance: 0.03727

Train Epoch: 27 
task: sign, mean loss: 0.22008, accuracy: 0.92935, avg. loss over tasks: 0.22008, lr: 0.000289228031029578
Diversity Loss - Mean: -0.08150, Variance: 0.01079
Semantic Loss - Mean: 0.29016, Variance: 0.00811

Test Epoch: 27 
task: sign, mean loss: 1.27491, accuracy: 0.60355, avg. loss over tasks: 1.27491
Diversity Loss - Mean: -0.09149, Variance: 0.01287
Semantic Loss - Mean: 1.15195, Variance: 0.03652

Train Epoch: 28 
task: sign, mean loss: 0.10438, accuracy: 0.97826, avg. loss over tasks: 0.10438, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.08619, Variance: 0.01082
Semantic Loss - Mean: 0.15172, Variance: 0.00796

Test Epoch: 28 
task: sign, mean loss: 1.14997, accuracy: 0.68047, avg. loss over tasks: 1.14997
Diversity Loss - Mean: -0.08619, Variance: 0.01299
Semantic Loss - Mean: 1.06766, Variance: 0.03586

Train Epoch: 29 
task: sign, mean loss: 0.10421, accuracy: 0.95652, avg. loss over tasks: 0.10421, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.08111, Variance: 0.01083
Semantic Loss - Mean: 0.17155, Variance: 0.00790

Test Epoch: 29 
task: sign, mean loss: 1.22111, accuracy: 0.67456, avg. loss over tasks: 1.22111
Diversity Loss - Mean: -0.08462, Variance: 0.01308
Semantic Loss - Mean: 1.13684, Variance: 0.03518

Train Epoch: 30 
task: sign, mean loss: 0.07465, accuracy: 0.98370, avg. loss over tasks: 0.07465, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.07720, Variance: 0.01083
Semantic Loss - Mean: 0.12196, Variance: 0.00770

Test Epoch: 30 
task: sign, mean loss: 1.61750, accuracy: 0.66272, avg. loss over tasks: 1.61750
Diversity Loss - Mean: -0.05829, Variance: 0.01312
Semantic Loss - Mean: 1.51112, Variance: 0.03646

Train Epoch: 31 
task: sign, mean loss: 0.07201, accuracy: 0.96739, avg. loss over tasks: 0.07201, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.07543, Variance: 0.01081
Semantic Loss - Mean: 0.12908, Variance: 0.00757

Test Epoch: 31 
task: sign, mean loss: 2.16677, accuracy: 0.47929, avg. loss over tasks: 2.16677
Diversity Loss - Mean: -0.05391, Variance: 0.01311
Semantic Loss - Mean: 1.95539, Variance: 0.03775

Train Epoch: 32 
task: sign, mean loss: 0.06825, accuracy: 0.97283, avg. loss over tasks: 0.06825, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.08036, Variance: 0.01081
Semantic Loss - Mean: 0.10002, Variance: 0.00741

Test Epoch: 32 
task: sign, mean loss: 2.48063, accuracy: 0.47337, avg. loss over tasks: 2.48063
Diversity Loss - Mean: -0.04725, Variance: 0.01310
Semantic Loss - Mean: 1.99308, Variance: 0.03994

Train Epoch: 33 
task: sign, mean loss: 0.07602, accuracy: 0.98370, avg. loss over tasks: 0.07602, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.07992, Variance: 0.01080
Semantic Loss - Mean: 0.10425, Variance: 0.00726

Test Epoch: 33 
task: sign, mean loss: 2.15879, accuracy: 0.61538, avg. loss over tasks: 2.15879
Diversity Loss - Mean: -0.06642, Variance: 0.01309
Semantic Loss - Mean: 1.76507, Variance: 0.04128

Train Epoch: 34 
task: sign, mean loss: 0.11989, accuracy: 0.96196, avg. loss over tasks: 0.11989, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.08642, Variance: 0.01081
Semantic Loss - Mean: 0.14814, Variance: 0.00716

Test Epoch: 34 
task: sign, mean loss: 2.03657, accuracy: 0.58580, avg. loss over tasks: 2.03657
Diversity Loss - Mean: -0.09665, Variance: 0.01313
Semantic Loss - Mean: 1.84532, Variance: 0.04199

Train Epoch: 35 
task: sign, mean loss: 0.11274, accuracy: 0.96196, avg. loss over tasks: 0.11274, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.09319, Variance: 0.01082
Semantic Loss - Mean: 0.20070, Variance: 0.00720

Test Epoch: 35 
task: sign, mean loss: 1.69061, accuracy: 0.63314, avg. loss over tasks: 1.69061
Diversity Loss - Mean: -0.07976, Variance: 0.01316
Semantic Loss - Mean: 1.39775, Variance: 0.04183

Train Epoch: 36 
task: sign, mean loss: 0.07660, accuracy: 0.97283, avg. loss over tasks: 0.07660, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.08867, Variance: 0.01082
Semantic Loss - Mean: 0.11943, Variance: 0.00710

Test Epoch: 36 
task: sign, mean loss: 1.89549, accuracy: 0.56213, avg. loss over tasks: 1.89549
Diversity Loss - Mean: -0.06942, Variance: 0.01319
Semantic Loss - Mean: 1.75236, Variance: 0.04323

Train Epoch: 37 
task: sign, mean loss: 0.29562, accuracy: 0.90761, avg. loss over tasks: 0.29562, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.09139, Variance: 0.01085
Semantic Loss - Mean: 0.32902, Variance: 0.00709

Test Epoch: 37 
task: sign, mean loss: 1.81110, accuracy: 0.51479, avg. loss over tasks: 1.81110
Diversity Loss - Mean: -0.07882, Variance: 0.01319
Semantic Loss - Mean: 1.50609, Variance: 0.04387

Train Epoch: 38 
task: sign, mean loss: 0.43518, accuracy: 0.83696, avg. loss over tasks: 0.43518, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.09789, Variance: 0.01090
Semantic Loss - Mean: 0.47999, Variance: 0.00718

Test Epoch: 38 
task: sign, mean loss: 2.35573, accuracy: 0.51479, avg. loss over tasks: 2.35573
Diversity Loss - Mean: -0.08008, Variance: 0.01318
Semantic Loss - Mean: 1.95829, Variance: 0.04386

Train Epoch: 39 
task: sign, mean loss: 0.32582, accuracy: 0.88587, avg. loss over tasks: 0.32582, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.09725, Variance: 0.01091
Semantic Loss - Mean: 0.36856, Variance: 0.00714

Test Epoch: 39 
task: sign, mean loss: 2.34387, accuracy: 0.63314, avg. loss over tasks: 2.34387
Diversity Loss - Mean: -0.10333, Variance: 0.01320
Semantic Loss - Mean: 1.88902, Variance: 0.04350

Train Epoch: 40 
task: sign, mean loss: 0.20585, accuracy: 0.91848, avg. loss over tasks: 0.20585, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.08935, Variance: 0.01090
Semantic Loss - Mean: 0.25565, Variance: 0.00702

Test Epoch: 40 
task: sign, mean loss: 2.60497, accuracy: 0.36095, avg. loss over tasks: 2.60497
Diversity Loss - Mean: -0.08546, Variance: 0.01321
Semantic Loss - Mean: 1.87270, Variance: 0.04321

Train Epoch: 41 
task: sign, mean loss: 0.14371, accuracy: 0.92935, avg. loss over tasks: 0.14371, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.09286, Variance: 0.01091
Semantic Loss - Mean: 0.21538, Variance: 0.00700

Test Epoch: 41 
task: sign, mean loss: 3.20097, accuracy: 0.36095, avg. loss over tasks: 3.20097
Diversity Loss - Mean: -0.06843, Variance: 0.01325
Semantic Loss - Mean: 2.89759, Variance: 0.04492

Train Epoch: 42 
task: sign, mean loss: 0.22039, accuracy: 0.94022, avg. loss over tasks: 0.22039, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.09646, Variance: 0.01094
Semantic Loss - Mean: 0.27218, Variance: 0.00695

Test Epoch: 42 
task: sign, mean loss: 1.75192, accuracy: 0.47929, avg. loss over tasks: 1.75192
Diversity Loss - Mean: -0.09011, Variance: 0.01331
Semantic Loss - Mean: 1.32774, Variance: 0.04536

Train Epoch: 43 
task: sign, mean loss: 0.16920, accuracy: 0.94565, avg. loss over tasks: 0.16920, lr: 0.000260757131773478
Diversity Loss - Mean: -0.09953, Variance: 0.01097
Semantic Loss - Mean: 0.18094, Variance: 0.00690

Test Epoch: 43 
task: sign, mean loss: 1.06680, accuracy: 0.61538, avg. loss over tasks: 1.06680
Diversity Loss - Mean: -0.08665, Variance: 0.01347
Semantic Loss - Mean: 1.03901, Variance: 0.04483

Train Epoch: 44 
task: sign, mean loss: 0.10316, accuracy: 0.97283, avg. loss over tasks: 0.10316, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.10096, Variance: 0.01101
Semantic Loss - Mean: 0.12264, Variance: 0.00680

Test Epoch: 44 
task: sign, mean loss: 1.23697, accuracy: 0.56805, avg. loss over tasks: 1.23697
Diversity Loss - Mean: -0.08329, Variance: 0.01361
Semantic Loss - Mean: 1.20128, Variance: 0.04455

Train Epoch: 45 
task: sign, mean loss: 0.21853, accuracy: 0.90217, avg. loss over tasks: 0.21853, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.10181, Variance: 0.01105
Semantic Loss - Mean: 0.26613, Variance: 0.00690

Test Epoch: 45 
task: sign, mean loss: 2.77481, accuracy: 0.22485, avg. loss over tasks: 2.77481
Diversity Loss - Mean: -0.04233, Variance: 0.01376
Semantic Loss - Mean: 2.33478, Variance: 0.04530

Train Epoch: 46 
task: sign, mean loss: 0.20655, accuracy: 0.91304, avg. loss over tasks: 0.20655, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.08955, Variance: 0.01108
Semantic Loss - Mean: 0.29021, Variance: 0.00702

Test Epoch: 46 
task: sign, mean loss: 1.81129, accuracy: 0.47929, avg. loss over tasks: 1.81129
Diversity Loss - Mean: -0.04837, Variance: 0.01380
Semantic Loss - Mean: 1.51632, Variance: 0.04735

Train Epoch: 47 
task: sign, mean loss: 0.13912, accuracy: 0.94022, avg. loss over tasks: 0.13912, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.09363, Variance: 0.01109
Semantic Loss - Mean: 0.22819, Variance: 0.00718

Test Epoch: 47 
task: sign, mean loss: 1.11704, accuracy: 0.71598, avg. loss over tasks: 1.11704
Diversity Loss - Mean: -0.08310, Variance: 0.01384
Semantic Loss - Mean: 1.06454, Variance: 0.04790

Train Epoch: 48 
task: sign, mean loss: 0.13741, accuracy: 0.95109, avg. loss over tasks: 0.13741, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.09162, Variance: 0.01110
Semantic Loss - Mean: 0.19501, Variance: 0.00718

Test Epoch: 48 
task: sign, mean loss: 1.55140, accuracy: 0.47929, avg. loss over tasks: 1.55140
Diversity Loss - Mean: -0.06405, Variance: 0.01384
Semantic Loss - Mean: 1.39999, Variance: 0.04851

Train Epoch: 49 
task: sign, mean loss: 0.04968, accuracy: 0.98913, avg. loss over tasks: 0.04968, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.08750, Variance: 0.01109
Semantic Loss - Mean: 0.10642, Variance: 0.00729

Test Epoch: 49 
task: sign, mean loss: 1.20775, accuracy: 0.75740, avg. loss over tasks: 1.20775
Diversity Loss - Mean: -0.09055, Variance: 0.01384
Semantic Loss - Mean: 0.98911, Variance: 0.04916

Train Epoch: 50 
task: sign, mean loss: 0.10448, accuracy: 0.97283, avg. loss over tasks: 0.10448, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.09326, Variance: 0.01108
Semantic Loss - Mean: 0.15407, Variance: 0.00737

Test Epoch: 50 
task: sign, mean loss: 0.97839, accuracy: 0.85207, avg. loss over tasks: 0.97839
Diversity Loss - Mean: -0.09633, Variance: 0.01387
Semantic Loss - Mean: 0.78712, Variance: 0.04869

Train Epoch: 51 
task: sign, mean loss: 0.06742, accuracy: 0.97283, avg. loss over tasks: 0.06742, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.09420, Variance: 0.01109
Semantic Loss - Mean: 0.14653, Variance: 0.00745

Test Epoch: 51 
task: sign, mean loss: 0.99775, accuracy: 0.69231, avg. loss over tasks: 0.99775
Diversity Loss - Mean: -0.07489, Variance: 0.01390
Semantic Loss - Mean: 1.05446, Variance: 0.04911

Train Epoch: 52 
task: sign, mean loss: 0.03319, accuracy: 0.98913, avg. loss over tasks: 0.03319, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.09219, Variance: 0.01108
Semantic Loss - Mean: 0.08859, Variance: 0.00743

Test Epoch: 52 
task: sign, mean loss: 1.59589, accuracy: 0.69822, avg. loss over tasks: 1.59589
Diversity Loss - Mean: -0.07445, Variance: 0.01390
Semantic Loss - Mean: 1.47827, Variance: 0.04914

Train Epoch: 53 
task: sign, mean loss: 0.05743, accuracy: 0.97826, avg. loss over tasks: 0.05743, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.09538, Variance: 0.01106
Semantic Loss - Mean: 0.16953, Variance: 0.00775

Test Epoch: 53 
task: sign, mean loss: 1.05652, accuracy: 0.82840, avg. loss over tasks: 1.05652
Diversity Loss - Mean: -0.09407, Variance: 0.01393
Semantic Loss - Mean: 0.88891, Variance: 0.04850

Train Epoch: 54 
task: sign, mean loss: 0.04801, accuracy: 0.98913, avg. loss over tasks: 0.04801, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.09088, Variance: 0.01104
Semantic Loss - Mean: 0.14187, Variance: 0.00776

Test Epoch: 54 
task: sign, mean loss: 1.23605, accuracy: 0.72781, avg. loss over tasks: 1.23605
Diversity Loss - Mean: -0.07792, Variance: 0.01394
Semantic Loss - Mean: 1.22074, Variance: 0.05016

Train Epoch: 55 
task: sign, mean loss: 0.10242, accuracy: 0.96739, avg. loss over tasks: 0.10242, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.09773, Variance: 0.01104
Semantic Loss - Mean: 0.13623, Variance: 0.00779

Test Epoch: 55 
task: sign, mean loss: 1.26367, accuracy: 0.72189, avg. loss over tasks: 1.26367
Diversity Loss - Mean: -0.08471, Variance: 0.01396
Semantic Loss - Mean: 1.06438, Variance: 0.05104

Train Epoch: 56 
task: sign, mean loss: 0.06393, accuracy: 0.97826, avg. loss over tasks: 0.06393, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.09217, Variance: 0.01104
Semantic Loss - Mean: 0.11598, Variance: 0.00774

Test Epoch: 56 
task: sign, mean loss: 1.81629, accuracy: 0.53846, avg. loss over tasks: 1.81629
Diversity Loss - Mean: -0.07757, Variance: 0.01395
Semantic Loss - Mean: 1.67933, Variance: 0.05290

Train Epoch: 57 
task: sign, mean loss: 0.15255, accuracy: 0.95652, avg. loss over tasks: 0.15255, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.09866, Variance: 0.01106
Semantic Loss - Mean: 0.19823, Variance: 0.00784

Test Epoch: 57 
task: sign, mean loss: 0.92789, accuracy: 0.73964, avg. loss over tasks: 0.92789
Diversity Loss - Mean: -0.10014, Variance: 0.01398
Semantic Loss - Mean: 0.85393, Variance: 0.05263

Train Epoch: 58 
task: sign, mean loss: 0.39155, accuracy: 0.88043, avg. loss over tasks: 0.39155, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.09965, Variance: 0.01108
Semantic Loss - Mean: 0.43498, Variance: 0.00814

Test Epoch: 58 
task: sign, mean loss: 1.41679, accuracy: 0.40828, avg. loss over tasks: 1.41679
Diversity Loss - Mean: -0.08223, Variance: 0.01398
Semantic Loss - Mean: 1.20545, Variance: 0.05512

Train Epoch: 59 
task: sign, mean loss: 0.25778, accuracy: 0.89674, avg. loss over tasks: 0.25778, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.10205, Variance: 0.01110
Semantic Loss - Mean: 0.30256, Variance: 0.00824

Test Epoch: 59 
task: sign, mean loss: 1.02021, accuracy: 0.71598, avg. loss over tasks: 1.02021
Diversity Loss - Mean: -0.08714, Variance: 0.01399
Semantic Loss - Mean: 0.76607, Variance: 0.05453

Train Epoch: 60 
task: sign, mean loss: 0.13628, accuracy: 0.94565, avg. loss over tasks: 0.13628, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.10784, Variance: 0.01114
Semantic Loss - Mean: 0.19856, Variance: 0.00831

Test Epoch: 60 
task: sign, mean loss: 0.61077, accuracy: 0.79290, avg. loss over tasks: 0.61077
Diversity Loss - Mean: -0.10350, Variance: 0.01404
Semantic Loss - Mean: 0.59820, Variance: 0.05386

Train Epoch: 61 
task: sign, mean loss: 0.12982, accuracy: 0.95652, avg. loss over tasks: 0.12982, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.11325, Variance: 0.01118
Semantic Loss - Mean: 0.19988, Variance: 0.00841

Test Epoch: 61 
task: sign, mean loss: 0.92763, accuracy: 0.76331, avg. loss over tasks: 0.92763
Diversity Loss - Mean: -0.10800, Variance: 0.01407
Semantic Loss - Mean: 0.83368, Variance: 0.05350

Train Epoch: 62 
task: sign, mean loss: 0.05430, accuracy: 0.99457, avg. loss over tasks: 0.05430, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.10967, Variance: 0.01121
Semantic Loss - Mean: 0.13001, Variance: 0.00850

Test Epoch: 62 
task: sign, mean loss: 0.76996, accuracy: 0.78698, avg. loss over tasks: 0.76996
Diversity Loss - Mean: -0.10819, Variance: 0.01412
Semantic Loss - Mean: 0.74818, Variance: 0.05298

Train Epoch: 63 
task: sign, mean loss: 0.03036, accuracy: 0.99457, avg. loss over tasks: 0.03036, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.10804, Variance: 0.01123
Semantic Loss - Mean: 0.07787, Variance: 0.00845

Test Epoch: 63 
task: sign, mean loss: 0.89996, accuracy: 0.76331, avg. loss over tasks: 0.89996
Diversity Loss - Mean: -0.10563, Variance: 0.01415
Semantic Loss - Mean: 0.83629, Variance: 0.05280

Train Epoch: 64 
task: sign, mean loss: 0.02116, accuracy: 0.99457, avg. loss over tasks: 0.02116, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.10858, Variance: 0.01125
Semantic Loss - Mean: 0.07510, Variance: 0.00847

Test Epoch: 64 
task: sign, mean loss: 0.90726, accuracy: 0.81065, avg. loss over tasks: 0.90726
Diversity Loss - Mean: -0.10643, Variance: 0.01418
Semantic Loss - Mean: 0.82678, Variance: 0.05236

Train Epoch: 65 
task: sign, mean loss: 0.00586, accuracy: 1.00000, avg. loss over tasks: 0.00586, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.10871, Variance: 0.01126
Semantic Loss - Mean: 0.04588, Variance: 0.00841

Test Epoch: 65 
task: sign, mean loss: 0.99288, accuracy: 0.81657, avg. loss over tasks: 0.99288
Diversity Loss - Mean: -0.10093, Variance: 0.01421
Semantic Loss - Mean: 0.96836, Variance: 0.05206

Train Epoch: 66 
task: sign, mean loss: 0.00896, accuracy: 1.00000, avg. loss over tasks: 0.00896, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.10838, Variance: 0.01127
Semantic Loss - Mean: 0.05878, Variance: 0.00839

Test Epoch: 66 
task: sign, mean loss: 0.89459, accuracy: 0.81657, avg. loss over tasks: 0.89459
Diversity Loss - Mean: -0.10663, Variance: 0.01424
Semantic Loss - Mean: 0.82823, Variance: 0.05149

Train Epoch: 67 
task: sign, mean loss: 0.00869, accuracy: 1.00000, avg. loss over tasks: 0.00869, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.11120, Variance: 0.01128
Semantic Loss - Mean: 0.04752, Variance: 0.00835

Test Epoch: 67 
task: sign, mean loss: 0.90096, accuracy: 0.81657, avg. loss over tasks: 0.90096
Diversity Loss - Mean: -0.10717, Variance: 0.01427
Semantic Loss - Mean: 0.82197, Variance: 0.05103

Train Epoch: 68 
task: sign, mean loss: 0.16414, accuracy: 0.94565, avg. loss over tasks: 0.16414, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.11113, Variance: 0.01129
Semantic Loss - Mean: 0.21392, Variance: 0.00856

Test Epoch: 68 
task: sign, mean loss: 1.21805, accuracy: 0.77515, avg. loss over tasks: 1.21805
Diversity Loss - Mean: -0.10815, Variance: 0.01428
Semantic Loss - Mean: 1.16597, Variance: 0.05068

Train Epoch: 69 
task: sign, mean loss: 0.08257, accuracy: 0.97283, avg. loss over tasks: 0.08257, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.11477, Variance: 0.01131
Semantic Loss - Mean: 0.13357, Variance: 0.00857

Test Epoch: 69 
task: sign, mean loss: 1.11153, accuracy: 0.75148, avg. loss over tasks: 1.11153
Diversity Loss - Mean: -0.10802, Variance: 0.01430
Semantic Loss - Mean: 1.15531, Variance: 0.05056

Train Epoch: 70 
task: sign, mean loss: 0.05665, accuracy: 0.98370, avg. loss over tasks: 0.05665, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.11574, Variance: 0.01133
Semantic Loss - Mean: 0.08727, Variance: 0.00852

Test Epoch: 70 
task: sign, mean loss: 1.14064, accuracy: 0.79882, avg. loss over tasks: 1.14064
Diversity Loss - Mean: -0.11182, Variance: 0.01433
Semantic Loss - Mean: 1.19597, Variance: 0.05073

Train Epoch: 71 
task: sign, mean loss: 0.02117, accuracy: 0.99457, avg. loss over tasks: 0.02117, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.11636, Variance: 0.01134
Semantic Loss - Mean: 0.05243, Variance: 0.00846

Test Epoch: 71 
task: sign, mean loss: 1.19317, accuracy: 0.78698, avg. loss over tasks: 1.19317
Diversity Loss - Mean: -0.11294, Variance: 0.01435
Semantic Loss - Mean: 1.12372, Variance: 0.05097

Train Epoch: 72 
task: sign, mean loss: 0.01793, accuracy: 0.99457, avg. loss over tasks: 0.01793, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.11687, Variance: 0.01136
Semantic Loss - Mean: 0.06443, Variance: 0.00843

Test Epoch: 72 
task: sign, mean loss: 1.13516, accuracy: 0.77515, avg. loss over tasks: 1.13516
Diversity Loss - Mean: -0.11201, Variance: 0.01436
Semantic Loss - Mean: 1.06148, Variance: 0.05098

Train Epoch: 73 
task: sign, mean loss: 0.01753, accuracy: 1.00000, avg. loss over tasks: 0.01753, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.11626, Variance: 0.01137
Semantic Loss - Mean: 0.05022, Variance: 0.00837

Test Epoch: 73 
task: sign, mean loss: 1.04419, accuracy: 0.79882, avg. loss over tasks: 1.04419
Diversity Loss - Mean: -0.11371, Variance: 0.01437
Semantic Loss - Mean: 0.94132, Variance: 0.05065

Train Epoch: 74 
task: sign, mean loss: 0.01583, accuracy: 0.99457, avg. loss over tasks: 0.01583, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.11568, Variance: 0.01138
Semantic Loss - Mean: 0.04626, Variance: 0.00829

Test Epoch: 74 
task: sign, mean loss: 1.25355, accuracy: 0.79290, avg. loss over tasks: 1.25355
Diversity Loss - Mean: -0.11071, Variance: 0.01437
Semantic Loss - Mean: 1.17131, Variance: 0.05051

Train Epoch: 75 
task: sign, mean loss: 0.02224, accuracy: 0.98913, avg. loss over tasks: 0.02224, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.11613, Variance: 0.01139
Semantic Loss - Mean: 0.06235, Variance: 0.00825

Test Epoch: 75 
task: sign, mean loss: 1.28897, accuracy: 0.76923, avg. loss over tasks: 1.28897
Diversity Loss - Mean: -0.11193, Variance: 0.01437
Semantic Loss - Mean: 1.11723, Variance: 0.05024

Train Epoch: 76 
task: sign, mean loss: 0.00579, accuracy: 1.00000, avg. loss over tasks: 0.00579, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.11568, Variance: 0.01139
Semantic Loss - Mean: 0.05820, Variance: 0.00827

Test Epoch: 76 
task: sign, mean loss: 1.22852, accuracy: 0.76923, avg. loss over tasks: 1.22852
Diversity Loss - Mean: -0.11132, Variance: 0.01437
Semantic Loss - Mean: 1.16334, Variance: 0.05019

Train Epoch: 77 
task: sign, mean loss: 0.00480, accuracy: 1.00000, avg. loss over tasks: 0.00480, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.11694, Variance: 0.01140
Semantic Loss - Mean: 0.06132, Variance: 0.00836

Test Epoch: 77 
task: sign, mean loss: 1.25376, accuracy: 0.77515, avg. loss over tasks: 1.25376
Diversity Loss - Mean: -0.10739, Variance: 0.01438
Semantic Loss - Mean: 1.37450, Variance: 0.05088

Train Epoch: 78 
task: sign, mean loss: 0.00878, accuracy: 0.99457, avg. loss over tasks: 0.00878, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.11838, Variance: 0.01141
Semantic Loss - Mean: 0.03359, Variance: 0.00827

Test Epoch: 78 
task: sign, mean loss: 1.35606, accuracy: 0.71006, avg. loss over tasks: 1.35606
Diversity Loss - Mean: -0.11026, Variance: 0.01439
Semantic Loss - Mean: 1.53612, Variance: 0.05137

Train Epoch: 79 
task: sign, mean loss: 0.00431, accuracy: 1.00000, avg. loss over tasks: 0.00431, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.11872, Variance: 0.01142
Semantic Loss - Mean: 0.02748, Variance: 0.00818

Test Epoch: 79 
task: sign, mean loss: 1.44285, accuracy: 0.68639, avg. loss over tasks: 1.44285
Diversity Loss - Mean: -0.11162, Variance: 0.01440
Semantic Loss - Mean: 1.47314, Variance: 0.05169

Train Epoch: 80 
task: sign, mean loss: 0.00209, accuracy: 1.00000, avg. loss over tasks: 0.00209, lr: 0.00015015
Diversity Loss - Mean: -0.11949, Variance: 0.01142
Semantic Loss - Mean: 0.02690, Variance: 0.00811

Test Epoch: 80 
task: sign, mean loss: 1.32120, accuracy: 0.72189, avg. loss over tasks: 1.32120
Diversity Loss - Mean: -0.11467, Variance: 0.01442
Semantic Loss - Mean: 1.32677, Variance: 0.05182

Train Epoch: 81 
task: sign, mean loss: 0.09446, accuracy: 0.98913, avg. loss over tasks: 0.09446, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.11987, Variance: 0.01143
Semantic Loss - Mean: 0.12547, Variance: 0.00821

Test Epoch: 81 
task: sign, mean loss: 1.47085, accuracy: 0.67456, avg. loss over tasks: 1.47085
Diversity Loss - Mean: -0.11641, Variance: 0.01443
Semantic Loss - Mean: 1.64358, Variance: 0.05302

Train Epoch: 82 
task: sign, mean loss: 0.07371, accuracy: 0.97826, avg. loss over tasks: 0.07371, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.12228, Variance: 0.01145
Semantic Loss - Mean: 0.13692, Variance: 0.00831

Test Epoch: 82 
task: sign, mean loss: 1.28323, accuracy: 0.75148, avg. loss over tasks: 1.28323
Diversity Loss - Mean: -0.11959, Variance: 0.01445
Semantic Loss - Mean: 1.36244, Variance: 0.05336

Train Epoch: 83 
task: sign, mean loss: 0.03088, accuracy: 0.98913, avg. loss over tasks: 0.03088, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12046, Variance: 0.01146
Semantic Loss - Mean: 0.09871, Variance: 0.00839

Test Epoch: 83 
task: sign, mean loss: 1.10655, accuracy: 0.71006, avg. loss over tasks: 1.10655
Diversity Loss - Mean: -0.10964, Variance: 0.01445
Semantic Loss - Mean: 1.43957, Variance: 0.05343

Train Epoch: 84 
task: sign, mean loss: 0.04526, accuracy: 0.98370, avg. loss over tasks: 0.04526, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.11815, Variance: 0.01146
Semantic Loss - Mean: 0.11782, Variance: 0.00842

Test Epoch: 84 
task: sign, mean loss: 0.85284, accuracy: 0.78107, avg. loss over tasks: 0.85284
Diversity Loss - Mean: -0.11818, Variance: 0.01446
Semantic Loss - Mean: 0.87783, Variance: 0.05320

Train Epoch: 85 
task: sign, mean loss: 0.01368, accuracy: 0.98913, avg. loss over tasks: 0.01368, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.12104, Variance: 0.01146
Semantic Loss - Mean: 0.03307, Variance: 0.00835

Test Epoch: 85 
task: sign, mean loss: 0.81776, accuracy: 0.78107, avg. loss over tasks: 0.81776
Diversity Loss - Mean: -0.12100, Variance: 0.01447
Semantic Loss - Mean: 0.88010, Variance: 0.05293

Train Epoch: 86 
task: sign, mean loss: 0.02793, accuracy: 0.99457, avg. loss over tasks: 0.02793, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.11996, Variance: 0.01147
Semantic Loss - Mean: 0.09813, Variance: 0.00845

Test Epoch: 86 
task: sign, mean loss: 1.00271, accuracy: 0.79882, avg. loss over tasks: 1.00271
Diversity Loss - Mean: -0.11687, Variance: 0.01448
Semantic Loss - Mean: 1.09067, Variance: 0.05278

Train Epoch: 87 
task: sign, mean loss: 0.03752, accuracy: 0.98913, avg. loss over tasks: 0.03752, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.12093, Variance: 0.01148
Semantic Loss - Mean: 0.09564, Variance: 0.00845

Test Epoch: 87 
task: sign, mean loss: 0.62291, accuracy: 0.85799, avg. loss over tasks: 0.62291
Diversity Loss - Mean: -0.11684, Variance: 0.01451
Semantic Loss - Mean: 0.61373, Variance: 0.05239

Train Epoch: 88 
task: sign, mean loss: 0.03904, accuracy: 0.98913, avg. loss over tasks: 0.03904, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.12019, Variance: 0.01149
Semantic Loss - Mean: 0.08539, Variance: 0.00841

Test Epoch: 88 
task: sign, mean loss: 0.94071, accuracy: 0.78698, avg. loss over tasks: 0.94071
Diversity Loss - Mean: -0.10388, Variance: 0.01450
Semantic Loss - Mean: 1.16065, Variance: 0.05245

Train Epoch: 89 
task: sign, mean loss: 0.02695, accuracy: 0.99457, avg. loss over tasks: 0.02695, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.12109, Variance: 0.01150
Semantic Loss - Mean: 0.04125, Variance: 0.00836

Test Epoch: 89 
task: sign, mean loss: 1.07196, accuracy: 0.75148, avg. loss over tasks: 1.07196
Diversity Loss - Mean: -0.11060, Variance: 0.01450
Semantic Loss - Mean: 1.33643, Variance: 0.05278

Train Epoch: 90 
task: sign, mean loss: 0.00861, accuracy: 1.00000, avg. loss over tasks: 0.00861, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.12113, Variance: 0.01151
Semantic Loss - Mean: 0.06140, Variance: 0.00834

Test Epoch: 90 
task: sign, mean loss: 1.24058, accuracy: 0.74556, avg. loss over tasks: 1.24058
Diversity Loss - Mean: -0.11256, Variance: 0.01450
Semantic Loss - Mean: 1.45294, Variance: 0.05313

Train Epoch: 91 
task: sign, mean loss: 0.01777, accuracy: 0.99457, avg. loss over tasks: 0.01777, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.12133, Variance: 0.01151
Semantic Loss - Mean: 0.06284, Variance: 0.00833

Test Epoch: 91 
task: sign, mean loss: 0.98340, accuracy: 0.79290, avg. loss over tasks: 0.98340
Diversity Loss - Mean: -0.12248, Variance: 0.01452
Semantic Loss - Mean: 1.00353, Variance: 0.05294

Train Epoch: 92 
task: sign, mean loss: 0.00408, accuracy: 1.00000, avg. loss over tasks: 0.00408, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.12075, Variance: 0.01151
Semantic Loss - Mean: 0.04559, Variance: 0.00832

Test Epoch: 92 
task: sign, mean loss: 1.05562, accuracy: 0.79290, avg. loss over tasks: 1.05562
Diversity Loss - Mean: -0.12305, Variance: 0.01453
Semantic Loss - Mean: 1.06529, Variance: 0.05292

Train Epoch: 93 
task: sign, mean loss: 0.00455, accuracy: 1.00000, avg. loss over tasks: 0.00455, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.12251, Variance: 0.01152
Semantic Loss - Mean: 0.02990, Variance: 0.00830

Test Epoch: 93 
task: sign, mean loss: 0.94968, accuracy: 0.82840, avg. loss over tasks: 0.94968
Diversity Loss - Mean: -0.12527, Variance: 0.01454
Semantic Loss - Mean: 0.91054, Variance: 0.05278

Train Epoch: 94 
task: sign, mean loss: 0.00342, accuracy: 1.00000, avg. loss over tasks: 0.00342, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.12235, Variance: 0.01152
Semantic Loss - Mean: 0.02382, Variance: 0.00824

Test Epoch: 94 
task: sign, mean loss: 0.94171, accuracy: 0.83432, avg. loss over tasks: 0.94171
Diversity Loss - Mean: -0.12371, Variance: 0.01455
Semantic Loss - Mean: 0.98332, Variance: 0.05271

Train Epoch: 95 
task: sign, mean loss: 0.00376, accuracy: 1.00000, avg. loss over tasks: 0.00376, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.12423, Variance: 0.01153
Semantic Loss - Mean: 0.03181, Variance: 0.00819

Test Epoch: 95 
task: sign, mean loss: 1.11115, accuracy: 0.79882, avg. loss over tasks: 1.11115
Diversity Loss - Mean: -0.12032, Variance: 0.01455
Semantic Loss - Mean: 1.27360, Variance: 0.05288

Train Epoch: 96 
task: sign, mean loss: 0.00897, accuracy: 1.00000, avg. loss over tasks: 0.00897, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.12399, Variance: 0.01153
Semantic Loss - Mean: 0.04368, Variance: 0.00814

Test Epoch: 96 
task: sign, mean loss: 0.81245, accuracy: 0.82840, avg. loss over tasks: 0.81245
Diversity Loss - Mean: -0.12269, Variance: 0.01457
Semantic Loss - Mean: 0.87189, Variance: 0.05270

Train Epoch: 97 
task: sign, mean loss: 0.00448, accuracy: 1.00000, avg. loss over tasks: 0.00448, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.12393, Variance: 0.01154
Semantic Loss - Mean: 0.04726, Variance: 0.00814

Test Epoch: 97 
task: sign, mean loss: 0.92500, accuracy: 0.82249, avg. loss over tasks: 0.92500
Diversity Loss - Mean: -0.12391, Variance: 0.01458
Semantic Loss - Mean: 0.89514, Variance: 0.05253

Train Epoch: 98 
task: sign, mean loss: 0.00429, accuracy: 1.00000, avg. loss over tasks: 0.00429, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.12486, Variance: 0.01154
Semantic Loss - Mean: 0.02882, Variance: 0.00811

Test Epoch: 98 
task: sign, mean loss: 1.07719, accuracy: 0.81065, avg. loss over tasks: 1.07719
Diversity Loss - Mean: -0.12463, Variance: 0.01459
Semantic Loss - Mean: 1.01489, Variance: 0.05245

Train Epoch: 99 
task: sign, mean loss: 0.00238, accuracy: 1.00000, avg. loss over tasks: 0.00238, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.12463, Variance: 0.01155
Semantic Loss - Mean: 0.03542, Variance: 0.00807

Test Epoch: 99 
task: sign, mean loss: 1.06479, accuracy: 0.79882, avg. loss over tasks: 1.06479
Diversity Loss - Mean: -0.12174, Variance: 0.01460
Semantic Loss - Mean: 1.08326, Variance: 0.05241

Train Epoch: 100 
task: sign, mean loss: 0.01027, accuracy: 0.99457, avg. loss over tasks: 0.01027, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.12490, Variance: 0.01156
Semantic Loss - Mean: 0.05778, Variance: 0.00804

Test Epoch: 100 
task: sign, mean loss: 0.87701, accuracy: 0.84615, avg. loss over tasks: 0.87701
Diversity Loss - Mean: -0.12387, Variance: 0.01462
Semantic Loss - Mean: 0.91555, Variance: 0.05233

Train Epoch: 101 
task: sign, mean loss: 0.07410, accuracy: 0.97283, avg. loss over tasks: 0.07410, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.12464, Variance: 0.01156
Semantic Loss - Mean: 0.09674, Variance: 0.00807

Test Epoch: 101 
task: sign, mean loss: 0.77226, accuracy: 0.84615, avg. loss over tasks: 0.77226
Diversity Loss - Mean: -0.12116, Variance: 0.01462
Semantic Loss - Mean: 0.92045, Variance: 0.05278

Train Epoch: 102 
task: sign, mean loss: 0.01577, accuracy: 0.99457, avg. loss over tasks: 0.01577, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.12537, Variance: 0.01157
Semantic Loss - Mean: 0.05884, Variance: 0.00808

Test Epoch: 102 
task: sign, mean loss: 0.67920, accuracy: 0.82249, avg. loss over tasks: 0.67920
Diversity Loss - Mean: -0.12144, Variance: 0.01464
Semantic Loss - Mean: 0.75903, Variance: 0.05308

Train Epoch: 103 
task: sign, mean loss: 0.02202, accuracy: 0.99457, avg. loss over tasks: 0.02202, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.12558, Variance: 0.01158
Semantic Loss - Mean: 0.04205, Variance: 0.00802

Test Epoch: 103 
task: sign, mean loss: 0.85093, accuracy: 0.83432, avg. loss over tasks: 0.85093
Diversity Loss - Mean: -0.12332, Variance: 0.01465
Semantic Loss - Mean: 0.92435, Variance: 0.05294

Train Epoch: 104 
task: sign, mean loss: 0.00410, accuracy: 1.00000, avg. loss over tasks: 0.00410, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.12555, Variance: 0.01158
Semantic Loss - Mean: 0.03625, Variance: 0.00799

Test Epoch: 104 
task: sign, mean loss: 0.80842, accuracy: 0.82249, avg. loss over tasks: 0.80842
Diversity Loss - Mean: -0.12428, Variance: 0.01467
Semantic Loss - Mean: 0.85278, Variance: 0.05272

Train Epoch: 105 
task: sign, mean loss: 0.00395, accuracy: 1.00000, avg. loss over tasks: 0.00395, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.12599, Variance: 0.01159
Semantic Loss - Mean: 0.03106, Variance: 0.00794

Test Epoch: 105 
task: sign, mean loss: 0.91700, accuracy: 0.79882, avg. loss over tasks: 0.91700
Diversity Loss - Mean: -0.12236, Variance: 0.01468
Semantic Loss - Mean: 1.07293, Variance: 0.05255

Train Epoch: 106 
task: sign, mean loss: 0.00651, accuracy: 1.00000, avg. loss over tasks: 0.00651, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.12696, Variance: 0.01159
Semantic Loss - Mean: 0.03922, Variance: 0.00793

Test Epoch: 106 
task: sign, mean loss: 0.81224, accuracy: 0.82249, avg. loss over tasks: 0.81224
Diversity Loss - Mean: -0.12401, Variance: 0.01469
Semantic Loss - Mean: 0.93571, Variance: 0.05229

Train Epoch: 107 
task: sign, mean loss: 0.01344, accuracy: 0.99457, avg. loss over tasks: 0.01344, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.12722, Variance: 0.01160
Semantic Loss - Mean: 0.05068, Variance: 0.00795

Test Epoch: 107 
task: sign, mean loss: 0.84774, accuracy: 0.82249, avg. loss over tasks: 0.84774
Diversity Loss - Mean: -0.12546, Variance: 0.01471
Semantic Loss - Mean: 0.94504, Variance: 0.05196

Train Epoch: 108 
task: sign, mean loss: 0.01995, accuracy: 0.99457, avg. loss over tasks: 0.01995, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.12659, Variance: 0.01161
Semantic Loss - Mean: 0.05857, Variance: 0.00794

Test Epoch: 108 
task: sign, mean loss: 0.87052, accuracy: 0.83432, avg. loss over tasks: 0.87052
Diversity Loss - Mean: -0.12496, Variance: 0.01472
Semantic Loss - Mean: 0.86851, Variance: 0.05165

Train Epoch: 109 
task: sign, mean loss: 0.01268, accuracy: 0.99457, avg. loss over tasks: 0.01268, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.12588, Variance: 0.01162
Semantic Loss - Mean: 0.05451, Variance: 0.00793

Test Epoch: 109 
task: sign, mean loss: 0.79889, accuracy: 0.82840, avg. loss over tasks: 0.79889
Diversity Loss - Mean: -0.12501, Variance: 0.01474
Semantic Loss - Mean: 0.79601, Variance: 0.05129

Train Epoch: 110 
task: sign, mean loss: 0.03961, accuracy: 0.98913, avg. loss over tasks: 0.03961, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.12631, Variance: 0.01162
Semantic Loss - Mean: 0.05936, Variance: 0.00789

Test Epoch: 110 
task: sign, mean loss: 0.80264, accuracy: 0.82249, avg. loss over tasks: 0.80264
Diversity Loss - Mean: -0.12370, Variance: 0.01475
Semantic Loss - Mean: 0.79457, Variance: 0.05093

Train Epoch: 111 
task: sign, mean loss: 0.01345, accuracy: 0.99457, avg. loss over tasks: 0.01345, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.12540, Variance: 0.01162
Semantic Loss - Mean: 0.06279, Variance: 0.00790

Test Epoch: 111 
task: sign, mean loss: 0.86760, accuracy: 0.81065, avg. loss over tasks: 0.86760
Diversity Loss - Mean: -0.12322, Variance: 0.01477
Semantic Loss - Mean: 0.85598, Variance: 0.05062

Train Epoch: 112 
task: sign, mean loss: 0.00181, accuracy: 1.00000, avg. loss over tasks: 0.00181, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.12684, Variance: 0.01163
Semantic Loss - Mean: 0.03204, Variance: 0.00786

Test Epoch: 112 
task: sign, mean loss: 0.87828, accuracy: 0.81065, avg. loss over tasks: 0.87828
Diversity Loss - Mean: -0.12183, Variance: 0.01478
Semantic Loss - Mean: 0.92522, Variance: 0.05042

Train Epoch: 113 
task: sign, mean loss: 0.00195, accuracy: 1.00000, avg. loss over tasks: 0.00195, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.12775, Variance: 0.01163
Semantic Loss - Mean: 0.02524, Variance: 0.00783

Test Epoch: 113 
task: sign, mean loss: 0.87833, accuracy: 0.79882, avg. loss over tasks: 0.87833
Diversity Loss - Mean: -0.12104, Variance: 0.01479
Semantic Loss - Mean: 0.97103, Variance: 0.05035

Train Epoch: 114 
task: sign, mean loss: 0.00157, accuracy: 1.00000, avg. loss over tasks: 0.00157, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.12646, Variance: 0.01164
Semantic Loss - Mean: 0.03974, Variance: 0.00781

Test Epoch: 114 
task: sign, mean loss: 0.93178, accuracy: 0.78698, avg. loss over tasks: 0.93178
Diversity Loss - Mean: -0.12238, Variance: 0.01480
Semantic Loss - Mean: 1.00939, Variance: 0.05026

Train Epoch: 115 
task: sign, mean loss: 0.00343, accuracy: 1.00000, avg. loss over tasks: 0.00343, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.12697, Variance: 0.01164
Semantic Loss - Mean: 0.03252, Variance: 0.00776

Test Epoch: 115 
task: sign, mean loss: 1.03209, accuracy: 0.78698, avg. loss over tasks: 1.03209
Diversity Loss - Mean: -0.12265, Variance: 0.01481
Semantic Loss - Mean: 1.15542, Variance: 0.05024

Train Epoch: 116 
task: sign, mean loss: 0.01966, accuracy: 0.99457, avg. loss over tasks: 0.01966, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.12764, Variance: 0.01164
Semantic Loss - Mean: 0.04639, Variance: 0.00772

Test Epoch: 116 
task: sign, mean loss: 0.82800, accuracy: 0.81657, avg. loss over tasks: 0.82800
Diversity Loss - Mean: -0.12543, Variance: 0.01483
Semantic Loss - Mean: 0.86694, Variance: 0.05001

Train Epoch: 117 
task: sign, mean loss: 0.00386, accuracy: 1.00000, avg. loss over tasks: 0.00386, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.12843, Variance: 0.01165
Semantic Loss - Mean: 0.03558, Variance: 0.00768

Test Epoch: 117 
task: sign, mean loss: 0.79220, accuracy: 0.81065, avg. loss over tasks: 0.79220
Diversity Loss - Mean: -0.12356, Variance: 0.01484
Semantic Loss - Mean: 0.83273, Variance: 0.04981

Train Epoch: 118 
task: sign, mean loss: 0.00397, accuracy: 1.00000, avg. loss over tasks: 0.00397, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.12801, Variance: 0.01166
Semantic Loss - Mean: 0.02453, Variance: 0.00764

Test Epoch: 118 
task: sign, mean loss: 0.84611, accuracy: 0.79882, avg. loss over tasks: 0.84611
Diversity Loss - Mean: -0.12338, Variance: 0.01485
Semantic Loss - Mean: 0.92554, Variance: 0.04965

Train Epoch: 119 
task: sign, mean loss: 0.00152, accuracy: 1.00000, avg. loss over tasks: 0.00152, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.12827, Variance: 0.01166
Semantic Loss - Mean: 0.03059, Variance: 0.00761

Test Epoch: 119 
task: sign, mean loss: 0.75900, accuracy: 0.79290, avg. loss over tasks: 0.75900
Diversity Loss - Mean: -0.12371, Variance: 0.01486
Semantic Loss - Mean: 0.83591, Variance: 0.04949

Train Epoch: 120 
task: sign, mean loss: 0.00156, accuracy: 1.00000, avg. loss over tasks: 0.00156, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.12860, Variance: 0.01167
Semantic Loss - Mean: 0.04501, Variance: 0.00758

Test Epoch: 120 
task: sign, mean loss: 0.80646, accuracy: 0.81657, avg. loss over tasks: 0.80646
Diversity Loss - Mean: -0.12430, Variance: 0.01487
Semantic Loss - Mean: 0.91541, Variance: 0.04933

Train Epoch: 121 
task: sign, mean loss: 0.00184, accuracy: 1.00000, avg. loss over tasks: 0.00184, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.12893, Variance: 0.01167
Semantic Loss - Mean: 0.03278, Variance: 0.00755

Test Epoch: 121 
task: sign, mean loss: 0.84738, accuracy: 0.80473, avg. loss over tasks: 0.84738
Diversity Loss - Mean: -0.12326, Variance: 0.01488
Semantic Loss - Mean: 1.00875, Variance: 0.04920

Train Epoch: 122 
task: sign, mean loss: 0.00284, accuracy: 1.00000, avg. loss over tasks: 0.00284, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.12856, Variance: 0.01168
Semantic Loss - Mean: 0.03981, Variance: 0.00754

Test Epoch: 122 
task: sign, mean loss: 0.82561, accuracy: 0.81657, avg. loss over tasks: 0.82561
Diversity Loss - Mean: -0.12466, Variance: 0.01489
Semantic Loss - Mean: 0.95527, Variance: 0.04898

Train Epoch: 123 
task: sign, mean loss: 0.00467, accuracy: 1.00000, avg. loss over tasks: 0.00467, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.12925, Variance: 0.01168
Semantic Loss - Mean: 0.02232, Variance: 0.00750

Test Epoch: 123 
task: sign, mean loss: 0.84864, accuracy: 0.81065, avg. loss over tasks: 0.84864
Diversity Loss - Mean: -0.12345, Variance: 0.01490
Semantic Loss - Mean: 1.00623, Variance: 0.04874

Train Epoch: 124 
task: sign, mean loss: 0.00213, accuracy: 1.00000, avg. loss over tasks: 0.00213, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.12899, Variance: 0.01169
Semantic Loss - Mean: 0.02504, Variance: 0.00746

Test Epoch: 124 
task: sign, mean loss: 0.76412, accuracy: 0.81657, avg. loss over tasks: 0.76412
Diversity Loss - Mean: -0.12396, Variance: 0.01491
Semantic Loss - Mean: 0.88221, Variance: 0.04849

Train Epoch: 125 
task: sign, mean loss: 0.00169, accuracy: 1.00000, avg. loss over tasks: 0.00169, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.12958, Variance: 0.01170
Semantic Loss - Mean: 0.02113, Variance: 0.00742

Test Epoch: 125 
task: sign, mean loss: 0.83308, accuracy: 0.80473, avg. loss over tasks: 0.83308
Diversity Loss - Mean: -0.12375, Variance: 0.01492
Semantic Loss - Mean: 0.97379, Variance: 0.04831

Train Epoch: 126 
task: sign, mean loss: 0.00123, accuracy: 1.00000, avg. loss over tasks: 0.00123, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.12922, Variance: 0.01170
Semantic Loss - Mean: 0.02402, Variance: 0.00738

Test Epoch: 126 
task: sign, mean loss: 0.77074, accuracy: 0.81065, avg. loss over tasks: 0.77074
Diversity Loss - Mean: -0.12468, Variance: 0.01494
Semantic Loss - Mean: 0.86750, Variance: 0.04808

Train Epoch: 127 
task: sign, mean loss: 0.00207, accuracy: 1.00000, avg. loss over tasks: 0.00207, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.12923, Variance: 0.01171
Semantic Loss - Mean: 0.01737, Variance: 0.00734

Test Epoch: 127 
task: sign, mean loss: 0.80862, accuracy: 0.81065, avg. loss over tasks: 0.80862
Diversity Loss - Mean: -0.12493, Variance: 0.01495
Semantic Loss - Mean: 0.94751, Variance: 0.04787

Train Epoch: 128 
task: sign, mean loss: 0.00083, accuracy: 1.00000, avg. loss over tasks: 0.00083, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.12938, Variance: 0.01172
Semantic Loss - Mean: 0.01833, Variance: 0.00729

Test Epoch: 128 
task: sign, mean loss: 0.87551, accuracy: 0.79290, avg. loss over tasks: 0.87551
Diversity Loss - Mean: -0.12376, Variance: 0.01496
Semantic Loss - Mean: 1.05703, Variance: 0.04769

Train Epoch: 129 
task: sign, mean loss: 0.00202, accuracy: 1.00000, avg. loss over tasks: 0.00202, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.12982, Variance: 0.01172
Semantic Loss - Mean: 0.02066, Variance: 0.00725

Test Epoch: 129 
task: sign, mean loss: 0.88457, accuracy: 0.80473, avg. loss over tasks: 0.88457
Diversity Loss - Mean: -0.12453, Variance: 0.01497
Semantic Loss - Mean: 1.02811, Variance: 0.04748

Train Epoch: 130 
task: sign, mean loss: 0.00206, accuracy: 1.00000, avg. loss over tasks: 0.00206, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.12950, Variance: 0.01173
Semantic Loss - Mean: 0.01845, Variance: 0.00720

Test Epoch: 130 
task: sign, mean loss: 0.90976, accuracy: 0.81065, avg. loss over tasks: 0.90976
Diversity Loss - Mean: -0.12437, Variance: 0.01498
Semantic Loss - Mean: 1.10044, Variance: 0.04732

Train Epoch: 131 
task: sign, mean loss: 0.00261, accuracy: 1.00000, avg. loss over tasks: 0.00261, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.12949, Variance: 0.01174
Semantic Loss - Mean: 0.02573, Variance: 0.00716

Test Epoch: 131 
task: sign, mean loss: 0.94754, accuracy: 0.79882, avg. loss over tasks: 0.94754
Diversity Loss - Mean: -0.12431, Variance: 0.01498
Semantic Loss - Mean: 1.12139, Variance: 0.04713

Train Epoch: 132 
task: sign, mean loss: 0.00180, accuracy: 1.00000, avg. loss over tasks: 0.00180, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.12999, Variance: 0.01174
Semantic Loss - Mean: 0.02073, Variance: 0.00712

Test Epoch: 132 
task: sign, mean loss: 0.98604, accuracy: 0.80473, avg. loss over tasks: 0.98604
Diversity Loss - Mean: -0.12422, Variance: 0.01499
Semantic Loss - Mean: 1.15516, Variance: 0.04692

Train Epoch: 133 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.12972, Variance: 0.01174
Semantic Loss - Mean: 0.01623, Variance: 0.00708

Test Epoch: 133 
task: sign, mean loss: 0.97079, accuracy: 0.80473, avg. loss over tasks: 0.97079
Diversity Loss - Mean: -0.12434, Variance: 0.01500
Semantic Loss - Mean: 1.13717, Variance: 0.04670

Train Epoch: 134 
task: sign, mean loss: 0.00057, accuracy: 1.00000, avg. loss over tasks: 0.00057, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13005, Variance: 0.01175
Semantic Loss - Mean: 0.01460, Variance: 0.00703

Test Epoch: 134 
task: sign, mean loss: 0.94105, accuracy: 0.80473, avg. loss over tasks: 0.94105
Diversity Loss - Mean: -0.12493, Variance: 0.01501
Semantic Loss - Mean: 1.09619, Variance: 0.04647

Train Epoch: 135 
task: sign, mean loss: 0.00164, accuracy: 1.00000, avg. loss over tasks: 0.00164, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.12961, Variance: 0.01175
Semantic Loss - Mean: 0.02814, Variance: 0.00702

Test Epoch: 135 
task: sign, mean loss: 1.00378, accuracy: 0.79882, avg. loss over tasks: 1.00378
Diversity Loss - Mean: -0.12496, Variance: 0.01502
Semantic Loss - Mean: 1.09472, Variance: 0.04623

Train Epoch: 136 
task: sign, mean loss: 0.00057, accuracy: 1.00000, avg. loss over tasks: 0.00057, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13026, Variance: 0.01176
Semantic Loss - Mean: 0.02035, Variance: 0.00699

Test Epoch: 136 
task: sign, mean loss: 0.89356, accuracy: 0.80473, avg. loss over tasks: 0.89356
Diversity Loss - Mean: -0.12519, Variance: 0.01503
Semantic Loss - Mean: 0.99441, Variance: 0.04598

Train Epoch: 137 
task: sign, mean loss: 0.00248, accuracy: 1.00000, avg. loss over tasks: 0.00248, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.12886, Variance: 0.01176
Semantic Loss - Mean: 0.02982, Variance: 0.00695

Test Epoch: 137 
task: sign, mean loss: 0.93022, accuracy: 0.81065, avg. loss over tasks: 0.93022
Diversity Loss - Mean: -0.12471, Variance: 0.01503
Semantic Loss - Mean: 1.06080, Variance: 0.04573

Train Epoch: 138 
task: sign, mean loss: 0.00183, accuracy: 1.00000, avg. loss over tasks: 0.00183, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.12967, Variance: 0.01177
Semantic Loss - Mean: 0.03544, Variance: 0.00692

Test Epoch: 138 
task: sign, mean loss: 0.85646, accuracy: 0.81065, avg. loss over tasks: 0.85646
Diversity Loss - Mean: -0.12461, Variance: 0.01504
Semantic Loss - Mean: 1.02733, Variance: 0.04548

Train Epoch: 139 
task: sign, mean loss: 0.00166, accuracy: 1.00000, avg. loss over tasks: 0.00166, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13002, Variance: 0.01178
Semantic Loss - Mean: 0.02905, Variance: 0.00689

Test Epoch: 139 
task: sign, mean loss: 0.95274, accuracy: 0.80473, avg. loss over tasks: 0.95274
Diversity Loss - Mean: -0.12435, Variance: 0.01505
Semantic Loss - Mean: 1.10910, Variance: 0.04524

Train Epoch: 140 
task: sign, mean loss: 0.03399, accuracy: 0.98913, avg. loss over tasks: 0.03399, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.12960, Variance: 0.01178
Semantic Loss - Mean: 0.04353, Variance: 0.00687

Test Epoch: 140 
task: sign, mean loss: 0.89998, accuracy: 0.81065, avg. loss over tasks: 0.89998
Diversity Loss - Mean: -0.12584, Variance: 0.01506
Semantic Loss - Mean: 1.01891, Variance: 0.04500

Train Epoch: 141 
task: sign, mean loss: 0.00116, accuracy: 1.00000, avg. loss over tasks: 0.00116, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.12967, Variance: 0.01179
Semantic Loss - Mean: 0.02384, Variance: 0.00685

Test Epoch: 141 
task: sign, mean loss: 0.88868, accuracy: 0.81657, avg. loss over tasks: 0.88868
Diversity Loss - Mean: -0.12578, Variance: 0.01507
Semantic Loss - Mean: 0.99581, Variance: 0.04475

Train Epoch: 142 
task: sign, mean loss: 0.00127, accuracy: 1.00000, avg. loss over tasks: 0.00127, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.12906, Variance: 0.01179
Semantic Loss - Mean: 0.04902, Variance: 0.00690

Test Epoch: 142 
task: sign, mean loss: 0.87644, accuracy: 0.81065, avg. loss over tasks: 0.87644
Diversity Loss - Mean: -0.12654, Variance: 0.01508
Semantic Loss - Mean: 0.95866, Variance: 0.04451

Train Epoch: 143 
task: sign, mean loss: 0.00117, accuracy: 1.00000, avg. loss over tasks: 0.00117, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13018, Variance: 0.01180
Semantic Loss - Mean: 0.01159, Variance: 0.00686

Test Epoch: 143 
task: sign, mean loss: 0.92086, accuracy: 0.81065, avg. loss over tasks: 0.92086
Diversity Loss - Mean: -0.12499, Variance: 0.01509
Semantic Loss - Mean: 1.08990, Variance: 0.04428

Train Epoch: 144 
task: sign, mean loss: 0.00130, accuracy: 1.00000, avg. loss over tasks: 0.00130, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.12965, Variance: 0.01180
Semantic Loss - Mean: 0.01736, Variance: 0.00682

Test Epoch: 144 
task: sign, mean loss: 0.93816, accuracy: 0.81065, avg. loss over tasks: 0.93816
Diversity Loss - Mean: -0.12529, Variance: 0.01510
Semantic Loss - Mean: 1.11350, Variance: 0.04405

Train Epoch: 145 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13001, Variance: 0.01181
Semantic Loss - Mean: 0.01759, Variance: 0.00678

Test Epoch: 145 
task: sign, mean loss: 0.92994, accuracy: 0.80473, avg. loss over tasks: 0.92994
Diversity Loss - Mean: -0.12577, Variance: 0.01510
Semantic Loss - Mean: 1.08331, Variance: 0.04383

Train Epoch: 146 
task: sign, mean loss: 0.00294, accuracy: 1.00000, avg. loss over tasks: 0.00294, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.12971, Variance: 0.01181
Semantic Loss - Mean: 0.02031, Variance: 0.00675

Test Epoch: 146 
task: sign, mean loss: 1.04388, accuracy: 0.79882, avg. loss over tasks: 1.04388
Diversity Loss - Mean: -0.12437, Variance: 0.01511
Semantic Loss - Mean: 1.21743, Variance: 0.04361

Train Epoch: 147 
task: sign, mean loss: 0.00221, accuracy: 1.00000, avg. loss over tasks: 0.00221, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13027, Variance: 0.01182
Semantic Loss - Mean: 0.01667, Variance: 0.00671

Test Epoch: 147 
task: sign, mean loss: 0.90838, accuracy: 0.81657, avg. loss over tasks: 0.90838
Diversity Loss - Mean: -0.12504, Variance: 0.01512
Semantic Loss - Mean: 1.07995, Variance: 0.04339

Train Epoch: 148 
task: sign, mean loss: 0.00189, accuracy: 1.00000, avg. loss over tasks: 0.00189, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.12934, Variance: 0.01182
Semantic Loss - Mean: 0.03095, Variance: 0.00669

Test Epoch: 148 
task: sign, mean loss: 0.91287, accuracy: 0.80473, avg. loss over tasks: 0.91287
Diversity Loss - Mean: -0.12516, Variance: 0.01513
Semantic Loss - Mean: 1.07080, Variance: 0.04318

Train Epoch: 149 
task: sign, mean loss: 0.00053, accuracy: 1.00000, avg. loss over tasks: 0.00053, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.12942, Variance: 0.01183
Semantic Loss - Mean: 0.02117, Variance: 0.00666

Test Epoch: 149 
task: sign, mean loss: 0.92164, accuracy: 0.80473, avg. loss over tasks: 0.92164
Diversity Loss - Mean: -0.12587, Variance: 0.01513
Semantic Loss - Mean: 1.03454, Variance: 0.04296

Train Epoch: 150 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 3e-07
Diversity Loss - Mean: -0.12999, Variance: 0.01183
Semantic Loss - Mean: 0.01523, Variance: 0.00662

Test Epoch: 150 
task: sign, mean loss: 0.89879, accuracy: 0.79882, avg. loss over tasks: 0.89879
Diversity Loss - Mean: -0.12571, Variance: 0.01514
Semantic Loss - Mean: 0.98899, Variance: 0.04274

