Used config:
{'B': 16,
 'B_seq': 16,
 'D': 128,
 'D_inner': 512,
 'D_k': 16,
 'D_v': 16,
 'H': 8,
 'I': 100,
 'M': 100,
 'N': 3600,
 'attn_dropout': 0.1,
 'data_dir': 'data/megapixel_mnist/dsets/megapixel_mnist_1500',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.001,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 1,
 'n_class': 10,
 'n_epoch': 100,
 'n_epoch_warmup': 10,
 'n_res_blocks': 2,
 'n_token': 4,
 'n_worker': 2,
 'patch_size': [50, 50],
 'patch_stride': [50, 50],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': False,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'majority'},
           'task1': {'act_fn': 'softmax',
                     'id': 1,
                     'metric': 'accuracy',
                     'name': 'max'},
           'task2': {'act_fn': 'softmax',
                     'id': 2,
                     'metric': 'accuracy',
                     'name': 'top'},
           'task3': {'act_fn': 'sigmoid',
                     'id': 3,
                     'metric': 'multilabel_accuracy',
                     'name': 'multi'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': True,
 'wd': 0.1}
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: majority, mean loss: 2.36778, accuracy: 0.10650, task: max, mean loss: 2.14509, accuracy: 0.23600, task: top, mean loss: 2.33722, accuracy: 0.11350, task: multi, mean loss: 0.67096, multilabel_accuracy: 0.00250, avg. loss over tasks: 1.88026, lr: 0.0001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 1 
task: majority, mean loss: 2.31732, accuracy: 0.10100, task: max, mean loss: 1.89338, accuracy: 0.27400, task: top, mean loss: 2.30699, accuracy: 0.10100, task: multi, mean loss: 0.60337, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78027
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 2 
task: majority, mean loss: 2.32406, accuracy: 0.08950, task: max, mean loss: 1.86033, accuracy: 0.24400, task: top, mean loss: 2.32750, accuracy: 0.09700, task: multi, mean loss: 0.60620, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77952, lr: 0.0002
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 2 
task: majority, mean loss: 2.32219, accuracy: 0.08900, task: max, mean loss: 1.88016, accuracy: 0.27400, task: top, mean loss: 2.33729, accuracy: 0.09700, task: multi, mean loss: 0.60117, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78520
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 3 
task: majority, mean loss: 2.33107, accuracy: 0.10100, task: max, mean loss: 1.85122, accuracy: 0.25250, task: top, mean loss: 2.32816, accuracy: 0.11450, task: multi, mean loss: 0.60646, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77923, lr: 0.0003
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 3 
task: majority, mean loss: 2.31976, accuracy: 0.09000, task: max, mean loss: 1.86306, accuracy: 0.27400, task: top, mean loss: 2.31261, accuracy: 0.09700, task: multi, mean loss: 0.60098, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77410
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 4 
task: majority, mean loss: 2.33053, accuracy: 0.09450, task: max, mean loss: 1.84879, accuracy: 0.25150, task: top, mean loss: 2.32800, accuracy: 0.10550, task: multi, mean loss: 0.60558, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77822, lr: 0.0004
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 4 
task: majority, mean loss: 2.34639, accuracy: 0.08900, task: max, mean loss: 1.86091, accuracy: 0.27400, task: top, mean loss: 2.33503, accuracy: 0.10100, task: multi, mean loss: 0.60314, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78637
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 5 
task: majority, mean loss: 2.33067, accuracy: 0.09600, task: max, mean loss: 1.85010, accuracy: 0.24900, task: top, mean loss: 2.32136, accuracy: 0.11150, task: multi, mean loss: 0.60564, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77694, lr: 0.0005
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 5 
task: majority, mean loss: 2.32794, accuracy: 0.09400, task: max, mean loss: 1.86259, accuracy: 0.21300, task: top, mean loss: 2.31846, accuracy: 0.13000, task: multi, mean loss: 0.60094, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77748
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 6 
task: majority, mean loss: 2.33335, accuracy: 0.10050, task: max, mean loss: 1.84121, accuracy: 0.25900, task: top, mean loss: 2.32659, accuracy: 0.10200, task: multi, mean loss: 0.60556, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77667, lr: 0.0006
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 6 
task: majority, mean loss: 2.31214, accuracy: 0.10600, task: max, mean loss: 1.86632, accuracy: 0.27300, task: top, mean loss: 2.32452, accuracy: 0.10100, task: multi, mean loss: 0.60129, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77607
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 7 
task: majority, mean loss: 2.32624, accuracy: 0.10100, task: max, mean loss: 1.84076, accuracy: 0.26150, task: top, mean loss: 2.32690, accuracy: 0.10300, task: multi, mean loss: 0.60456, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77462, lr: 0.0007
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 7 
task: majority, mean loss: 2.31359, accuracy: 0.09400, task: max, mean loss: 1.91080, accuracy: 0.21300, task: top, mean loss: 2.30957, accuracy: 0.10700, task: multi, mean loss: 0.60457, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78463
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 8 
task: majority, mean loss: 2.32492, accuracy: 0.10600, task: max, mean loss: 1.84424, accuracy: 0.25650, task: top, mean loss: 2.32038, accuracy: 0.10950, task: multi, mean loss: 0.60563, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77379, lr: 0.0008
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 8 
task: majority, mean loss: 2.33464, accuracy: 0.10100, task: max, mean loss: 1.86603, accuracy: 0.27400, task: top, mean loss: 2.30314, accuracy: 0.09600, task: multi, mean loss: 0.60220, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77650
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 9 
task: majority, mean loss: 2.32788, accuracy: 0.09150, task: max, mean loss: 1.84345, accuracy: 0.25400, task: top, mean loss: 2.31832, accuracy: 0.09950, task: multi, mean loss: 0.60502, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77367, lr: 0.0009
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 9 
task: majority, mean loss: 2.30901, accuracy: 0.11100, task: max, mean loss: 1.85551, accuracy: 0.27400, task: top, mean loss: 2.31545, accuracy: 0.10300, task: multi, mean loss: 0.60036, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77008
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 10 
task: majority, mean loss: 2.32915, accuracy: 0.09150, task: max, mean loss: 1.84305, accuracy: 0.25500, task: top, mean loss: 2.32135, accuracy: 0.09000, task: multi, mean loss: 0.60545, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77475, lr: 0.001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 10 
task: majority, mean loss: 2.31810, accuracy: 0.10100, task: max, mean loss: 1.86221, accuracy: 0.27400, task: top, mean loss: 2.30863, accuracy: 0.09800, task: multi, mean loss: 0.60164, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77264
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 11 
task: majority, mean loss: 2.32040, accuracy: 0.09050, task: max, mean loss: 1.83910, accuracy: 0.25250, task: top, mean loss: 2.31759, accuracy: 0.10400, task: multi, mean loss: 0.60463, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77043, lr: 0.0009996957180960382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 11 
task: majority, mean loss: 2.32477, accuracy: 0.10600, task: max, mean loss: 1.86452, accuracy: 0.22800, task: top, mean loss: 2.33696, accuracy: 0.10300, task: multi, mean loss: 0.60253, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78220
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 12 
task: majority, mean loss: 2.28133, accuracy: 0.12700, task: max, mean loss: 1.83084, accuracy: 0.26250, task: top, mean loss: 2.28623, accuracy: 0.13900, task: multi, mean loss: 0.60450, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.75072, lr: 0.0009987832431047822
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 12 
task: majority, mean loss: 2.33937, accuracy: 0.09300, task: max, mean loss: 1.86913, accuracy: 0.21300, task: top, mean loss: 2.37015, accuracy: 0.10300, task: multi, mean loss: 0.60121, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.79497
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 13 
task: majority, mean loss: 2.24085, accuracy: 0.13600, task: max, mean loss: 1.82505, accuracy: 0.26750, task: top, mean loss: 2.26625, accuracy: 0.13000, task: multi, mean loss: 0.60006, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.73305, lr: 0.0009972636867364526
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 13 
task: majority, mean loss: 2.29080, accuracy: 0.11600, task: max, mean loss: 1.85735, accuracy: 0.27500, task: top, mean loss: 2.29010, accuracy: 0.14700, task: multi, mean loss: 0.59988, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.75953
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 14 
task: majority, mean loss: 2.21034, accuracy: 0.14150, task: max, mean loss: 1.79886, accuracy: 0.29300, task: top, mean loss: 2.23621, accuracy: 0.14500, task: multi, mean loss: 0.59253, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.70949, lr: 0.0009951389003364144
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 14 
task: majority, mean loss: 2.22153, accuracy: 0.12700, task: max, mean loss: 1.84444, accuracy: 0.25700, task: top, mean loss: 2.25462, accuracy: 0.13800, task: multi, mean loss: 0.59021, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.72770
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 15 
task: majority, mean loss: 2.19369, accuracy: 0.16050, task: max, mean loss: 1.80517, accuracy: 0.27450, task: top, mean loss: 2.23591, accuracy: 0.15400, task: multi, mean loss: 0.59264, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.70685, lr: 0.000992411472629598
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 15 
task: majority, mean loss: 2.20862, accuracy: 0.15700, task: max, mean loss: 1.84110, accuracy: 0.27400, task: top, mean loss: 2.22983, accuracy: 0.18200, task: multi, mean loss: 0.58640, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.71649
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 16 
task: majority, mean loss: 2.17232, accuracy: 0.16700, task: max, mean loss: 1.79987, accuracy: 0.28150, task: top, mean loss: 2.20690, accuracy: 0.16450, task: multi, mean loss: 0.59026, multilabel_accuracy: 0.00050, avg. loss over tasks: 1.69234, lr: 0.000989084726566536
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 16 
task: majority, mean loss: 2.29043, accuracy: 0.14000, task: max, mean loss: 1.85672, accuracy: 0.27300, task: top, mean loss: 2.31022, accuracy: 0.15800, task: multi, mean loss: 0.59971, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76427
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 17 
task: majority, mean loss: 2.15915, accuracy: 0.16300, task: max, mean loss: 1.78879, accuracy: 0.29250, task: top, mean loss: 2.18751, accuracy: 0.17600, task: multi, mean loss: 0.58929, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.68118, lr: 0.00098516271527486
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 17 
task: majority, mean loss: 2.27236, accuracy: 0.16000, task: max, mean loss: 1.93031, accuracy: 0.26700, task: top, mean loss: 2.30611, accuracy: 0.13600, task: multi, mean loss: 0.59621, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77625
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 18 
task: majority, mean loss: 2.12487, accuracy: 0.18750, task: max, mean loss: 1.78692, accuracy: 0.29900, task: top, mean loss: 2.15507, accuracy: 0.16550, task: multi, mean loss: 0.58673, multilabel_accuracy: 0.00050, avg. loss over tasks: 1.66340, lr: 0.0009806502171211902
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 18 
task: majority, mean loss: 2.36194, accuracy: 0.11400, task: max, mean loss: 1.84441, accuracy: 0.25100, task: top, mean loss: 2.33128, accuracy: 0.12600, task: multi, mean loss: 0.59745, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78377
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 19 
task: majority, mean loss: 2.07268, accuracy: 0.20750, task: max, mean loss: 1.76501, accuracy: 0.31100, task: top, mean loss: 2.12382, accuracy: 0.19050, task: multi, mean loss: 0.58284, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.63609, lr: 0.0009755527298894293
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 19 
task: majority, mean loss: 2.22767, accuracy: 0.15900, task: max, mean loss: 1.82343, accuracy: 0.27100, task: top, mean loss: 2.28179, accuracy: 0.13000, task: multi, mean loss: 0.58769, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.73015
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 20 
task: majority, mean loss: 2.03705, accuracy: 0.23250, task: max, mean loss: 1.73170, accuracy: 0.33600, task: top, mean loss: 2.09502, accuracy: 0.21800, task: multi, mean loss: 0.58036, multilabel_accuracy: 0.00150, avg. loss over tasks: 1.61103, lr: 0.0009698764640825613
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 20 
task: majority, mean loss: 2.03520, accuracy: 0.25400, task: max, mean loss: 1.73811, accuracy: 0.36000, task: top, mean loss: 2.15061, accuracy: 0.19700, task: multi, mean loss: 0.57312, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.62426
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 21 
task: majority, mean loss: 1.95453, accuracy: 0.27350, task: max, mean loss: 1.66641, accuracy: 0.37350, task: top, mean loss: 2.04797, accuracy: 0.24800, task: multi, mean loss: 0.57540, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.56108, lr: 0.0009636283353561103
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 21 
task: majority, mean loss: 2.07698, accuracy: 0.23900, task: max, mean loss: 1.66810, accuracy: 0.38500, task: top, mean loss: 2.15415, accuracy: 0.22600, task: multi, mean loss: 0.59651, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.62393
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 22 
task: majority, mean loss: 1.85351, accuracy: 0.31550, task: max, mean loss: 1.57956, accuracy: 0.42550, task: top, mean loss: 1.97891, accuracy: 0.27700, task: multi, mean loss: 0.56680, multilabel_accuracy: 0.00150, avg. loss over tasks: 1.49470, lr: 0.0009568159560924791
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 22 
task: majority, mean loss: 2.02894, accuracy: 0.25700, task: max, mean loss: 1.65634, accuracy: 0.41400, task: top, mean loss: 2.09874, accuracy: 0.25200, task: multi, mean loss: 0.56574, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.58744
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 23 
task: majority, mean loss: 1.76170, accuracy: 0.35250, task: max, mean loss: 1.50351, accuracy: 0.46550, task: top, mean loss: 1.89888, accuracy: 0.30850, task: multi, mean loss: 0.54846, multilabel_accuracy: 0.00800, avg. loss over tasks: 1.42814, lr: 0.000949447626126434
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 23 
task: majority, mean loss: 2.34756, accuracy: 0.19900, task: max, mean loss: 1.87805, accuracy: 0.29000, task: top, mean loss: 2.08771, accuracy: 0.22400, task: multi, mean loss: 0.59547, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.72720
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 24 
task: majority, mean loss: 1.68265, accuracy: 0.38200, task: max, mean loss: 1.45094, accuracy: 0.47700, task: top, mean loss: 1.78647, accuracy: 0.35850, task: multi, mean loss: 0.53395, multilabel_accuracy: 0.00850, avg. loss over tasks: 1.36350, lr: 0.000941532322633034
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 24 
task: majority, mean loss: 1.79151, accuracy: 0.36700, task: max, mean loss: 1.57100, accuracy: 0.42600, task: top, mean loss: 1.90753, accuracy: 0.30600, task: multi, mean loss: 0.54363, multilabel_accuracy: 0.00400, avg. loss over tasks: 1.45342
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 25 
task: majority, mean loss: 1.53175, accuracy: 0.45100, task: max, mean loss: 1.35800, accuracy: 0.51150, task: top, mean loss: 1.69931, accuracy: 0.40150, task: multi, mean loss: 0.51188, multilabel_accuracy: 0.01300, avg. loss over tasks: 1.27523, lr: 0.0009330796891903273
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 25 
task: majority, mean loss: 1.83835, accuracy: 0.36300, task: max, mean loss: 1.62709, accuracy: 0.42100, task: top, mean loss: 1.89490, accuracy: 0.33900, task: multi, mean loss: 0.53492, multilabel_accuracy: 0.01800, avg. loss over tasks: 1.47382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 26 
task: majority, mean loss: 1.39805, accuracy: 0.51950, task: max, mean loss: 1.24011, accuracy: 0.55750, task: top, mean loss: 1.52663, accuracy: 0.47000, task: multi, mean loss: 0.48631, multilabel_accuracy: 0.03300, avg. loss over tasks: 1.16277, lr: 0.0009241000240301347
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 26 
task: majority, mean loss: 1.94008, accuracy: 0.33900, task: max, mean loss: 1.57035, accuracy: 0.43200, task: top, mean loss: 1.97197, accuracy: 0.32800, task: multi, mean loss: 0.53785, multilabel_accuracy: 0.02000, avg. loss over tasks: 1.50506
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 27 
task: majority, mean loss: 1.23974, accuracy: 0.55750, task: max, mean loss: 1.11733, accuracy: 0.60650, task: top, mean loss: 1.35059, accuracy: 0.54400, task: multi, mean loss: 0.45986, multilabel_accuracy: 0.03800, avg. loss over tasks: 1.04188, lr: 0.0009146042674912433
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 27 
task: majority, mean loss: 1.63547, accuracy: 0.48300, task: max, mean loss: 1.15147, accuracy: 0.58800, task: top, mean loss: 1.66534, accuracy: 0.46800, task: multi, mean loss: 0.48205, multilabel_accuracy: 0.02700, avg. loss over tasks: 1.23358
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 28 
task: majority, mean loss: 1.09285, accuracy: 0.62450, task: max, mean loss: 1.05073, accuracy: 0.62350, task: top, mean loss: 1.23197, accuracy: 0.58800, task: multi, mean loss: 0.44239, multilabel_accuracy: 0.05050, avg. loss over tasks: 0.95448, lr: 0.0009046039886902862
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 28 
task: majority, mean loss: 2.13983, accuracy: 0.33800, task: max, mean loss: 1.43820, accuracy: 0.49300, task: top, mean loss: 2.04211, accuracy: 0.34300, task: multi, mean loss: 0.55861, multilabel_accuracy: 0.02800, avg. loss over tasks: 1.54469
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 29 
task: majority, mean loss: 0.95544, accuracy: 0.65850, task: max, mean loss: 0.95571, accuracy: 0.66200, task: top, mean loss: 1.09047, accuracy: 0.64150, task: multi, mean loss: 0.41803, multilabel_accuracy: 0.06100, avg. loss over tasks: 0.85491, lr: 0.0008941113714265576
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 29 
task: majority, mean loss: 1.14404, accuracy: 0.59200, task: max, mean loss: 1.12518, accuracy: 0.58300, task: top, mean loss: 1.33851, accuracy: 0.54500, task: multi, mean loss: 0.43758, multilabel_accuracy: 0.06800, avg. loss over tasks: 1.01133
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 30 
task: majority, mean loss: 0.85183, accuracy: 0.70100, task: max, mean loss: 0.90276, accuracy: 0.67150, task: top, mean loss: 1.02465, accuracy: 0.66300, task: multi, mean loss: 0.40463, multilabel_accuracy: 0.08700, avg. loss over tasks: 0.79597, lr: 0.0008831391993379295
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 30 
task: majority, mean loss: 1.21602, accuracy: 0.58500, task: max, mean loss: 1.12135, accuracy: 0.60800, task: top, mean loss: 1.32283, accuracy: 0.56500, task: multi, mean loss: 0.42994, multilabel_accuracy: 0.07200, avg. loss over tasks: 1.02253
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 31 
task: majority, mean loss: 0.70903, accuracy: 0.74750, task: max, mean loss: 0.82528, accuracy: 0.70600, task: top, mean loss: 0.89002, accuracy: 0.70100, task: multi, mean loss: 0.38529, multilabel_accuracy: 0.09600, avg. loss over tasks: 0.70240, lr: 0.0008717008403259585
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 31 
task: majority, mean loss: 1.14164, accuracy: 0.58400, task: max, mean loss: 1.07090, accuracy: 0.63000, task: top, mean loss: 1.33024, accuracy: 0.57900, task: multi, mean loss: 0.42687, multilabel_accuracy: 0.07800, avg. loss over tasks: 0.99241
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 32 
task: majority, mean loss: 0.65025, accuracy: 0.77300, task: max, mean loss: 0.78196, accuracy: 0.73000, task: top, mean loss: 0.80888, accuracy: 0.73900, task: multi, mean loss: 0.37528, multilabel_accuracy: 0.10500, avg. loss over tasks: 0.65409, lr: 0.0008598102302691562
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 32 
task: majority, mean loss: 1.09012, accuracy: 0.62100, task: max, mean loss: 1.20760, accuracy: 0.56800, task: top, mean loss: 1.25679, accuracy: 0.59200, task: multi, mean loss: 0.42893, multilabel_accuracy: 0.09900, avg. loss over tasks: 0.99586
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 33 
task: majority, mean loss: 0.60712, accuracy: 0.78550, task: max, mean loss: 0.73077, accuracy: 0.74850, task: top, mean loss: 0.74813, accuracy: 0.76350, task: multi, mean loss: 0.36509, multilabel_accuracy: 0.12550, avg. loss over tasks: 0.61278, lr: 0.0008474818560442692
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 33 
task: majority, mean loss: 0.78099, accuracy: 0.72000, task: max, mean loss: 0.89484, accuracy: 0.67700, task: top, mean loss: 1.05421, accuracy: 0.64900, task: multi, mean loss: 0.38186, multilabel_accuracy: 0.11500, avg. loss over tasks: 0.77798
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 34 
task: majority, mean loss: 0.54910, accuracy: 0.81100, task: max, mean loss: 0.65788, accuracy: 0.76700, task: top, mean loss: 0.66138, accuracy: 0.77800, task: multi, mean loss: 0.34640, multilabel_accuracy: 0.14850, avg. loss over tasks: 0.55369, lr: 0.0008347307378762497
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 34 
task: majority, mean loss: 0.80759, accuracy: 0.73600, task: max, mean loss: 0.88059, accuracy: 0.68900, task: top, mean loss: 1.08484, accuracy: 0.65200, task: multi, mean loss: 0.38891, multilabel_accuracy: 0.12300, avg. loss over tasks: 0.79048
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 35 
task: majority, mean loss: 0.49048, accuracy: 0.82550, task: max, mean loss: 0.63776, accuracy: 0.78300, task: top, mean loss: 0.61225, accuracy: 0.81050, task: multi, mean loss: 0.33665, multilabel_accuracy: 0.15300, avg. loss over tasks: 0.51929, lr: 0.0008215724110384265
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 35 
task: majority, mean loss: 0.60480, accuracy: 0.80200, task: max, mean loss: 0.87788, accuracy: 0.69200, task: top, mean loss: 0.96432, accuracy: 0.69700, task: multi, mean loss: 0.36018, multilabel_accuracy: 0.14000, avg. loss over tasks: 0.70180
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 36 
task: majority, mean loss: 0.42834, accuracy: 0.85800, task: max, mean loss: 0.56669, accuracy: 0.80650, task: top, mean loss: 0.54717, accuracy: 0.83300, task: multi, mean loss: 0.32141, multilabel_accuracy: 0.17800, avg. loss over tasks: 0.46590, lr: 0.0008080229069251663
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 36 
task: majority, mean loss: 0.73358, accuracy: 0.76300, task: max, mean loss: 0.92812, accuracy: 0.69100, task: top, mean loss: 0.99560, accuracy: 0.69300, task: multi, mean loss: 0.35593, multilabel_accuracy: 0.16700, avg. loss over tasks: 0.75331
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 37 
task: majority, mean loss: 0.36996, accuracy: 0.87250, task: max, mean loss: 0.54611, accuracy: 0.81450, task: top, mean loss: 0.47539, accuracy: 0.85100, task: multi, mean loss: 0.30291, multilabel_accuracy: 0.19700, avg. loss over tasks: 0.42359, lr: 0.0007940987335200903
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 37 
task: majority, mean loss: 0.76684, accuracy: 0.73600, task: max, mean loss: 0.93515, accuracy: 0.69100, task: top, mean loss: 0.98745, accuracy: 0.69100, task: multi, mean loss: 0.36108, multilabel_accuracy: 0.16700, avg. loss over tasks: 0.76263
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 38 
task: majority, mean loss: 0.31315, accuracy: 0.89050, task: max, mean loss: 0.47895, accuracy: 0.84150, task: top, mean loss: 0.42186, accuracy: 0.85850, task: multi, mean loss: 0.29448, multilabel_accuracy: 0.24000, avg. loss over tasks: 0.37711, lr: 0.0007798168552836382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 38 
task: majority, mean loss: 0.72502, accuracy: 0.75100, task: max, mean loss: 0.92079, accuracy: 0.70500, task: top, mean loss: 0.96851, accuracy: 0.72300, task: multi, mean loss: 0.35345, multilabel_accuracy: 0.18000, avg. loss over tasks: 0.74194
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 39 
task: majority, mean loss: 0.27049, accuracy: 0.90200, task: max, mean loss: 0.43228, accuracy: 0.85050, task: top, mean loss: 0.33225, accuracy: 0.89650, task: multi, mean loss: 0.27962, multilabel_accuracy: 0.24600, avg. loss over tasks: 0.32866, lr: 0.0007651946724844859
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 39 
task: majority, mean loss: 0.58618, accuracy: 0.81400, task: max, mean loss: 0.89183, accuracy: 0.71400, task: top, mean loss: 0.94221, accuracy: 0.72100, task: multi, mean loss: 0.32855, multilabel_accuracy: 0.19700, avg. loss over tasks: 0.68719
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 40 
task: majority, mean loss: 0.24336, accuracy: 0.91550, task: max, mean loss: 0.40326, accuracy: 0.86650, task: top, mean loss: 0.31389, accuracy: 0.89850, task: multi, mean loss: 0.27263, multilabel_accuracy: 0.25700, avg. loss over tasks: 0.30829, lr: 0.00075025
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 40 
task: majority, mean loss: 0.94116, accuracy: 0.70600, task: max, mean loss: 0.92904, accuracy: 0.71300, task: top, mean loss: 1.17147, accuracy: 0.69500, task: multi, mean loss: 0.36852, multilabel_accuracy: 0.15700, avg. loss over tasks: 0.85255
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 41 
task: majority, mean loss: 0.24315, accuracy: 0.90950, task: max, mean loss: 0.38337, accuracy: 0.87550, task: top, mean loss: 0.27976, accuracy: 0.91000, task: multi, mean loss: 0.27151, multilabel_accuracy: 0.26200, avg. loss over tasks: 0.29445, lr: 0.0007350010456115524
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 41 
task: majority, mean loss: 0.54839, accuracy: 0.82000, task: max, mean loss: 0.94632, accuracy: 0.70200, task: top, mean loss: 0.89011, accuracy: 0.75400, task: multi, mean loss: 0.31527, multilabel_accuracy: 0.21900, avg. loss over tasks: 0.67502
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 42 
task: majority, mean loss: 0.23578, accuracy: 0.91450, task: max, mean loss: 0.34939, accuracy: 0.88250, task: top, mean loss: 0.26569, accuracy: 0.91300, task: multi, mean loss: 0.26191, multilabel_accuracy: 0.28000, avg. loss over tasks: 0.27819, lr: 0.0007194663878211441
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 42 
task: majority, mean loss: 0.70042, accuracy: 0.77000, task: max, mean loss: 0.86046, accuracy: 0.72400, task: top, mean loss: 0.99902, accuracy: 0.72400, task: multi, mean loss: 0.33106, multilabel_accuracy: 0.20600, avg. loss over tasks: 0.72274
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 43 
task: majority, mean loss: 0.18729, accuracy: 0.93750, task: max, mean loss: 0.27921, accuracy: 0.90100, task: top, mean loss: 0.21097, accuracy: 0.92800, task: multi, mean loss: 0.24635, multilabel_accuracy: 0.30800, avg. loss over tasks: 0.23095, lr: 0.0007036649532163622
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 43 
task: majority, mean loss: 0.48599, accuracy: 0.84000, task: max, mean loss: 0.94189, accuracy: 0.72500, task: top, mean loss: 0.89936, accuracy: 0.75200, task: multi, mean loss: 0.30040, multilabel_accuracy: 0.25400, avg. loss over tasks: 0.65691
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 44 
task: majority, mean loss: 0.23077, accuracy: 0.91700, task: max, mean loss: 0.25686, accuracy: 0.91550, task: top, mean loss: 0.21854, accuracy: 0.93050, task: multi, mean loss: 0.24626, multilabel_accuracy: 0.30300, avg. loss over tasks: 0.23811, lr: 0.0006876159934112482
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 44 
task: majority, mean loss: 0.77652, accuracy: 0.77000, task: max, mean loss: 0.82389, accuracy: 0.75200, task: top, mean loss: 0.96381, accuracy: 0.75600, task: multi, mean loss: 0.31788, multilabel_accuracy: 0.24700, avg. loss over tasks: 0.72053
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 45 
task: majority, mean loss: 0.17741, accuracy: 0.94300, task: max, mean loss: 0.25793, accuracy: 0.91650, task: top, mean loss: 0.21711, accuracy: 0.92850, task: multi, mean loss: 0.23753, multilabel_accuracy: 0.33250, avg. loss over tasks: 0.22249, lr: 0.0006713390615911716
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 45 
task: majority, mean loss: 0.60475, accuracy: 0.80800, task: max, mean loss: 0.98893, accuracy: 0.72500, task: top, mean loss: 1.09185, accuracy: 0.73800, task: multi, mean loss: 0.32071, multilabel_accuracy: 0.24900, avg. loss over tasks: 0.75156
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 46 
task: majority, mean loss: 0.18826, accuracy: 0.93050, task: max, mean loss: 0.24056, accuracy: 0.92400, task: top, mean loss: 0.20532, accuracy: 0.93250, task: multi, mean loss: 0.23672, multilabel_accuracy: 0.33850, avg. loss over tasks: 0.21772, lr: 0.0006548539886902863
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 46 
task: majority, mean loss: 0.41321, accuracy: 0.86600, task: max, mean loss: 0.72434, accuracy: 0.76700, task: top, mean loss: 0.87289, accuracy: 0.77500, task: multi, mean loss: 0.28041, multilabel_accuracy: 0.29400, avg. loss over tasks: 0.57271
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 47 
task: majority, mean loss: 0.14837, accuracy: 0.94800, task: max, mean loss: 0.21587, accuracy: 0.93100, task: top, mean loss: 0.16519, accuracy: 0.94900, task: multi, mean loss: 0.22240, multilabel_accuracy: 0.36650, avg. loss over tasks: 0.18796, lr: 0.0006381808592305911
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 47 
task: majority, mean loss: 0.57213, accuracy: 0.81900, task: max, mean loss: 0.91927, accuracy: 0.73000, task: top, mean loss: 1.01114, accuracy: 0.75000, task: multi, mean loss: 0.31188, multilabel_accuracy: 0.21600, avg. loss over tasks: 0.70360
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 48 
task: majority, mean loss: 0.13279, accuracy: 0.95700, task: max, mean loss: 0.17258, accuracy: 0.94400, task: top, mean loss: 0.14953, accuracy: 0.94850, task: multi, mean loss: 0.21414, multilabel_accuracy: 0.37450, avg. loss over tasks: 0.16726, lr: 0.0006213399868520341
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 48 
task: majority, mean loss: 0.73124, accuracy: 0.78200, task: max, mean loss: 1.02514, accuracy: 0.74500, task: top, mean loss: 1.01735, accuracy: 0.74300, task: multi, mean loss: 0.32094, multilabel_accuracy: 0.27200, avg. loss over tasks: 0.77367
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 49 
task: majority, mean loss: 0.14184, accuracy: 0.94950, task: max, mean loss: 0.18032, accuracy: 0.93850, task: top, mean loss: 0.13455, accuracy: 0.95850, task: multi, mean loss: 0.21383, multilabel_accuracy: 0.37550, avg. loss over tasks: 0.16764, lr: 0.0006043518895634708
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 49 
task: majority, mean loss: 0.50086, accuracy: 0.84300, task: max, mean loss: 0.87041, accuracy: 0.74700, task: top, mean loss: 0.93938, accuracy: 0.76800, task: multi, mean loss: 0.29155, multilabel_accuracy: 0.26700, avg. loss over tasks: 0.65055
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 50 
task: majority, mean loss: 0.11393, accuracy: 0.96100, task: max, mean loss: 0.14862, accuracy: 0.95100, task: top, mean loss: 0.11824, accuracy: 0.96200, task: multi, mean loss: 0.19976, multilabel_accuracy: 0.42350, avg. loss over tasks: 0.14514, lr: 0.0005872372647446318
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 50 
task: majority, mean loss: 0.69498, accuracy: 0.80000, task: max, mean loss: 0.84251, accuracy: 0.76300, task: top, mean loss: 0.99026, accuracy: 0.76400, task: multi, mean loss: 0.30848, multilabel_accuracy: 0.27100, avg. loss over tasks: 0.70906
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 51 
task: majority, mean loss: 0.09657, accuracy: 0.96700, task: max, mean loss: 0.14302, accuracy: 0.95250, task: top, mean loss: 0.10864, accuracy: 0.96850, task: multi, mean loss: 0.20212, multilabel_accuracy: 0.40500, avg. loss over tasks: 0.13758, lr: 0.0005700169639295527
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 51 
task: majority, mean loss: 0.56283, accuracy: 0.83400, task: max, mean loss: 0.85626, accuracy: 0.77400, task: top, mean loss: 0.89833, accuracy: 0.78300, task: multi, mean loss: 0.27964, multilabel_accuracy: 0.32000, avg. loss over tasks: 0.64926
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 52 
task: majority, mean loss: 0.11371, accuracy: 0.96100, task: max, mean loss: 0.12776, accuracy: 0.95850, task: top, mean loss: 0.09157, accuracy: 0.97400, task: multi, mean loss: 0.19688, multilabel_accuracy: 0.42850, avg. loss over tasks: 0.13248, lr: 0.0005527119674021931
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 52 
task: majority, mean loss: 0.49105, accuracy: 0.85200, task: max, mean loss: 0.86066, accuracy: 0.75800, task: top, mean loss: 0.97731, accuracy: 0.77300, task: multi, mean loss: 0.28681, multilabel_accuracy: 0.29400, avg. loss over tasks: 0.65396
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 53 
task: majority, mean loss: 0.09822, accuracy: 0.96400, task: max, mean loss: 0.12211, accuracy: 0.95950, task: top, mean loss: 0.11137, accuracy: 0.96700, task: multi, mean loss: 0.19198, multilabel_accuracy: 0.43100, avg. loss over tasks: 0.13092, lr: 0.0005353433586351906
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 53 
task: majority, mean loss: 0.54517, accuracy: 0.83600, task: max, mean loss: 0.77660, accuracy: 0.79700, task: top, mean loss: 0.99128, accuracy: 0.77600, task: multi, mean loss: 0.26730, multilabel_accuracy: 0.32500, avg. loss over tasks: 0.64509
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 54 
task: majority, mean loss: 0.10554, accuracy: 0.96450, task: max, mean loss: 0.12136, accuracy: 0.96400, task: top, mean loss: 0.10232, accuracy: 0.97200, task: multi, mean loss: 0.19132, multilabel_accuracy: 0.42900, avg. loss over tasks: 0.13013, lr: 0.0005179322986028993
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 54 
task: majority, mean loss: 0.41571, accuracy: 0.87600, task: max, mean loss: 0.79483, accuracy: 0.77200, task: top, mean loss: 0.88767, accuracy: 0.79500, task: multi, mean loss: 0.26339, multilabel_accuracy: 0.31900, avg. loss over tasks: 0.59040
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 55 
task: majority, mean loss: 0.09602, accuracy: 0.96500, task: max, mean loss: 0.09788, accuracy: 0.97200, task: top, mean loss: 0.08875, accuracy: 0.97050, task: multi, mean loss: 0.17956, multilabel_accuracy: 0.47000, avg. loss over tasks: 0.11555, lr: 0.0005004999999999999
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 55 
task: majority, mean loss: 0.37883, accuracy: 0.87800, task: max, mean loss: 0.79323, accuracy: 0.79200, task: top, mean loss: 0.93930, accuracy: 0.78300, task: multi, mean loss: 0.25267, multilabel_accuracy: 0.35600, avg. loss over tasks: 0.59101
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 56 
task: majority, mean loss: 0.08153, accuracy: 0.97350, task: max, mean loss: 0.08648, accuracy: 0.97700, task: top, mean loss: 0.07365, accuracy: 0.97450, task: multi, mean loss: 0.17776, multilabel_accuracy: 0.46450, avg. loss over tasks: 0.10485, lr: 0.00048306770139710083
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 56 
task: majority, mean loss: 0.40502, accuracy: 0.86700, task: max, mean loss: 0.92798, accuracy: 0.77200, task: top, mean loss: 0.94193, accuracy: 0.78200, task: multi, mean loss: 0.25803, multilabel_accuracy: 0.32600, avg. loss over tasks: 0.63324
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 57 
task: majority, mean loss: 0.07467, accuracy: 0.97500, task: max, mean loss: 0.08521, accuracy: 0.97600, task: top, mean loss: 0.06853, accuracy: 0.98200, task: multi, mean loss: 0.17093, multilabel_accuracy: 0.48900, avg. loss over tasks: 0.09983, lr: 0.0004656566413648094
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 57 
task: majority, mean loss: 0.45589, accuracy: 0.85000, task: max, mean loss: 0.82699, accuracy: 0.78900, task: top, mean loss: 0.94869, accuracy: 0.80000, task: multi, mean loss: 0.25782, multilabel_accuracy: 0.33200, avg. loss over tasks: 0.62235
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 58 
task: majority, mean loss: 0.07363, accuracy: 0.97700, task: max, mean loss: 0.07217, accuracy: 0.97950, task: top, mean loss: 0.06108, accuracy: 0.98150, task: multi, mean loss: 0.16975, multilabel_accuracy: 0.47600, avg. loss over tasks: 0.09416, lr: 0.0004482880325978072
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 58 
task: majority, mean loss: 0.34838, accuracy: 0.89100, task: max, mean loss: 0.85097, accuracy: 0.79600, task: top, mean loss: 0.94180, accuracy: 0.79000, task: multi, mean loss: 0.25658, multilabel_accuracy: 0.35800, avg. loss over tasks: 0.59943
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 59 
task: majority, mean loss: 0.06675, accuracy: 0.97800, task: max, mean loss: 0.07279, accuracy: 0.97700, task: top, mean loss: 0.05967, accuracy: 0.98350, task: multi, mean loss: 0.16906, multilabel_accuracy: 0.49250, avg. loss over tasks: 0.09207, lr: 0.0004309830360704473
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 59 
task: majority, mean loss: 0.34352, accuracy: 0.89200, task: max, mean loss: 0.85927, accuracy: 0.78700, task: top, mean loss: 0.86827, accuracy: 0.80100, task: multi, mean loss: 0.25553, multilabel_accuracy: 0.35400, avg. loss over tasks: 0.58165
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 60 
task: majority, mean loss: 0.05998, accuracy: 0.98050, task: max, mean loss: 0.07265, accuracy: 0.97700, task: top, mean loss: 0.04736, accuracy: 0.98500, task: multi, mean loss: 0.16259, multilabel_accuracy: 0.50400, avg. loss over tasks: 0.08565, lr: 0.00041376273525536844
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 60 
task: majority, mean loss: 0.40092, accuracy: 0.87500, task: max, mean loss: 0.84183, accuracy: 0.78300, task: top, mean loss: 0.93722, accuracy: 0.79700, task: multi, mean loss: 0.25788, multilabel_accuracy: 0.36700, avg. loss over tasks: 0.60946
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 61 
task: majority, mean loss: 0.05419, accuracy: 0.98350, task: max, mean loss: 0.05643, accuracy: 0.98150, task: top, mean loss: 0.05964, accuracy: 0.98100, task: multi, mean loss: 0.15511, multilabel_accuracy: 0.53150, avg. loss over tasks: 0.08134, lr: 0.00039664811043652916
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 61 
task: majority, mean loss: 0.31585, accuracy: 0.90000, task: max, mean loss: 0.85276, accuracy: 0.80000, task: top, mean loss: 0.90677, accuracy: 0.80000, task: multi, mean loss: 0.23578, multilabel_accuracy: 0.39600, avg. loss over tasks: 0.57779
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 62 
task: majority, mean loss: 0.05850, accuracy: 0.98100, task: max, mean loss: 0.04962, accuracy: 0.98650, task: top, mean loss: 0.04063, accuracy: 0.99000, task: multi, mean loss: 0.15433, multilabel_accuracy: 0.53800, avg. loss over tasks: 0.07577, lr: 0.00037966001314796593
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 62 
task: majority, mean loss: 0.35658, accuracy: 0.89800, task: max, mean loss: 0.87094, accuracy: 0.79700, task: top, mean loss: 0.91879, accuracy: 0.79400, task: multi, mean loss: 0.25773, multilabel_accuracy: 0.37900, avg. loss over tasks: 0.60101
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 63 
task: majority, mean loss: 0.05210, accuracy: 0.98400, task: max, mean loss: 0.05291, accuracy: 0.98550, task: top, mean loss: 0.03184, accuracy: 0.99250, task: multi, mean loss: 0.15146, multilabel_accuracy: 0.53750, avg. loss over tasks: 0.07208, lr: 0.00036281914076940894
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 63 
task: majority, mean loss: 0.32734, accuracy: 0.91700, task: max, mean loss: 0.83599, accuracy: 0.80800, task: top, mean loss: 0.86556, accuracy: 0.81100, task: multi, mean loss: 0.23962, multilabel_accuracy: 0.40800, avg. loss over tasks: 0.56713
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 64 
task: majority, mean loss: 0.04302, accuracy: 0.98450, task: max, mean loss: 0.04065, accuracy: 0.99000, task: top, mean loss: 0.02409, accuracy: 0.99450, task: multi, mean loss: 0.14630, multilabel_accuracy: 0.55700, avg. loss over tasks: 0.06352, lr: 0.00034614601130971383
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 64 
task: majority, mean loss: 0.31174, accuracy: 0.90500, task: max, mean loss: 0.86223, accuracy: 0.79200, task: top, mean loss: 0.94636, accuracy: 0.80300, task: multi, mean loss: 0.23993, multilabel_accuracy: 0.38500, avg. loss over tasks: 0.59006
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 65 
task: majority, mean loss: 0.05680, accuracy: 0.98050, task: max, mean loss: 0.05463, accuracy: 0.98300, task: top, mean loss: 0.03659, accuracy: 0.99000, task: multi, mean loss: 0.14685, multilabel_accuracy: 0.55200, avg. loss over tasks: 0.07372, lr: 0.0003296609384088285
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 65 
task: majority, mean loss: 0.29148, accuracy: 0.90500, task: max, mean loss: 0.81494, accuracy: 0.80600, task: top, mean loss: 0.88758, accuracy: 0.81000, task: multi, mean loss: 0.23563, multilabel_accuracy: 0.40000, avg. loss over tasks: 0.55741
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 66 
task: majority, mean loss: 0.04519, accuracy: 0.98650, task: max, mean loss: 0.03760, accuracy: 0.99100, task: top, mean loss: 0.03984, accuracy: 0.99000, task: multi, mean loss: 0.14173, multilabel_accuracy: 0.57200, avg. loss over tasks: 0.06609, lr: 0.00031338400658875205
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 66 
task: majority, mean loss: 0.25910, accuracy: 0.91100, task: max, mean loss: 0.82494, accuracy: 0.81500, task: top, mean loss: 0.87951, accuracy: 0.80600, task: multi, mean loss: 0.22656, multilabel_accuracy: 0.41100, avg. loss over tasks: 0.54753
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 67 
task: majority, mean loss: 0.03083, accuracy: 0.99000, task: max, mean loss: 0.03374, accuracy: 0.99000, task: top, mean loss: 0.03435, accuracy: 0.98800, task: multi, mean loss: 0.13576, multilabel_accuracy: 0.57650, avg. loss over tasks: 0.05867, lr: 0.00029733504678363786
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 67 
task: majority, mean loss: 0.27088, accuracy: 0.91700, task: max, mean loss: 0.90678, accuracy: 0.78700, task: top, mean loss: 0.95385, accuracy: 0.80500, task: multi, mean loss: 0.23441, multilabel_accuracy: 0.41500, avg. loss over tasks: 0.59148
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 68 
task: majority, mean loss: 0.02495, accuracy: 0.99300, task: max, mean loss: 0.02413, accuracy: 0.99550, task: top, mean loss: 0.02584, accuracy: 0.99250, task: multi, mean loss: 0.13351, multilabel_accuracy: 0.58900, avg. loss over tasks: 0.05211, lr: 0.0002815336121788558
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 68 
task: majority, mean loss: 0.26069, accuracy: 0.91500, task: max, mean loss: 0.80766, accuracy: 0.81100, task: top, mean loss: 0.88275, accuracy: 0.81200, task: multi, mean loss: 0.22493, multilabel_accuracy: 0.42600, avg. loss over tasks: 0.54401
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 69 
task: majority, mean loss: 0.02072, accuracy: 0.99550, task: max, mean loss: 0.02748, accuracy: 0.99250, task: top, mean loss: 0.01922, accuracy: 0.99450, task: multi, mean loss: 0.13102, multilabel_accuracy: 0.60050, avg. loss over tasks: 0.04961, lr: 0.0002659989543884475
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 69 
task: majority, mean loss: 0.28536, accuracy: 0.91200, task: max, mean loss: 0.84570, accuracy: 0.79400, task: top, mean loss: 0.90285, accuracy: 0.81000, task: multi, mean loss: 0.23097, multilabel_accuracy: 0.38500, avg. loss over tasks: 0.56622
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 70 
task: majority, mean loss: 0.02950, accuracy: 0.99050, task: max, mean loss: 0.03120, accuracy: 0.99150, task: top, mean loss: 0.02202, accuracy: 0.99600, task: multi, mean loss: 0.12856, multilabel_accuracy: 0.61600, avg. loss over tasks: 0.05282, lr: 0.0002507500000000001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 70 
task: majority, mean loss: 0.28174, accuracy: 0.91100, task: max, mean loss: 0.88686, accuracy: 0.78700, task: top, mean loss: 0.92712, accuracy: 0.81500, task: multi, mean loss: 0.22448, multilabel_accuracy: 0.41900, avg. loss over tasks: 0.58005
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 71 
task: majority, mean loss: 0.03828, accuracy: 0.98800, task: max, mean loss: 0.02596, accuracy: 0.99200, task: top, mean loss: 0.03145, accuracy: 0.99100, task: multi, mean loss: 0.12811, multilabel_accuracy: 0.61650, avg. loss over tasks: 0.05595, lr: 0.0002358053275155142
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 71 
task: majority, mean loss: 0.26705, accuracy: 0.91900, task: max, mean loss: 0.86941, accuracy: 0.80600, task: top, mean loss: 0.88327, accuracy: 0.82100, task: multi, mean loss: 0.22541, multilabel_accuracy: 0.43100, avg. loss over tasks: 0.56129
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 72 
task: majority, mean loss: 0.02892, accuracy: 0.99050, task: max, mean loss: 0.02516, accuracy: 0.99350, task: top, mean loss: 0.01461, accuracy: 0.99700, task: multi, mean loss: 0.12039, multilabel_accuracy: 0.62950, avg. loss over tasks: 0.04727, lr: 0.00022118314471636204
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 72 
task: majority, mean loss: 0.27810, accuracy: 0.92100, task: max, mean loss: 0.84770, accuracy: 0.81400, task: top, mean loss: 0.87958, accuracy: 0.81600, task: multi, mean loss: 0.22310, multilabel_accuracy: 0.44200, avg. loss over tasks: 0.55712
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 73 
task: majority, mean loss: 0.02453, accuracy: 0.99050, task: max, mean loss: 0.02279, accuracy: 0.99350, task: top, mean loss: 0.01974, accuracy: 0.99450, task: multi, mean loss: 0.11651, multilabel_accuracy: 0.63700, avg. loss over tasks: 0.04589, lr: 0.00020690126647990973
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 73 
task: majority, mean loss: 0.29686, accuracy: 0.91300, task: max, mean loss: 0.89142, accuracy: 0.80900, task: top, mean loss: 0.97993, accuracy: 0.80900, task: multi, mean loss: 0.23592, multilabel_accuracy: 0.39800, avg. loss over tasks: 0.60103
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 74 
task: majority, mean loss: 0.01934, accuracy: 0.99500, task: max, mean loss: 0.02085, accuracy: 0.99450, task: top, mean loss: 0.01452, accuracy: 0.99700, task: multi, mean loss: 0.12080, multilabel_accuracy: 0.62500, avg. loss over tasks: 0.04388, lr: 0.00019297709307483367
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 74 
task: majority, mean loss: 0.23094, accuracy: 0.92900, task: max, mean loss: 0.83582, accuracy: 0.81800, task: top, mean loss: 0.90030, accuracy: 0.82100, task: multi, mean loss: 0.21752, multilabel_accuracy: 0.44200, avg. loss over tasks: 0.54615
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 75 
task: majority, mean loss: 0.03071, accuracy: 0.98900, task: max, mean loss: 0.01925, accuracy: 0.99500, task: top, mean loss: 0.01779, accuracy: 0.99600, task: multi, mean loss: 0.11097, multilabel_accuracy: 0.64700, avg. loss over tasks: 0.04468, lr: 0.0001794275889615736
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 75 
task: majority, mean loss: 0.27389, accuracy: 0.90800, task: max, mean loss: 0.83907, accuracy: 0.81400, task: top, mean loss: 0.95413, accuracy: 0.81000, task: multi, mean loss: 0.21999, multilabel_accuracy: 0.43100, avg. loss over tasks: 0.57177
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 76 
task: majority, mean loss: 0.01955, accuracy: 0.99500, task: max, mean loss: 0.01366, accuracy: 0.99650, task: top, mean loss: 0.02168, accuracy: 0.99400, task: multi, mean loss: 0.11598, multilabel_accuracy: 0.63800, avg. loss over tasks: 0.04272, lr: 0.0001662692621237503
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 76 
task: majority, mean loss: 0.27762, accuracy: 0.91800, task: max, mean loss: 0.84682, accuracy: 0.81800, task: top, mean loss: 0.89797, accuracy: 0.81900, task: multi, mean loss: 0.22481, multilabel_accuracy: 0.44300, avg. loss over tasks: 0.56180
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 77 
task: majority, mean loss: 0.01942, accuracy: 0.99500, task: max, mean loss: 0.01528, accuracy: 0.99600, task: top, mean loss: 0.00976, accuracy: 0.99800, task: multi, mean loss: 0.10300, multilabel_accuracy: 0.67700, avg. loss over tasks: 0.03687, lr: 0.00015351814395573083
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 77 
task: majority, mean loss: 0.25952, accuracy: 0.92400, task: max, mean loss: 0.84752, accuracy: 0.81600, task: top, mean loss: 0.89837, accuracy: 0.82000, task: multi, mean loss: 0.21631, multilabel_accuracy: 0.45600, avg. loss over tasks: 0.55543
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 78 
task: majority, mean loss: 0.02508, accuracy: 0.99200, task: max, mean loss: 0.01256, accuracy: 0.99750, task: top, mean loss: 0.01015, accuracy: 0.99850, task: multi, mean loss: 0.10321, multilabel_accuracy: 0.67950, avg. loss over tasks: 0.03775, lr: 0.00014118976973084385
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 78 
task: majority, mean loss: 0.25685, accuracy: 0.92200, task: max, mean loss: 0.83011, accuracy: 0.81000, task: top, mean loss: 0.91709, accuracy: 0.82100, task: multi, mean loss: 0.21696, multilabel_accuracy: 0.44800, avg. loss over tasks: 0.55525
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 79 
task: majority, mean loss: 0.01815, accuracy: 0.99450, task: max, mean loss: 0.01616, accuracy: 0.99600, task: top, mean loss: 0.01301, accuracy: 0.99650, task: multi, mean loss: 0.10419, multilabel_accuracy: 0.68300, avg. loss over tasks: 0.03788, lr: 0.0001292991596740417
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 79 
task: majority, mean loss: 0.23088, accuracy: 0.92300, task: max, mean loss: 0.86291, accuracy: 0.81500, task: top, mean loss: 0.92293, accuracy: 0.81300, task: multi, mean loss: 0.21755, multilabel_accuracy: 0.44600, avg. loss over tasks: 0.55857
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 80 
task: majority, mean loss: 0.01376, accuracy: 0.99700, task: max, mean loss: 0.01611, accuracy: 0.99500, task: top, mean loss: 0.01717, accuracy: 0.99650, task: multi, mean loss: 0.10570, multilabel_accuracy: 0.67200, avg. loss over tasks: 0.03818, lr: 0.00011786080066207054
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 80 
task: majority, mean loss: 0.23717, accuracy: 0.92900, task: max, mean loss: 0.84802, accuracy: 0.82000, task: top, mean loss: 0.88418, accuracy: 0.83200, task: multi, mean loss: 0.21548, multilabel_accuracy: 0.45600, avg. loss over tasks: 0.54621
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 81 
task: majority, mean loss: 0.00991, accuracy: 0.99900, task: max, mean loss: 0.01091, accuracy: 0.99750, task: top, mean loss: 0.01329, accuracy: 0.99750, task: multi, mean loss: 0.09881, multilabel_accuracy: 0.70250, avg. loss over tasks: 0.03323, lr: 0.00010688862857344241
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 81 
task: majority, mean loss: 0.24096, accuracy: 0.92500, task: max, mean loss: 0.82399, accuracy: 0.81700, task: top, mean loss: 0.91733, accuracy: 0.81900, task: multi, mean loss: 0.21239, multilabel_accuracy: 0.46500, avg. loss over tasks: 0.54867
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 82 
task: majority, mean loss: 0.01315, accuracy: 0.99600, task: max, mean loss: 0.01129, accuracy: 0.99750, task: top, mean loss: 0.01156, accuracy: 0.99700, task: multi, mean loss: 0.09954, multilabel_accuracy: 0.68750, avg. loss over tasks: 0.03388, lr: 9.63960113097138e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 82 
task: majority, mean loss: 0.23090, accuracy: 0.92700, task: max, mean loss: 0.81532, accuracy: 0.82300, task: top, mean loss: 0.89813, accuracy: 0.82100, task: multi, mean loss: 0.21033, multilabel_accuracy: 0.46100, avg. loss over tasks: 0.53867
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 83 
task: majority, mean loss: 0.00940, accuracy: 0.99850, task: max, mean loss: 0.01277, accuracy: 0.99700, task: top, mean loss: 0.00651, accuracy: 0.99850, task: multi, mean loss: 0.09537, multilabel_accuracy: 0.71400, avg. loss over tasks: 0.03101, lr: 8.639573250875671e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 83 
task: majority, mean loss: 0.22609, accuracy: 0.93600, task: max, mean loss: 0.84976, accuracy: 0.81500, task: top, mean loss: 0.88301, accuracy: 0.82600, task: multi, mean loss: 0.21217, multilabel_accuracy: 0.45500, avg. loss over tasks: 0.54276
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 84 
task: majority, mean loss: 0.01012, accuracy: 0.99850, task: max, mean loss: 0.00715, accuracy: 0.99950, task: top, mean loss: 0.00971, accuracy: 0.99850, task: multi, mean loss: 0.09296, multilabel_accuracy: 0.72050, avg. loss over tasks: 0.02999, lr: 7.689997596986524e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 84 
task: majority, mean loss: 0.22009, accuracy: 0.93000, task: max, mean loss: 0.87752, accuracy: 0.80800, task: top, mean loss: 0.90618, accuracy: 0.82400, task: multi, mean loss: 0.21368, multilabel_accuracy: 0.45100, avg. loss over tasks: 0.55437
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 85 
task: majority, mean loss: 0.01598, accuracy: 0.99650, task: max, mean loss: 0.00988, accuracy: 0.99700, task: top, mean loss: 0.01502, accuracy: 0.99750, task: multi, mean loss: 0.09382, multilabel_accuracy: 0.72300, avg. loss over tasks: 0.03367, lr: 6.792031080967287e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 85 
task: majority, mean loss: 0.24762, accuracy: 0.92700, task: max, mean loss: 0.86489, accuracy: 0.82200, task: top, mean loss: 0.91814, accuracy: 0.82400, task: multi, mean loss: 0.21316, multilabel_accuracy: 0.46700, avg. loss over tasks: 0.56095
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 86 
task: majority, mean loss: 0.00985, accuracy: 0.99850, task: max, mean loss: 0.01013, accuracy: 0.99700, task: top, mean loss: 0.01074, accuracy: 0.99700, task: multi, mean loss: 0.09010, multilabel_accuracy: 0.72300, avg. loss over tasks: 0.03021, lr: 5.946767736696608e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 86 
task: majority, mean loss: 0.24168, accuracy: 0.92600, task: max, mean loss: 0.83254, accuracy: 0.82800, task: top, mean loss: 0.93628, accuracy: 0.82600, task: multi, mean loss: 0.21114, multilabel_accuracy: 0.45700, avg. loss over tasks: 0.55541
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 87 
task: majority, mean loss: 0.01116, accuracy: 0.99850, task: max, mean loss: 0.00823, accuracy: 0.99850, task: top, mean loss: 0.01252, accuracy: 0.99650, task: multi, mean loss: 0.09408, multilabel_accuracy: 0.71600, avg. loss over tasks: 0.03150, lr: 5.155237387356618e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 87 
task: majority, mean loss: 0.24468, accuracy: 0.91800, task: max, mean loss: 0.83477, accuracy: 0.82000, task: top, mean loss: 0.91914, accuracy: 0.81700, task: multi, mean loss: 0.21225, multilabel_accuracy: 0.45500, avg. loss over tasks: 0.55271
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 88 
task: majority, mean loss: 0.00757, accuracy: 0.99900, task: max, mean loss: 0.00599, accuracy: 0.99900, task: top, mean loss: 0.00517, accuracy: 0.99950, task: multi, mean loss: 0.08727, multilabel_accuracy: 0.73850, avg. loss over tasks: 0.02650, lr: 4.418404390752081e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 88 
task: majority, mean loss: 0.23579, accuracy: 0.92700, task: max, mean loss: 0.83427, accuracy: 0.82900, task: top, mean loss: 0.90170, accuracy: 0.82200, task: multi, mean loss: 0.21249, multilabel_accuracy: 0.46100, avg. loss over tasks: 0.54606
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 89 
task: majority, mean loss: 0.01145, accuracy: 0.99750, task: max, mean loss: 0.01170, accuracy: 0.99700, task: top, mean loss: 0.00635, accuracy: 0.99900, task: multi, mean loss: 0.08919, multilabel_accuracy: 0.72700, avg. loss over tasks: 0.02967, lr: 3.7371664643889735e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 89 
task: majority, mean loss: 0.23629, accuracy: 0.92700, task: max, mean loss: 0.84611, accuracy: 0.82700, task: top, mean loss: 0.92323, accuracy: 0.81900, task: multi, mean loss: 0.21291, multilabel_accuracy: 0.45900, avg. loss over tasks: 0.55464
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 90 
task: majority, mean loss: 0.01043, accuracy: 0.99800, task: max, mean loss: 0.01178, accuracy: 0.99800, task: top, mean loss: 0.00969, accuracy: 0.99700, task: multi, mean loss: 0.08796, multilabel_accuracy: 0.72350, avg. loss over tasks: 0.02996, lr: 3.11235359174388e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 90 
task: majority, mean loss: 0.22071, accuracy: 0.92600, task: max, mean loss: 0.83446, accuracy: 0.82000, task: top, mean loss: 0.92166, accuracy: 0.81900, task: multi, mean loss: 0.21044, multilabel_accuracy: 0.47400, avg. loss over tasks: 0.54682
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 91 
task: majority, mean loss: 0.01029, accuracy: 0.99800, task: max, mean loss: 0.00578, accuracy: 0.99850, task: top, mean loss: 0.00595, accuracy: 0.99900, task: multi, mean loss: 0.08561, multilabel_accuracy: 0.74650, avg. loss over tasks: 0.02691, lr: 2.544727011057081e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 91 
task: majority, mean loss: 0.22624, accuracy: 0.92600, task: max, mean loss: 0.83649, accuracy: 0.81700, task: top, mean loss: 0.92597, accuracy: 0.82000, task: multi, mean loss: 0.21090, multilabel_accuracy: 0.48100, avg. loss over tasks: 0.54990
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 92 
task: majority, mean loss: 0.00933, accuracy: 0.99800, task: max, mean loss: 0.00696, accuracy: 0.99850, task: top, mean loss: 0.00900, accuracy: 0.99750, task: multi, mean loss: 0.08581, multilabel_accuracy: 0.74050, avg. loss over tasks: 0.02777, lr: 2.0349782878809714e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 92 
task: majority, mean loss: 0.22183, accuracy: 0.93200, task: max, mean loss: 0.85522, accuracy: 0.81900, task: top, mean loss: 0.91838, accuracy: 0.81800, task: multi, mean loss: 0.21077, multilabel_accuracy: 0.47500, avg. loss over tasks: 0.55155
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 93 
task: majority, mean loss: 0.01083, accuracy: 0.99850, task: max, mean loss: 0.01055, accuracy: 0.99850, task: top, mean loss: 0.00535, accuracy: 0.99950, task: multi, mean loss: 0.08504, multilabel_accuracy: 0.74650, avg. loss over tasks: 0.02794, lr: 1.583728472513976e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 93 
task: majority, mean loss: 0.22704, accuracy: 0.92900, task: max, mean loss: 0.85846, accuracy: 0.82300, task: top, mean loss: 0.91937, accuracy: 0.82100, task: multi, mean loss: 0.21136, multilabel_accuracy: 0.47200, avg. loss over tasks: 0.55406
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 94 
task: majority, mean loss: 0.00851, accuracy: 0.99850, task: max, mean loss: 0.00808, accuracy: 0.99850, task: top, mean loss: 0.00886, accuracy: 0.99700, task: multi, mean loss: 0.08581, multilabel_accuracy: 0.74400, avg. loss over tasks: 0.02782, lr: 1.1915273433464114e-05
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 94 
task: majority, mean loss: 0.22464, accuracy: 0.93100, task: max, mean loss: 0.85356, accuracy: 0.82000, task: top, mean loss: 0.91829, accuracy: 0.82300, task: multi, mean loss: 0.20931, multilabel_accuracy: 0.46800, avg. loss over tasks: 0.55145
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 95 
task: majority, mean loss: 0.00808, accuracy: 0.99900, task: max, mean loss: 0.00417, accuracy: 1.00000, task: top, mean loss: 0.00458, accuracy: 0.99950, task: multi, mean loss: 0.08519, multilabel_accuracy: 0.74850, avg. loss over tasks: 0.02550, lr: 8.588527370402095e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 95 
task: majority, mean loss: 0.22172, accuracy: 0.92500, task: max, mean loss: 0.84724, accuracy: 0.81900, task: top, mean loss: 0.92257, accuracy: 0.82300, task: multi, mean loss: 0.20878, multilabel_accuracy: 0.46900, avg. loss over tasks: 0.55008
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 96 
task: majority, mean loss: 0.00831, accuracy: 0.99850, task: max, mean loss: 0.00787, accuracy: 0.99750, task: top, mean loss: 0.00971, accuracy: 0.99850, task: multi, mean loss: 0.08372, multilabel_accuracy: 0.75650, avg. loss over tasks: 0.02740, lr: 5.86109966358566e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 96 
task: majority, mean loss: 0.21865, accuracy: 0.92900, task: max, mean loss: 0.85757, accuracy: 0.82000, task: top, mean loss: 0.93121, accuracy: 0.82100, task: multi, mean loss: 0.21034, multilabel_accuracy: 0.46500, avg. loss over tasks: 0.55444
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 97 
task: majority, mean loss: 0.01084, accuracy: 0.99750, task: max, mean loss: 0.00894, accuracy: 0.99650, task: top, mean loss: 0.00617, accuracy: 0.99900, task: multi, mean loss: 0.08389, multilabel_accuracy: 0.75550, avg. loss over tasks: 0.02746, lr: 3.7363132635474912e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 97 
task: majority, mean loss: 0.22123, accuracy: 0.92500, task: max, mean loss: 0.85214, accuracy: 0.81600, task: top, mean loss: 0.92826, accuracy: 0.81900, task: multi, mean loss: 0.21030, multilabel_accuracy: 0.46900, avg. loss over tasks: 0.55298
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 98 
task: majority, mean loss: 0.01004, accuracy: 0.99800, task: max, mean loss: 0.00799, accuracy: 0.99900, task: top, mean loss: 0.00433, accuracy: 1.00000, task: multi, mean loss: 0.08660, multilabel_accuracy: 0.73000, avg. loss over tasks: 0.02724, lr: 2.2167568952178134e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 98 
task: majority, mean loss: 0.23044, accuracy: 0.92700, task: max, mean loss: 0.85238, accuracy: 0.81700, task: top, mean loss: 0.91365, accuracy: 0.81900, task: multi, mean loss: 0.21030, multilabel_accuracy: 0.47500, avg. loss over tasks: 0.55169
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 99 
task: majority, mean loss: 0.01027, accuracy: 0.99750, task: max, mean loss: 0.00443, accuracy: 0.99950, task: top, mean loss: 0.00541, accuracy: 0.99950, task: multi, mean loss: 0.08221, multilabel_accuracy: 0.75650, avg. loss over tasks: 0.02558, lr: 1.3042819039616668e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 99 
task: majority, mean loss: 0.23022, accuracy: 0.93100, task: max, mean loss: 0.85560, accuracy: 0.82000, task: top, mean loss: 0.91163, accuracy: 0.82200, task: multi, mean loss: 0.21042, multilabel_accuracy: 0.47900, avg. loss over tasks: 0.55197
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 100 
task: majority, mean loss: 0.00817, accuracy: 0.99900, task: max, mean loss: 0.00544, accuracy: 0.99950, task: top, mean loss: 0.00561, accuracy: 0.99900, task: multi, mean loss: 0.08177, multilabel_accuracy: 0.75950, avg. loss over tasks: 0.02525, lr: 1e-06
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 100 
task: majority, mean loss: 0.21980, accuracy: 0.93100, task: max, mean loss: 0.84881, accuracy: 0.81900, task: top, mean loss: 0.91506, accuracy: 0.82000, task: multi, mean loss: 0.20885, multilabel_accuracy: 0.47600, avg. loss over tasks: 0.54813
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

