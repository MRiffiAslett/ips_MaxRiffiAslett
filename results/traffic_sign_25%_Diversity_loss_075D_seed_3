Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09218, accuracy: 0.63587, avg. loss over tasks: 1.09218, lr: 3e-05
Diversity Loss - Mean: -0.01177, Variance: 0.01049
Semantic Loss - Mean: 1.43110, Variance: 0.07349

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17936, accuracy: 0.66272, avg. loss over tasks: 1.17936
Diversity Loss - Mean: -0.03403, Variance: 0.01258
Semantic Loss - Mean: 1.16101, Variance: 0.05311

Train Epoch: 2 
task: sign, mean loss: 0.96182, accuracy: 0.67391, avg. loss over tasks: 0.96182, lr: 6e-05
Diversity Loss - Mean: -0.02934, Variance: 0.01049
Semantic Loss - Mean: 0.98207, Variance: 0.03962

Test Epoch: 2 
task: sign, mean loss: 1.10012, accuracy: 0.65680, avg. loss over tasks: 1.10012
Diversity Loss - Mean: -0.04930, Variance: 0.01242
Semantic Loss - Mean: 1.13783, Variance: 0.03224

Train Epoch: 3 
task: sign, mean loss: 0.81047, accuracy: 0.70109, avg. loss over tasks: 0.81047, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.06163, Variance: 0.01043
Semantic Loss - Mean: 0.99312, Variance: 0.02755

Test Epoch: 3 
task: sign, mean loss: 1.27208, accuracy: 0.57396, avg. loss over tasks: 1.27208
Diversity Loss - Mean: -0.08098, Variance: 0.01168
Semantic Loss - Mean: 1.10759, Variance: 0.02942

Train Epoch: 4 
task: sign, mean loss: 0.74168, accuracy: 0.67391, avg. loss over tasks: 0.74168, lr: 0.00012
Diversity Loss - Mean: -0.08802, Variance: 0.01028
Semantic Loss - Mean: 0.88976, Variance: 0.02130

Test Epoch: 4 
task: sign, mean loss: 1.56869, accuracy: 0.47929, avg. loss over tasks: 1.56869
Diversity Loss - Mean: -0.08963, Variance: 0.01118
Semantic Loss - Mean: 1.10012, Variance: 0.02385

Train Epoch: 5 
task: sign, mean loss: 0.71963, accuracy: 0.70652, avg. loss over tasks: 0.71963, lr: 0.00015
Diversity Loss - Mean: -0.08281, Variance: 0.01009
Semantic Loss - Mean: 0.78294, Variance: 0.01741

Test Epoch: 5 
task: sign, mean loss: 1.65674, accuracy: 0.55030, avg. loss over tasks: 1.65674
Diversity Loss - Mean: -0.08806, Variance: 0.01083
Semantic Loss - Mean: 1.21890, Variance: 0.02211

Train Epoch: 6 
task: sign, mean loss: 0.68684, accuracy: 0.77174, avg. loss over tasks: 0.68684, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.07571, Variance: 0.01002
Semantic Loss - Mean: 0.70984, Variance: 0.01501

Test Epoch: 6 
task: sign, mean loss: 2.02740, accuracy: 0.65089, avg. loss over tasks: 2.02740
Diversity Loss - Mean: -0.06966, Variance: 0.01130
Semantic Loss - Mean: 1.46558, Variance: 0.02190

Train Epoch: 7 
task: sign, mean loss: 0.58731, accuracy: 0.77174, avg. loss over tasks: 0.58731, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.08133, Variance: 0.01015
Semantic Loss - Mean: 0.62023, Variance: 0.01317

Test Epoch: 7 
task: sign, mean loss: 2.06878, accuracy: 0.65680, avg. loss over tasks: 2.06878
Diversity Loss - Mean: -0.06977, Variance: 0.01153
Semantic Loss - Mean: 1.34998, Variance: 0.02153

Train Epoch: 8 
task: sign, mean loss: 0.55634, accuracy: 0.83696, avg. loss over tasks: 0.55634, lr: 0.00024
Diversity Loss - Mean: -0.04885, Variance: 0.01007
Semantic Loss - Mean: 0.62112, Variance: 0.01209

Test Epoch: 8 
task: sign, mean loss: 1.97180, accuracy: 0.52663, avg. loss over tasks: 1.97180
Diversity Loss - Mean: -0.05239, Variance: 0.01134
Semantic Loss - Mean: 1.45412, Variance: 0.02338

Train Epoch: 9 
task: sign, mean loss: 0.70669, accuracy: 0.74457, avg. loss over tasks: 0.70669, lr: 0.00027
Diversity Loss - Mean: -0.06007, Variance: 0.01002
Semantic Loss - Mean: 0.67402, Variance: 0.01122

Test Epoch: 9 
task: sign, mean loss: 2.96833, accuracy: 0.28402, avg. loss over tasks: 2.96833
Diversity Loss - Mean: -0.02203, Variance: 0.01110
Semantic Loss - Mean: 2.33747, Variance: 0.02459

Train Epoch: 10 
task: sign, mean loss: 0.86173, accuracy: 0.68478, avg. loss over tasks: 0.86173, lr: 0.0003
Diversity Loss - Mean: -0.07846, Variance: 0.01007
Semantic Loss - Mean: 0.77195, Variance: 0.01041

Test Epoch: 10 
task: sign, mean loss: 2.69314, accuracy: 0.22485, avg. loss over tasks: 2.69314
Diversity Loss - Mean: -0.03856, Variance: 0.01123
Semantic Loss - Mean: 2.09304, Variance: 0.02588

Train Epoch: 11 
task: sign, mean loss: 0.70877, accuracy: 0.71196, avg. loss over tasks: 0.70877, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.06904, Variance: 0.01009
Semantic Loss - Mean: 0.66598, Variance: 0.00978

Test Epoch: 11 
task: sign, mean loss: 2.09178, accuracy: 0.66272, avg. loss over tasks: 2.09178
Diversity Loss - Mean: -0.08200, Variance: 0.01135
Semantic Loss - Mean: 1.84133, Variance: 0.02540

Train Epoch: 12 
task: sign, mean loss: 0.53241, accuracy: 0.79348, avg. loss over tasks: 0.53241, lr: 0.000299849111021216
Diversity Loss - Mean: -0.07644, Variance: 0.01018
Semantic Loss - Mean: 0.57543, Variance: 0.00927

Test Epoch: 12 
task: sign, mean loss: 2.28977, accuracy: 0.37870, avg. loss over tasks: 2.28977
Diversity Loss - Mean: -0.04944, Variance: 0.01129
Semantic Loss - Mean: 1.81576, Variance: 0.02566

Train Epoch: 13 
task: sign, mean loss: 0.50508, accuracy: 0.82609, avg. loss over tasks: 0.50508, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.07652, Variance: 0.01026
Semantic Loss - Mean: 0.56502, Variance: 0.00902

Test Epoch: 13 
task: sign, mean loss: 1.53074, accuracy: 0.59172, avg. loss over tasks: 1.53074
Diversity Loss - Mean: -0.07513, Variance: 0.01138
Semantic Loss - Mean: 1.38904, Variance: 0.02633

Train Epoch: 14 
task: sign, mean loss: 0.27892, accuracy: 0.89674, avg. loss over tasks: 0.27892, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.06669, Variance: 0.01034
Semantic Loss - Mean: 0.33304, Variance: 0.00873

Test Epoch: 14 
task: sign, mean loss: 1.93644, accuracy: 0.44970, avg. loss over tasks: 1.93644
Diversity Loss - Mean: -0.04261, Variance: 0.01161
Semantic Loss - Mean: 1.73046, Variance: 0.03231

Train Epoch: 15 
task: sign, mean loss: 0.21840, accuracy: 0.91304, avg. loss over tasks: 0.21840, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.06079, Variance: 0.01040
Semantic Loss - Mean: 0.26315, Variance: 0.00841

Test Epoch: 15 
task: sign, mean loss: 1.40691, accuracy: 0.62722, avg. loss over tasks: 1.40691
Diversity Loss - Mean: -0.06941, Variance: 0.01178
Semantic Loss - Mean: 1.20554, Variance: 0.03287

Train Epoch: 16 
task: sign, mean loss: 0.16575, accuracy: 0.94022, avg. loss over tasks: 0.16575, lr: 0.000298643821800925
Diversity Loss - Mean: -0.06020, Variance: 0.01042
Semantic Loss - Mean: 0.23264, Variance: 0.00813

Test Epoch: 16 
task: sign, mean loss: 1.85619, accuracy: 0.71006, avg. loss over tasks: 1.85619
Diversity Loss - Mean: -0.09332, Variance: 0.01200
Semantic Loss - Mean: 1.61582, Variance: 0.03207

Train Epoch: 17 
task: sign, mean loss: 0.10776, accuracy: 0.95109, avg. loss over tasks: 0.10776, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.06942, Variance: 0.01046
Semantic Loss - Mean: 0.15002, Variance: 0.00794

Test Epoch: 17 
task: sign, mean loss: 1.64874, accuracy: 0.72189, avg. loss over tasks: 1.64874
Diversity Loss - Mean: -0.09273, Variance: 0.01216
Semantic Loss - Mean: 1.42747, Variance: 0.03204

Train Epoch: 18 
task: sign, mean loss: 0.20084, accuracy: 0.94565, avg. loss over tasks: 0.20084, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.07389, Variance: 0.01051
Semantic Loss - Mean: 0.23457, Variance: 0.00815

Test Epoch: 18 
task: sign, mean loss: 1.45660, accuracy: 0.62130, avg. loss over tasks: 1.45660
Diversity Loss - Mean: -0.07211, Variance: 0.01231
Semantic Loss - Mean: 1.20476, Variance: 0.03579

Train Epoch: 19 
task: sign, mean loss: 0.20888, accuracy: 0.91304, avg. loss over tasks: 0.20888, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.08134, Variance: 0.01056
Semantic Loss - Mean: 0.27010, Variance: 0.00810

Test Epoch: 19 
task: sign, mean loss: 2.62361, accuracy: 0.41420, avg. loss over tasks: 2.62361
Diversity Loss - Mean: -0.06356, Variance: 0.01227
Semantic Loss - Mean: 1.92451, Variance: 0.03857

Train Epoch: 20 
task: sign, mean loss: 0.22580, accuracy: 0.91304, avg. loss over tasks: 0.22580, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.08552, Variance: 0.01062
Semantic Loss - Mean: 0.26827, Variance: 0.00824

Test Epoch: 20 
task: sign, mean loss: 2.65902, accuracy: 0.37870, avg. loss over tasks: 2.65902
Diversity Loss - Mean: -0.02990, Variance: 0.01237
Semantic Loss - Mean: 2.23018, Variance: 0.04125

Train Epoch: 21 
task: sign, mean loss: 0.32407, accuracy: 0.86957, avg. loss over tasks: 0.32407, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.08188, Variance: 0.01070
Semantic Loss - Mean: 0.38164, Variance: 0.00843

Test Epoch: 21 
task: sign, mean loss: 1.35900, accuracy: 0.57396, avg. loss over tasks: 1.35900
Diversity Loss - Mean: -0.07230, Variance: 0.01253
Semantic Loss - Mean: 1.02552, Variance: 0.04245

Train Epoch: 22 
task: sign, mean loss: 0.41813, accuracy: 0.86413, avg. loss over tasks: 0.41813, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.09292, Variance: 0.01083
Semantic Loss - Mean: 0.51652, Variance: 0.00865

Test Epoch: 22 
task: sign, mean loss: 1.52730, accuracy: 0.60947, avg. loss over tasks: 1.52730
Diversity Loss - Mean: -0.07712, Variance: 0.01271
Semantic Loss - Mean: 1.26998, Variance: 0.04407

Train Epoch: 23 
task: sign, mean loss: 0.30254, accuracy: 0.87500, avg. loss over tasks: 0.30254, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.10272, Variance: 0.01098
Semantic Loss - Mean: 0.36252, Variance: 0.00883

Test Epoch: 23 
task: sign, mean loss: 0.95081, accuracy: 0.60355, avg. loss over tasks: 0.95081
Diversity Loss - Mean: -0.08903, Variance: 0.01284
Semantic Loss - Mean: 0.82295, Variance: 0.04321

Train Epoch: 24 
task: sign, mean loss: 0.25488, accuracy: 0.92391, avg. loss over tasks: 0.25488, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.10223, Variance: 0.01108
Semantic Loss - Mean: 0.31737, Variance: 0.00882

Test Epoch: 24 
task: sign, mean loss: 0.90577, accuracy: 0.71006, avg. loss over tasks: 0.90577
Diversity Loss - Mean: -0.07856, Variance: 0.01294
Semantic Loss - Mean: 0.94939, Variance: 0.04359

Train Epoch: 25 
task: sign, mean loss: 0.24638, accuracy: 0.92391, avg. loss over tasks: 0.24638, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.09633, Variance: 0.01113
Semantic Loss - Mean: 0.29959, Variance: 0.00903

Test Epoch: 25 
task: sign, mean loss: 1.61490, accuracy: 0.56213, avg. loss over tasks: 1.61490
Diversity Loss - Mean: -0.08729, Variance: 0.01296
Semantic Loss - Mean: 1.41319, Variance: 0.04385

Train Epoch: 26 
task: sign, mean loss: 0.32430, accuracy: 0.91304, avg. loss over tasks: 0.32430, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.10223, Variance: 0.01122
Semantic Loss - Mean: 0.36384, Variance: 0.00885

Test Epoch: 26 
task: sign, mean loss: 2.78195, accuracy: 0.50296, avg. loss over tasks: 2.78195
Diversity Loss - Mean: -0.08091, Variance: 0.01298
Semantic Loss - Mean: 2.42508, Variance: 0.04476

Train Epoch: 27 
task: sign, mean loss: 0.54447, accuracy: 0.80978, avg. loss over tasks: 0.54447, lr: 0.000289228031029578
Diversity Loss - Mean: -0.10441, Variance: 0.01133
Semantic Loss - Mean: 0.60058, Variance: 0.00885

Test Epoch: 27 
task: sign, mean loss: 3.37043, accuracy: 0.28994, avg. loss over tasks: 3.37043
Diversity Loss - Mean: -0.04013, Variance: 0.01298
Semantic Loss - Mean: 2.93162, Variance: 0.04854

Train Epoch: 28 
task: sign, mean loss: 0.46751, accuracy: 0.81522, avg. loss over tasks: 0.46751, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.10495, Variance: 0.01139
Semantic Loss - Mean: 0.50817, Variance: 0.00876

Test Epoch: 28 
task: sign, mean loss: 1.88438, accuracy: 0.62130, avg. loss over tasks: 1.88438
Diversity Loss - Mean: -0.11714, Variance: 0.01312
Semantic Loss - Mean: 1.68478, Variance: 0.04761

Train Epoch: 29 
task: sign, mean loss: 0.30479, accuracy: 0.88587, avg. loss over tasks: 0.30479, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.11270, Variance: 0.01148
Semantic Loss - Mean: 0.35742, Variance: 0.00856

Test Epoch: 29 
task: sign, mean loss: 1.97366, accuracy: 0.53254, avg. loss over tasks: 1.97366
Diversity Loss - Mean: -0.11950, Variance: 0.01321
Semantic Loss - Mean: 1.61866, Variance: 0.04673

Train Epoch: 30 
task: sign, mean loss: 0.25878, accuracy: 0.90761, avg. loss over tasks: 0.25878, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.10597, Variance: 0.01151
Semantic Loss - Mean: 0.27361, Variance: 0.00835

Test Epoch: 30 
task: sign, mean loss: 2.82759, accuracy: 0.37278, avg. loss over tasks: 2.82759
Diversity Loss - Mean: -0.10565, Variance: 0.01332
Semantic Loss - Mean: 2.43341, Variance: 0.04595

Train Epoch: 31 
task: sign, mean loss: 0.26349, accuracy: 0.92391, avg. loss over tasks: 0.26349, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.10413, Variance: 0.01155
Semantic Loss - Mean: 0.30703, Variance: 0.00826

Test Epoch: 31 
task: sign, mean loss: 2.15879, accuracy: 0.52663, avg. loss over tasks: 2.15879
Diversity Loss - Mean: -0.11788, Variance: 0.01347
Semantic Loss - Mean: 1.82473, Variance: 0.04642

Train Epoch: 32 
task: sign, mean loss: 0.18373, accuracy: 0.91848, avg. loss over tasks: 0.18373, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.10607, Variance: 0.01158
Semantic Loss - Mean: 0.22479, Variance: 0.00806

Test Epoch: 32 
task: sign, mean loss: 2.25144, accuracy: 0.51479, avg. loss over tasks: 2.25144
Diversity Loss - Mean: -0.11173, Variance: 0.01354
Semantic Loss - Mean: 1.99930, Variance: 0.04616

Train Epoch: 33 
task: sign, mean loss: 0.21164, accuracy: 0.92935, avg. loss over tasks: 0.21164, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.10769, Variance: 0.01160
Semantic Loss - Mean: 0.23720, Variance: 0.00791

Test Epoch: 33 
task: sign, mean loss: 2.78378, accuracy: 0.39645, avg. loss over tasks: 2.78378
Diversity Loss - Mean: -0.10594, Variance: 0.01362
Semantic Loss - Mean: 2.25785, Variance: 0.04571

Train Epoch: 34 
task: sign, mean loss: 0.17000, accuracy: 0.94022, avg. loss over tasks: 0.17000, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.11182, Variance: 0.01164
Semantic Loss - Mean: 0.20981, Variance: 0.00779

Test Epoch: 34 
task: sign, mean loss: 2.48068, accuracy: 0.46746, avg. loss over tasks: 2.48068
Diversity Loss - Mean: -0.11612, Variance: 0.01375
Semantic Loss - Mean: 1.94318, Variance: 0.04618

Train Epoch: 35 
task: sign, mean loss: 0.11366, accuracy: 0.96196, avg. loss over tasks: 0.11366, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.11279, Variance: 0.01168
Semantic Loss - Mean: 0.14691, Variance: 0.00772

Test Epoch: 35 
task: sign, mean loss: 2.17100, accuracy: 0.45562, avg. loss over tasks: 2.17100
Diversity Loss - Mean: -0.09688, Variance: 0.01389
Semantic Loss - Mean: 1.82377, Variance: 0.04600

Train Epoch: 36 
task: sign, mean loss: 0.08457, accuracy: 0.97283, avg. loss over tasks: 0.08457, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.11039, Variance: 0.01171
Semantic Loss - Mean: 0.11160, Variance: 0.00756

Test Epoch: 36 
task: sign, mean loss: 1.39896, accuracy: 0.54438, avg. loss over tasks: 1.39896
Diversity Loss - Mean: -0.10452, Variance: 0.01396
Semantic Loss - Mean: 1.31730, Variance: 0.04627

Train Epoch: 37 
task: sign, mean loss: 0.15085, accuracy: 0.92935, avg. loss over tasks: 0.15085, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.11229, Variance: 0.01177
Semantic Loss - Mean: 0.19413, Variance: 0.00754

Test Epoch: 37 
task: sign, mean loss: 1.57894, accuracy: 0.61538, avg. loss over tasks: 1.57894
Diversity Loss - Mean: -0.10867, Variance: 0.01402
Semantic Loss - Mean: 1.41492, Variance: 0.04684

Train Epoch: 38 
task: sign, mean loss: 0.13865, accuracy: 0.94565, avg. loss over tasks: 0.13865, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.10843, Variance: 0.01181
Semantic Loss - Mean: 0.24758, Variance: 0.00761

Test Epoch: 38 
task: sign, mean loss: 1.91509, accuracy: 0.50296, avg. loss over tasks: 1.91509
Diversity Loss - Mean: -0.09819, Variance: 0.01405
Semantic Loss - Mean: 1.77607, Variance: 0.04731

Train Epoch: 39 
task: sign, mean loss: 0.13664, accuracy: 0.95109, avg. loss over tasks: 0.13664, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.10710, Variance: 0.01183
Semantic Loss - Mean: 0.16306, Variance: 0.00749

Test Epoch: 39 
task: sign, mean loss: 1.23840, accuracy: 0.57396, avg. loss over tasks: 1.23840
Diversity Loss - Mean: -0.10239, Variance: 0.01413
Semantic Loss - Mean: 0.99259, Variance: 0.04723

Train Epoch: 40 
task: sign, mean loss: 0.04827, accuracy: 0.97826, avg. loss over tasks: 0.04827, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.11007, Variance: 0.01187
Semantic Loss - Mean: 0.09046, Variance: 0.00736

Test Epoch: 40 
task: sign, mean loss: 1.53306, accuracy: 0.71006, avg. loss over tasks: 1.53306
Diversity Loss - Mean: -0.11516, Variance: 0.01419
Semantic Loss - Mean: 1.29617, Variance: 0.04732

Train Epoch: 41 
task: sign, mean loss: 0.06038, accuracy: 0.97283, avg. loss over tasks: 0.06038, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.10760, Variance: 0.01189
Semantic Loss - Mean: 0.08425, Variance: 0.00724

Test Epoch: 41 
task: sign, mean loss: 1.49792, accuracy: 0.69231, avg. loss over tasks: 1.49792
Diversity Loss - Mean: -0.11489, Variance: 0.01426
Semantic Loss - Mean: 1.32100, Variance: 0.04861

Train Epoch: 42 
task: sign, mean loss: 0.09910, accuracy: 0.97826, avg. loss over tasks: 0.09910, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.11475, Variance: 0.01195
Semantic Loss - Mean: 0.09586, Variance: 0.00721

Test Epoch: 42 
task: sign, mean loss: 1.37941, accuracy: 0.60355, avg. loss over tasks: 1.37941
Diversity Loss - Mean: -0.10656, Variance: 0.01434
Semantic Loss - Mean: 1.48908, Variance: 0.04847

Train Epoch: 43 
task: sign, mean loss: 0.08303, accuracy: 0.97283, avg. loss over tasks: 0.08303, lr: 0.000260757131773478
Diversity Loss - Mean: -0.11139, Variance: 0.01197
Semantic Loss - Mean: 0.15411, Variance: 0.00717

Test Epoch: 43 
task: sign, mean loss: 1.89861, accuracy: 0.45562, avg. loss over tasks: 1.89861
Diversity Loss - Mean: -0.10201, Variance: 0.01441
Semantic Loss - Mean: 1.36638, Variance: 0.04920

Train Epoch: 44 
task: sign, mean loss: 0.03460, accuracy: 0.98370, avg. loss over tasks: 0.03460, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.11333, Variance: 0.01200
Semantic Loss - Mean: 0.08089, Variance: 0.00733

Test Epoch: 44 
task: sign, mean loss: 1.48830, accuracy: 0.63905, avg. loss over tasks: 1.48830
Diversity Loss - Mean: -0.11319, Variance: 0.01449
Semantic Loss - Mean: 1.21223, Variance: 0.04884

Train Epoch: 45 
task: sign, mean loss: 0.04278, accuracy: 0.98913, avg. loss over tasks: 0.04278, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.11197, Variance: 0.01202
Semantic Loss - Mean: 0.09017, Variance: 0.00739

Test Epoch: 45 
task: sign, mean loss: 1.13162, accuracy: 0.75740, avg. loss over tasks: 1.13162
Diversity Loss - Mean: -0.11692, Variance: 0.01456
Semantic Loss - Mean: 1.01715, Variance: 0.04925

Train Epoch: 46 
task: sign, mean loss: 0.02528, accuracy: 0.98913, avg. loss over tasks: 0.02528, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.11322, Variance: 0.01202
Semantic Loss - Mean: 0.04489, Variance: 0.00726

Test Epoch: 46 
task: sign, mean loss: 1.80046, accuracy: 0.56805, avg. loss over tasks: 1.80046
Diversity Loss - Mean: -0.10873, Variance: 0.01460
Semantic Loss - Mean: 1.77823, Variance: 0.05073

Train Epoch: 47 
task: sign, mean loss: 0.01598, accuracy: 1.00000, avg. loss over tasks: 0.01598, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.11414, Variance: 0.01202
Semantic Loss - Mean: 0.05416, Variance: 0.00715

Test Epoch: 47 
task: sign, mean loss: 1.57591, accuracy: 0.71006, avg. loss over tasks: 1.57591
Diversity Loss - Mean: -0.11499, Variance: 0.01464
Semantic Loss - Mean: 1.43364, Variance: 0.05138

Train Epoch: 48 
task: sign, mean loss: 0.04000, accuracy: 0.98370, avg. loss over tasks: 0.04000, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.11727, Variance: 0.01203
Semantic Loss - Mean: 0.07816, Variance: 0.00710

Test Epoch: 48 
task: sign, mean loss: 1.21013, accuracy: 0.74556, avg. loss over tasks: 1.21013
Diversity Loss - Mean: -0.11378, Variance: 0.01465
Semantic Loss - Mean: 1.10961, Variance: 0.05184

Train Epoch: 49 
task: sign, mean loss: 0.02488, accuracy: 0.99457, avg. loss over tasks: 0.02488, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.11852, Variance: 0.01203
Semantic Loss - Mean: 0.03127, Variance: 0.00699

Test Epoch: 49 
task: sign, mean loss: 1.09128, accuracy: 0.73373, avg. loss over tasks: 1.09128
Diversity Loss - Mean: -0.11892, Variance: 0.01466
Semantic Loss - Mean: 0.95455, Variance: 0.05147

Train Epoch: 50 
task: sign, mean loss: 0.01877, accuracy: 0.98913, avg. loss over tasks: 0.01877, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.12097, Variance: 0.01203
Semantic Loss - Mean: 0.04025, Variance: 0.00688

Test Epoch: 50 
task: sign, mean loss: 1.14826, accuracy: 0.72781, avg. loss over tasks: 1.14826
Diversity Loss - Mean: -0.12342, Variance: 0.01470
Semantic Loss - Mean: 1.09746, Variance: 0.05163

Train Epoch: 51 
task: sign, mean loss: 0.00513, accuracy: 1.00000, avg. loss over tasks: 0.00513, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.12139, Variance: 0.01202
Semantic Loss - Mean: 0.03411, Variance: 0.00678

Test Epoch: 51 
task: sign, mean loss: 1.15938, accuracy: 0.76331, avg. loss over tasks: 1.15938
Diversity Loss - Mean: -0.11889, Variance: 0.01470
Semantic Loss - Mean: 0.98470, Variance: 0.05116

Train Epoch: 52 
task: sign, mean loss: 0.00568, accuracy: 1.00000, avg. loss over tasks: 0.00568, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.12348, Variance: 0.01202
Semantic Loss - Mean: 0.02521, Variance: 0.00667

Test Epoch: 52 
task: sign, mean loss: 1.17028, accuracy: 0.77515, avg. loss over tasks: 1.17028
Diversity Loss - Mean: -0.12431, Variance: 0.01472
Semantic Loss - Mean: 0.93631, Variance: 0.05139

Train Epoch: 53 
task: sign, mean loss: 0.00559, accuracy: 1.00000, avg. loss over tasks: 0.00559, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.12380, Variance: 0.01203
Semantic Loss - Mean: 0.02494, Variance: 0.00656

Test Epoch: 53 
task: sign, mean loss: 1.21676, accuracy: 0.76923, avg. loss over tasks: 1.21676
Diversity Loss - Mean: -0.12646, Variance: 0.01475
Semantic Loss - Mean: 1.03171, Variance: 0.05118

Train Epoch: 54 
task: sign, mean loss: 0.00855, accuracy: 1.00000, avg. loss over tasks: 0.00855, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.12451, Variance: 0.01203
Semantic Loss - Mean: 0.02246, Variance: 0.00645

Test Epoch: 54 
task: sign, mean loss: 1.26877, accuracy: 0.73373, avg. loss over tasks: 1.26877
Diversity Loss - Mean: -0.12403, Variance: 0.01479
Semantic Loss - Mean: 1.28789, Variance: 0.05233

Train Epoch: 55 
task: sign, mean loss: 0.00448, accuracy: 1.00000, avg. loss over tasks: 0.00448, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.12554, Variance: 0.01203
Semantic Loss - Mean: 0.01947, Variance: 0.00638

Test Epoch: 55 
task: sign, mean loss: 1.32819, accuracy: 0.76331, avg. loss over tasks: 1.32819
Diversity Loss - Mean: -0.12733, Variance: 0.01483
Semantic Loss - Mean: 1.21522, Variance: 0.05265

Train Epoch: 56 
task: sign, mean loss: 0.00284, accuracy: 1.00000, avg. loss over tasks: 0.00284, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.12654, Variance: 0.01203
Semantic Loss - Mean: 0.01195, Variance: 0.00627

Test Epoch: 56 
task: sign, mean loss: 1.41755, accuracy: 0.75148, avg. loss over tasks: 1.41755
Diversity Loss - Mean: -0.12734, Variance: 0.01486
Semantic Loss - Mean: 1.35630, Variance: 0.05344

Train Epoch: 57 
task: sign, mean loss: 0.00745, accuracy: 0.99457, avg. loss over tasks: 0.00745, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.12852, Variance: 0.01203
Semantic Loss - Mean: 0.02021, Variance: 0.00618

Test Epoch: 57 
task: sign, mean loss: 1.43405, accuracy: 0.69822, avg. loss over tasks: 1.43405
Diversity Loss - Mean: -0.12715, Variance: 0.01489
Semantic Loss - Mean: 1.46726, Variance: 0.05445

Train Epoch: 58 
task: sign, mean loss: 0.00134, accuracy: 1.00000, avg. loss over tasks: 0.00134, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.12989, Variance: 0.01204
Semantic Loss - Mean: 0.01048, Variance: 0.00608

Test Epoch: 58 
task: sign, mean loss: 1.46841, accuracy: 0.71006, avg. loss over tasks: 1.46841
Diversity Loss - Mean: -0.12955, Variance: 0.01493
Semantic Loss - Mean: 1.39103, Variance: 0.05424

Train Epoch: 59 
task: sign, mean loss: 0.01099, accuracy: 1.00000, avg. loss over tasks: 0.01099, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.13051, Variance: 0.01205
Semantic Loss - Mean: 0.02962, Variance: 0.00602

Test Epoch: 59 
task: sign, mean loss: 1.27257, accuracy: 0.73373, avg. loss over tasks: 1.27257
Diversity Loss - Mean: -0.12990, Variance: 0.01496
Semantic Loss - Mean: 1.23225, Variance: 0.05563

Train Epoch: 60 
task: sign, mean loss: 0.01514, accuracy: 0.98913, avg. loss over tasks: 0.01514, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.12953, Variance: 0.01206
Semantic Loss - Mean: 0.03940, Variance: 0.00594

Test Epoch: 60 
task: sign, mean loss: 0.88101, accuracy: 0.75148, avg. loss over tasks: 0.88101
Diversity Loss - Mean: -0.12841, Variance: 0.01498
Semantic Loss - Mean: 0.91733, Variance: 0.05626

Train Epoch: 61 
task: sign, mean loss: 0.00496, accuracy: 1.00000, avg. loss over tasks: 0.00496, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.12962, Variance: 0.01206
Semantic Loss - Mean: 0.03609, Variance: 0.00588

Test Epoch: 61 
task: sign, mean loss: 1.14152, accuracy: 0.71006, avg. loss over tasks: 1.14152
Diversity Loss - Mean: -0.12815, Variance: 0.01501
Semantic Loss - Mean: 1.08200, Variance: 0.05750

Train Epoch: 62 
task: sign, mean loss: 0.00489, accuracy: 1.00000, avg. loss over tasks: 0.00489, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.12931, Variance: 0.01206
Semantic Loss - Mean: 0.02902, Variance: 0.00583

Test Epoch: 62 
task: sign, mean loss: 1.10726, accuracy: 0.73373, avg. loss over tasks: 1.10726
Diversity Loss - Mean: -0.12932, Variance: 0.01503
Semantic Loss - Mean: 0.99894, Variance: 0.05864

Train Epoch: 63 
task: sign, mean loss: 0.04411, accuracy: 0.99457, avg. loss over tasks: 0.04411, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.12915, Variance: 0.01206
Semantic Loss - Mean: 0.05412, Variance: 0.00582

Test Epoch: 63 
task: sign, mean loss: 1.56894, accuracy: 0.62722, avg. loss over tasks: 1.56894
Diversity Loss - Mean: -0.12793, Variance: 0.01507
Semantic Loss - Mean: 1.60133, Variance: 0.06234

Train Epoch: 64 
task: sign, mean loss: 0.00466, accuracy: 1.00000, avg. loss over tasks: 0.00466, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.13020, Variance: 0.01208
Semantic Loss - Mean: 0.05390, Variance: 0.00602

Test Epoch: 64 
task: sign, mean loss: 1.35264, accuracy: 0.66272, avg. loss over tasks: 1.35264
Diversity Loss - Mean: -0.12509, Variance: 0.01512
Semantic Loss - Mean: 1.59953, Variance: 0.06808

Train Epoch: 65 
task: sign, mean loss: 0.07361, accuracy: 0.95652, avg. loss over tasks: 0.07361, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.13016, Variance: 0.01210
Semantic Loss - Mean: 0.12006, Variance: 0.00607

Test Epoch: 65 
task: sign, mean loss: 1.72193, accuracy: 0.49704, avg. loss over tasks: 1.72193
Diversity Loss - Mean: -0.12403, Variance: 0.01514
Semantic Loss - Mean: 2.10659, Variance: 0.07827

Train Epoch: 66 
task: sign, mean loss: 0.07818, accuracy: 0.97283, avg. loss over tasks: 0.07818, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.12811, Variance: 0.01211
Semantic Loss - Mean: 0.16645, Variance: 0.00627

Test Epoch: 66 
task: sign, mean loss: 0.59201, accuracy: 0.81657, avg. loss over tasks: 0.59201
Diversity Loss - Mean: -0.12618, Variance: 0.01517
Semantic Loss - Mean: 0.63008, Variance: 0.07790

Train Epoch: 67 
task: sign, mean loss: 0.08779, accuracy: 0.96739, avg. loss over tasks: 0.08779, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.12890, Variance: 0.01213
Semantic Loss - Mean: 0.12205, Variance: 0.00639

Test Epoch: 67 
task: sign, mean loss: 1.02759, accuracy: 0.81065, avg. loss over tasks: 1.02759
Diversity Loss - Mean: -0.12436, Variance: 0.01518
Semantic Loss - Mean: 1.04780, Variance: 0.07834

Train Epoch: 68 
task: sign, mean loss: 0.30927, accuracy: 0.92935, avg. loss over tasks: 0.30927, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.12993, Variance: 0.01214
Semantic Loss - Mean: 0.31995, Variance: 0.00672

Test Epoch: 68 
task: sign, mean loss: 1.23786, accuracy: 0.74556, avg. loss over tasks: 1.23786
Diversity Loss - Mean: -0.12868, Variance: 0.01521
Semantic Loss - Mean: 1.04214, Variance: 0.07939

Train Epoch: 69 
task: sign, mean loss: 0.13849, accuracy: 0.92935, avg. loss over tasks: 0.13849, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.12705, Variance: 0.01214
Semantic Loss - Mean: 0.24974, Variance: 0.00676

Test Epoch: 69 
task: sign, mean loss: 1.20369, accuracy: 0.63314, avg. loss over tasks: 1.20369
Diversity Loss - Mean: -0.12325, Variance: 0.01522
Semantic Loss - Mean: 1.19342, Variance: 0.08054

Train Epoch: 70 
task: sign, mean loss: 0.03598, accuracy: 0.99457, avg. loss over tasks: 0.03598, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.12872, Variance: 0.01215
Semantic Loss - Mean: 0.07112, Variance: 0.00673

Test Epoch: 70 
task: sign, mean loss: 1.29094, accuracy: 0.57396, avg. loss over tasks: 1.29094
Diversity Loss - Mean: -0.12029, Variance: 0.01521
Semantic Loss - Mean: 1.13735, Variance: 0.08068

Train Epoch: 71 
task: sign, mean loss: 0.10491, accuracy: 0.97826, avg. loss over tasks: 0.10491, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.12809, Variance: 0.01217
Semantic Loss - Mean: 0.16808, Variance: 0.00675

Test Epoch: 71 
task: sign, mean loss: 1.53792, accuracy: 0.47929, avg. loss over tasks: 1.53792
Diversity Loss - Mean: -0.11799, Variance: 0.01519
Semantic Loss - Mean: 1.58226, Variance: 0.08345

Train Epoch: 72 
task: sign, mean loss: 0.03945, accuracy: 0.98913, avg. loss over tasks: 0.03945, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.12621, Variance: 0.01218
Semantic Loss - Mean: 0.10538, Variance: 0.00675

Test Epoch: 72 
task: sign, mean loss: 0.66414, accuracy: 0.74556, avg. loss over tasks: 0.66414
Diversity Loss - Mean: -0.11887, Variance: 0.01520
Semantic Loss - Mean: 0.80229, Variance: 0.08365

Train Epoch: 73 
task: sign, mean loss: 0.04970, accuracy: 0.97283, avg. loss over tasks: 0.04970, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.12608, Variance: 0.01218
Semantic Loss - Mean: 0.12156, Variance: 0.00689

Test Epoch: 73 
task: sign, mean loss: 0.66406, accuracy: 0.77515, avg. loss over tasks: 0.66406
Diversity Loss - Mean: -0.12097, Variance: 0.01522
Semantic Loss - Mean: 0.65689, Variance: 0.08297

Train Epoch: 74 
task: sign, mean loss: 0.16807, accuracy: 0.95652, avg. loss over tasks: 0.16807, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.12395, Variance: 0.01219
Semantic Loss - Mean: 0.26596, Variance: 0.00701

Test Epoch: 74 
task: sign, mean loss: 1.23702, accuracy: 0.72189, avg. loss over tasks: 1.23702
Diversity Loss - Mean: -0.11627, Variance: 0.01522
Semantic Loss - Mean: 1.33031, Variance: 0.08270

Train Epoch: 75 
task: sign, mean loss: 0.22100, accuracy: 0.94022, avg. loss over tasks: 0.22100, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.12542, Variance: 0.01220
Semantic Loss - Mean: 0.31597, Variance: 0.00760

Test Epoch: 75 
task: sign, mean loss: 1.35900, accuracy: 0.77515, avg. loss over tasks: 1.35900
Diversity Loss - Mean: -0.12593, Variance: 0.01520
Semantic Loss - Mean: 1.18421, Variance: 0.08237

Train Epoch: 76 
task: sign, mean loss: 0.37813, accuracy: 0.88587, avg. loss over tasks: 0.37813, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.12642, Variance: 0.01223
Semantic Loss - Mean: 0.45064, Variance: 0.00795

Test Epoch: 76 
task: sign, mean loss: 1.63076, accuracy: 0.44379, avg. loss over tasks: 1.63076
Diversity Loss - Mean: -0.11666, Variance: 0.01522
Semantic Loss - Mean: 2.25125, Variance: 0.08454

Train Epoch: 77 
task: sign, mean loss: 0.07964, accuracy: 0.97283, avg. loss over tasks: 0.07964, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.12556, Variance: 0.01224
Semantic Loss - Mean: 0.15126, Variance: 0.00801

Test Epoch: 77 
task: sign, mean loss: 0.99946, accuracy: 0.62130, avg. loss over tasks: 0.99946
Diversity Loss - Mean: -0.12087, Variance: 0.01525
Semantic Loss - Mean: 1.26967, Variance: 0.08548

Train Epoch: 78 
task: sign, mean loss: 0.05299, accuracy: 0.99457, avg. loss over tasks: 0.05299, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.12694, Variance: 0.01225
Semantic Loss - Mean: 0.13024, Variance: 0.00812

Test Epoch: 78 
task: sign, mean loss: 1.02398, accuracy: 0.55621, avg. loss over tasks: 1.02398
Diversity Loss - Mean: -0.12167, Variance: 0.01529
Semantic Loss - Mean: 1.04966, Variance: 0.08513

Train Epoch: 79 
task: sign, mean loss: 0.03155, accuracy: 0.99457, avg. loss over tasks: 0.03155, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.12672, Variance: 0.01226
Semantic Loss - Mean: 0.07694, Variance: 0.00810

Test Epoch: 79 
task: sign, mean loss: 0.63113, accuracy: 0.81657, avg. loss over tasks: 0.63113
Diversity Loss - Mean: -0.12407, Variance: 0.01532
Semantic Loss - Mean: 0.71964, Variance: 0.08461

Train Epoch: 80 
task: sign, mean loss: 0.03536, accuracy: 0.98370, avg. loss over tasks: 0.03536, lr: 0.00015015
Diversity Loss - Mean: -0.12720, Variance: 0.01226
Semantic Loss - Mean: 0.08016, Variance: 0.00809

Test Epoch: 80 
task: sign, mean loss: 0.61989, accuracy: 0.84615, avg. loss over tasks: 0.61989
Diversity Loss - Mean: -0.12337, Variance: 0.01535
Semantic Loss - Mean: 0.73732, Variance: 0.08412

Train Epoch: 81 
task: sign, mean loss: 0.19915, accuracy: 0.97283, avg. loss over tasks: 0.19915, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.12759, Variance: 0.01226
Semantic Loss - Mean: 0.20875, Variance: 0.00817

Test Epoch: 81 
task: sign, mean loss: 0.61617, accuracy: 0.82249, avg. loss over tasks: 0.61617
Diversity Loss - Mean: -0.12019, Variance: 0.01538
Semantic Loss - Mean: 1.01745, Variance: 0.08505

Train Epoch: 82 
task: sign, mean loss: 0.04719, accuracy: 0.97283, avg. loss over tasks: 0.04719, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.12887, Variance: 0.01227
Semantic Loss - Mean: 0.09607, Variance: 0.00817

Test Epoch: 82 
task: sign, mean loss: 0.83696, accuracy: 0.79290, avg. loss over tasks: 0.83696
Diversity Loss - Mean: -0.12517, Variance: 0.01540
Semantic Loss - Mean: 1.12780, Variance: 0.08536

Train Epoch: 83 
task: sign, mean loss: 0.07414, accuracy: 0.96739, avg. loss over tasks: 0.07414, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12995, Variance: 0.01227
Semantic Loss - Mean: 0.14115, Variance: 0.00824

Test Epoch: 83 
task: sign, mean loss: 1.16860, accuracy: 0.77515, avg. loss over tasks: 1.16860
Diversity Loss - Mean: -0.12980, Variance: 0.01541
Semantic Loss - Mean: 1.16531, Variance: 0.08496

Train Epoch: 84 
task: sign, mean loss: 0.07513, accuracy: 0.97826, avg. loss over tasks: 0.07513, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.12741, Variance: 0.01227
Semantic Loss - Mean: 0.13789, Variance: 0.00825

Test Epoch: 84 
task: sign, mean loss: 0.57364, accuracy: 0.84024, avg. loss over tasks: 0.57364
Diversity Loss - Mean: -0.12899, Variance: 0.01543
Semantic Loss - Mean: 0.70253, Variance: 0.08438

Train Epoch: 85 
task: sign, mean loss: 0.01222, accuracy: 1.00000, avg. loss over tasks: 0.01222, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.12913, Variance: 0.01227
Semantic Loss - Mean: 0.05496, Variance: 0.00821

Test Epoch: 85 
task: sign, mean loss: 0.59457, accuracy: 0.79290, avg. loss over tasks: 0.59457
Diversity Loss - Mean: -0.12703, Variance: 0.01546
Semantic Loss - Mean: 0.89294, Variance: 0.08413

Train Epoch: 86 
task: sign, mean loss: 0.02428, accuracy: 0.99457, avg. loss over tasks: 0.02428, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.12921, Variance: 0.01228
Semantic Loss - Mean: 0.06620, Variance: 0.00819

Test Epoch: 86 
task: sign, mean loss: 0.90721, accuracy: 0.65680, avg. loss over tasks: 0.90721
Diversity Loss - Mean: -0.12543, Variance: 0.01549
Semantic Loss - Mean: 1.49941, Variance: 0.08407

Train Epoch: 87 
task: sign, mean loss: 0.00723, accuracy: 1.00000, avg. loss over tasks: 0.00723, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.12946, Variance: 0.01228
Semantic Loss - Mean: 0.04273, Variance: 0.00814

Test Epoch: 87 
task: sign, mean loss: 0.82912, accuracy: 0.76923, avg. loss over tasks: 0.82912
Diversity Loss - Mean: -0.12722, Variance: 0.01551
Semantic Loss - Mean: 1.05389, Variance: 0.08336

Train Epoch: 88 
task: sign, mean loss: 0.00673, accuracy: 1.00000, avg. loss over tasks: 0.00673, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.13105, Variance: 0.01228
Semantic Loss - Mean: 0.03468, Variance: 0.00810

Test Epoch: 88 
task: sign, mean loss: 0.74795, accuracy: 0.78698, avg. loss over tasks: 0.74795
Diversity Loss - Mean: -0.12832, Variance: 0.01552
Semantic Loss - Mean: 0.88993, Variance: 0.08273

Train Epoch: 89 
task: sign, mean loss: 0.01058, accuracy: 1.00000, avg. loss over tasks: 0.01058, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.13059, Variance: 0.01229
Semantic Loss - Mean: 0.04393, Variance: 0.00804

Test Epoch: 89 
task: sign, mean loss: 0.67420, accuracy: 0.81065, avg. loss over tasks: 0.67420
Diversity Loss - Mean: -0.12853, Variance: 0.01553
Semantic Loss - Mean: 0.78193, Variance: 0.08212

Train Epoch: 90 
task: sign, mean loss: 0.00439, accuracy: 1.00000, avg. loss over tasks: 0.00439, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.13107, Variance: 0.01230
Semantic Loss - Mean: 0.03298, Variance: 0.00799

Test Epoch: 90 
task: sign, mean loss: 0.72148, accuracy: 0.82249, avg. loss over tasks: 0.72148
Diversity Loss - Mean: -0.12731, Variance: 0.01554
Semantic Loss - Mean: 0.79483, Variance: 0.08151

Train Epoch: 91 
task: sign, mean loss: 0.02096, accuracy: 0.99457, avg. loss over tasks: 0.02096, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.13128, Variance: 0.01230
Semantic Loss - Mean: 0.04008, Variance: 0.00794

Test Epoch: 91 
task: sign, mean loss: 0.74273, accuracy: 0.81065, avg. loss over tasks: 0.74273
Diversity Loss - Mean: -0.12729, Variance: 0.01554
Semantic Loss - Mean: 0.88936, Variance: 0.08085

Train Epoch: 92 
task: sign, mean loss: 0.00456, accuracy: 1.00000, avg. loss over tasks: 0.00456, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.13049, Variance: 0.01230
Semantic Loss - Mean: 0.04522, Variance: 0.00791

Test Epoch: 92 
task: sign, mean loss: 0.82421, accuracy: 0.79290, avg. loss over tasks: 0.82421
Diversity Loss - Mean: -0.12754, Variance: 0.01555
Semantic Loss - Mean: 1.04854, Variance: 0.08050

Train Epoch: 93 
task: sign, mean loss: 0.00424, accuracy: 1.00000, avg. loss over tasks: 0.00424, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.13146, Variance: 0.01229
Semantic Loss - Mean: 0.03149, Variance: 0.00788

Test Epoch: 93 
task: sign, mean loss: 0.77648, accuracy: 0.81657, avg. loss over tasks: 0.77648
Diversity Loss - Mean: -0.12908, Variance: 0.01556
Semantic Loss - Mean: 0.86576, Variance: 0.07995

Train Epoch: 94 
task: sign, mean loss: 0.00311, accuracy: 1.00000, avg. loss over tasks: 0.00311, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.13138, Variance: 0.01229
Semantic Loss - Mean: 0.02547, Variance: 0.00783

Test Epoch: 94 
task: sign, mean loss: 0.83332, accuracy: 0.81065, avg. loss over tasks: 0.83332
Diversity Loss - Mean: -0.12852, Variance: 0.01556
Semantic Loss - Mean: 1.02633, Variance: 0.08008

Train Epoch: 95 
task: sign, mean loss: 0.00215, accuracy: 1.00000, avg. loss over tasks: 0.00215, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.13203, Variance: 0.01229
Semantic Loss - Mean: 0.02321, Variance: 0.00776

Test Epoch: 95 
task: sign, mean loss: 0.87545, accuracy: 0.79290, avg. loss over tasks: 0.87545
Diversity Loss - Mean: -0.12743, Variance: 0.01556
Semantic Loss - Mean: 1.19048, Variance: 0.08075

Train Epoch: 96 
task: sign, mean loss: 0.00370, accuracy: 1.00000, avg. loss over tasks: 0.00370, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.13215, Variance: 0.01229
Semantic Loss - Mean: 0.03237, Variance: 0.00772

Test Epoch: 96 
task: sign, mean loss: 0.76973, accuracy: 0.79290, avg. loss over tasks: 0.76973
Diversity Loss - Mean: -0.12854, Variance: 0.01557
Semantic Loss - Mean: 1.01964, Variance: 0.08045

Train Epoch: 97 
task: sign, mean loss: 0.00378, accuracy: 1.00000, avg. loss over tasks: 0.00378, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.13213, Variance: 0.01229
Semantic Loss - Mean: 0.03032, Variance: 0.00767

Test Epoch: 97 
task: sign, mean loss: 0.87760, accuracy: 0.79882, avg. loss over tasks: 0.87760
Diversity Loss - Mean: -0.12938, Variance: 0.01558
Semantic Loss - Mean: 1.05800, Variance: 0.07997

Train Epoch: 98 
task: sign, mean loss: 0.00447, accuracy: 1.00000, avg. loss over tasks: 0.00447, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.13316, Variance: 0.01229
Semantic Loss - Mean: 0.02142, Variance: 0.00761

Test Epoch: 98 
task: sign, mean loss: 0.85274, accuracy: 0.80473, avg. loss over tasks: 0.85274
Diversity Loss - Mean: -0.13031, Variance: 0.01559
Semantic Loss - Mean: 1.01094, Variance: 0.07953

Train Epoch: 99 
task: sign, mean loss: 0.00337, accuracy: 1.00000, avg. loss over tasks: 0.00337, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.13314, Variance: 0.01229
Semantic Loss - Mean: 0.01627, Variance: 0.00755

Test Epoch: 99 
task: sign, mean loss: 0.80530, accuracy: 0.79290, avg. loss over tasks: 0.80530
Diversity Loss - Mean: -0.13042, Variance: 0.01560
Semantic Loss - Mean: 1.01819, Variance: 0.07897

Train Epoch: 100 
task: sign, mean loss: 0.00741, accuracy: 1.00000, avg. loss over tasks: 0.00741, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.13299, Variance: 0.01229
Semantic Loss - Mean: 0.03535, Variance: 0.00751

Test Epoch: 100 
task: sign, mean loss: 0.72639, accuracy: 0.81065, avg. loss over tasks: 0.72639
Diversity Loss - Mean: -0.13006, Variance: 0.01561
Semantic Loss - Mean: 1.01835, Variance: 0.07841

Train Epoch: 101 
task: sign, mean loss: 0.00155, accuracy: 1.00000, avg. loss over tasks: 0.00155, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.13338, Variance: 0.01229
Semantic Loss - Mean: 0.02815, Variance: 0.00746

Test Epoch: 101 
task: sign, mean loss: 0.86408, accuracy: 0.81065, avg. loss over tasks: 0.86408
Diversity Loss - Mean: -0.13006, Variance: 0.01562
Semantic Loss - Mean: 1.26367, Variance: 0.07812

Train Epoch: 102 
task: sign, mean loss: 0.00153, accuracy: 1.00000, avg. loss over tasks: 0.00153, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.13323, Variance: 0.01229
Semantic Loss - Mean: 0.02067, Variance: 0.00742

Test Epoch: 102 
task: sign, mean loss: 0.90823, accuracy: 0.79882, avg. loss over tasks: 0.90823
Diversity Loss - Mean: -0.12964, Variance: 0.01564
Semantic Loss - Mean: 1.37950, Variance: 0.07809

Train Epoch: 103 
task: sign, mean loss: 0.00245, accuracy: 1.00000, avg. loss over tasks: 0.00245, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.13309, Variance: 0.01229
Semantic Loss - Mean: 0.02039, Variance: 0.00736

Test Epoch: 103 
task: sign, mean loss: 0.94312, accuracy: 0.81065, avg. loss over tasks: 0.94312
Diversity Loss - Mean: -0.13052, Variance: 0.01565
Semantic Loss - Mean: 1.32374, Variance: 0.07775

Train Epoch: 104 
task: sign, mean loss: 0.00217, accuracy: 1.00000, avg. loss over tasks: 0.00217, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.13367, Variance: 0.01228
Semantic Loss - Mean: 0.02025, Variance: 0.00729

Test Epoch: 104 
task: sign, mean loss: 0.82274, accuracy: 0.80473, avg. loss over tasks: 0.82274
Diversity Loss - Mean: -0.13103, Variance: 0.01566
Semantic Loss - Mean: 1.12357, Variance: 0.07740

Train Epoch: 105 
task: sign, mean loss: 0.00131, accuracy: 1.00000, avg. loss over tasks: 0.00131, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.13391, Variance: 0.01228
Semantic Loss - Mean: 0.01316, Variance: 0.00723

Test Epoch: 105 
task: sign, mean loss: 0.86764, accuracy: 0.81657, avg. loss over tasks: 0.86764
Diversity Loss - Mean: -0.13164, Variance: 0.01567
Semantic Loss - Mean: 1.16809, Variance: 0.07723

Train Epoch: 106 
task: sign, mean loss: 0.00162, accuracy: 1.00000, avg. loss over tasks: 0.00162, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.13425, Variance: 0.01228
Semantic Loss - Mean: 0.02162, Variance: 0.00719

Test Epoch: 106 
task: sign, mean loss: 0.81254, accuracy: 0.79882, avg. loss over tasks: 0.81254
Diversity Loss - Mean: -0.13109, Variance: 0.01569
Semantic Loss - Mean: 1.18682, Variance: 0.07718

Train Epoch: 107 
task: sign, mean loss: 0.00118, accuracy: 1.00000, avg. loss over tasks: 0.00118, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.13429, Variance: 0.01228
Semantic Loss - Mean: 0.01174, Variance: 0.00713

Test Epoch: 107 
task: sign, mean loss: 0.92785, accuracy: 0.81657, avg. loss over tasks: 0.92785
Diversity Loss - Mean: -0.13112, Variance: 0.01570
Semantic Loss - Mean: 1.34892, Variance: 0.07703

Train Epoch: 108 
task: sign, mean loss: 0.00094, accuracy: 1.00000, avg. loss over tasks: 0.00094, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.13451, Variance: 0.01228
Semantic Loss - Mean: 0.01400, Variance: 0.00708

Test Epoch: 108 
task: sign, mean loss: 0.87045, accuracy: 0.82249, avg. loss over tasks: 0.87045
Diversity Loss - Mean: -0.13175, Variance: 0.01571
Semantic Loss - Mean: 1.23022, Variance: 0.07672

Train Epoch: 109 
task: sign, mean loss: 0.00164, accuracy: 1.00000, avg. loss over tasks: 0.00164, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.13418, Variance: 0.01227
Semantic Loss - Mean: 0.02117, Variance: 0.00702

Test Epoch: 109 
task: sign, mean loss: 0.90324, accuracy: 0.80473, avg. loss over tasks: 0.90324
Diversity Loss - Mean: -0.13161, Variance: 0.01572
Semantic Loss - Mean: 1.30677, Variance: 0.07642

Train Epoch: 110 
task: sign, mean loss: 0.00740, accuracy: 1.00000, avg. loss over tasks: 0.00740, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.13474, Variance: 0.01227
Semantic Loss - Mean: 0.01946, Variance: 0.00697

Test Epoch: 110 
task: sign, mean loss: 0.94256, accuracy: 0.80473, avg. loss over tasks: 0.94256
Diversity Loss - Mean: -0.13151, Variance: 0.01574
Semantic Loss - Mean: 1.44434, Variance: 0.07607

Train Epoch: 111 
task: sign, mean loss: 0.00108, accuracy: 1.00000, avg. loss over tasks: 0.00108, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.13431, Variance: 0.01227
Semantic Loss - Mean: 0.01711, Variance: 0.00691

Test Epoch: 111 
task: sign, mean loss: 0.87464, accuracy: 0.79882, avg. loss over tasks: 0.87464
Diversity Loss - Mean: -0.13226, Variance: 0.01575
Semantic Loss - Mean: 1.23340, Variance: 0.07558

Train Epoch: 112 
task: sign, mean loss: 0.00100, accuracy: 1.00000, avg. loss over tasks: 0.00100, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.13519, Variance: 0.01227
Semantic Loss - Mean: 0.00875, Variance: 0.00685

Test Epoch: 112 
task: sign, mean loss: 0.82691, accuracy: 0.79882, avg. loss over tasks: 0.82691
Diversity Loss - Mean: -0.13221, Variance: 0.01576
Semantic Loss - Mean: 1.15957, Variance: 0.07515

Train Epoch: 113 
task: sign, mean loss: 0.00220, accuracy: 1.00000, avg. loss over tasks: 0.00220, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.13553, Variance: 0.01227
Semantic Loss - Mean: 0.01336, Variance: 0.00679

Test Epoch: 113 
task: sign, mean loss: 0.81001, accuracy: 0.80473, avg. loss over tasks: 0.81001
Diversity Loss - Mean: -0.13271, Variance: 0.01577
Semantic Loss - Mean: 1.14314, Variance: 0.07483

Train Epoch: 114 
task: sign, mean loss: 0.00163, accuracy: 1.00000, avg. loss over tasks: 0.00163, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.13487, Variance: 0.01227
Semantic Loss - Mean: 0.01678, Variance: 0.00675

Test Epoch: 114 
task: sign, mean loss: 0.81764, accuracy: 0.82840, avg. loss over tasks: 0.81764
Diversity Loss - Mean: -0.13279, Variance: 0.01579
Semantic Loss - Mean: 1.12238, Variance: 0.07448

Train Epoch: 115 
task: sign, mean loss: 0.00139, accuracy: 1.00000, avg. loss over tasks: 0.00139, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.13476, Variance: 0.01227
Semantic Loss - Mean: 0.01619, Variance: 0.00671

Test Epoch: 115 
task: sign, mean loss: 0.94776, accuracy: 0.77515, avg. loss over tasks: 0.94776
Diversity Loss - Mean: -0.13285, Variance: 0.01580
Semantic Loss - Mean: 1.36595, Variance: 0.07453

Train Epoch: 116 
task: sign, mean loss: 0.00226, accuracy: 1.00000, avg. loss over tasks: 0.00226, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.13515, Variance: 0.01227
Semantic Loss - Mean: 0.01546, Variance: 0.00666

Test Epoch: 116 
task: sign, mean loss: 0.76634, accuracy: 0.80473, avg. loss over tasks: 0.76634
Diversity Loss - Mean: -0.13359, Variance: 0.01581
Semantic Loss - Mean: 1.01827, Variance: 0.07425

Train Epoch: 117 
task: sign, mean loss: 0.00084, accuracy: 1.00000, avg. loss over tasks: 0.00084, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.13521, Variance: 0.01227
Semantic Loss - Mean: 0.01117, Variance: 0.00661

Test Epoch: 117 
task: sign, mean loss: 0.78999, accuracy: 0.81065, avg. loss over tasks: 0.78999
Diversity Loss - Mean: -0.13339, Variance: 0.01583
Semantic Loss - Mean: 1.02615, Variance: 0.07398

Train Epoch: 118 
task: sign, mean loss: 0.00119, accuracy: 1.00000, avg. loss over tasks: 0.00119, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.13558, Variance: 0.01227
Semantic Loss - Mean: 0.00884, Variance: 0.00655

Test Epoch: 118 
task: sign, mean loss: 0.84291, accuracy: 0.78698, avg. loss over tasks: 0.84291
Diversity Loss - Mean: -0.13346, Variance: 0.01584
Semantic Loss - Mean: 1.17597, Variance: 0.07393

Train Epoch: 119 
task: sign, mean loss: 0.00139, accuracy: 1.00000, avg. loss over tasks: 0.00139, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.13567, Variance: 0.01227
Semantic Loss - Mean: 0.00909, Variance: 0.00650

Test Epoch: 119 
task: sign, mean loss: 0.79512, accuracy: 0.80473, avg. loss over tasks: 0.79512
Diversity Loss - Mean: -0.13331, Variance: 0.01585
Semantic Loss - Mean: 1.15480, Variance: 0.07396

Train Epoch: 120 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.13530, Variance: 0.01227
Semantic Loss - Mean: 0.02659, Variance: 0.00648

Test Epoch: 120 
task: sign, mean loss: 0.83611, accuracy: 0.79882, avg. loss over tasks: 0.83611
Diversity Loss - Mean: -0.13347, Variance: 0.01587
Semantic Loss - Mean: 1.23070, Variance: 0.07392

Train Epoch: 121 
task: sign, mean loss: 0.00127, accuracy: 1.00000, avg. loss over tasks: 0.00127, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.13549, Variance: 0.01227
Semantic Loss - Mean: 0.01968, Variance: 0.00643

Test Epoch: 121 
task: sign, mean loss: 0.85714, accuracy: 0.76923, avg. loss over tasks: 0.85714
Diversity Loss - Mean: -0.13328, Variance: 0.01588
Semantic Loss - Mean: 1.27239, Variance: 0.07387

Train Epoch: 122 
task: sign, mean loss: 0.00361, accuracy: 1.00000, avg. loss over tasks: 0.00361, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.13575, Variance: 0.01227
Semantic Loss - Mean: 0.01261, Variance: 0.00639

Test Epoch: 122 
task: sign, mean loss: 0.87340, accuracy: 0.78107, avg. loss over tasks: 0.87340
Diversity Loss - Mean: -0.13329, Variance: 0.01589
Semantic Loss - Mean: 1.35490, Variance: 0.07386

Train Epoch: 123 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.13613, Variance: 0.01228
Semantic Loss - Mean: 0.01305, Variance: 0.00635

Test Epoch: 123 
task: sign, mean loss: 0.99720, accuracy: 0.76331, avg. loss over tasks: 0.99720
Diversity Loss - Mean: -0.13288, Variance: 0.01591
Semantic Loss - Mean: 1.58606, Variance: 0.07388

Train Epoch: 124 
task: sign, mean loss: 0.00227, accuracy: 1.00000, avg. loss over tasks: 0.00227, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.13592, Variance: 0.01227
Semantic Loss - Mean: 0.01197, Variance: 0.00630

Test Epoch: 124 
task: sign, mean loss: 0.82390, accuracy: 0.79882, avg. loss over tasks: 0.82390
Diversity Loss - Mean: -0.13315, Variance: 0.01592
Semantic Loss - Mean: 1.23661, Variance: 0.07365

Train Epoch: 125 
task: sign, mean loss: 0.00084, accuracy: 1.00000, avg. loss over tasks: 0.00084, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.13618, Variance: 0.01228
Semantic Loss - Mean: 0.00897, Variance: 0.00625

Test Epoch: 125 
task: sign, mean loss: 0.89946, accuracy: 0.77515, avg. loss over tasks: 0.89946
Diversity Loss - Mean: -0.13287, Variance: 0.01593
Semantic Loss - Mean: 1.41452, Variance: 0.07351

Train Epoch: 126 
task: sign, mean loss: 0.00122, accuracy: 1.00000, avg. loss over tasks: 0.00122, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.13605, Variance: 0.01227
Semantic Loss - Mean: 0.01337, Variance: 0.00621

Test Epoch: 126 
task: sign, mean loss: 0.82274, accuracy: 0.78698, avg. loss over tasks: 0.82274
Diversity Loss - Mean: -0.13337, Variance: 0.01594
Semantic Loss - Mean: 1.19332, Variance: 0.07325

Train Epoch: 127 
task: sign, mean loss: 0.00142, accuracy: 1.00000, avg. loss over tasks: 0.00142, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.13606, Variance: 0.01228
Semantic Loss - Mean: 0.00947, Variance: 0.00616

Test Epoch: 127 
task: sign, mean loss: 0.93297, accuracy: 0.76923, avg. loss over tasks: 0.93297
Diversity Loss - Mean: -0.13365, Variance: 0.01595
Semantic Loss - Mean: 1.35968, Variance: 0.07304

Train Epoch: 128 
task: sign, mean loss: 0.00097, accuracy: 1.00000, avg. loss over tasks: 0.00097, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.13625, Variance: 0.01228
Semantic Loss - Mean: 0.01196, Variance: 0.00612

Test Epoch: 128 
task: sign, mean loss: 0.97201, accuracy: 0.77515, avg. loss over tasks: 0.97201
Diversity Loss - Mean: -0.13355, Variance: 0.01596
Semantic Loss - Mean: 1.41877, Variance: 0.07283

Train Epoch: 129 
task: sign, mean loss: 0.00079, accuracy: 1.00000, avg. loss over tasks: 0.00079, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.13632, Variance: 0.01228
Semantic Loss - Mean: 0.00723, Variance: 0.00607

Test Epoch: 129 
task: sign, mean loss: 0.90881, accuracy: 0.77515, avg. loss over tasks: 0.90881
Diversity Loss - Mean: -0.13381, Variance: 0.01598
Semantic Loss - Mean: 1.30286, Variance: 0.07254

Train Epoch: 130 
task: sign, mean loss: 0.00107, accuracy: 1.00000, avg. loss over tasks: 0.00107, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.13644, Variance: 0.01228
Semantic Loss - Mean: 0.00708, Variance: 0.00602

Test Epoch: 130 
task: sign, mean loss: 0.95672, accuracy: 0.76923, avg. loss over tasks: 0.95672
Diversity Loss - Mean: -0.13384, Variance: 0.01599
Semantic Loss - Mean: 1.36330, Variance: 0.07229

Train Epoch: 131 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.13636, Variance: 0.01228
Semantic Loss - Mean: 0.01399, Variance: 0.00598

Test Epoch: 131 
task: sign, mean loss: 0.97366, accuracy: 0.76923, avg. loss over tasks: 0.97366
Diversity Loss - Mean: -0.13376, Variance: 0.01600
Semantic Loss - Mean: 1.33643, Variance: 0.07201

Train Epoch: 132 
task: sign, mean loss: 0.00059, accuracy: 1.00000, avg. loss over tasks: 0.00059, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.13642, Variance: 0.01228
Semantic Loss - Mean: 0.00542, Variance: 0.00594

Test Epoch: 132 
task: sign, mean loss: 0.96439, accuracy: 0.76923, avg. loss over tasks: 0.96439
Diversity Loss - Mean: -0.13400, Variance: 0.01601
Semantic Loss - Mean: 1.37361, Variance: 0.07175

Train Epoch: 133 
task: sign, mean loss: 0.00088, accuracy: 1.00000, avg. loss over tasks: 0.00088, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.13667, Variance: 0.01228
Semantic Loss - Mean: 0.00682, Variance: 0.00590

Test Epoch: 133 
task: sign, mean loss: 0.99774, accuracy: 0.75148, avg. loss over tasks: 0.99774
Diversity Loss - Mean: -0.13371, Variance: 0.01602
Semantic Loss - Mean: 1.43246, Variance: 0.07151

Train Epoch: 134 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13663, Variance: 0.01228
Semantic Loss - Mean: 0.01126, Variance: 0.00586

Test Epoch: 134 
task: sign, mean loss: 0.93657, accuracy: 0.76923, avg. loss over tasks: 0.93657
Diversity Loss - Mean: -0.13387, Variance: 0.01603
Semantic Loss - Mean: 1.34156, Variance: 0.07123

Train Epoch: 135 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.13623, Variance: 0.01228
Semantic Loss - Mean: 0.01210, Variance: 0.00583

Test Epoch: 135 
task: sign, mean loss: 0.98384, accuracy: 0.76331, avg. loss over tasks: 0.98384
Diversity Loss - Mean: -0.13389, Variance: 0.01604
Semantic Loss - Mean: 1.35820, Variance: 0.07096

Train Epoch: 136 
task: sign, mean loss: 0.00043, accuracy: 1.00000, avg. loss over tasks: 0.00043, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13652, Variance: 0.01228
Semantic Loss - Mean: 0.01005, Variance: 0.00579

Test Epoch: 136 
task: sign, mean loss: 0.92078, accuracy: 0.77515, avg. loss over tasks: 0.92078
Diversity Loss - Mean: -0.13399, Variance: 0.01605
Semantic Loss - Mean: 1.32050, Variance: 0.07069

Train Epoch: 137 
task: sign, mean loss: 0.00087, accuracy: 1.00000, avg. loss over tasks: 0.00087, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13621, Variance: 0.01228
Semantic Loss - Mean: 0.01039, Variance: 0.00575

Test Epoch: 137 
task: sign, mean loss: 0.85076, accuracy: 0.78107, avg. loss over tasks: 0.85076
Diversity Loss - Mean: -0.13384, Variance: 0.01606
Semantic Loss - Mean: 1.26591, Variance: 0.07046

Train Epoch: 138 
task: sign, mean loss: 0.00731, accuracy: 0.99457, avg. loss over tasks: 0.00731, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13645, Variance: 0.01229
Semantic Loss - Mean: 0.01470, Variance: 0.00572

Test Epoch: 138 
task: sign, mean loss: 0.90651, accuracy: 0.76923, avg. loss over tasks: 0.90651
Diversity Loss - Mean: -0.13379, Variance: 0.01607
Semantic Loss - Mean: 1.33037, Variance: 0.07019

Train Epoch: 139 
task: sign, mean loss: 0.00147, accuracy: 1.00000, avg. loss over tasks: 0.00147, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13629, Variance: 0.01229
Semantic Loss - Mean: 0.01645, Variance: 0.00568

Test Epoch: 139 
task: sign, mean loss: 0.92823, accuracy: 0.75740, avg. loss over tasks: 0.92823
Diversity Loss - Mean: -0.13400, Variance: 0.01608
Semantic Loss - Mean: 1.35456, Variance: 0.06992

Train Epoch: 140 
task: sign, mean loss: 0.00675, accuracy: 1.00000, avg. loss over tasks: 0.00675, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.13628, Variance: 0.01229
Semantic Loss - Mean: 0.02312, Variance: 0.00565

Test Epoch: 140 
task: sign, mean loss: 0.94368, accuracy: 0.78107, avg. loss over tasks: 0.94368
Diversity Loss - Mean: -0.13432, Variance: 0.01609
Semantic Loss - Mean: 1.31177, Variance: 0.06963

Train Epoch: 141 
task: sign, mean loss: 0.00090, accuracy: 1.00000, avg. loss over tasks: 0.00090, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.13627, Variance: 0.01229
Semantic Loss - Mean: 0.01673, Variance: 0.00564

Test Epoch: 141 
task: sign, mean loss: 0.88343, accuracy: 0.76923, avg. loss over tasks: 0.88343
Diversity Loss - Mean: -0.13435, Variance: 0.01610
Semantic Loss - Mean: 1.22798, Variance: 0.06934

Train Epoch: 142 
task: sign, mean loss: 0.00228, accuracy: 1.00000, avg. loss over tasks: 0.00228, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.13621, Variance: 0.01229
Semantic Loss - Mean: 0.02392, Variance: 0.00564

Test Epoch: 142 
task: sign, mean loss: 0.82458, accuracy: 0.79290, avg. loss over tasks: 0.82458
Diversity Loss - Mean: -0.13447, Variance: 0.01611
Semantic Loss - Mean: 1.10844, Variance: 0.06901

Train Epoch: 143 
task: sign, mean loss: 0.00085, accuracy: 1.00000, avg. loss over tasks: 0.00085, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13679, Variance: 0.01229
Semantic Loss - Mean: 0.00563, Variance: 0.00560

Test Epoch: 143 
task: sign, mean loss: 0.86421, accuracy: 0.77515, avg. loss over tasks: 0.86421
Diversity Loss - Mean: -0.13406, Variance: 0.01612
Semantic Loss - Mean: 1.26645, Variance: 0.06876

Train Epoch: 144 
task: sign, mean loss: 0.00156, accuracy: 1.00000, avg. loss over tasks: 0.00156, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.13666, Variance: 0.01229
Semantic Loss - Mean: 0.00728, Variance: 0.00556

Test Epoch: 144 
task: sign, mean loss: 0.96406, accuracy: 0.77515, avg. loss over tasks: 0.96406
Diversity Loss - Mean: -0.13405, Variance: 0.01613
Semantic Loss - Mean: 1.42202, Variance: 0.06854

Train Epoch: 145 
task: sign, mean loss: 0.00085, accuracy: 1.00000, avg. loss over tasks: 0.00085, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13646, Variance: 0.01229
Semantic Loss - Mean: 0.01195, Variance: 0.00553

Test Epoch: 145 
task: sign, mean loss: 0.95303, accuracy: 0.78107, avg. loss over tasks: 0.95303
Diversity Loss - Mean: -0.13393, Variance: 0.01614
Semantic Loss - Mean: 1.41733, Variance: 0.06832

Train Epoch: 146 
task: sign, mean loss: 0.00137, accuracy: 1.00000, avg. loss over tasks: 0.00137, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.13646, Variance: 0.01229
Semantic Loss - Mean: 0.01231, Variance: 0.00550

Test Epoch: 146 
task: sign, mean loss: 1.04913, accuracy: 0.75740, avg. loss over tasks: 1.04913
Diversity Loss - Mean: -0.13374, Variance: 0.01615
Semantic Loss - Mean: 1.51234, Variance: 0.06813

Train Epoch: 147 
task: sign, mean loss: 0.00076, accuracy: 1.00000, avg. loss over tasks: 0.00076, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13655, Variance: 0.01229
Semantic Loss - Mean: 0.00718, Variance: 0.00547

Test Epoch: 147 
task: sign, mean loss: 0.98936, accuracy: 0.76923, avg. loss over tasks: 0.98936
Diversity Loss - Mean: -0.13387, Variance: 0.01616
Semantic Loss - Mean: 1.46998, Variance: 0.06795

Train Epoch: 148 
task: sign, mean loss: 0.00284, accuracy: 1.00000, avg. loss over tasks: 0.00284, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.13626, Variance: 0.01229
Semantic Loss - Mean: 0.01266, Variance: 0.00543

Test Epoch: 148 
task: sign, mean loss: 0.97743, accuracy: 0.76331, avg. loss over tasks: 0.97743
Diversity Loss - Mean: -0.13402, Variance: 0.01617
Semantic Loss - Mean: 1.40098, Variance: 0.06772

Train Epoch: 149 
task: sign, mean loss: 0.00243, accuracy: 1.00000, avg. loss over tasks: 0.00243, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.13652, Variance: 0.01229
Semantic Loss - Mean: 0.01255, Variance: 0.00540

Test Epoch: 149 
task: sign, mean loss: 0.91910, accuracy: 0.75740, avg. loss over tasks: 0.91910
Diversity Loss - Mean: -0.13419, Variance: 0.01617
Semantic Loss - Mean: 1.29952, Variance: 0.06747

Train Epoch: 150 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 3e-07
Diversity Loss - Mean: -0.13651, Variance: 0.01229
Semantic Loss - Mean: 0.00595, Variance: 0.00536

Test Epoch: 150 
task: sign, mean loss: 0.93875, accuracy: 0.76331, avg. loss over tasks: 0.93875
Diversity Loss - Mean: -0.13398, Variance: 0.01618
Semantic Loss - Mean: 1.30786, Variance: 0.06722

