Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09233, accuracy: 0.63587, avg. loss over tasks: 1.09233, lr: 3e-05
Diversity Loss - Mean: -0.00979, Variance: 0.01050
Semantic Loss - Mean: 1.43119, Variance: 0.07282

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17542, accuracy: 0.66272, avg. loss over tasks: 1.17542
Diversity Loss - Mean: -0.02887, Variance: 0.01238
Semantic Loss - Mean: 1.16174, Variance: 0.05394

Train Epoch: 2 
task: sign, mean loss: 0.96305, accuracy: 0.66848, avg. loss over tasks: 0.96305, lr: 6e-05
Diversity Loss - Mean: -0.01577, Variance: 0.01045
Semantic Loss - Mean: 0.98025, Variance: 0.03939

Test Epoch: 2 
task: sign, mean loss: 1.10699, accuracy: 0.66272, avg. loss over tasks: 1.10699
Diversity Loss - Mean: -0.02479, Variance: 0.01194
Semantic Loss - Mean: 1.15008, Variance: 0.03282

Train Epoch: 3 
task: sign, mean loss: 0.79985, accuracy: 0.69022, avg. loss over tasks: 0.79985, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.02777, Variance: 0.01026
Semantic Loss - Mean: 0.99525, Variance: 0.02726

Test Epoch: 3 
task: sign, mean loss: 1.26868, accuracy: 0.59172, avg. loss over tasks: 1.26868
Diversity Loss - Mean: -0.04719, Variance: 0.01127
Semantic Loss - Mean: 1.11456, Variance: 0.02925

Train Epoch: 4 
task: sign, mean loss: 0.73548, accuracy: 0.72283, avg. loss over tasks: 0.73548, lr: 0.00012
Diversity Loss - Mean: -0.04633, Variance: 0.00994
Semantic Loss - Mean: 0.88389, Variance: 0.02108

Test Epoch: 4 
task: sign, mean loss: 1.75082, accuracy: 0.33728, avg. loss over tasks: 1.75082
Diversity Loss - Mean: -0.03372, Variance: 0.01049
Semantic Loss - Mean: 1.12765, Variance: 0.02439

Train Epoch: 5 
task: sign, mean loss: 0.73512, accuracy: 0.71196, avg. loss over tasks: 0.73512, lr: 0.00015
Diversity Loss - Mean: -0.03793, Variance: 0.00960
Semantic Loss - Mean: 0.78122, Variance: 0.01727

Test Epoch: 5 
task: sign, mean loss: 2.14029, accuracy: 0.36095, avg. loss over tasks: 2.14029
Diversity Loss - Mean: -0.04380, Variance: 0.01008
Semantic Loss - Mean: 1.28349, Variance: 0.02282

Train Epoch: 6 
task: sign, mean loss: 0.66659, accuracy: 0.76087, avg. loss over tasks: 0.66659, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.04070, Variance: 0.00951
Semantic Loss - Mean: 0.71574, Variance: 0.01472

Test Epoch: 6 
task: sign, mean loss: 2.16436, accuracy: 0.65680, avg. loss over tasks: 2.16436
Diversity Loss - Mean: 0.00230, Variance: 0.01067
Semantic Loss - Mean: 1.53427, Variance: 0.02156

Train Epoch: 7 
task: sign, mean loss: 0.57943, accuracy: 0.75000, avg. loss over tasks: 0.57943, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.04012, Variance: 0.00952
Semantic Loss - Mean: 0.56888, Variance: 0.01295

Test Epoch: 7 
task: sign, mean loss: 1.89186, accuracy: 0.63314, avg. loss over tasks: 1.89186
Diversity Loss - Mean: -0.03280, Variance: 0.01070
Semantic Loss - Mean: 1.47890, Variance: 0.02174

Train Epoch: 8 
task: sign, mean loss: 0.52134, accuracy: 0.77717, avg. loss over tasks: 0.52134, lr: 0.00024
Diversity Loss - Mean: -0.01185, Variance: 0.00925
Semantic Loss - Mean: 0.57779, Variance: 0.01199

Test Epoch: 8 
task: sign, mean loss: 1.48688, accuracy: 0.60947, avg. loss over tasks: 1.48688
Diversity Loss - Mean: -0.04816, Variance: 0.01052
Semantic Loss - Mean: 1.22835, Variance: 0.02181

Train Epoch: 9 
task: sign, mean loss: 0.81860, accuracy: 0.77174, avg. loss over tasks: 0.81860, lr: 0.00027
Diversity Loss - Mean: -0.01691, Variance: 0.00894
Semantic Loss - Mean: 0.73091, Variance: 0.01133

Test Epoch: 9 
task: sign, mean loss: 3.43474, accuracy: 0.13609, avg. loss over tasks: 3.43474
Diversity Loss - Mean: 0.06755, Variance: 0.01043
Semantic Loss - Mean: 2.55624, Variance: 0.03426

Train Epoch: 10 
task: sign, mean loss: 0.97967, accuracy: 0.62500, avg. loss over tasks: 0.97967, lr: 0.0003
Diversity Loss - Mean: -0.06020, Variance: 0.00889
Semantic Loss - Mean: 0.89408, Variance: 0.01082

Test Epoch: 10 
task: sign, mean loss: 4.12067, accuracy: 0.11834, avg. loss over tasks: 4.12067
Diversity Loss - Mean: 0.13912, Variance: 0.01072
Semantic Loss - Mean: 2.91435, Variance: 0.04779

Train Epoch: 11 
task: sign, mean loss: 0.77349, accuracy: 0.70652, avg. loss over tasks: 0.77349, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.03629, Variance: 0.00890
Semantic Loss - Mean: 0.75057, Variance: 0.01029

Test Epoch: 11 
task: sign, mean loss: 0.68951, accuracy: 0.75148, avg. loss over tasks: 0.68951
Diversity Loss - Mean: 0.02725, Variance: 0.01086
Semantic Loss - Mean: 0.76959, Variance: 0.04440

Train Epoch: 12 
task: sign, mean loss: 0.52411, accuracy: 0.80435, avg. loss over tasks: 0.52411, lr: 0.000299849111021216
Diversity Loss - Mean: -0.01040, Variance: 0.00897
Semantic Loss - Mean: 0.57391, Variance: 0.01035

Test Epoch: 12 
task: sign, mean loss: 1.14315, accuracy: 0.62130, avg. loss over tasks: 1.14315
Diversity Loss - Mean: 0.02622, Variance: 0.01111
Semantic Loss - Mean: 1.15396, Variance: 0.04254

Train Epoch: 13 
task: sign, mean loss: 0.45317, accuracy: 0.83152, avg. loss over tasks: 0.45317, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.00076, Variance: 0.00904
Semantic Loss - Mean: 0.55177, Variance: 0.01062

Test Epoch: 13 
task: sign, mean loss: 0.46279, accuracy: 0.82249, avg. loss over tasks: 0.46279
Diversity Loss - Mean: -0.01991, Variance: 0.01114
Semantic Loss - Mean: 0.54432, Variance: 0.04206

Train Epoch: 14 
task: sign, mean loss: 0.35086, accuracy: 0.86957, avg. loss over tasks: 0.35086, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.01097, Variance: 0.00910
Semantic Loss - Mean: 0.42928, Variance: 0.01076

Test Epoch: 14 
task: sign, mean loss: 1.90637, accuracy: 0.31361, avg. loss over tasks: 1.90637
Diversity Loss - Mean: 0.04796, Variance: 0.01127
Semantic Loss - Mean: 1.77934, Variance: 0.04962

Train Epoch: 15 
task: sign, mean loss: 0.77242, accuracy: 0.78261, avg. loss over tasks: 0.77242, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.03366, Variance: 0.00923
Semantic Loss - Mean: 0.78374, Variance: 0.01118

Test Epoch: 15 
task: sign, mean loss: 3.39388, accuracy: 0.11834, avg. loss over tasks: 3.39388
Diversity Loss - Mean: 0.04770, Variance: 0.01188
Semantic Loss - Mean: 2.53007, Variance: 0.09643

Train Epoch: 16 
task: sign, mean loss: 1.01547, accuracy: 0.70109, avg. loss over tasks: 1.01547, lr: 0.000298643821800925
Diversity Loss - Mean: -0.08378, Variance: 0.00957
Semantic Loss - Mean: 1.02985, Variance: 0.01133

Test Epoch: 16 
task: sign, mean loss: 1.37962, accuracy: 0.18935, avg. loss over tasks: 1.37962
Diversity Loss - Mean: -0.02346, Variance: 0.01217
Semantic Loss - Mean: 1.39678, Variance: 0.09630

Train Epoch: 17 
task: sign, mean loss: 0.74316, accuracy: 0.71196, avg. loss over tasks: 0.74316, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.07893, Variance: 0.00981
Semantic Loss - Mean: 0.74862, Variance: 0.01082

Test Epoch: 17 
task: sign, mean loss: 0.76741, accuracy: 0.74556, avg. loss over tasks: 0.76741
Diversity Loss - Mean: -0.06075, Variance: 0.01235
Semantic Loss - Mean: 0.80670, Variance: 0.09115

Train Epoch: 18 
task: sign, mean loss: 0.54472, accuracy: 0.83152, avg. loss over tasks: 0.54472, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.05730, Variance: 0.00995
Semantic Loss - Mean: 0.59159, Variance: 0.01043

Test Epoch: 18 
task: sign, mean loss: 0.73314, accuracy: 0.75148, avg. loss over tasks: 0.73314
Diversity Loss - Mean: -0.06123, Variance: 0.01245
Semantic Loss - Mean: 0.72781, Variance: 0.08662

Train Epoch: 19 
task: sign, mean loss: 0.32493, accuracy: 0.85870, avg. loss over tasks: 0.32493, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.03783, Variance: 0.01008
Semantic Loss - Mean: 0.39077, Variance: 0.01031

Test Epoch: 19 
task: sign, mean loss: 0.69841, accuracy: 0.77515, avg. loss over tasks: 0.69841
Diversity Loss - Mean: 0.00737, Variance: 0.01249
Semantic Loss - Mean: 0.64347, Variance: 0.08242

Train Epoch: 20 
task: sign, mean loss: 0.25246, accuracy: 0.90217, avg. loss over tasks: 0.25246, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.02417, Variance: 0.01014
Semantic Loss - Mean: 0.33174, Variance: 0.01035

Test Epoch: 20 
task: sign, mean loss: 0.66063, accuracy: 0.78698, avg. loss over tasks: 0.66063
Diversity Loss - Mean: 0.00299, Variance: 0.01248
Semantic Loss - Mean: 0.65663, Variance: 0.07982

Train Epoch: 21 
task: sign, mean loss: 0.25794, accuracy: 0.90217, avg. loss over tasks: 0.25794, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.01717, Variance: 0.01015
Semantic Loss - Mean: 0.31620, Variance: 0.01024

Test Epoch: 21 
task: sign, mean loss: 0.64239, accuracy: 0.81657, avg. loss over tasks: 0.64239
Diversity Loss - Mean: 0.01370, Variance: 0.01248
Semantic Loss - Mean: 0.67458, Variance: 0.08020

Train Epoch: 22 
task: sign, mean loss: 0.21388, accuracy: 0.89130, avg. loss over tasks: 0.21388, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.03359, Variance: 0.01019
Semantic Loss - Mean: 0.33040, Variance: 0.01034

Test Epoch: 22 
task: sign, mean loss: 0.45023, accuracy: 0.82249, avg. loss over tasks: 0.45023
Diversity Loss - Mean: -0.03267, Variance: 0.01255
Semantic Loss - Mean: 0.54515, Variance: 0.07908

Train Epoch: 23 
task: sign, mean loss: 0.15918, accuracy: 0.94022, avg. loss over tasks: 0.15918, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.03193, Variance: 0.01021
Semantic Loss - Mean: 0.24900, Variance: 0.01027

Test Epoch: 23 
task: sign, mean loss: 0.79904, accuracy: 0.81065, avg. loss over tasks: 0.79904
Diversity Loss - Mean: -0.02182, Variance: 0.01258
Semantic Loss - Mean: 0.76118, Variance: 0.08024

Train Epoch: 24 
task: sign, mean loss: 0.13474, accuracy: 0.94565, avg. loss over tasks: 0.13474, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.03071, Variance: 0.01023
Semantic Loss - Mean: 0.23014, Variance: 0.01030

Test Epoch: 24 
task: sign, mean loss: 0.88445, accuracy: 0.71598, avg. loss over tasks: 0.88445
Diversity Loss - Mean: -0.02175, Variance: 0.01260
Semantic Loss - Mean: 0.91958, Variance: 0.08121

Train Epoch: 25 
task: sign, mean loss: 0.07990, accuracy: 0.96739, avg. loss over tasks: 0.07990, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.02877, Variance: 0.01024
Semantic Loss - Mean: 0.16377, Variance: 0.01041

Test Epoch: 25 
task: sign, mean loss: 0.94477, accuracy: 0.69231, avg. loss over tasks: 0.94477
Diversity Loss - Mean: -0.02218, Variance: 0.01262
Semantic Loss - Mean: 1.03034, Variance: 0.08297

Train Epoch: 26 
task: sign, mean loss: 0.09810, accuracy: 0.95652, avg. loss over tasks: 0.09810, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.02777, Variance: 0.01025
Semantic Loss - Mean: 0.18798, Variance: 0.01036

Test Epoch: 26 
task: sign, mean loss: 0.76496, accuracy: 0.79882, avg. loss over tasks: 0.76496
Diversity Loss - Mean: -0.02296, Variance: 0.01262
Semantic Loss - Mean: 0.79088, Variance: 0.08117

Train Epoch: 27 
task: sign, mean loss: 0.09019, accuracy: 0.97283, avg. loss over tasks: 0.09019, lr: 0.000289228031029578
Diversity Loss - Mean: -0.01837, Variance: 0.01022
Semantic Loss - Mean: 0.26768, Variance: 0.01124

Test Epoch: 27 
task: sign, mean loss: 0.83713, accuracy: 0.78107, avg. loss over tasks: 0.83713
Diversity Loss - Mean: 0.01444, Variance: 0.01259
Semantic Loss - Mean: 0.83620, Variance: 0.08143

Train Epoch: 28 
task: sign, mean loss: 0.06300, accuracy: 0.97283, avg. loss over tasks: 0.06300, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.01018, Variance: 0.01021
Semantic Loss - Mean: 0.17406, Variance: 0.01115

Test Epoch: 28 
task: sign, mean loss: 0.84336, accuracy: 0.81065, avg. loss over tasks: 0.84336
Diversity Loss - Mean: -0.00120, Variance: 0.01259
Semantic Loss - Mean: 0.73930, Variance: 0.08277

Train Epoch: 29 
task: sign, mean loss: 0.17590, accuracy: 0.95652, avg. loss over tasks: 0.17590, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.02083, Variance: 0.01024
Semantic Loss - Mean: 0.28229, Variance: 0.01163

Test Epoch: 29 
task: sign, mean loss: 0.66336, accuracy: 0.80473, avg. loss over tasks: 0.66336
Diversity Loss - Mean: -0.01354, Variance: 0.01263
Semantic Loss - Mean: 0.60149, Variance: 0.08109

Train Epoch: 30 
task: sign, mean loss: 0.42202, accuracy: 0.88043, avg. loss over tasks: 0.42202, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.01880, Variance: 0.01028
Semantic Loss - Mean: 0.44338, Variance: 0.01185

Test Epoch: 30 
task: sign, mean loss: 0.98116, accuracy: 0.72781, avg. loss over tasks: 0.98116
Diversity Loss - Mean: 0.02855, Variance: 0.01259
Semantic Loss - Mean: 1.04186, Variance: 0.08464

Train Epoch: 31 
task: sign, mean loss: 0.38779, accuracy: 0.85326, avg. loss over tasks: 0.38779, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.04091, Variance: 0.01033
Semantic Loss - Mean: 0.44314, Variance: 0.01204

Test Epoch: 31 
task: sign, mean loss: 0.71468, accuracy: 0.82249, avg. loss over tasks: 0.71468
Diversity Loss - Mean: -0.02843, Variance: 0.01254
Semantic Loss - Mean: 0.75118, Variance: 0.08401

Train Epoch: 32 
task: sign, mean loss: 0.21570, accuracy: 0.88587, avg. loss over tasks: 0.21570, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.05864, Variance: 0.01040
Semantic Loss - Mean: 0.34547, Variance: 0.01223

Test Epoch: 32 
task: sign, mean loss: 0.59645, accuracy: 0.81065, avg. loss over tasks: 0.59645
Diversity Loss - Mean: -0.03763, Variance: 0.01253
Semantic Loss - Mean: 0.63678, Variance: 0.08213

Train Epoch: 33 
task: sign, mean loss: 0.15108, accuracy: 0.94022, avg. loss over tasks: 0.15108, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.05602, Variance: 0.01047
Semantic Loss - Mean: 0.24176, Variance: 0.01216

Test Epoch: 33 
task: sign, mean loss: 0.76889, accuracy: 0.81065, avg. loss over tasks: 0.76889
Diversity Loss - Mean: -0.03598, Variance: 0.01251
Semantic Loss - Mean: 0.69364, Variance: 0.07982

Train Epoch: 34 
task: sign, mean loss: 0.29577, accuracy: 0.86957, avg. loss over tasks: 0.29577, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.05570, Variance: 0.01054
Semantic Loss - Mean: 0.35163, Variance: 0.01233

Test Epoch: 34 
task: sign, mean loss: 1.66715, accuracy: 0.73964, avg. loss over tasks: 1.66715
Diversity Loss - Mean: -0.03631, Variance: 0.01240
Semantic Loss - Mean: 1.44306, Variance: 0.08005

Train Epoch: 35 
task: sign, mean loss: 0.14407, accuracy: 0.93478, avg. loss over tasks: 0.14407, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.06418, Variance: 0.01059
Semantic Loss - Mean: 0.22564, Variance: 0.01237

Test Epoch: 35 
task: sign, mean loss: 0.63625, accuracy: 0.81065, avg. loss over tasks: 0.63625
Diversity Loss - Mean: -0.04063, Variance: 0.01239
Semantic Loss - Mean: 0.61040, Variance: 0.07805

Train Epoch: 36 
task: sign, mean loss: 0.10565, accuracy: 0.96196, avg. loss over tasks: 0.10565, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.04926, Variance: 0.01061
Semantic Loss - Mean: 0.17665, Variance: 0.01218

Test Epoch: 36 
task: sign, mean loss: 0.58446, accuracy: 0.81065, avg. loss over tasks: 0.58446
Diversity Loss - Mean: -0.01423, Variance: 0.01240
Semantic Loss - Mean: 0.51359, Variance: 0.07660

Train Epoch: 37 
task: sign, mean loss: 0.08788, accuracy: 0.97283, avg. loss over tasks: 0.08788, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.04803, Variance: 0.01062
Semantic Loss - Mean: 0.15991, Variance: 0.01205

Test Epoch: 37 
task: sign, mean loss: 0.53983, accuracy: 0.84024, avg. loss over tasks: 0.53983
Diversity Loss - Mean: -0.01564, Variance: 0.01241
Semantic Loss - Mean: 0.46247, Variance: 0.07494

Train Epoch: 38 
task: sign, mean loss: 0.07118, accuracy: 0.98370, avg. loss over tasks: 0.07118, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.04692, Variance: 0.01062
Semantic Loss - Mean: 0.14828, Variance: 0.01194

Test Epoch: 38 
task: sign, mean loss: 0.62328, accuracy: 0.82840, avg. loss over tasks: 0.62328
Diversity Loss - Mean: -0.00715, Variance: 0.01240
Semantic Loss - Mean: 0.58210, Variance: 0.07347

Train Epoch: 39 
task: sign, mean loss: 0.03116, accuracy: 0.99457, avg. loss over tasks: 0.03116, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.04780, Variance: 0.01061
Semantic Loss - Mean: 0.09450, Variance: 0.01182

Test Epoch: 39 
task: sign, mean loss: 0.73903, accuracy: 0.81657, avg. loss over tasks: 0.73903
Diversity Loss - Mean: -0.03255, Variance: 0.01242
Semantic Loss - Mean: 0.63170, Variance: 0.07343

Train Epoch: 40 
task: sign, mean loss: 0.02410, accuracy: 0.99457, avg. loss over tasks: 0.02410, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.04653, Variance: 0.01060
Semantic Loss - Mean: 0.09014, Variance: 0.01169

Test Epoch: 40 
task: sign, mean loss: 0.92330, accuracy: 0.81065, avg. loss over tasks: 0.92330
Diversity Loss - Mean: -0.03777, Variance: 0.01243
Semantic Loss - Mean: 0.82065, Variance: 0.07454

Train Epoch: 41 
task: sign, mean loss: 0.01677, accuracy: 1.00000, avg. loss over tasks: 0.01677, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.04953, Variance: 0.01058
Semantic Loss - Mean: 0.08867, Variance: 0.01172

Test Epoch: 41 
task: sign, mean loss: 0.82616, accuracy: 0.82840, avg. loss over tasks: 0.82616
Diversity Loss - Mean: -0.02349, Variance: 0.01244
Semantic Loss - Mean: 0.72688, Variance: 0.07457

Train Epoch: 42 
task: sign, mean loss: 0.03500, accuracy: 0.98913, avg. loss over tasks: 0.03500, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.05241, Variance: 0.01058
Semantic Loss - Mean: 0.10381, Variance: 0.01166

Test Epoch: 42 
task: sign, mean loss: 1.07369, accuracy: 0.80473, avg. loss over tasks: 1.07369
Diversity Loss - Mean: -0.02903, Variance: 0.01246
Semantic Loss - Mean: 0.96372, Variance: 0.07855

Train Epoch: 43 
task: sign, mean loss: 0.07544, accuracy: 0.97826, avg. loss over tasks: 0.07544, lr: 0.000260757131773478
Diversity Loss - Mean: -0.05624, Variance: 0.01059
Semantic Loss - Mean: 0.13508, Variance: 0.01167

Test Epoch: 43 
task: sign, mean loss: 0.37371, accuracy: 0.85207, avg. loss over tasks: 0.37371
Diversity Loss - Mean: -0.02987, Variance: 0.01249
Semantic Loss - Mean: 0.50800, Variance: 0.07733

Train Epoch: 44 
task: sign, mean loss: 0.08372, accuracy: 0.97283, avg. loss over tasks: 0.08372, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.05589, Variance: 0.01062
Semantic Loss - Mean: 0.16337, Variance: 0.01167

Test Epoch: 44 
task: sign, mean loss: 0.62636, accuracy: 0.84615, avg. loss over tasks: 0.62636
Diversity Loss - Mean: -0.05210, Variance: 0.01251
Semantic Loss - Mean: 0.62360, Variance: 0.07656

Train Epoch: 45 
task: sign, mean loss: 0.16101, accuracy: 0.92935, avg. loss over tasks: 0.16101, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.06350, Variance: 0.01065
Semantic Loss - Mean: 0.26040, Variance: 0.01177

Test Epoch: 45 
task: sign, mean loss: 0.60402, accuracy: 0.82840, avg. loss over tasks: 0.60402
Diversity Loss - Mean: -0.03881, Variance: 0.01253
Semantic Loss - Mean: 0.62738, Variance: 0.07543

Train Epoch: 46 
task: sign, mean loss: 0.32293, accuracy: 0.89674, avg. loss over tasks: 0.32293, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.05794, Variance: 0.01068
Semantic Loss - Mean: 0.36983, Variance: 0.01189

Test Epoch: 46 
task: sign, mean loss: 1.61836, accuracy: 0.75148, avg. loss over tasks: 1.61836
Diversity Loss - Mean: -0.02520, Variance: 0.01251
Semantic Loss - Mean: 1.60614, Variance: 0.07550

Train Epoch: 47 
task: sign, mean loss: 0.20054, accuracy: 0.90761, avg. loss over tasks: 0.20054, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.06596, Variance: 0.01072
Semantic Loss - Mean: 0.28272, Variance: 0.01210

Test Epoch: 47 
task: sign, mean loss: 1.22424, accuracy: 0.75148, avg. loss over tasks: 1.22424
Diversity Loss - Mean: -0.04696, Variance: 0.01253
Semantic Loss - Mean: 1.36889, Variance: 0.07757

Train Epoch: 48 
task: sign, mean loss: 0.19028, accuracy: 0.90761, avg. loss over tasks: 0.19028, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.06992, Variance: 0.01075
Semantic Loss - Mean: 0.25753, Variance: 0.01201

Test Epoch: 48 
task: sign, mean loss: 0.42296, accuracy: 0.82840, avg. loss over tasks: 0.42296
Diversity Loss - Mean: -0.05279, Variance: 0.01257
Semantic Loss - Mean: 0.41595, Variance: 0.07606

Train Epoch: 49 
task: sign, mean loss: 0.11769, accuracy: 0.97283, avg. loss over tasks: 0.11769, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.07175, Variance: 0.01079
Semantic Loss - Mean: 0.16277, Variance: 0.01205

Test Epoch: 49 
task: sign, mean loss: 0.53186, accuracy: 0.84024, avg. loss over tasks: 0.53186
Diversity Loss - Mean: -0.06294, Variance: 0.01260
Semantic Loss - Mean: 0.47271, Variance: 0.07497

Train Epoch: 50 
task: sign, mean loss: 0.13515, accuracy: 0.97826, avg. loss over tasks: 0.13515, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.07217, Variance: 0.01080
Semantic Loss - Mean: 0.20098, Variance: 0.01209

Test Epoch: 50 
task: sign, mean loss: 0.61725, accuracy: 0.85799, avg. loss over tasks: 0.61725
Diversity Loss - Mean: -0.06581, Variance: 0.01260
Semantic Loss - Mean: 0.51423, Variance: 0.07431

Train Epoch: 51 
task: sign, mean loss: 0.09813, accuracy: 0.95652, avg. loss over tasks: 0.09813, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.06805, Variance: 0.01081
Semantic Loss - Mean: 0.15608, Variance: 0.01201

Test Epoch: 51 
task: sign, mean loss: 0.54945, accuracy: 0.82840, avg. loss over tasks: 0.54945
Diversity Loss - Mean: -0.04905, Variance: 0.01261
Semantic Loss - Mean: 0.51653, Variance: 0.07482

Train Epoch: 52 
task: sign, mean loss: 0.06003, accuracy: 0.98370, avg. loss over tasks: 0.06003, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.07289, Variance: 0.01083
Semantic Loss - Mean: 0.12189, Variance: 0.01197

Test Epoch: 52 
task: sign, mean loss: 0.64511, accuracy: 0.81065, avg. loss over tasks: 0.64511
Diversity Loss - Mean: -0.06003, Variance: 0.01265
Semantic Loss - Mean: 0.62915, Variance: 0.07428

Train Epoch: 53 
task: sign, mean loss: 0.11056, accuracy: 0.97283, avg. loss over tasks: 0.11056, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.07345, Variance: 0.01085
Semantic Loss - Mean: 0.17623, Variance: 0.01202

Test Epoch: 53 
task: sign, mean loss: 0.37242, accuracy: 0.89941, avg. loss over tasks: 0.37242
Diversity Loss - Mean: -0.06866, Variance: 0.01270
Semantic Loss - Mean: 0.39283, Variance: 0.07312

Train Epoch: 54 
task: sign, mean loss: 0.04165, accuracy: 0.98913, avg. loss over tasks: 0.04165, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.06868, Variance: 0.01086
Semantic Loss - Mean: 0.11728, Variance: 0.01206

Test Epoch: 54 
task: sign, mean loss: 0.44232, accuracy: 0.86391, avg. loss over tasks: 0.44232
Diversity Loss - Mean: -0.07864, Variance: 0.01276
Semantic Loss - Mean: 0.54023, Variance: 0.07275

Train Epoch: 55 
task: sign, mean loss: 0.04415, accuracy: 0.98913, avg. loss over tasks: 0.04415, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.06554, Variance: 0.01086
Semantic Loss - Mean: 0.12574, Variance: 0.01211

Test Epoch: 55 
task: sign, mean loss: 0.38167, accuracy: 0.88166, avg. loss over tasks: 0.38167
Diversity Loss - Mean: -0.06065, Variance: 0.01278
Semantic Loss - Mean: 0.41309, Variance: 0.07175

Train Epoch: 56 
task: sign, mean loss: 0.08052, accuracy: 0.98913, avg. loss over tasks: 0.08052, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.06925, Variance: 0.01087
Semantic Loss - Mean: 0.13018, Variance: 0.01204

Test Epoch: 56 
task: sign, mean loss: 1.15625, accuracy: 0.80473, avg. loss over tasks: 1.15625
Diversity Loss - Mean: -0.07021, Variance: 0.01277
Semantic Loss - Mean: 0.97214, Variance: 0.07074

Train Epoch: 57 
task: sign, mean loss: 0.02167, accuracy: 0.99457, avg. loss over tasks: 0.02167, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.07320, Variance: 0.01087
Semantic Loss - Mean: 0.07588, Variance: 0.01196

Test Epoch: 57 
task: sign, mean loss: 0.61790, accuracy: 0.84615, avg. loss over tasks: 0.61790
Diversity Loss - Mean: -0.06581, Variance: 0.01278
Semantic Loss - Mean: 0.54996, Variance: 0.06993

Train Epoch: 58 
task: sign, mean loss: 0.06246, accuracy: 0.98370, avg. loss over tasks: 0.06246, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.07277, Variance: 0.01088
Semantic Loss - Mean: 0.11545, Variance: 0.01190

Test Epoch: 58 
task: sign, mean loss: 0.70192, accuracy: 0.85207, avg. loss over tasks: 0.70192
Diversity Loss - Mean: -0.07722, Variance: 0.01280
Semantic Loss - Mean: 0.61776, Variance: 0.06895

Train Epoch: 59 
task: sign, mean loss: 0.03691, accuracy: 0.98913, avg. loss over tasks: 0.03691, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.07851, Variance: 0.01090
Semantic Loss - Mean: 0.08637, Variance: 0.01189

Test Epoch: 59 
task: sign, mean loss: 0.64818, accuracy: 0.84024, avg. loss over tasks: 0.64818
Diversity Loss - Mean: -0.07642, Variance: 0.01283
Semantic Loss - Mean: 0.60541, Variance: 0.06838

Train Epoch: 60 
task: sign, mean loss: 0.04374, accuracy: 0.98913, avg. loss over tasks: 0.04374, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.07547, Variance: 0.01091
Semantic Loss - Mean: 0.09705, Variance: 0.01186

Test Epoch: 60 
task: sign, mean loss: 0.51147, accuracy: 0.85799, avg. loss over tasks: 0.51147
Diversity Loss - Mean: -0.07625, Variance: 0.01288
Semantic Loss - Mean: 0.51033, Variance: 0.06837

Train Epoch: 61 
task: sign, mean loss: 0.13914, accuracy: 0.95109, avg. loss over tasks: 0.13914, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.08368, Variance: 0.01093
Semantic Loss - Mean: 0.19220, Variance: 0.01200

Test Epoch: 61 
task: sign, mean loss: 2.62864, accuracy: 0.40237, avg. loss over tasks: 2.62864
Diversity Loss - Mean: -0.04128, Variance: 0.01298
Semantic Loss - Mean: 2.75621, Variance: 0.07839

Train Epoch: 62 
task: sign, mean loss: 0.25086, accuracy: 0.93478, avg. loss over tasks: 0.25086, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.08523, Variance: 0.01096
Semantic Loss - Mean: 0.30303, Variance: 0.01223

Test Epoch: 62 
task: sign, mean loss: 0.83218, accuracy: 0.82249, avg. loss over tasks: 0.83218
Diversity Loss - Mean: -0.07503, Variance: 0.01302
Semantic Loss - Mean: 0.74843, Variance: 0.07756

Train Epoch: 63 
task: sign, mean loss: 0.10371, accuracy: 0.95652, avg. loss over tasks: 0.10371, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.08990, Variance: 0.01100
Semantic Loss - Mean: 0.17926, Variance: 0.01230

Test Epoch: 63 
task: sign, mean loss: 0.80541, accuracy: 0.79290, avg. loss over tasks: 0.80541
Diversity Loss - Mean: -0.04297, Variance: 0.01305
Semantic Loss - Mean: 0.91570, Variance: 0.07872

Train Epoch: 64 
task: sign, mean loss: 0.08008, accuracy: 0.97283, avg. loss over tasks: 0.08008, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.08515, Variance: 0.01103
Semantic Loss - Mean: 0.16170, Variance: 0.01241

Test Epoch: 64 
task: sign, mean loss: 0.36399, accuracy: 0.91716, avg. loss over tasks: 0.36399
Diversity Loss - Mean: -0.08067, Variance: 0.01308
Semantic Loss - Mean: 0.38750, Variance: 0.07769

Train Epoch: 65 
task: sign, mean loss: 0.04266, accuracy: 0.99457, avg. loss over tasks: 0.04266, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.08161, Variance: 0.01105
Semantic Loss - Mean: 0.14120, Variance: 0.01242

Test Epoch: 65 
task: sign, mean loss: 0.71321, accuracy: 0.85799, avg. loss over tasks: 0.71321
Diversity Loss - Mean: -0.07897, Variance: 0.01312
Semantic Loss - Mean: 0.65889, Variance: 0.07718

Train Epoch: 66 
task: sign, mean loss: 0.08867, accuracy: 0.97283, avg. loss over tasks: 0.08867, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.08117, Variance: 0.01106
Semantic Loss - Mean: 0.16440, Variance: 0.01251

Test Epoch: 66 
task: sign, mean loss: 0.49075, accuracy: 0.88166, avg. loss over tasks: 0.49075
Diversity Loss - Mean: -0.07449, Variance: 0.01317
Semantic Loss - Mean: 0.39953, Variance: 0.07664

Train Epoch: 67 
task: sign, mean loss: 0.07586, accuracy: 0.97826, avg. loss over tasks: 0.07586, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.08215, Variance: 0.01107
Semantic Loss - Mean: 0.11999, Variance: 0.01244

Test Epoch: 67 
task: sign, mean loss: 0.59998, accuracy: 0.85207, avg. loss over tasks: 0.59998
Diversity Loss - Mean: -0.06996, Variance: 0.01319
Semantic Loss - Mean: 0.50921, Variance: 0.07620

Train Epoch: 68 
task: sign, mean loss: 0.17077, accuracy: 0.95652, avg. loss over tasks: 0.17077, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.08036, Variance: 0.01108
Semantic Loss - Mean: 0.24452, Variance: 0.01252

Test Epoch: 68 
task: sign, mean loss: 0.50014, accuracy: 0.85207, avg. loss over tasks: 0.50014
Diversity Loss - Mean: -0.07310, Variance: 0.01324
Semantic Loss - Mean: 0.50770, Variance: 0.07544

Train Epoch: 69 
task: sign, mean loss: 0.10260, accuracy: 0.96739, avg. loss over tasks: 0.10260, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.09345, Variance: 0.01110
Semantic Loss - Mean: 0.16737, Variance: 0.01254

Test Epoch: 69 
task: sign, mean loss: 0.74313, accuracy: 0.83432, avg. loss over tasks: 0.74313
Diversity Loss - Mean: -0.07074, Variance: 0.01326
Semantic Loss - Mean: 0.68847, Variance: 0.07492

Train Epoch: 70 
task: sign, mean loss: 0.08754, accuracy: 0.97826, avg. loss over tasks: 0.08754, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.08827, Variance: 0.01111
Semantic Loss - Mean: 0.15423, Variance: 0.01246

Test Epoch: 70 
task: sign, mean loss: 0.67918, accuracy: 0.84615, avg. loss over tasks: 0.67918
Diversity Loss - Mean: -0.09182, Variance: 0.01329
Semantic Loss - Mean: 0.67737, Variance: 0.07497

Train Epoch: 71 
task: sign, mean loss: 0.06375, accuracy: 0.98913, avg. loss over tasks: 0.06375, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.09596, Variance: 0.01113
Semantic Loss - Mean: 0.11866, Variance: 0.01244

Test Epoch: 71 
task: sign, mean loss: 0.82742, accuracy: 0.77515, avg. loss over tasks: 0.82742
Diversity Loss - Mean: -0.08066, Variance: 0.01337
Semantic Loss - Mean: 1.03834, Variance: 0.07599

Train Epoch: 72 
task: sign, mean loss: 0.05825, accuracy: 0.99457, avg. loss over tasks: 0.05825, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.09237, Variance: 0.01115
Semantic Loss - Mean: 0.11951, Variance: 0.01239

Test Epoch: 72 
task: sign, mean loss: 0.44112, accuracy: 0.86982, avg. loss over tasks: 0.44112
Diversity Loss - Mean: -0.09080, Variance: 0.01340
Semantic Loss - Mean: 0.47143, Variance: 0.07516

Train Epoch: 73 
task: sign, mean loss: 0.01981, accuracy: 1.00000, avg. loss over tasks: 0.01981, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.08996, Variance: 0.01116
Semantic Loss - Mean: 0.09010, Variance: 0.01242

Test Epoch: 73 
task: sign, mean loss: 0.27790, accuracy: 0.92308, avg. loss over tasks: 0.27790
Diversity Loss - Mean: -0.09385, Variance: 0.01345
Semantic Loss - Mean: 0.28173, Variance: 0.07422

Train Epoch: 74 
task: sign, mean loss: 0.01307, accuracy: 1.00000, avg. loss over tasks: 0.01307, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.08924, Variance: 0.01118
Semantic Loss - Mean: 0.07032, Variance: 0.01234

Test Epoch: 74 
task: sign, mean loss: 0.38832, accuracy: 0.88757, avg. loss over tasks: 0.38832
Diversity Loss - Mean: -0.09105, Variance: 0.01349
Semantic Loss - Mean: 0.42595, Variance: 0.07336

Train Epoch: 75 
task: sign, mean loss: 0.01534, accuracy: 0.99457, avg. loss over tasks: 0.01534, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.08797, Variance: 0.01118
Semantic Loss - Mean: 0.08473, Variance: 0.01237

Test Epoch: 75 
task: sign, mean loss: 0.28269, accuracy: 0.92308, avg. loss over tasks: 0.28269
Diversity Loss - Mean: -0.08721, Variance: 0.01353
Semantic Loss - Mean: 0.32179, Variance: 0.07258

Train Epoch: 76 
task: sign, mean loss: 0.02086, accuracy: 0.98913, avg. loss over tasks: 0.02086, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.08730, Variance: 0.01119
Semantic Loss - Mean: 0.07796, Variance: 0.01234

Test Epoch: 76 
task: sign, mean loss: 0.34120, accuracy: 0.92899, avg. loss over tasks: 0.34120
Diversity Loss - Mean: -0.09284, Variance: 0.01356
Semantic Loss - Mean: 0.33060, Variance: 0.07198

Train Epoch: 77 
task: sign, mean loss: 0.01348, accuracy: 0.99457, avg. loss over tasks: 0.01348, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.08601, Variance: 0.01119
Semantic Loss - Mean: 0.07207, Variance: 0.01234

Test Epoch: 77 
task: sign, mean loss: 0.35386, accuracy: 0.91716, avg. loss over tasks: 0.35386
Diversity Loss - Mean: -0.09340, Variance: 0.01360
Semantic Loss - Mean: 0.32582, Variance: 0.07137

Train Epoch: 78 
task: sign, mean loss: 0.01810, accuracy: 1.00000, avg. loss over tasks: 0.01810, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.08900, Variance: 0.01121
Semantic Loss - Mean: 0.07645, Variance: 0.01233

Test Epoch: 78 
task: sign, mean loss: 0.44129, accuracy: 0.88757, avg. loss over tasks: 0.44129
Diversity Loss - Mean: -0.09409, Variance: 0.01363
Semantic Loss - Mean: 0.38361, Variance: 0.07068

Train Epoch: 79 
task: sign, mean loss: 0.01797, accuracy: 0.98913, avg. loss over tasks: 0.01797, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.09089, Variance: 0.01122
Semantic Loss - Mean: 0.05711, Variance: 0.01223

Test Epoch: 79 
task: sign, mean loss: 0.54732, accuracy: 0.88757, avg. loss over tasks: 0.54732
Diversity Loss - Mean: -0.09018, Variance: 0.01367
Semantic Loss - Mean: 0.47830, Variance: 0.06996

Train Epoch: 80 
task: sign, mean loss: 0.02671, accuracy: 0.99457, avg. loss over tasks: 0.02671, lr: 0.00015015
Diversity Loss - Mean: -0.09246, Variance: 0.01123
Semantic Loss - Mean: 0.06836, Variance: 0.01213

Test Epoch: 80 
task: sign, mean loss: 0.35738, accuracy: 0.89349, avg. loss over tasks: 0.35738
Diversity Loss - Mean: -0.08636, Variance: 0.01370
Semantic Loss - Mean: 0.37409, Variance: 0.06919

Train Epoch: 81 
task: sign, mean loss: 0.08181, accuracy: 0.98913, avg. loss over tasks: 0.08181, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.09260, Variance: 0.01125
Semantic Loss - Mean: 0.12629, Variance: 0.01219

Test Epoch: 81 
task: sign, mean loss: 0.34729, accuracy: 0.91716, avg. loss over tasks: 0.34729
Diversity Loss - Mean: -0.07983, Variance: 0.01372
Semantic Loss - Mean: 0.30205, Variance: 0.06845

Train Epoch: 82 
task: sign, mean loss: 0.01199, accuracy: 1.00000, avg. loss over tasks: 0.01199, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.09113, Variance: 0.01126
Semantic Loss - Mean: 0.06831, Variance: 0.01214

Test Epoch: 82 
task: sign, mean loss: 0.57619, accuracy: 0.89941, avg. loss over tasks: 0.57619
Diversity Loss - Mean: -0.08126, Variance: 0.01373
Semantic Loss - Mean: 0.43786, Variance: 0.06790

Train Epoch: 83 
task: sign, mean loss: 0.01108, accuracy: 1.00000, avg. loss over tasks: 0.01108, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.09117, Variance: 0.01127
Semantic Loss - Mean: 0.08532, Variance: 0.01221

Test Epoch: 83 
task: sign, mean loss: 0.65342, accuracy: 0.85207, avg. loss over tasks: 0.65342
Diversity Loss - Mean: -0.08658, Variance: 0.01375
Semantic Loss - Mean: 0.59538, Variance: 0.06783

Train Epoch: 84 
task: sign, mean loss: 0.10442, accuracy: 0.97826, avg. loss over tasks: 0.10442, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.09201, Variance: 0.01128
Semantic Loss - Mean: 0.15063, Variance: 0.01219

Test Epoch: 84 
task: sign, mean loss: 0.57517, accuracy: 0.91124, avg. loss over tasks: 0.57517
Diversity Loss - Mean: -0.08011, Variance: 0.01378
Semantic Loss - Mean: 0.45618, Variance: 0.06759

Train Epoch: 85 
task: sign, mean loss: 0.07625, accuracy: 0.96739, avg. loss over tasks: 0.07625, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.08809, Variance: 0.01128
Semantic Loss - Mean: 0.09644, Variance: 0.01210

Test Epoch: 85 
task: sign, mean loss: 0.77656, accuracy: 0.85799, avg. loss over tasks: 0.77656
Diversity Loss - Mean: -0.08596, Variance: 0.01380
Semantic Loss - Mean: 0.71103, Variance: 0.06796

Train Epoch: 86 
task: sign, mean loss: 0.02328, accuracy: 0.98913, avg. loss over tasks: 0.02328, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.09087, Variance: 0.01128
Semantic Loss - Mean: 0.07952, Variance: 0.01203

Test Epoch: 86 
task: sign, mean loss: 1.17405, accuracy: 0.80473, avg. loss over tasks: 1.17405
Diversity Loss - Mean: -0.08567, Variance: 0.01380
Semantic Loss - Mean: 0.93817, Variance: 0.06906

Train Epoch: 87 
task: sign, mean loss: 0.03079, accuracy: 0.99457, avg. loss over tasks: 0.03079, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.08964, Variance: 0.01128
Semantic Loss - Mean: 0.06826, Variance: 0.01197

Test Epoch: 87 
task: sign, mean loss: 0.65422, accuracy: 0.86391, avg. loss over tasks: 0.65422
Diversity Loss - Mean: -0.09605, Variance: 0.01382
Semantic Loss - Mean: 0.46066, Variance: 0.06874

Train Epoch: 88 
task: sign, mean loss: 0.01088, accuracy: 1.00000, avg. loss over tasks: 0.01088, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.08915, Variance: 0.01129
Semantic Loss - Mean: 0.06728, Variance: 0.01194

Test Epoch: 88 
task: sign, mean loss: 0.42500, accuracy: 0.91124, avg. loss over tasks: 0.42500
Diversity Loss - Mean: -0.09562, Variance: 0.01384
Semantic Loss - Mean: 0.37878, Variance: 0.06816

Train Epoch: 89 
task: sign, mean loss: 0.00472, accuracy: 1.00000, avg. loss over tasks: 0.00472, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.09194, Variance: 0.01129
Semantic Loss - Mean: 0.04475, Variance: 0.01186

Test Epoch: 89 
task: sign, mean loss: 0.50369, accuracy: 0.89941, avg. loss over tasks: 0.50369
Diversity Loss - Mean: -0.09201, Variance: 0.01387
Semantic Loss - Mean: 0.46735, Variance: 0.06764

Train Epoch: 90 
task: sign, mean loss: 0.00981, accuracy: 0.99457, avg. loss over tasks: 0.00981, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.09435, Variance: 0.01130
Semantic Loss - Mean: 0.04818, Variance: 0.01178

Test Epoch: 90 
task: sign, mean loss: 0.49292, accuracy: 0.89941, avg. loss over tasks: 0.49292
Diversity Loss - Mean: -0.09362, Variance: 0.01389
Semantic Loss - Mean: 0.44340, Variance: 0.06700

Train Epoch: 91 
task: sign, mean loss: 0.02153, accuracy: 0.99457, avg. loss over tasks: 0.02153, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.09313, Variance: 0.01130
Semantic Loss - Mean: 0.04110, Variance: 0.01173

Test Epoch: 91 
task: sign, mean loss: 0.40425, accuracy: 0.89349, avg. loss over tasks: 0.40425
Diversity Loss - Mean: -0.09289, Variance: 0.01393
Semantic Loss - Mean: 0.37323, Variance: 0.06632

Train Epoch: 92 
task: sign, mean loss: 0.00278, accuracy: 1.00000, avg. loss over tasks: 0.00278, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.09515, Variance: 0.01131
Semantic Loss - Mean: 0.04755, Variance: 0.01168

Test Epoch: 92 
task: sign, mean loss: 0.45377, accuracy: 0.88166, avg. loss over tasks: 0.45377
Diversity Loss - Mean: -0.09290, Variance: 0.01395
Semantic Loss - Mean: 0.42608, Variance: 0.06575

Train Epoch: 93 
task: sign, mean loss: 0.00205, accuracy: 1.00000, avg. loss over tasks: 0.00205, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.09613, Variance: 0.01131
Semantic Loss - Mean: 0.02091, Variance: 0.01157

Test Epoch: 93 
task: sign, mean loss: 0.47720, accuracy: 0.87574, avg. loss over tasks: 0.47720
Diversity Loss - Mean: -0.09564, Variance: 0.01399
Semantic Loss - Mean: 0.42799, Variance: 0.06521

Train Epoch: 94 
task: sign, mean loss: 0.00258, accuracy: 1.00000, avg. loss over tasks: 0.00258, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.09610, Variance: 0.01132
Semantic Loss - Mean: 0.02197, Variance: 0.01146

Test Epoch: 94 
task: sign, mean loss: 0.48306, accuracy: 0.88166, avg. loss over tasks: 0.48306
Diversity Loss - Mean: -0.09706, Variance: 0.01401
Semantic Loss - Mean: 0.43145, Variance: 0.06465

Train Epoch: 95 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.09808, Variance: 0.01133
Semantic Loss - Mean: 0.02851, Variance: 0.01141

Test Epoch: 95 
task: sign, mean loss: 0.49993, accuracy: 0.88757, avg. loss over tasks: 0.49993
Diversity Loss - Mean: -0.09707, Variance: 0.01404
Semantic Loss - Mean: 0.45137, Variance: 0.06415

Train Epoch: 96 
task: sign, mean loss: 0.00173, accuracy: 1.00000, avg. loss over tasks: 0.00173, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.09763, Variance: 0.01134
Semantic Loss - Mean: 0.02376, Variance: 0.01132

Test Epoch: 96 
task: sign, mean loss: 0.42142, accuracy: 0.89349, avg. loss over tasks: 0.42142
Diversity Loss - Mean: -0.09548, Variance: 0.01408
Semantic Loss - Mean: 0.39329, Variance: 0.06357

Train Epoch: 97 
task: sign, mean loss: 0.00710, accuracy: 0.99457, avg. loss over tasks: 0.00710, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.10006, Variance: 0.01134
Semantic Loss - Mean: 0.04015, Variance: 0.01125

Test Epoch: 97 
task: sign, mean loss: 0.39463, accuracy: 0.91124, avg. loss over tasks: 0.39463
Diversity Loss - Mean: -0.09461, Variance: 0.01412
Semantic Loss - Mean: 0.37455, Variance: 0.06298

Train Epoch: 98 
task: sign, mean loss: 0.01080, accuracy: 0.99457, avg. loss over tasks: 0.01080, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.10035, Variance: 0.01135
Semantic Loss - Mean: 0.04349, Variance: 0.01121

Test Epoch: 98 
task: sign, mean loss: 0.50931, accuracy: 0.88757, avg. loss over tasks: 0.50931
Diversity Loss - Mean: -0.10116, Variance: 0.01414
Semantic Loss - Mean: 0.47074, Variance: 0.06249

Train Epoch: 99 
task: sign, mean loss: 0.02765, accuracy: 0.99457, avg. loss over tasks: 0.02765, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.10075, Variance: 0.01136
Semantic Loss - Mean: 0.04959, Variance: 0.01113

Test Epoch: 99 
task: sign, mean loss: 0.61920, accuracy: 0.87574, avg. loss over tasks: 0.61920
Diversity Loss - Mean: -0.10081, Variance: 0.01416
Semantic Loss - Mean: 0.59591, Variance: 0.06208

Train Epoch: 100 
task: sign, mean loss: 0.00334, accuracy: 1.00000, avg. loss over tasks: 0.00334, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.09923, Variance: 0.01136
Semantic Loss - Mean: 0.05613, Variance: 0.01114

Test Epoch: 100 
task: sign, mean loss: 0.45788, accuracy: 0.89349, avg. loss over tasks: 0.45788
Diversity Loss - Mean: -0.09874, Variance: 0.01419
Semantic Loss - Mean: 0.42783, Variance: 0.06152

Train Epoch: 101 
task: sign, mean loss: 0.00391, accuracy: 1.00000, avg. loss over tasks: 0.00391, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.10115, Variance: 0.01137
Semantic Loss - Mean: 0.03167, Variance: 0.01109

Test Epoch: 101 
task: sign, mean loss: 0.46423, accuracy: 0.89349, avg. loss over tasks: 0.46423
Diversity Loss - Mean: -0.10064, Variance: 0.01422
Semantic Loss - Mean: 0.44094, Variance: 0.06102

Train Epoch: 102 
task: sign, mean loss: 0.00209, accuracy: 1.00000, avg. loss over tasks: 0.00209, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.09980, Variance: 0.01137
Semantic Loss - Mean: 0.03384, Variance: 0.01105

Test Epoch: 102 
task: sign, mean loss: 0.46047, accuracy: 0.90533, avg. loss over tasks: 0.46047
Diversity Loss - Mean: -0.10222, Variance: 0.01425
Semantic Loss - Mean: 0.43313, Variance: 0.06054

Train Epoch: 103 
task: sign, mean loss: 0.00206, accuracy: 1.00000, avg. loss over tasks: 0.00206, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.10064, Variance: 0.01138
Semantic Loss - Mean: 0.02231, Variance: 0.01097

Test Epoch: 103 
task: sign, mean loss: 0.46496, accuracy: 0.89941, avg. loss over tasks: 0.46496
Diversity Loss - Mean: -0.10065, Variance: 0.01427
Semantic Loss - Mean: 0.43165, Variance: 0.06003

Train Epoch: 104 
task: sign, mean loss: 0.00110, accuracy: 1.00000, avg. loss over tasks: 0.00110, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.10261, Variance: 0.01139
Semantic Loss - Mean: 0.01924, Variance: 0.01088

Test Epoch: 104 
task: sign, mean loss: 0.42433, accuracy: 0.90533, avg. loss over tasks: 0.42433
Diversity Loss - Mean: -0.10126, Variance: 0.01430
Semantic Loss - Mean: 0.39542, Variance: 0.05952

Train Epoch: 105 
task: sign, mean loss: 0.00176, accuracy: 1.00000, avg. loss over tasks: 0.00176, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.10317, Variance: 0.01139
Semantic Loss - Mean: 0.02094, Variance: 0.01080

Test Epoch: 105 
task: sign, mean loss: 0.44975, accuracy: 0.90533, avg. loss over tasks: 0.44975
Diversity Loss - Mean: -0.10010, Variance: 0.01433
Semantic Loss - Mean: 0.42015, Variance: 0.05901

Train Epoch: 106 
task: sign, mean loss: 0.00085, accuracy: 1.00000, avg. loss over tasks: 0.00085, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.10485, Variance: 0.01140
Semantic Loss - Mean: 0.02112, Variance: 0.01071

Test Epoch: 106 
task: sign, mean loss: 0.42155, accuracy: 0.90533, avg. loss over tasks: 0.42155
Diversity Loss - Mean: -0.10013, Variance: 0.01436
Semantic Loss - Mean: 0.39722, Variance: 0.05850

Train Epoch: 107 
task: sign, mean loss: 0.00260, accuracy: 1.00000, avg. loss over tasks: 0.00260, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.10407, Variance: 0.01141
Semantic Loss - Mean: 0.02236, Variance: 0.01064

Test Epoch: 107 
task: sign, mean loss: 0.45988, accuracy: 0.89941, avg. loss over tasks: 0.45988
Diversity Loss - Mean: -0.10306, Variance: 0.01438
Semantic Loss - Mean: 0.41468, Variance: 0.05801

Train Epoch: 108 
task: sign, mean loss: 0.00276, accuracy: 1.00000, avg. loss over tasks: 0.00276, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.10452, Variance: 0.01141
Semantic Loss - Mean: 0.02275, Variance: 0.01055

Test Epoch: 108 
task: sign, mean loss: 0.45270, accuracy: 0.89349, avg. loss over tasks: 0.45270
Diversity Loss - Mean: -0.10426, Variance: 0.01441
Semantic Loss - Mean: 0.41311, Variance: 0.05754

Train Epoch: 109 
task: sign, mean loss: 0.00257, accuracy: 1.00000, avg. loss over tasks: 0.00257, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.10486, Variance: 0.01142
Semantic Loss - Mean: 0.03281, Variance: 0.01050

Test Epoch: 109 
task: sign, mean loss: 0.49432, accuracy: 0.89941, avg. loss over tasks: 0.49432
Diversity Loss - Mean: -0.10288, Variance: 0.01444
Semantic Loss - Mean: 0.45822, Variance: 0.05707

Train Epoch: 110 
task: sign, mean loss: 0.00167, accuracy: 1.00000, avg. loss over tasks: 0.00167, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.10458, Variance: 0.01143
Semantic Loss - Mean: 0.01048, Variance: 0.01041

Test Epoch: 110 
task: sign, mean loss: 0.48241, accuracy: 0.89941, avg. loss over tasks: 0.48241
Diversity Loss - Mean: -0.10341, Variance: 0.01446
Semantic Loss - Mean: 0.45375, Variance: 0.05658

Train Epoch: 111 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.10517, Variance: 0.01143
Semantic Loss - Mean: 0.02719, Variance: 0.01035

Test Epoch: 111 
task: sign, mean loss: 0.41999, accuracy: 0.90533, avg. loss over tasks: 0.41999
Diversity Loss - Mean: -0.10250, Variance: 0.01448
Semantic Loss - Mean: 0.39573, Variance: 0.05611

Train Epoch: 112 
task: sign, mean loss: 0.00061, accuracy: 1.00000, avg. loss over tasks: 0.00061, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.10572, Variance: 0.01144
Semantic Loss - Mean: 0.01479, Variance: 0.01027

Test Epoch: 112 
task: sign, mean loss: 0.43941, accuracy: 0.89349, avg. loss over tasks: 0.43941
Diversity Loss - Mean: -0.10121, Variance: 0.01451
Semantic Loss - Mean: 0.42123, Variance: 0.05568

Train Epoch: 113 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.10630, Variance: 0.01145
Semantic Loss - Mean: 0.01998, Variance: 0.01020

Test Epoch: 113 
task: sign, mean loss: 0.42373, accuracy: 0.90533, avg. loss over tasks: 0.42373
Diversity Loss - Mean: -0.09957, Variance: 0.01453
Semantic Loss - Mean: 0.42582, Variance: 0.05528

Train Epoch: 114 
task: sign, mean loss: 0.00115, accuracy: 1.00000, avg. loss over tasks: 0.00115, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.10524, Variance: 0.01146
Semantic Loss - Mean: 0.01847, Variance: 0.01013

Test Epoch: 114 
task: sign, mean loss: 0.42762, accuracy: 0.89941, avg. loss over tasks: 0.42762
Diversity Loss - Mean: -0.10088, Variance: 0.01456
Semantic Loss - Mean: 0.43759, Variance: 0.05492

Train Epoch: 115 
task: sign, mean loss: 0.00175, accuracy: 1.00000, avg. loss over tasks: 0.00175, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.10574, Variance: 0.01146
Semantic Loss - Mean: 0.01688, Variance: 0.01005

Test Epoch: 115 
task: sign, mean loss: 0.47400, accuracy: 0.90533, avg. loss over tasks: 0.47400
Diversity Loss - Mean: -0.10154, Variance: 0.01457
Semantic Loss - Mean: 0.47785, Variance: 0.05463

Train Epoch: 116 
task: sign, mean loss: 0.00335, accuracy: 1.00000, avg. loss over tasks: 0.00335, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.10554, Variance: 0.01147
Semantic Loss - Mean: 0.01562, Variance: 0.00996

Test Epoch: 116 
task: sign, mean loss: 0.37272, accuracy: 0.91124, avg. loss over tasks: 0.37272
Diversity Loss - Mean: -0.10269, Variance: 0.01460
Semantic Loss - Mean: 0.37758, Variance: 0.05425

Train Epoch: 117 
task: sign, mean loss: 0.00098, accuracy: 1.00000, avg. loss over tasks: 0.00098, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.10610, Variance: 0.01148
Semantic Loss - Mean: 0.01656, Variance: 0.00989

Test Epoch: 117 
task: sign, mean loss: 0.38146, accuracy: 0.91716, avg. loss over tasks: 0.38146
Diversity Loss - Mean: -0.10264, Variance: 0.01462
Semantic Loss - Mean: 0.37801, Variance: 0.05386

Train Epoch: 118 
task: sign, mean loss: 0.00119, accuracy: 1.00000, avg. loss over tasks: 0.00119, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.10693, Variance: 0.01148
Semantic Loss - Mean: 0.01512, Variance: 0.00982

Test Epoch: 118 
task: sign, mean loss: 0.39070, accuracy: 0.92899, avg. loss over tasks: 0.39070
Diversity Loss - Mean: -0.10301, Variance: 0.01464
Semantic Loss - Mean: 0.37618, Variance: 0.05347

Train Epoch: 119 
task: sign, mean loss: 0.00059, accuracy: 1.00000, avg. loss over tasks: 0.00059, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.10729, Variance: 0.01149
Semantic Loss - Mean: 0.01735, Variance: 0.00975

Test Epoch: 119 
task: sign, mean loss: 0.37103, accuracy: 0.92308, avg. loss over tasks: 0.37103
Diversity Loss - Mean: -0.10186, Variance: 0.01467
Semantic Loss - Mean: 0.35769, Variance: 0.05307

Train Epoch: 120 
task: sign, mean loss: 0.00071, accuracy: 1.00000, avg. loss over tasks: 0.00071, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.10668, Variance: 0.01150
Semantic Loss - Mean: 0.01596, Variance: 0.00968

Test Epoch: 120 
task: sign, mean loss: 0.37031, accuracy: 0.92899, avg. loss over tasks: 0.37031
Diversity Loss - Mean: -0.10517, Variance: 0.01469
Semantic Loss - Mean: 0.35181, Variance: 0.05265

Train Epoch: 121 
task: sign, mean loss: 0.00110, accuracy: 1.00000, avg. loss over tasks: 0.00110, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.10845, Variance: 0.01151
Semantic Loss - Mean: 0.02844, Variance: 0.00962

Test Epoch: 121 
task: sign, mean loss: 0.37520, accuracy: 0.91716, avg. loss over tasks: 0.37520
Diversity Loss - Mean: -0.10014, Variance: 0.01472
Semantic Loss - Mean: 0.38741, Variance: 0.05230

Train Epoch: 122 
task: sign, mean loss: 0.00077, accuracy: 1.00000, avg. loss over tasks: 0.00077, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.10752, Variance: 0.01152
Semantic Loss - Mean: 0.01780, Variance: 0.00956

Test Epoch: 122 
task: sign, mean loss: 0.39309, accuracy: 0.91716, avg. loss over tasks: 0.39309
Diversity Loss - Mean: -0.10377, Variance: 0.01474
Semantic Loss - Mean: 0.38292, Variance: 0.05192

Train Epoch: 123 
task: sign, mean loss: 0.00054, accuracy: 1.00000, avg. loss over tasks: 0.00054, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.10816, Variance: 0.01152
Semantic Loss - Mean: 0.01157, Variance: 0.00949

Test Epoch: 123 
task: sign, mean loss: 0.37766, accuracy: 0.92899, avg. loss over tasks: 0.37766
Diversity Loss - Mean: -0.10604, Variance: 0.01476
Semantic Loss - Mean: 0.35651, Variance: 0.05152

Train Epoch: 124 
task: sign, mean loss: 0.00085, accuracy: 1.00000, avg. loss over tasks: 0.00085, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.10838, Variance: 0.01153
Semantic Loss - Mean: 0.01184, Variance: 0.00941

Test Epoch: 124 
task: sign, mean loss: 0.36934, accuracy: 0.93491, avg. loss over tasks: 0.36934
Diversity Loss - Mean: -0.10209, Variance: 0.01479
Semantic Loss - Mean: 0.36145, Variance: 0.05114

Train Epoch: 125 
task: sign, mean loss: 0.00052, accuracy: 1.00000, avg. loss over tasks: 0.00052, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.10890, Variance: 0.01154
Semantic Loss - Mean: 0.01008, Variance: 0.00934

Test Epoch: 125 
task: sign, mean loss: 0.38197, accuracy: 0.92308, avg. loss over tasks: 0.38197
Diversity Loss - Mean: -0.10363, Variance: 0.01481
Semantic Loss - Mean: 0.36878, Variance: 0.05076

Train Epoch: 126 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.10743, Variance: 0.01154
Semantic Loss - Mean: 0.01695, Variance: 0.00928

Test Epoch: 126 
task: sign, mean loss: 0.36114, accuracy: 0.91716, avg. loss over tasks: 0.36114
Diversity Loss - Mean: -0.10242, Variance: 0.01483
Semantic Loss - Mean: 0.34977, Variance: 0.05038

Train Epoch: 127 
task: sign, mean loss: 0.01184, accuracy: 0.99457, avg. loss over tasks: 0.01184, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.10900, Variance: 0.01156
Semantic Loss - Mean: 0.02008, Variance: 0.00922

Test Epoch: 127 
task: sign, mean loss: 0.34681, accuracy: 0.94083, avg. loss over tasks: 0.34681
Diversity Loss - Mean: -0.10430, Variance: 0.01486
Semantic Loss - Mean: 0.33100, Variance: 0.05001

Train Epoch: 128 
task: sign, mean loss: 0.00059, accuracy: 1.00000, avg. loss over tasks: 0.00059, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.10935, Variance: 0.01156
Semantic Loss - Mean: 0.01154, Variance: 0.00916

Test Epoch: 128 
task: sign, mean loss: 0.32508, accuracy: 0.92899, avg. loss over tasks: 0.32508
Diversity Loss - Mean: -0.10617, Variance: 0.01488
Semantic Loss - Mean: 0.30832, Variance: 0.04963

Train Epoch: 129 
task: sign, mean loss: 0.00169, accuracy: 1.00000, avg. loss over tasks: 0.00169, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.10847, Variance: 0.01157
Semantic Loss - Mean: 0.01070, Variance: 0.00909

Test Epoch: 129 
task: sign, mean loss: 0.32824, accuracy: 0.93491, avg. loss over tasks: 0.32824
Diversity Loss - Mean: -0.10690, Variance: 0.01490
Semantic Loss - Mean: 0.30673, Variance: 0.04927

Train Epoch: 130 
task: sign, mean loss: 0.00130, accuracy: 1.00000, avg. loss over tasks: 0.00130, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.10964, Variance: 0.01158
Semantic Loss - Mean: 0.01455, Variance: 0.00903

Test Epoch: 130 
task: sign, mean loss: 0.30808, accuracy: 0.92899, avg. loss over tasks: 0.30808
Diversity Loss - Mean: -0.10502, Variance: 0.01492
Semantic Loss - Mean: 0.29983, Variance: 0.04891

Train Epoch: 131 
task: sign, mean loss: 0.00173, accuracy: 1.00000, avg. loss over tasks: 0.00173, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.10991, Variance: 0.01159
Semantic Loss - Mean: 0.01207, Variance: 0.00897

Test Epoch: 131 
task: sign, mean loss: 0.32639, accuracy: 0.91716, avg. loss over tasks: 0.32639
Diversity Loss - Mean: -0.10495, Variance: 0.01494
Semantic Loss - Mean: 0.31525, Variance: 0.04856

Train Epoch: 132 
task: sign, mean loss: 0.00256, accuracy: 1.00000, avg. loss over tasks: 0.00256, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.10965, Variance: 0.01159
Semantic Loss - Mean: 0.00958, Variance: 0.00890

Test Epoch: 132 
task: sign, mean loss: 0.31481, accuracy: 0.92899, avg. loss over tasks: 0.31481
Diversity Loss - Mean: -0.10601, Variance: 0.01496
Semantic Loss - Mean: 0.30700, Variance: 0.04821

Train Epoch: 133 
task: sign, mean loss: 0.00158, accuracy: 1.00000, avg. loss over tasks: 0.00158, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.10998, Variance: 0.01160
Semantic Loss - Mean: 0.01406, Variance: 0.00884

Test Epoch: 133 
task: sign, mean loss: 0.33715, accuracy: 0.91716, avg. loss over tasks: 0.33715
Diversity Loss - Mean: -0.10368, Variance: 0.01498
Semantic Loss - Mean: 0.33321, Variance: 0.04788

Train Epoch: 134 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.10883, Variance: 0.01161
Semantic Loss - Mean: 0.00533, Variance: 0.00877

Test Epoch: 134 
task: sign, mean loss: 0.31573, accuracy: 0.92899, avg. loss over tasks: 0.31573
Diversity Loss - Mean: -0.10715, Variance: 0.01500
Semantic Loss - Mean: 0.30585, Variance: 0.04754

Train Epoch: 135 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.10855, Variance: 0.01161
Semantic Loss - Mean: 0.01635, Variance: 0.00873

Test Epoch: 135 
task: sign, mean loss: 0.34517, accuracy: 0.91716, avg. loss over tasks: 0.34517
Diversity Loss - Mean: -0.10741, Variance: 0.01502
Semantic Loss - Mean: 0.31607, Variance: 0.04721

Train Epoch: 136 
task: sign, mean loss: 0.00045, accuracy: 1.00000, avg. loss over tasks: 0.00045, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.11062, Variance: 0.01162
Semantic Loss - Mean: 0.00956, Variance: 0.00866

Test Epoch: 136 
task: sign, mean loss: 0.30970, accuracy: 0.92899, avg. loss over tasks: 0.30970
Diversity Loss - Mean: -0.10645, Variance: 0.01504
Semantic Loss - Mean: 0.29715, Variance: 0.04688

Train Epoch: 137 
task: sign, mean loss: 0.00058, accuracy: 1.00000, avg. loss over tasks: 0.00058, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.10889, Variance: 0.01163
Semantic Loss - Mean: 0.01302, Variance: 0.00861

Test Epoch: 137 
task: sign, mean loss: 0.32867, accuracy: 0.92308, avg. loss over tasks: 0.32867
Diversity Loss - Mean: -0.10460, Variance: 0.01506
Semantic Loss - Mean: 0.32047, Variance: 0.04656

Train Epoch: 138 
task: sign, mean loss: 0.00148, accuracy: 1.00000, avg. loss over tasks: 0.00148, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.11007, Variance: 0.01163
Semantic Loss - Mean: 0.01988, Variance: 0.00856

Test Epoch: 138 
task: sign, mean loss: 0.32310, accuracy: 0.92308, avg. loss over tasks: 0.32310
Diversity Loss - Mean: -0.10402, Variance: 0.01508
Semantic Loss - Mean: 0.32697, Variance: 0.04625

Train Epoch: 139 
task: sign, mean loss: 0.00054, accuracy: 1.00000, avg. loss over tasks: 0.00054, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.10959, Variance: 0.01164
Semantic Loss - Mean: 0.01051, Variance: 0.00850

Test Epoch: 139 
task: sign, mean loss: 0.32323, accuracy: 0.92308, avg. loss over tasks: 0.32323
Diversity Loss - Mean: -0.10549, Variance: 0.01510
Semantic Loss - Mean: 0.32058, Variance: 0.04594

Train Epoch: 140 
task: sign, mean loss: 0.06148, accuracy: 0.98913, avg. loss over tasks: 0.06148, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.11096, Variance: 0.01165
Semantic Loss - Mean: 0.05558, Variance: 0.00845

Test Epoch: 140 
task: sign, mean loss: 0.32405, accuracy: 0.93491, avg. loss over tasks: 0.32405
Diversity Loss - Mean: -0.10556, Variance: 0.01512
Semantic Loss - Mean: 0.31925, Variance: 0.04563

Train Epoch: 141 
task: sign, mean loss: 0.00090, accuracy: 1.00000, avg. loss over tasks: 0.00090, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.10986, Variance: 0.01166
Semantic Loss - Mean: 0.00765, Variance: 0.00840

Test Epoch: 141 
task: sign, mean loss: 0.31591, accuracy: 0.94083, avg. loss over tasks: 0.31591
Diversity Loss - Mean: -0.10402, Variance: 0.01513
Semantic Loss - Mean: 0.31770, Variance: 0.04533

Train Epoch: 142 
task: sign, mean loss: 0.00076, accuracy: 1.00000, avg. loss over tasks: 0.00076, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.10850, Variance: 0.01167
Semantic Loss - Mean: 0.01136, Variance: 0.00834

Test Epoch: 142 
task: sign, mean loss: 0.30822, accuracy: 0.92308, avg. loss over tasks: 0.30822
Diversity Loss - Mean: -0.10498, Variance: 0.01516
Semantic Loss - Mean: 0.30330, Variance: 0.04503

Train Epoch: 143 
task: sign, mean loss: 0.00080, accuracy: 1.00000, avg. loss over tasks: 0.00080, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.11098, Variance: 0.01167
Semantic Loss - Mean: 0.00682, Variance: 0.00828

Test Epoch: 143 
task: sign, mean loss: 0.32270, accuracy: 0.94083, avg. loss over tasks: 0.32270
Diversity Loss - Mean: -0.10525, Variance: 0.01517
Semantic Loss - Mean: 0.31320, Variance: 0.04473

Train Epoch: 144 
task: sign, mean loss: 0.00067, accuracy: 1.00000, avg. loss over tasks: 0.00067, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.11008, Variance: 0.01168
Semantic Loss - Mean: 0.01023, Variance: 0.00823

Test Epoch: 144 
task: sign, mean loss: 0.32472, accuracy: 0.92899, avg. loss over tasks: 0.32472
Diversity Loss - Mean: -0.10484, Variance: 0.01519
Semantic Loss - Mean: 0.33151, Variance: 0.04445

Train Epoch: 145 
task: sign, mean loss: 0.00078, accuracy: 1.00000, avg. loss over tasks: 0.00078, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.10954, Variance: 0.01168
Semantic Loss - Mean: 0.00820, Variance: 0.00817

Test Epoch: 145 
task: sign, mean loss: 0.32504, accuracy: 0.92899, avg. loss over tasks: 0.32504
Diversity Loss - Mean: -0.10579, Variance: 0.01521
Semantic Loss - Mean: 0.32326, Variance: 0.04416

Train Epoch: 146 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.10955, Variance: 0.01169
Semantic Loss - Mean: 0.01989, Variance: 0.00814

Test Epoch: 146 
task: sign, mean loss: 0.33321, accuracy: 0.91716, avg. loss over tasks: 0.33321
Diversity Loss - Mean: -0.10315, Variance: 0.01522
Semantic Loss - Mean: 0.33502, Variance: 0.04389

Train Epoch: 147 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.10972, Variance: 0.01170
Semantic Loss - Mean: 0.01611, Variance: 0.00809

Test Epoch: 147 
task: sign, mean loss: 0.32062, accuracy: 0.92308, avg. loss over tasks: 0.32062
Diversity Loss - Mean: -0.10483, Variance: 0.01524
Semantic Loss - Mean: 0.32409, Variance: 0.04362

Train Epoch: 148 
task: sign, mean loss: 0.00104, accuracy: 1.00000, avg. loss over tasks: 0.00104, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.10876, Variance: 0.01170
Semantic Loss - Mean: 0.00763, Variance: 0.00804

Test Epoch: 148 
task: sign, mean loss: 0.33518, accuracy: 0.92308, avg. loss over tasks: 0.33518
Diversity Loss - Mean: -0.10563, Variance: 0.01526
Semantic Loss - Mean: 0.32507, Variance: 0.04334

Train Epoch: 149 
task: sign, mean loss: 0.00134, accuracy: 1.00000, avg. loss over tasks: 0.00134, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.10893, Variance: 0.01171
Semantic Loss - Mean: 0.03354, Variance: 0.00802

Test Epoch: 149 
task: sign, mean loss: 0.32233, accuracy: 0.92308, avg. loss over tasks: 0.32233
Diversity Loss - Mean: -0.10673, Variance: 0.01527
Semantic Loss - Mean: 0.30902, Variance: 0.04306

Train Epoch: 150 
task: sign, mean loss: 0.00077, accuracy: 1.00000, avg. loss over tasks: 0.00077, lr: 3e-07
Diversity Loss - Mean: -0.10996, Variance: 0.01171
Semantic Loss - Mean: 0.01718, Variance: 0.00798

Test Epoch: 150 
task: sign, mean loss: 0.31633, accuracy: 0.92308, avg. loss over tasks: 0.31633
Diversity Loss - Mean: -0.10585, Variance: 0.01529
Semantic Loss - Mean: 0.30558, Variance: 0.04279

