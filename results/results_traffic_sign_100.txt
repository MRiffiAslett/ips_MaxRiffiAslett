Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09239, accuracy: 0.63587, avg. loss over tasks: 1.09239, lr: 3e-05
Diversity Loss - Mean: -0.01173, Variance: 0.01049
Semantic Loss - Mean: 1.43029, Variance: 0.07361

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17982, accuracy: 0.66272, avg. loss over tasks: 1.17982
Diversity Loss - Mean: -0.03406, Variance: 0.01260
Semantic Loss - Mean: 1.16209, Variance: 0.05335

Train Epoch: 2 
task: sign, mean loss: 0.96519, accuracy: 0.67391, avg. loss over tasks: 0.96519, lr: 6e-05
Diversity Loss - Mean: -0.02905, Variance: 0.01042
Semantic Loss - Mean: 0.98252, Variance: 0.03967

Test Epoch: 2 
task: sign, mean loss: 1.09636, accuracy: 0.65680, avg. loss over tasks: 1.09636
Diversity Loss - Mean: -0.04883, Variance: 0.01235
Semantic Loss - Mean: 1.13857, Variance: 0.03216

Train Epoch: 3 
task: sign, mean loss: 0.78176, accuracy: 0.70109, avg. loss over tasks: 0.78176, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.06198, Variance: 0.01037
Semantic Loss - Mean: 0.98952, Variance: 0.02757

Test Epoch: 3 
task: sign, mean loss: 1.27270, accuracy: 0.55030, avg. loss over tasks: 1.27270
Diversity Loss - Mean: -0.08228, Variance: 0.01155
Semantic Loss - Mean: 1.10399, Variance: 0.02906

Train Epoch: 4 
task: sign, mean loss: 0.75549, accuracy: 0.70109, avg. loss over tasks: 0.75549, lr: 0.00012
Diversity Loss - Mean: -0.08797, Variance: 0.01022
Semantic Loss - Mean: 0.89193, Variance: 0.02132

Test Epoch: 4 
task: sign, mean loss: 1.69901, accuracy: 0.43787, avg. loss over tasks: 1.69901
Diversity Loss - Mean: -0.08253, Variance: 0.01090
Semantic Loss - Mean: 1.12648, Variance: 0.02427

Train Epoch: 5 
task: sign, mean loss: 0.74499, accuracy: 0.70109, avg. loss over tasks: 0.74499, lr: 0.00015
Diversity Loss - Mean: -0.08556, Variance: 0.00994
Semantic Loss - Mean: 0.79017, Variance: 0.01736

Test Epoch: 5 
task: sign, mean loss: 2.03275, accuracy: 0.32544, avg. loss over tasks: 2.03275
Diversity Loss - Mean: -0.08359, Variance: 0.01033
Semantic Loss - Mean: 1.26028, Variance: 0.02207

Train Epoch: 6 
task: sign, mean loss: 0.70360, accuracy: 0.77717, avg. loss over tasks: 0.70360, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.06969, Variance: 0.00968
Semantic Loss - Mean: 0.72135, Variance: 0.01505

Test Epoch: 6 
task: sign, mean loss: 1.89561, accuracy: 0.41420, avg. loss over tasks: 1.89561
Diversity Loss - Mean: -0.07677, Variance: 0.01010
Semantic Loss - Mean: 1.35997, Variance: 0.02190

Train Epoch: 7 
task: sign, mean loss: 0.63015, accuracy: 0.75000, avg. loss over tasks: 0.63015, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.08401, Variance: 0.00975
Semantic Loss - Mean: 0.66532, Variance: 0.01323

Test Epoch: 7 
task: sign, mean loss: 2.05282, accuracy: 0.25444, avg. loss over tasks: 2.05282
Diversity Loss - Mean: -0.04118, Variance: 0.00996
Semantic Loss - Mean: 1.68506, Variance: 0.02748

Train Epoch: 8 
task: sign, mean loss: 0.52919, accuracy: 0.80978, avg. loss over tasks: 0.52919, lr: 0.00024
Diversity Loss - Mean: -0.06034, Variance: 0.00969
Semantic Loss - Mean: 0.57639, Variance: 0.01203

Test Epoch: 8 
task: sign, mean loss: 2.10461, accuracy: 0.46746, avg. loss over tasks: 2.10461
Diversity Loss - Mean: -0.03121, Variance: 0.00994
Semantic Loss - Mean: 1.44101, Variance: 0.02769

Train Epoch: 9 
task: sign, mean loss: 0.63687, accuracy: 0.78804, avg. loss over tasks: 0.63687, lr: 0.00027
Diversity Loss - Mean: -0.06396, Variance: 0.00964
Semantic Loss - Mean: 0.60128, Variance: 0.01114

Test Epoch: 9 
task: sign, mean loss: 2.91810, accuracy: 0.40828, avg. loss over tasks: 2.91810
Diversity Loss - Mean: -0.03065, Variance: 0.01024
Semantic Loss - Mean: 2.24601, Variance: 0.03091

Train Epoch: 10 
task: sign, mean loss: 0.71930, accuracy: 0.74457, avg. loss over tasks: 0.71930, lr: 0.0003
Diversity Loss - Mean: -0.07075, Variance: 0.00970
Semantic Loss - Mean: 0.69704, Variance: 0.01080

Test Epoch: 10 
task: sign, mean loss: 2.65259, accuracy: 0.66272, avg. loss over tasks: 2.65259
Diversity Loss - Mean: -0.04396, Variance: 0.01046
Semantic Loss - Mean: 2.05406, Variance: 0.03126

Train Epoch: 11 
task: sign, mean loss: 0.70412, accuracy: 0.71196, avg. loss over tasks: 0.70412, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.05618, Variance: 0.00979
Semantic Loss - Mean: 0.66239, Variance: 0.01045

Test Epoch: 11 
task: sign, mean loss: 2.19169, accuracy: 0.66864, avg. loss over tasks: 2.19169
Diversity Loss - Mean: -0.05251, Variance: 0.01095
Semantic Loss - Mean: 1.98892, Variance: 0.03180

Train Epoch: 12 
task: sign, mean loss: 0.68753, accuracy: 0.75543, avg. loss over tasks: 0.68753, lr: 0.000299849111021216
Diversity Loss - Mean: -0.06956, Variance: 0.01000
Semantic Loss - Mean: 0.69493, Variance: 0.00990

Test Epoch: 12 
task: sign, mean loss: 2.27510, accuracy: 0.40237, avg. loss over tasks: 2.27510
Diversity Loss - Mean: -0.03601, Variance: 0.01109
Semantic Loss - Mean: 1.47811, Variance: 0.03218

Train Epoch: 13 
task: sign, mean loss: 0.60690, accuracy: 0.77174, avg. loss over tasks: 0.60690, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.08763, Variance: 0.01032
Semantic Loss - Mean: 0.63215, Variance: 0.00939

Test Epoch: 13 
task: sign, mean loss: 3.63854, accuracy: 0.15976, avg. loss over tasks: 3.63854
Diversity Loss - Mean: -0.00753, Variance: 0.01176
Semantic Loss - Mean: 2.56186, Variance: 0.04047

Train Epoch: 14 
task: sign, mean loss: 0.43776, accuracy: 0.84783, avg. loss over tasks: 0.43776, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.08264, Variance: 0.01051
Semantic Loss - Mean: 0.50118, Variance: 0.00899

Test Epoch: 14 
task: sign, mean loss: 2.87777, accuracy: 0.19527, avg. loss over tasks: 2.87777
Diversity Loss - Mean: -0.04208, Variance: 0.01223
Semantic Loss - Mean: 2.17375, Variance: 0.04096

Train Epoch: 15 
task: sign, mean loss: 0.33349, accuracy: 0.87500, avg. loss over tasks: 0.33349, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.07925, Variance: 0.01062
Semantic Loss - Mean: 0.38858, Variance: 0.00858

Test Epoch: 15 
task: sign, mean loss: 2.13473, accuracy: 0.49704, avg. loss over tasks: 2.13473
Diversity Loss - Mean: -0.08177, Variance: 0.01238
Semantic Loss - Mean: 1.76630, Variance: 0.03990

Train Epoch: 16 
task: sign, mean loss: 0.36005, accuracy: 0.87500, avg. loss over tasks: 0.36005, lr: 0.000298643821800925
Diversity Loss - Mean: -0.08047, Variance: 0.01072
Semantic Loss - Mean: 0.40939, Variance: 0.00826

Test Epoch: 16 
task: sign, mean loss: 2.28637, accuracy: 0.55621, avg. loss over tasks: 2.28637
Diversity Loss - Mean: -0.10066, Variance: 0.01255
Semantic Loss - Mean: 1.91561, Variance: 0.03880

Train Epoch: 17 
task: sign, mean loss: 0.36099, accuracy: 0.85870, avg. loss over tasks: 0.36099, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.08480, Variance: 0.01083
Semantic Loss - Mean: 0.40505, Variance: 0.00808

Test Epoch: 17 
task: sign, mean loss: 2.33297, accuracy: 0.50888, avg. loss over tasks: 2.33297
Diversity Loss - Mean: -0.08859, Variance: 0.01274
Semantic Loss - Mean: 1.98373, Variance: 0.03746

Train Epoch: 18 
task: sign, mean loss: 0.32567, accuracy: 0.88587, avg. loss over tasks: 0.32567, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.09176, Variance: 0.01096
Semantic Loss - Mean: 0.36127, Variance: 0.00785

Test Epoch: 18 
task: sign, mean loss: 2.13003, accuracy: 0.44970, avg. loss over tasks: 2.13003
Diversity Loss - Mean: -0.10444, Variance: 0.01297
Semantic Loss - Mean: 1.83687, Variance: 0.03760

Train Epoch: 19 
task: sign, mean loss: 0.27993, accuracy: 0.90761, avg. loss over tasks: 0.27993, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.09091, Variance: 0.01107
Semantic Loss - Mean: 0.33604, Variance: 0.00763

Test Epoch: 19 
task: sign, mean loss: 2.77825, accuracy: 0.32544, avg. loss over tasks: 2.77825
Diversity Loss - Mean: -0.07345, Variance: 0.01323
Semantic Loss - Mean: 2.53968, Variance: 0.03756

Train Epoch: 20 
task: sign, mean loss: 0.32086, accuracy: 0.87500, avg. loss over tasks: 0.32086, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.09345, Variance: 0.01115
Semantic Loss - Mean: 0.34450, Variance: 0.00738

Test Epoch: 20 
task: sign, mean loss: 3.22152, accuracy: 0.36095, avg. loss over tasks: 3.22152
Diversity Loss - Mean: -0.06845, Variance: 0.01352
Semantic Loss - Mean: 2.59266, Variance: 0.03743

Train Epoch: 21 
task: sign, mean loss: 0.23680, accuracy: 0.91304, avg. loss over tasks: 0.23680, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.09817, Variance: 0.01125
Semantic Loss - Mean: 0.25994, Variance: 0.00715

Test Epoch: 21 
task: sign, mean loss: 3.15019, accuracy: 0.39645, avg. loss over tasks: 3.15019
Diversity Loss - Mean: -0.07883, Variance: 0.01387
Semantic Loss - Mean: 2.25873, Variance: 0.04189

Train Epoch: 22 
task: sign, mean loss: 0.20856, accuracy: 0.91848, avg. loss over tasks: 0.20856, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.10038, Variance: 0.01133
Semantic Loss - Mean: 0.25956, Variance: 0.00712

Test Epoch: 22 
task: sign, mean loss: 2.82283, accuracy: 0.42604, avg. loss over tasks: 2.82283
Diversity Loss - Mean: -0.07829, Variance: 0.01412
Semantic Loss - Mean: 2.25836, Variance: 0.04420

Train Epoch: 23 
task: sign, mean loss: 0.07791, accuracy: 0.97283, avg. loss over tasks: 0.07791, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.09302, Variance: 0.01137
Semantic Loss - Mean: 0.12950, Variance: 0.00695

Test Epoch: 23 
task: sign, mean loss: 2.38204, accuracy: 0.44379, avg. loss over tasks: 2.38204
Diversity Loss - Mean: -0.09249, Variance: 0.01426
Semantic Loss - Mean: 2.03051, Variance: 0.04367

Train Epoch: 24 
task: sign, mean loss: 0.06580, accuracy: 0.98370, avg. loss over tasks: 0.06580, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.09224, Variance: 0.01140
Semantic Loss - Mean: 0.09678, Variance: 0.00669

Test Epoch: 24 
task: sign, mean loss: 3.73984, accuracy: 0.40237, avg. loss over tasks: 3.73984
Diversity Loss - Mean: -0.07493, Variance: 0.01441
Semantic Loss - Mean: 3.08343, Variance: 0.04569

Train Epoch: 25 
task: sign, mean loss: 0.06012, accuracy: 0.97826, avg. loss over tasks: 0.06012, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.09011, Variance: 0.01143
Semantic Loss - Mean: 0.07382, Variance: 0.00647

Test Epoch: 25 
task: sign, mean loss: 4.45092, accuracy: 0.43787, avg. loss over tasks: 4.45092
Diversity Loss - Mean: -0.07911, Variance: 0.01462
Semantic Loss - Mean: 2.91603, Variance: 0.04920

Train Epoch: 26 
task: sign, mean loss: 0.11104, accuracy: 0.97283, avg. loss over tasks: 0.11104, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.09713, Variance: 0.01150
Semantic Loss - Mean: 0.15127, Variance: 0.00646

Test Epoch: 26 
task: sign, mean loss: 2.88748, accuracy: 0.55030, avg. loss over tasks: 2.88748
Diversity Loss - Mean: -0.10575, Variance: 0.01473
Semantic Loss - Mean: 2.27583, Variance: 0.05056

Train Epoch: 27 
task: sign, mean loss: 0.28836, accuracy: 0.92391, avg. loss over tasks: 0.28836, lr: 0.000289228031029578
Diversity Loss - Mean: -0.10255, Variance: 0.01157
Semantic Loss - Mean: 0.36776, Variance: 0.00674

Test Epoch: 27 
task: sign, mean loss: 2.10623, accuracy: 0.67456, avg. loss over tasks: 2.10623
Diversity Loss - Mean: -0.12829, Variance: 0.01491
Semantic Loss - Mean: 1.79848, Variance: 0.05116

Train Epoch: 28 
task: sign, mean loss: 0.27195, accuracy: 0.90217, avg. loss over tasks: 0.27195, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.10829, Variance: 0.01165
Semantic Loss - Mean: 0.31914, Variance: 0.00673

Test Epoch: 28 
task: sign, mean loss: 2.22930, accuracy: 0.57396, avg. loss over tasks: 2.22930
Diversity Loss - Mean: -0.11682, Variance: 0.01504
Semantic Loss - Mean: 1.94967, Variance: 0.05161

Train Epoch: 29 
task: sign, mean loss: 0.27531, accuracy: 0.89674, avg. loss over tasks: 0.27531, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.11150, Variance: 0.01176
Semantic Loss - Mean: 0.35124, Variance: 0.00677

Test Epoch: 29 
task: sign, mean loss: 3.17536, accuracy: 0.32544, avg. loss over tasks: 3.17536
Diversity Loss - Mean: -0.08427, Variance: 0.01514
Semantic Loss - Mean: 2.82027, Variance: 0.05386

Train Epoch: 30 
task: sign, mean loss: 0.50263, accuracy: 0.79348, avg. loss over tasks: 0.50263, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.11230, Variance: 0.01185
Semantic Loss - Mean: 0.54365, Variance: 0.00679

Test Epoch: 30 
task: sign, mean loss: 2.34912, accuracy: 0.43195, avg. loss over tasks: 2.34912
Diversity Loss - Mean: -0.11125, Variance: 0.01517
Semantic Loss - Mean: 2.09815, Variance: 0.05389

Train Epoch: 31 
task: sign, mean loss: 0.34198, accuracy: 0.85326, avg. loss over tasks: 0.34198, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.11439, Variance: 0.01193
Semantic Loss - Mean: 0.41062, Variance: 0.00667

Test Epoch: 31 
task: sign, mean loss: 1.93850, accuracy: 0.61538, avg. loss over tasks: 1.93850
Diversity Loss - Mean: -0.10710, Variance: 0.01523
Semantic Loss - Mean: 1.68938, Variance: 0.05288

Train Epoch: 32 
task: sign, mean loss: 0.25168, accuracy: 0.91304, avg. loss over tasks: 0.25168, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.11021, Variance: 0.01196
Semantic Loss - Mean: 0.32391, Variance: 0.00656

Test Epoch: 32 
task: sign, mean loss: 2.79982, accuracy: 0.43195, avg. loss over tasks: 2.79982
Diversity Loss - Mean: -0.10457, Variance: 0.01524
Semantic Loss - Mean: 2.27923, Variance: 0.05205

Train Epoch: 33 
task: sign, mean loss: 0.13544, accuracy: 0.94022, avg. loss over tasks: 0.13544, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.10588, Variance: 0.01197
Semantic Loss - Mean: 0.16641, Variance: 0.00646

Test Epoch: 33 
task: sign, mean loss: 3.07867, accuracy: 0.42604, avg. loss over tasks: 3.07867
Diversity Loss - Mean: -0.10173, Variance: 0.01523
Semantic Loss - Mean: 2.46691, Variance: 0.05129

Train Epoch: 34 
task: sign, mean loss: 0.21893, accuracy: 0.91848, avg. loss over tasks: 0.21893, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.10898, Variance: 0.01198
Semantic Loss - Mean: 0.23178, Variance: 0.00645

Test Epoch: 34 
task: sign, mean loss: 2.13140, accuracy: 0.56213, avg. loss over tasks: 2.13140
Diversity Loss - Mean: -0.11524, Variance: 0.01521
Semantic Loss - Mean: 1.89385, Variance: 0.05042

Train Epoch: 35 
task: sign, mean loss: 0.13908, accuracy: 0.95109, avg. loss over tasks: 0.13908, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.11161, Variance: 0.01201
Semantic Loss - Mean: 0.19132, Variance: 0.00642

Test Epoch: 35 
task: sign, mean loss: 2.09746, accuracy: 0.52071, avg. loss over tasks: 2.09746
Diversity Loss - Mean: -0.09053, Variance: 0.01530
Semantic Loss - Mean: 1.92415, Variance: 0.05114

Train Epoch: 36 
task: sign, mean loss: 0.11367, accuracy: 0.95652, avg. loss over tasks: 0.11367, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.10454, Variance: 0.01200
Semantic Loss - Mean: 0.17729, Variance: 0.00635

Test Epoch: 36 
task: sign, mean loss: 2.18662, accuracy: 0.50888, avg. loss over tasks: 2.18662
Diversity Loss - Mean: -0.10491, Variance: 0.01533
Semantic Loss - Mean: 1.78849, Variance: 0.05195

Train Epoch: 37 
task: sign, mean loss: 0.14404, accuracy: 0.94565, avg. loss over tasks: 0.14404, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.11104, Variance: 0.01202
Semantic Loss - Mean: 0.17320, Variance: 0.00628

Test Epoch: 37 
task: sign, mean loss: 1.60807, accuracy: 0.68639, avg. loss over tasks: 1.60807
Diversity Loss - Mean: -0.12140, Variance: 0.01540
Semantic Loss - Mean: 1.40650, Variance: 0.05169

Train Epoch: 38 
task: sign, mean loss: 0.08558, accuracy: 0.97283, avg. loss over tasks: 0.08558, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.11117, Variance: 0.01206
Semantic Loss - Mean: 0.12057, Variance: 0.00631

Test Epoch: 38 
task: sign, mean loss: 1.86417, accuracy: 0.59763, avg. loss over tasks: 1.86417
Diversity Loss - Mean: -0.11256, Variance: 0.01543
Semantic Loss - Mean: 1.61884, Variance: 0.05209

Train Epoch: 39 
task: sign, mean loss: 0.05171, accuracy: 0.97826, avg. loss over tasks: 0.05171, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.10925, Variance: 0.01207
Semantic Loss - Mean: 0.08029, Variance: 0.00628

Test Epoch: 39 
task: sign, mean loss: 1.81024, accuracy: 0.59763, avg. loss over tasks: 1.81024
Diversity Loss - Mean: -0.11147, Variance: 0.01545
Semantic Loss - Mean: 1.60690, Variance: 0.05203

Train Epoch: 40 
task: sign, mean loss: 0.03865, accuracy: 0.99457, avg. loss over tasks: 0.03865, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.11384, Variance: 0.01209
Semantic Loss - Mean: 0.09045, Variance: 0.00630

Test Epoch: 40 
task: sign, mean loss: 2.61447, accuracy: 0.50296, avg. loss over tasks: 2.61447
Diversity Loss - Mean: -0.10734, Variance: 0.01546
Semantic Loss - Mean: 2.27790, Variance: 0.05252

Train Epoch: 41 
task: sign, mean loss: 0.01949, accuracy: 0.99457, avg. loss over tasks: 0.01949, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.11152, Variance: 0.01212
Semantic Loss - Mean: 0.06529, Variance: 0.00623

Test Epoch: 41 
task: sign, mean loss: 2.70292, accuracy: 0.52663, avg. loss over tasks: 2.70292
Diversity Loss - Mean: -0.11203, Variance: 0.01549
Semantic Loss - Mean: 2.05789, Variance: 0.05394

Train Epoch: 42 
task: sign, mean loss: 0.04495, accuracy: 0.98370, avg. loss over tasks: 0.04495, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.11291, Variance: 0.01214
Semantic Loss - Mean: 0.06942, Variance: 0.00620

Test Epoch: 42 
task: sign, mean loss: 2.60110, accuracy: 0.54438, avg. loss over tasks: 2.60110
Diversity Loss - Mean: -0.11172, Variance: 0.01552
Semantic Loss - Mean: 2.11246, Variance: 0.05548

Train Epoch: 43 
task: sign, mean loss: 0.06783, accuracy: 0.96739, avg. loss over tasks: 0.06783, lr: 0.000260757131773478
Diversity Loss - Mean: -0.11408, Variance: 0.01215
Semantic Loss - Mean: 0.08538, Variance: 0.00616

Test Epoch: 43 
task: sign, mean loss: 2.09783, accuracy: 0.62722, avg. loss over tasks: 2.09783
Diversity Loss - Mean: -0.12281, Variance: 0.01556
Semantic Loss - Mean: 1.79347, Variance: 0.05619

Train Epoch: 44 
task: sign, mean loss: 0.10872, accuracy: 0.95652, avg. loss over tasks: 0.10872, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.11826, Variance: 0.01218
Semantic Loss - Mean: 0.17133, Variance: 0.00625

Test Epoch: 44 
task: sign, mean loss: 1.69807, accuracy: 0.63905, avg. loss over tasks: 1.69807
Diversity Loss - Mean: -0.11345, Variance: 0.01559
Semantic Loss - Mean: 1.51561, Variance: 0.05600

Train Epoch: 45 
task: sign, mean loss: 0.97396, accuracy: 0.76630, avg. loss over tasks: 0.97396, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.12150, Variance: 0.01222
Semantic Loss - Mean: 0.92160, Variance: 0.00664

Test Epoch: 45 
task: sign, mean loss: 1.77878, accuracy: 0.55621, avg. loss over tasks: 1.77878
Diversity Loss - Mean: -0.12074, Variance: 0.01575
Semantic Loss - Mean: 1.59323, Variance: 0.05753

Train Epoch: 46 
task: sign, mean loss: 1.18913, accuracy: 0.61413, avg. loss over tasks: 1.18913, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.12651, Variance: 0.01233
Semantic Loss - Mean: 1.20700, Variance: 0.00661

Test Epoch: 46 
task: sign, mean loss: 1.69446, accuracy: 0.39053, avg. loss over tasks: 1.69446
Diversity Loss - Mean: -0.11908, Variance: 0.01582
Semantic Loss - Mean: 1.39863, Variance: 0.05766

Train Epoch: 47 
task: sign, mean loss: 1.07369, accuracy: 0.64674, avg. loss over tasks: 1.07369, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.12910, Variance: 0.01247
Semantic Loss - Mean: 1.07819, Variance: 0.00649

Test Epoch: 47 
task: sign, mean loss: 1.08858, accuracy: 0.66272, avg. loss over tasks: 1.08858
Diversity Loss - Mean: -0.13339, Variance: 0.01589
Semantic Loss - Mean: 1.09358, Variance: 0.05654

Train Epoch: 48 
task: sign, mean loss: 1.00183, accuracy: 0.65761, avg. loss over tasks: 1.00183, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.13195, Variance: 0.01262
Semantic Loss - Mean: 1.01428, Variance: 0.00637

Test Epoch: 48 
task: sign, mean loss: 1.05707, accuracy: 0.66272, avg. loss over tasks: 1.05707
Diversity Loss - Mean: -0.13486, Variance: 0.01597
Semantic Loss - Mean: 1.06120, Variance: 0.05541

Train Epoch: 49 
task: sign, mean loss: 1.03432, accuracy: 0.65761, avg. loss over tasks: 1.03432, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.13523, Variance: 0.01278
Semantic Loss - Mean: 1.02700, Variance: 0.00625

Test Epoch: 49 
task: sign, mean loss: 1.07151, accuracy: 0.66272, avg. loss over tasks: 1.07151
Diversity Loss - Mean: -0.13553, Variance: 0.01606
Semantic Loss - Mean: 1.07619, Variance: 0.05430

Train Epoch: 50 
task: sign, mean loss: 1.01723, accuracy: 0.65761, avg. loss over tasks: 1.01723, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.13851, Variance: 0.01296
Semantic Loss - Mean: 1.02516, Variance: 0.00613

Test Epoch: 50 
task: sign, mean loss: 1.07641, accuracy: 0.66272, avg. loss over tasks: 1.07641
Diversity Loss - Mean: -0.13581, Variance: 0.01615
Semantic Loss - Mean: 1.06514, Variance: 0.05323

Train Epoch: 51 
task: sign, mean loss: 0.98460, accuracy: 0.65761, avg. loss over tasks: 0.98460, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.13863, Variance: 0.01313
Semantic Loss - Mean: 0.99029, Variance: 0.00601

Test Epoch: 51 
task: sign, mean loss: 1.12690, accuracy: 0.66272, avg. loss over tasks: 1.12690
Diversity Loss - Mean: -0.13460, Variance: 0.01623
Semantic Loss - Mean: 1.11301, Variance: 0.05222

Train Epoch: 52 
task: sign, mean loss: 0.98842, accuracy: 0.65217, avg. loss over tasks: 0.98842, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.13873, Variance: 0.01327
Semantic Loss - Mean: 0.99069, Variance: 0.00591

Test Epoch: 52 
task: sign, mean loss: 1.17440, accuracy: 0.66272, avg. loss over tasks: 1.17440
Diversity Loss - Mean: -0.13228, Variance: 0.01629
Semantic Loss - Mean: 1.13850, Variance: 0.05126

Train Epoch: 53 
task: sign, mean loss: 1.05173, accuracy: 0.65761, avg. loss over tasks: 1.05173, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.13877, Variance: 0.01341
Semantic Loss - Mean: 1.05356, Variance: 0.00580

Test Epoch: 53 
task: sign, mean loss: 1.17010, accuracy: 0.66272, avg. loss over tasks: 1.17010
Diversity Loss - Mean: -0.13474, Variance: 0.01636
Semantic Loss - Mean: 1.14726, Variance: 0.05031

Train Epoch: 54 
task: sign, mean loss: 1.04025, accuracy: 0.67391, avg. loss over tasks: 1.04025, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.13898, Variance: 0.01354
Semantic Loss - Mean: 1.03904, Variance: 0.00570

Test Epoch: 54 
task: sign, mean loss: 1.17228, accuracy: 0.66272, avg. loss over tasks: 1.17228
Diversity Loss - Mean: -0.13285, Variance: 0.01641
Semantic Loss - Mean: 1.09514, Variance: 0.04940

Train Epoch: 55 
task: sign, mean loss: 1.04763, accuracy: 0.65761, avg. loss over tasks: 1.04763, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.13987, Variance: 0.01369
Semantic Loss - Mean: 1.05694, Variance: 0.00560

Test Epoch: 55 
task: sign, mean loss: 1.09835, accuracy: 0.66272, avg. loss over tasks: 1.09835
Diversity Loss - Mean: -0.13587, Variance: 0.01646
Semantic Loss - Mean: 1.06870, Variance: 0.04853

Train Epoch: 56 
task: sign, mean loss: 1.02738, accuracy: 0.66304, avg. loss over tasks: 1.02738, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.13937, Variance: 0.01382
Semantic Loss - Mean: 1.02726, Variance: 0.00551

Test Epoch: 56 
task: sign, mean loss: 1.17921, accuracy: 0.66272, avg. loss over tasks: 1.17921
Diversity Loss - Mean: -0.13457, Variance: 0.01652
Semantic Loss - Mean: 1.12293, Variance: 0.04769

Train Epoch: 57 
task: sign, mean loss: 0.98118, accuracy: 0.65761, avg. loss over tasks: 0.98118, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.14009, Variance: 0.01395
Semantic Loss - Mean: 0.97214, Variance: 0.00542

Test Epoch: 57 
task: sign, mean loss: 1.14249, accuracy: 0.66272, avg. loss over tasks: 1.14249
Diversity Loss - Mean: -0.13644, Variance: 0.01658
Semantic Loss - Mean: 1.09300, Variance: 0.04687

Train Epoch: 58 
task: sign, mean loss: 0.96264, accuracy: 0.65761, avg. loss over tasks: 0.96264, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.14004, Variance: 0.01408
Semantic Loss - Mean: 0.97208, Variance: 0.00533

Test Epoch: 58 
task: sign, mean loss: 1.11420, accuracy: 0.66272, avg. loss over tasks: 1.11420
Diversity Loss - Mean: -0.13759, Variance: 0.01664
Semantic Loss - Mean: 1.08075, Variance: 0.04609

Train Epoch: 59 
task: sign, mean loss: 0.98906, accuracy: 0.65217, avg. loss over tasks: 0.98906, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.13936, Variance: 0.01420
Semantic Loss - Mean: 0.98987, Variance: 0.00525

Test Epoch: 59 
task: sign, mean loss: 1.10209, accuracy: 0.66272, avg. loss over tasks: 1.10209
Diversity Loss - Mean: -0.13880, Variance: 0.01670
Semantic Loss - Mean: 1.07139, Variance: 0.04532

Train Epoch: 60 
task: sign, mean loss: 0.93695, accuracy: 0.65761, avg. loss over tasks: 0.93695, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.13901, Variance: 0.01432
Semantic Loss - Mean: 0.94158, Variance: 0.00517

Test Epoch: 60 
task: sign, mean loss: 1.07659, accuracy: 0.66272, avg. loss over tasks: 1.07659
Diversity Loss - Mean: -0.13987, Variance: 0.01676
Semantic Loss - Mean: 1.05510, Variance: 0.04458

Train Epoch: 61 
task: sign, mean loss: 0.97929, accuracy: 0.66848, avg. loss over tasks: 0.97929, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.13749, Variance: 0.01441
Semantic Loss - Mean: 0.97713, Variance: 0.00509

Test Epoch: 61 
task: sign, mean loss: 1.22907, accuracy: 0.62722, avg. loss over tasks: 1.22907
Diversity Loss - Mean: -0.13599, Variance: 0.01680
Semantic Loss - Mean: 1.18339, Variance: 0.04387

Train Epoch: 62 
task: sign, mean loss: 0.90110, accuracy: 0.71739, avg. loss over tasks: 0.90110, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.13556, Variance: 0.01448
Semantic Loss - Mean: 0.90347, Variance: 0.00502

Test Epoch: 62 
task: sign, mean loss: 1.24479, accuracy: 0.57988, avg. loss over tasks: 1.24479
Diversity Loss - Mean: -0.13231, Variance: 0.01683
Semantic Loss - Mean: 1.20768, Variance: 0.04319

Train Epoch: 63 
task: sign, mean loss: 0.89736, accuracy: 0.67935, avg. loss over tasks: 0.89736, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.13442, Variance: 0.01455
Semantic Loss - Mean: 0.90383, Variance: 0.00495

Test Epoch: 63 
task: sign, mean loss: 1.17114, accuracy: 0.65089, avg. loss over tasks: 1.17114
Diversity Loss - Mean: -0.13745, Variance: 0.01688
Semantic Loss - Mean: 1.14925, Variance: 0.04251

Train Epoch: 64 
task: sign, mean loss: 0.86515, accuracy: 0.69565, avg. loss over tasks: 0.86515, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.13549, Variance: 0.01462
Semantic Loss - Mean: 0.88683, Variance: 0.00488

Test Epoch: 64 
task: sign, mean loss: 1.16693, accuracy: 0.66272, avg. loss over tasks: 1.16693
Diversity Loss - Mean: -0.13858, Variance: 0.01692
Semantic Loss - Mean: 1.13823, Variance: 0.04186

Train Epoch: 65 
task: sign, mean loss: 0.83993, accuracy: 0.68478, avg. loss over tasks: 0.83993, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.13572, Variance: 0.01467
Semantic Loss - Mean: 0.85487, Variance: 0.00481

Test Epoch: 65 
task: sign, mean loss: 1.22082, accuracy: 0.66272, avg. loss over tasks: 1.22082
Diversity Loss - Mean: -0.13774, Variance: 0.01694
Semantic Loss - Mean: 1.20214, Variance: 0.04123

Train Epoch: 66 
task: sign, mean loss: 0.84437, accuracy: 0.71739, avg. loss over tasks: 0.84437, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.13476, Variance: 0.01471
Semantic Loss - Mean: 0.86081, Variance: 0.00475

Test Epoch: 66 
task: sign, mean loss: 1.31141, accuracy: 0.59763, avg. loss over tasks: 1.31141
Diversity Loss - Mean: -0.13599, Variance: 0.01696
Semantic Loss - Mean: 1.25913, Variance: 0.04063

Train Epoch: 67 
task: sign, mean loss: 0.79276, accuracy: 0.70109, avg. loss over tasks: 0.79276, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.13369, Variance: 0.01474
Semantic Loss - Mean: 0.81021, Variance: 0.00469

Test Epoch: 67 
task: sign, mean loss: 1.30873, accuracy: 0.63314, avg. loss over tasks: 1.30873
Diversity Loss - Mean: -0.13535, Variance: 0.01698
Semantic Loss - Mean: 1.25696, Variance: 0.04004

Train Epoch: 68 
task: sign, mean loss: 0.76451, accuracy: 0.72283, avg. loss over tasks: 0.76451, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.13389, Variance: 0.01477
Semantic Loss - Mean: 0.78523, Variance: 0.00463

Test Epoch: 68 
task: sign, mean loss: 1.43293, accuracy: 0.54438, avg. loss over tasks: 1.43293
Diversity Loss - Mean: -0.12995, Variance: 0.01697
Semantic Loss - Mean: 1.33746, Variance: 0.03951

Train Epoch: 69 
task: sign, mean loss: 0.79153, accuracy: 0.69565, avg. loss over tasks: 0.79153, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.13298, Variance: 0.01481
Semantic Loss - Mean: 0.80327, Variance: 0.00458

Test Epoch: 69 
task: sign, mean loss: 1.44398, accuracy: 0.63314, avg. loss over tasks: 1.44398
Diversity Loss - Mean: -0.13319, Variance: 0.01699
Semantic Loss - Mean: 1.34999, Variance: 0.03897

Train Epoch: 70 
task: sign, mean loss: 0.74393, accuracy: 0.70652, avg. loss over tasks: 0.74393, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.13251, Variance: 0.01483
Semantic Loss - Mean: 0.76951, Variance: 0.00454

Test Epoch: 70 
task: sign, mean loss: 1.83418, accuracy: 0.31953, avg. loss over tasks: 1.83418
Diversity Loss - Mean: -0.12107, Variance: 0.01697
Semantic Loss - Mean: 1.55862, Variance: 0.03850

Train Epoch: 71 
task: sign, mean loss: 0.75875, accuracy: 0.74457, avg. loss over tasks: 0.75875, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.13360, Variance: 0.01487
Semantic Loss - Mean: 0.77469, Variance: 0.00449

Test Epoch: 71 
task: sign, mean loss: 1.41750, accuracy: 0.65680, avg. loss over tasks: 1.41750
Diversity Loss - Mean: -0.13518, Variance: 0.01700
Semantic Loss - Mean: 1.35066, Variance: 0.03799

Train Epoch: 72 
task: sign, mean loss: 0.74828, accuracy: 0.69022, avg. loss over tasks: 0.74828, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.13198, Variance: 0.01491
Semantic Loss - Mean: 0.78047, Variance: 0.00445

Test Epoch: 72 
task: sign, mean loss: 1.38905, accuracy: 0.65680, avg. loss over tasks: 1.38905
Diversity Loss - Mean: -0.13494, Variance: 0.01702
Semantic Loss - Mean: 1.30557, Variance: 0.03754

Train Epoch: 73 
task: sign, mean loss: 0.78025, accuracy: 0.70652, avg. loss over tasks: 0.78025, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.13297, Variance: 0.01495
Semantic Loss - Mean: 0.80059, Variance: 0.00442

Test Epoch: 73 
task: sign, mean loss: 1.55289, accuracy: 0.64497, avg. loss over tasks: 1.55289
Diversity Loss - Mean: -0.13242, Variance: 0.01701
Semantic Loss - Mean: 1.41712, Variance: 0.03707

Train Epoch: 74 
task: sign, mean loss: 0.72382, accuracy: 0.70652, avg. loss over tasks: 0.72382, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.12919, Variance: 0.01497
Semantic Loss - Mean: 0.73032, Variance: 0.00439

Test Epoch: 74 
task: sign, mean loss: 1.70781, accuracy: 0.60355, avg. loss over tasks: 1.70781
Diversity Loss - Mean: -0.12912, Variance: 0.01701
Semantic Loss - Mean: 1.56068, Variance: 0.03662

Train Epoch: 75 
task: sign, mean loss: 0.67195, accuracy: 0.73913, avg. loss over tasks: 0.67195, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.12912, Variance: 0.01499
Semantic Loss - Mean: 0.70477, Variance: 0.00435

Test Epoch: 75 
task: sign, mean loss: 1.61596, accuracy: 0.63314, avg. loss over tasks: 1.61596
Diversity Loss - Mean: -0.12692, Variance: 0.01700
Semantic Loss - Mean: 1.49851, Variance: 0.03619

Train Epoch: 76 
task: sign, mean loss: 0.78484, accuracy: 0.69565, avg. loss over tasks: 0.78484, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.13148, Variance: 0.01501
Semantic Loss - Mean: 0.80168, Variance: 0.00431

