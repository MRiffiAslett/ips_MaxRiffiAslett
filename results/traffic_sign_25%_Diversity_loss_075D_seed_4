Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09174, accuracy: 0.63587, avg. loss over tasks: 1.09174, lr: 3e-05
Diversity Loss - Mean: -0.01167, Variance: 0.01051
Semantic Loss - Mean: 1.43131, Variance: 0.07300

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.18160, accuracy: 0.66272, avg. loss over tasks: 1.18160
Diversity Loss - Mean: -0.03380, Variance: 0.01253
Semantic Loss - Mean: 1.16317, Variance: 0.05402

Train Epoch: 2 
task: sign, mean loss: 0.96352, accuracy: 0.67935, avg. loss over tasks: 0.96352, lr: 6e-05
Diversity Loss - Mean: -0.02942, Variance: 0.01053
Semantic Loss - Mean: 0.98020, Variance: 0.03940

Test Epoch: 2 
task: sign, mean loss: 1.10694, accuracy: 0.66272, avg. loss over tasks: 1.10694
Diversity Loss - Mean: -0.04819, Variance: 0.01219
Semantic Loss - Mean: 1.13991, Variance: 0.03274

Train Epoch: 3 
task: sign, mean loss: 0.79501, accuracy: 0.70109, avg. loss over tasks: 0.79501, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.06118, Variance: 0.01044
Semantic Loss - Mean: 0.98878, Variance: 0.02734

Test Epoch: 3 
task: sign, mean loss: 1.27879, accuracy: 0.59763, avg. loss over tasks: 1.27879
Diversity Loss - Mean: -0.08215, Variance: 0.01156
Semantic Loss - Mean: 1.10626, Variance: 0.03017

Train Epoch: 4 
task: sign, mean loss: 0.76116, accuracy: 0.67391, avg. loss over tasks: 0.76116, lr: 0.00012
Diversity Loss - Mean: -0.08956, Variance: 0.01029
Semantic Loss - Mean: 0.89172, Variance: 0.02114

Test Epoch: 4 
task: sign, mean loss: 1.50139, accuracy: 0.45562, avg. loss over tasks: 1.50139
Diversity Loss - Mean: -0.08978, Variance: 0.01096
Semantic Loss - Mean: 1.09186, Variance: 0.02515

Train Epoch: 5 
task: sign, mean loss: 0.75696, accuracy: 0.67935, avg. loss over tasks: 0.75696, lr: 0.00015
Diversity Loss - Mean: -0.08255, Variance: 0.01010
Semantic Loss - Mean: 0.79600, Variance: 0.01732

Test Epoch: 5 
task: sign, mean loss: 1.70392, accuracy: 0.50888, avg. loss over tasks: 1.70392
Diversity Loss - Mean: -0.09016, Variance: 0.01069
Semantic Loss - Mean: 1.19621, Variance: 0.02334

Train Epoch: 6 
task: sign, mean loss: 0.72091, accuracy: 0.77717, avg. loss over tasks: 0.72091, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.07372, Variance: 0.01003
Semantic Loss - Mean: 0.72169, Variance: 0.01477

Test Epoch: 6 
task: sign, mean loss: 1.81281, accuracy: 0.65680, avg. loss over tasks: 1.81281
Diversity Loss - Mean: -0.07544, Variance: 0.01125
Semantic Loss - Mean: 1.42920, Variance: 0.02236

Train Epoch: 7 
task: sign, mean loss: 0.59324, accuracy: 0.78261, avg. loss over tasks: 0.59324, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.08159, Variance: 0.01019
Semantic Loss - Mean: 0.64848, Variance: 0.01299

Test Epoch: 7 
task: sign, mean loss: 1.72660, accuracy: 0.42012, avg. loss over tasks: 1.72660
Diversity Loss - Mean: -0.07152, Variance: 0.01113
Semantic Loss - Mean: 1.42940, Variance: 0.02286

Train Epoch: 8 
task: sign, mean loss: 0.61811, accuracy: 0.80435, avg. loss over tasks: 0.61811, lr: 0.00024
Diversity Loss - Mean: -0.06659, Variance: 0.01026
Semantic Loss - Mean: 0.66263, Variance: 0.01204

Test Epoch: 8 
task: sign, mean loss: 2.00730, accuracy: 0.66272, avg. loss over tasks: 2.00730
Diversity Loss - Mean: -0.02550, Variance: 0.01158
Semantic Loss - Mean: 1.64389, Variance: 0.02297

Train Epoch: 9 
task: sign, mean loss: 0.76468, accuracy: 0.72826, avg. loss over tasks: 0.76468, lr: 0.00027
Diversity Loss - Mean: -0.07971, Variance: 0.01029
Semantic Loss - Mean: 0.75169, Variance: 0.01127

Test Epoch: 9 
task: sign, mean loss: 1.48388, accuracy: 0.64497, avg. loss over tasks: 1.48388
Diversity Loss - Mean: -0.07143, Variance: 0.01184
Semantic Loss - Mean: 1.40319, Variance: 0.02458

Train Epoch: 10 
task: sign, mean loss: 0.83205, accuracy: 0.69022, avg. loss over tasks: 0.83205, lr: 0.0003
Diversity Loss - Mean: -0.07740, Variance: 0.01033
Semantic Loss - Mean: 0.70467, Variance: 0.01046

Test Epoch: 10 
task: sign, mean loss: 2.11028, accuracy: 0.52071, avg. loss over tasks: 2.11028
Diversity Loss - Mean: -0.07613, Variance: 0.01207
Semantic Loss - Mean: 1.61637, Variance: 0.02398

Train Epoch: 11 
task: sign, mean loss: 0.58636, accuracy: 0.80435, avg. loss over tasks: 0.58636, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.06716, Variance: 0.01038
Semantic Loss - Mean: 0.56323, Variance: 0.00994

Test Epoch: 11 
task: sign, mean loss: 2.04919, accuracy: 0.64497, avg. loss over tasks: 2.04919
Diversity Loss - Mean: -0.04574, Variance: 0.01227
Semantic Loss - Mean: 1.86113, Variance: 0.02356

Train Epoch: 12 
task: sign, mean loss: 0.52052, accuracy: 0.85326, avg. loss over tasks: 0.52052, lr: 0.000299849111021216
Diversity Loss - Mean: -0.04697, Variance: 0.01036
Semantic Loss - Mean: 0.54485, Variance: 0.00962

Test Epoch: 12 
task: sign, mean loss: 2.08379, accuracy: 0.52071, avg. loss over tasks: 2.08379
Diversity Loss - Mean: -0.07754, Variance: 0.01250
Semantic Loss - Mean: 1.91819, Variance: 0.02735

Train Epoch: 13 
task: sign, mean loss: 0.47903, accuracy: 0.85326, avg. loss over tasks: 0.47903, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.07213, Variance: 0.01042
Semantic Loss - Mean: 0.51183, Variance: 0.00920

Test Epoch: 13 
task: sign, mean loss: 1.52841, accuracy: 0.66272, avg. loss over tasks: 1.52841
Diversity Loss - Mean: -0.10917, Variance: 0.01279
Semantic Loss - Mean: 1.45074, Variance: 0.02893

Train Epoch: 14 
task: sign, mean loss: 0.54358, accuracy: 0.83696, avg. loss over tasks: 0.54358, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.08485, Variance: 0.01057
Semantic Loss - Mean: 0.53561, Variance: 0.00891

Test Epoch: 14 
task: sign, mean loss: 1.78455, accuracy: 0.40237, avg. loss over tasks: 1.78455
Diversity Loss - Mean: -0.07028, Variance: 0.01290
Semantic Loss - Mean: 1.68845, Variance: 0.02975

Train Epoch: 15 
task: sign, mean loss: 0.39216, accuracy: 0.85326, avg. loss over tasks: 0.39216, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.08298, Variance: 0.01062
Semantic Loss - Mean: 0.42658, Variance: 0.00860

Test Epoch: 15 
task: sign, mean loss: 2.47205, accuracy: 0.41420, avg. loss over tasks: 2.47205
Diversity Loss - Mean: -0.05659, Variance: 0.01304
Semantic Loss - Mean: 2.16359, Variance: 0.03488

Train Epoch: 16 
task: sign, mean loss: 0.42275, accuracy: 0.89130, avg. loss over tasks: 0.42275, lr: 0.000298643821800925
Diversity Loss - Mean: -0.08563, Variance: 0.01072
Semantic Loss - Mean: 0.46882, Variance: 0.00844

Test Epoch: 16 
task: sign, mean loss: 1.05240, accuracy: 0.67456, avg. loss over tasks: 1.05240
Diversity Loss - Mean: -0.08595, Variance: 0.01323
Semantic Loss - Mean: 1.01468, Variance: 0.03374

Train Epoch: 17 
task: sign, mean loss: 0.19550, accuracy: 0.92935, avg. loss over tasks: 0.19550, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.08020, Variance: 0.01077
Semantic Loss - Mean: 0.27247, Variance: 0.00837

Test Epoch: 17 
task: sign, mean loss: 1.60524, accuracy: 0.66272, avg. loss over tasks: 1.60524
Diversity Loss - Mean: -0.09798, Variance: 0.01333
Semantic Loss - Mean: 1.39901, Variance: 0.03269

Train Epoch: 18 
task: sign, mean loss: 0.39245, accuracy: 0.92391, avg. loss over tasks: 0.39245, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.07655, Variance: 0.01078
Semantic Loss - Mean: 0.42153, Variance: 0.00817

Test Epoch: 18 
task: sign, mean loss: 1.81639, accuracy: 0.62130, avg. loss over tasks: 1.81639
Diversity Loss - Mean: -0.09902, Variance: 0.01332
Semantic Loss - Mean: 1.63057, Variance: 0.03350

Train Epoch: 19 
task: sign, mean loss: 0.18862, accuracy: 0.92935, avg. loss over tasks: 0.18862, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.08658, Variance: 0.01082
Semantic Loss - Mean: 0.26032, Variance: 0.00809

Test Epoch: 19 
task: sign, mean loss: 1.08570, accuracy: 0.71598, avg. loss over tasks: 1.08570
Diversity Loss - Mean: -0.09076, Variance: 0.01328
Semantic Loss - Mean: 1.03593, Variance: 0.03238

Train Epoch: 20 
task: sign, mean loss: 0.16832, accuracy: 0.92935, avg. loss over tasks: 0.16832, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.09023, Variance: 0.01088
Semantic Loss - Mean: 0.22544, Variance: 0.00795

Test Epoch: 20 
task: sign, mean loss: 1.32055, accuracy: 0.68639, avg. loss over tasks: 1.32055
Diversity Loss - Mean: -0.07234, Variance: 0.01313
Semantic Loss - Mean: 1.28509, Variance: 0.03240

Train Epoch: 21 
task: sign, mean loss: 0.16759, accuracy: 0.95109, avg. loss over tasks: 0.16759, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.07869, Variance: 0.01088
Semantic Loss - Mean: 0.20418, Variance: 0.00780

Test Epoch: 21 
task: sign, mean loss: 1.44167, accuracy: 0.67456, avg. loss over tasks: 1.44167
Diversity Loss - Mean: -0.06339, Variance: 0.01302
Semantic Loss - Mean: 1.13086, Variance: 0.03307

Train Epoch: 22 
task: sign, mean loss: 0.13687, accuracy: 0.95109, avg. loss over tasks: 0.13687, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.08841, Variance: 0.01090
Semantic Loss - Mean: 0.21095, Variance: 0.00762

Test Epoch: 22 
task: sign, mean loss: 1.60699, accuracy: 0.56213, avg. loss over tasks: 1.60699
Diversity Loss - Mean: -0.05686, Variance: 0.01296
Semantic Loss - Mean: 1.46692, Variance: 0.03449

Train Epoch: 23 
task: sign, mean loss: 0.09110, accuracy: 0.95652, avg. loss over tasks: 0.09110, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.08285, Variance: 0.01090
Semantic Loss - Mean: 0.13468, Variance: 0.00739

Test Epoch: 23 
task: sign, mean loss: 1.17138, accuracy: 0.69822, avg. loss over tasks: 1.17138
Diversity Loss - Mean: -0.08947, Variance: 0.01295
Semantic Loss - Mean: 1.04592, Variance: 0.03438

Train Epoch: 24 
task: sign, mean loss: 0.07641, accuracy: 0.97826, avg. loss over tasks: 0.07641, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.09282, Variance: 0.01094
Semantic Loss - Mean: 0.11593, Variance: 0.00723

Test Epoch: 24 
task: sign, mean loss: 1.64134, accuracy: 0.69231, avg. loss over tasks: 1.64134
Diversity Loss - Mean: -0.08411, Variance: 0.01292
Semantic Loss - Mean: 1.56443, Variance: 0.03718

Train Epoch: 25 
task: sign, mean loss: 0.04826, accuracy: 0.98370, avg. loss over tasks: 0.04826, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.09208, Variance: 0.01098
Semantic Loss - Mean: 0.09373, Variance: 0.00704

Test Epoch: 25 
task: sign, mean loss: 1.28402, accuracy: 0.65680, avg. loss over tasks: 1.28402
Diversity Loss - Mean: -0.09034, Variance: 0.01289
Semantic Loss - Mean: 1.14908, Variance: 0.03771

Train Epoch: 26 
task: sign, mean loss: 0.02380, accuracy: 1.00000, avg. loss over tasks: 0.02380, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.09755, Variance: 0.01104
Semantic Loss - Mean: 0.07618, Variance: 0.00691

Test Epoch: 26 
task: sign, mean loss: 1.77691, accuracy: 0.69822, avg. loss over tasks: 1.77691
Diversity Loss - Mean: -0.09643, Variance: 0.01291
Semantic Loss - Mean: 1.43192, Variance: 0.03878

Train Epoch: 27 
task: sign, mean loss: 0.07797, accuracy: 0.96739, avg. loss over tasks: 0.07797, lr: 0.000289228031029578
Diversity Loss - Mean: -0.09753, Variance: 0.01109
Semantic Loss - Mean: 0.09986, Variance: 0.00695

Test Epoch: 27 
task: sign, mean loss: 2.11467, accuracy: 0.63314, avg. loss over tasks: 2.11467
Diversity Loss - Mean: -0.07586, Variance: 0.01284
Semantic Loss - Mean: 2.00904, Variance: 0.04274

Train Epoch: 28 
task: sign, mean loss: 0.25431, accuracy: 0.91304, avg. loss over tasks: 0.25431, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.09870, Variance: 0.01111
Semantic Loss - Mean: 0.26951, Variance: 0.00703

Test Epoch: 28 
task: sign, mean loss: 2.19217, accuracy: 0.63314, avg. loss over tasks: 2.19217
Diversity Loss - Mean: -0.09335, Variance: 0.01291
Semantic Loss - Mean: 1.87740, Variance: 0.04368

Train Epoch: 29 
task: sign, mean loss: 0.27254, accuracy: 0.90217, avg. loss over tasks: 0.27254, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.10758, Variance: 0.01116
Semantic Loss - Mean: 0.31927, Variance: 0.00755

Test Epoch: 29 
task: sign, mean loss: 1.23905, accuracy: 0.75740, avg. loss over tasks: 1.23905
Diversity Loss - Mean: -0.10369, Variance: 0.01312
Semantic Loss - Mean: 1.06248, Variance: 0.04373

Train Epoch: 30 
task: sign, mean loss: 0.31897, accuracy: 0.86957, avg. loss over tasks: 0.31897, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.10842, Variance: 0.01124
Semantic Loss - Mean: 0.37072, Variance: 0.00812

Test Epoch: 30 
task: sign, mean loss: 2.11230, accuracy: 0.72189, avg. loss over tasks: 2.11230
Diversity Loss - Mean: -0.11611, Variance: 0.01337
Semantic Loss - Mean: 1.82291, Variance: 0.04462

Train Epoch: 31 
task: sign, mean loss: 0.27179, accuracy: 0.88043, avg. loss over tasks: 0.27179, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.11129, Variance: 0.01131
Semantic Loss - Mean: 0.38037, Variance: 0.00831

Test Epoch: 31 
task: sign, mean loss: 1.42819, accuracy: 0.72189, avg. loss over tasks: 1.42819
Diversity Loss - Mean: -0.12290, Variance: 0.01351
Semantic Loss - Mean: 1.18242, Variance: 0.04546

Train Epoch: 32 
task: sign, mean loss: 0.25838, accuracy: 0.89674, avg. loss over tasks: 0.25838, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.11182, Variance: 0.01140
Semantic Loss - Mean: 0.33581, Variance: 0.00844

Test Epoch: 32 
task: sign, mean loss: 1.61534, accuracy: 0.66272, avg. loss over tasks: 1.61534
Diversity Loss - Mean: -0.09060, Variance: 0.01351
Semantic Loss - Mean: 1.48023, Variance: 0.04779

Train Epoch: 33 
task: sign, mean loss: 0.22784, accuracy: 0.89130, avg. loss over tasks: 0.22784, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.10845, Variance: 0.01145
Semantic Loss - Mean: 0.30981, Variance: 0.00856

Test Epoch: 33 
task: sign, mean loss: 1.45133, accuracy: 0.55621, avg. loss over tasks: 1.45133
Diversity Loss - Mean: -0.08856, Variance: 0.01347
Semantic Loss - Mean: 1.06396, Variance: 0.05046

Train Epoch: 34 
task: sign, mean loss: 0.32692, accuracy: 0.88587, avg. loss over tasks: 0.32692, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.11217, Variance: 0.01152
Semantic Loss - Mean: 0.36461, Variance: 0.00893

Test Epoch: 34 
task: sign, mean loss: 1.83999, accuracy: 0.63905, avg. loss over tasks: 1.83999
Diversity Loss - Mean: -0.10295, Variance: 0.01350
Semantic Loss - Mean: 1.53097, Variance: 0.05127

Train Epoch: 35 
task: sign, mean loss: 0.26844, accuracy: 0.90761, avg. loss over tasks: 0.26844, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.11437, Variance: 0.01160
Semantic Loss - Mean: 0.32853, Variance: 0.00897

Test Epoch: 35 
task: sign, mean loss: 0.65896, accuracy: 0.76331, avg. loss over tasks: 0.65896
Diversity Loss - Mean: -0.11708, Variance: 0.01355
Semantic Loss - Mean: 0.68304, Variance: 0.05042

Train Epoch: 36 
task: sign, mean loss: 0.35008, accuracy: 0.87500, avg. loss over tasks: 0.35008, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.11541, Variance: 0.01165
Semantic Loss - Mean: 0.40346, Variance: 0.00896

Test Epoch: 36 
task: sign, mean loss: 3.11624, accuracy: 0.27811, avg. loss over tasks: 3.11624
Diversity Loss - Mean: -0.08429, Variance: 0.01369
Semantic Loss - Mean: 2.25638, Variance: 0.05171

Train Epoch: 37 
task: sign, mean loss: 0.37877, accuracy: 0.84783, avg. loss over tasks: 0.37877, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.11593, Variance: 0.01172
Semantic Loss - Mean: 0.43174, Variance: 0.00907

Test Epoch: 37 
task: sign, mean loss: 2.45022, accuracy: 0.40237, avg. loss over tasks: 2.45022
Diversity Loss - Mean: -0.09046, Variance: 0.01368
Semantic Loss - Mean: 2.01272, Variance: 0.05449

Train Epoch: 38 
task: sign, mean loss: 0.36412, accuracy: 0.84783, avg. loss over tasks: 0.36412, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.11701, Variance: 0.01181
Semantic Loss - Mean: 0.42633, Variance: 0.00910

Test Epoch: 38 
task: sign, mean loss: 2.00834, accuracy: 0.72781, avg. loss over tasks: 2.00834
Diversity Loss - Mean: -0.10760, Variance: 0.01366
Semantic Loss - Mean: 1.57424, Variance: 0.05512

Train Epoch: 39 
task: sign, mean loss: 1.20360, accuracy: 0.61413, avg. loss over tasks: 1.20360, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.12016, Variance: 0.01191
Semantic Loss - Mean: 1.14185, Variance: 0.00909

Test Epoch: 39 
task: sign, mean loss: 1.59219, accuracy: 0.45562, avg. loss over tasks: 1.59219
Diversity Loss - Mean: -0.10454, Variance: 0.01361
Semantic Loss - Mean: 1.46492, Variance: 0.05458

Train Epoch: 40 
task: sign, mean loss: 0.81937, accuracy: 0.70109, avg. loss over tasks: 0.81937, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.12197, Variance: 0.01202
Semantic Loss - Mean: 0.83801, Variance: 0.00892

Test Epoch: 40 
task: sign, mean loss: 1.41788, accuracy: 0.45562, avg. loss over tasks: 1.41788
Diversity Loss - Mean: -0.10843, Variance: 0.01359
Semantic Loss - Mean: 1.36483, Variance: 0.05367

Train Epoch: 41 
task: sign, mean loss: 0.75434, accuracy: 0.72283, avg. loss over tasks: 0.75434, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.12306, Variance: 0.01215
Semantic Loss - Mean: 0.75687, Variance: 0.00874

Test Epoch: 41 
task: sign, mean loss: 1.68310, accuracy: 0.40828, avg. loss over tasks: 1.68310
Diversity Loss - Mean: -0.11073, Variance: 0.01360
Semantic Loss - Mean: 1.58658, Variance: 0.05304

Train Epoch: 42 
task: sign, mean loss: 0.80751, accuracy: 0.71739, avg. loss over tasks: 0.80751, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.12480, Variance: 0.01225
Semantic Loss - Mean: 0.81697, Variance: 0.00858

Test Epoch: 42 
task: sign, mean loss: 1.68450, accuracy: 0.43195, avg. loss over tasks: 1.68450
Diversity Loss - Mean: -0.11594, Variance: 0.01363
Semantic Loss - Mean: 1.65363, Variance: 0.05208

Train Epoch: 43 
task: sign, mean loss: 0.66897, accuracy: 0.76087, avg. loss over tasks: 0.66897, lr: 0.000260757131773478
Diversity Loss - Mean: -0.12549, Variance: 0.01235
Semantic Loss - Mean: 0.71357, Variance: 0.00841

Test Epoch: 43 
task: sign, mean loss: 1.19896, accuracy: 0.63905, avg. loss over tasks: 1.19896
Diversity Loss - Mean: -0.12878, Variance: 0.01365
Semantic Loss - Mean: 1.12903, Variance: 0.05095

Train Epoch: 44 
task: sign, mean loss: 0.67300, accuracy: 0.71196, avg. loss over tasks: 0.67300, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.12371, Variance: 0.01245
Semantic Loss - Mean: 0.68324, Variance: 0.00824

Test Epoch: 44 
task: sign, mean loss: 1.89013, accuracy: 0.39645, avg. loss over tasks: 1.89013
Diversity Loss - Mean: -0.11012, Variance: 0.01360
Semantic Loss - Mean: 1.72438, Variance: 0.04988

Train Epoch: 45 
task: sign, mean loss: 0.52723, accuracy: 0.76087, avg. loss over tasks: 0.52723, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.12362, Variance: 0.01250
Semantic Loss - Mean: 0.56952, Variance: 0.00808

Test Epoch: 45 
task: sign, mean loss: 1.66254, accuracy: 0.55621, avg. loss over tasks: 1.66254
Diversity Loss - Mean: -0.11933, Variance: 0.01358
Semantic Loss - Mean: 1.43572, Variance: 0.04905

Train Epoch: 46 
task: sign, mean loss: 0.36314, accuracy: 0.85326, avg. loss over tasks: 0.36314, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.12045, Variance: 0.01255
Semantic Loss - Mean: 0.39952, Variance: 0.00793

Test Epoch: 46 
task: sign, mean loss: 1.90380, accuracy: 0.55621, avg. loss over tasks: 1.90380
Diversity Loss - Mean: -0.12348, Variance: 0.01362
Semantic Loss - Mean: 1.70667, Variance: 0.04842

Train Epoch: 47 
task: sign, mean loss: 0.31761, accuracy: 0.86957, avg. loss over tasks: 0.31761, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.11674, Variance: 0.01258
Semantic Loss - Mean: 0.34194, Variance: 0.00782

Test Epoch: 47 
task: sign, mean loss: 1.60075, accuracy: 0.63905, avg. loss over tasks: 1.60075
Diversity Loss - Mean: -0.11459, Variance: 0.01361
Semantic Loss - Mean: 1.47136, Variance: 0.04748

Train Epoch: 48 
task: sign, mean loss: 0.34788, accuracy: 0.85870, avg. loss over tasks: 0.34788, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.11364, Variance: 0.01261
Semantic Loss - Mean: 0.39132, Variance: 0.00774

Test Epoch: 48 
task: sign, mean loss: 2.32404, accuracy: 0.56213, avg. loss over tasks: 2.32404
Diversity Loss - Mean: -0.11450, Variance: 0.01362
Semantic Loss - Mean: 2.23046, Variance: 0.04699

Train Epoch: 49 
task: sign, mean loss: 0.46051, accuracy: 0.83696, avg. loss over tasks: 0.46051, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.11792, Variance: 0.01264
Semantic Loss - Mean: 0.48339, Variance: 0.00774

Test Epoch: 49 
task: sign, mean loss: 2.77607, accuracy: 0.26627, avg. loss over tasks: 2.77607
Diversity Loss - Mean: -0.12360, Variance: 0.01364
Semantic Loss - Mean: 2.19729, Variance: 0.04670

Train Epoch: 50 
task: sign, mean loss: 0.33696, accuracy: 0.86413, avg. loss over tasks: 0.33696, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.12365, Variance: 0.01268
Semantic Loss - Mean: 0.36713, Variance: 0.00768

Test Epoch: 50 
task: sign, mean loss: 2.00678, accuracy: 0.63905, avg. loss over tasks: 2.00678
Diversity Loss - Mean: -0.13486, Variance: 0.01376
Semantic Loss - Mean: 1.74148, Variance: 0.04590

Train Epoch: 51 
task: sign, mean loss: 0.31498, accuracy: 0.86413, avg. loss over tasks: 0.31498, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.12162, Variance: 0.01274
Semantic Loss - Mean: 0.34998, Variance: 0.00761

Test Epoch: 51 
task: sign, mean loss: 2.06264, accuracy: 0.53254, avg. loss over tasks: 2.06264
Diversity Loss - Mean: -0.12127, Variance: 0.01382
Semantic Loss - Mean: 1.89623, Variance: 0.04549

Train Epoch: 52 
task: sign, mean loss: 0.16351, accuracy: 0.95652, avg. loss over tasks: 0.16351, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.12058, Variance: 0.01278
Semantic Loss - Mean: 0.20776, Variance: 0.00748

Test Epoch: 52 
task: sign, mean loss: 1.87359, accuracy: 0.55621, avg. loss over tasks: 1.87359
Diversity Loss - Mean: -0.12598, Variance: 0.01385
Semantic Loss - Mean: 1.73910, Variance: 0.04499

Train Epoch: 53 
task: sign, mean loss: 0.27832, accuracy: 0.91304, avg. loss over tasks: 0.27832, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.11786, Variance: 0.01282
Semantic Loss - Mean: 0.30063, Variance: 0.00738

Test Epoch: 53 
task: sign, mean loss: 2.60729, accuracy: 0.42604, avg. loss over tasks: 2.60729
Diversity Loss - Mean: -0.12392, Variance: 0.01386
Semantic Loss - Mean: 2.44744, Variance: 0.04431

Train Epoch: 54 
task: sign, mean loss: 0.24737, accuracy: 0.90217, avg. loss over tasks: 0.24737, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.11937, Variance: 0.01286
Semantic Loss - Mean: 0.30291, Variance: 0.00729

Test Epoch: 54 
task: sign, mean loss: 2.81268, accuracy: 0.52071, avg. loss over tasks: 2.81268
Diversity Loss - Mean: -0.12714, Variance: 0.01391
Semantic Loss - Mean: 2.33539, Variance: 0.04381

Train Epoch: 55 
task: sign, mean loss: 0.21936, accuracy: 0.90761, avg. loss over tasks: 0.21936, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.12118, Variance: 0.01289
Semantic Loss - Mean: 0.25851, Variance: 0.00722

Test Epoch: 55 
task: sign, mean loss: 3.89743, accuracy: 0.18935, avg. loss over tasks: 3.89743
Diversity Loss - Mean: -0.11266, Variance: 0.01392
Semantic Loss - Mean: 3.82283, Variance: 0.04439

Train Epoch: 56 
task: sign, mean loss: 0.22672, accuracy: 0.91304, avg. loss over tasks: 0.22672, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.12116, Variance: 0.01292
Semantic Loss - Mean: 0.28240, Variance: 0.00718

Test Epoch: 56 
task: sign, mean loss: 2.11721, accuracy: 0.57396, avg. loss over tasks: 2.11721
Diversity Loss - Mean: -0.11883, Variance: 0.01394
Semantic Loss - Mean: 1.93318, Variance: 0.04390

Train Epoch: 57 
task: sign, mean loss: 0.13482, accuracy: 0.94565, avg. loss over tasks: 0.13482, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.12063, Variance: 0.01295
Semantic Loss - Mean: 0.17500, Variance: 0.00709

Test Epoch: 57 
task: sign, mean loss: 2.00479, accuracy: 0.57396, avg. loss over tasks: 2.00479
Diversity Loss - Mean: -0.12707, Variance: 0.01397
Semantic Loss - Mean: 1.86571, Variance: 0.04334

Train Epoch: 58 
task: sign, mean loss: 0.10368, accuracy: 0.95652, avg. loss over tasks: 0.10368, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.12255, Variance: 0.01298
Semantic Loss - Mean: 0.12383, Variance: 0.00700

Test Epoch: 58 
task: sign, mean loss: 2.39985, accuracy: 0.44379, avg. loss over tasks: 2.39985
Diversity Loss - Mean: -0.12428, Variance: 0.01399
Semantic Loss - Mean: 2.39834, Variance: 0.04297

Train Epoch: 59 
task: sign, mean loss: 0.09359, accuracy: 0.98370, avg. loss over tasks: 0.09359, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.12069, Variance: 0.01300
Semantic Loss - Mean: 0.11367, Variance: 0.00690

Test Epoch: 59 
task: sign, mean loss: 2.83056, accuracy: 0.47929, avg. loss over tasks: 2.83056
Diversity Loss - Mean: -0.12873, Variance: 0.01401
Semantic Loss - Mean: 2.49515, Variance: 0.04278

Train Epoch: 60 
task: sign, mean loss: 0.13792, accuracy: 0.93478, avg. loss over tasks: 0.13792, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.12228, Variance: 0.01302
Semantic Loss - Mean: 0.15443, Variance: 0.00684

Test Epoch: 60 
task: sign, mean loss: 2.78719, accuracy: 0.44970, avg. loss over tasks: 2.78719
Diversity Loss - Mean: -0.12831, Variance: 0.01401
Semantic Loss - Mean: 2.43858, Variance: 0.04237

Train Epoch: 61 
task: sign, mean loss: 0.09270, accuracy: 0.96739, avg. loss over tasks: 0.09270, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.12371, Variance: 0.01304
Semantic Loss - Mean: 0.12933, Variance: 0.00680

Test Epoch: 61 
task: sign, mean loss: 2.22447, accuracy: 0.47929, avg. loss over tasks: 2.22447
Diversity Loss - Mean: -0.12546, Variance: 0.01400
Semantic Loss - Mean: 2.15681, Variance: 0.04212

Train Epoch: 62 
task: sign, mean loss: 0.17458, accuracy: 0.94022, avg. loss over tasks: 0.17458, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.12281, Variance: 0.01306
Semantic Loss - Mean: 0.19755, Variance: 0.00683

Test Epoch: 62 
task: sign, mean loss: 2.61051, accuracy: 0.42604, avg. loss over tasks: 2.61051
Diversity Loss - Mean: -0.12488, Variance: 0.01400
Semantic Loss - Mean: 2.52141, Variance: 0.04208

Train Epoch: 63 
task: sign, mean loss: 0.19021, accuracy: 0.94022, avg. loss over tasks: 0.19021, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.12520, Variance: 0.01307
Semantic Loss - Mean: 0.23289, Variance: 0.00680

Test Epoch: 63 
task: sign, mean loss: 3.11995, accuracy: 0.44970, avg. loss over tasks: 3.11995
Diversity Loss - Mean: -0.13172, Variance: 0.01401
Semantic Loss - Mean: 2.68117, Variance: 0.04175

Train Epoch: 64 
task: sign, mean loss: 0.13033, accuracy: 0.95109, avg. loss over tasks: 0.13033, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.12676, Variance: 0.01308
Semantic Loss - Mean: 0.16350, Variance: 0.00673

Test Epoch: 64 
task: sign, mean loss: 2.92218, accuracy: 0.49112, avg. loss over tasks: 2.92218
Diversity Loss - Mean: -0.13363, Variance: 0.01403
Semantic Loss - Mean: 2.49302, Variance: 0.04131

Train Epoch: 65 
task: sign, mean loss: 0.26829, accuracy: 0.90761, avg. loss over tasks: 0.26829, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.12775, Variance: 0.01309
Semantic Loss - Mean: 0.27401, Variance: 0.00672

Test Epoch: 65 
task: sign, mean loss: 3.18285, accuracy: 0.27811, avg. loss over tasks: 3.18285
Diversity Loss - Mean: -0.12528, Variance: 0.01408
Semantic Loss - Mean: 2.95873, Variance: 0.04184

Train Epoch: 66 
task: sign, mean loss: 0.26893, accuracy: 0.90761, avg. loss over tasks: 0.26893, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.12893, Variance: 0.01311
Semantic Loss - Mean: 0.32091, Variance: 0.00671

Test Epoch: 66 
task: sign, mean loss: 3.29007, accuracy: 0.23669, avg. loss over tasks: 3.29007
Diversity Loss - Mean: -0.12581, Variance: 0.01409
Semantic Loss - Mean: 2.96783, Variance: 0.04146

Train Epoch: 67 
task: sign, mean loss: 0.11479, accuracy: 0.96196, avg. loss over tasks: 0.11479, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.12888, Variance: 0.01312
Semantic Loss - Mean: 0.14679, Variance: 0.00664

Test Epoch: 67 
task: sign, mean loss: 2.78813, accuracy: 0.46154, avg. loss over tasks: 2.78813
Diversity Loss - Mean: -0.13253, Variance: 0.01412
Semantic Loss - Mean: 2.47587, Variance: 0.04119

Train Epoch: 68 
task: sign, mean loss: 0.11224, accuracy: 0.95109, avg. loss over tasks: 0.11224, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.12880, Variance: 0.01314
Semantic Loss - Mean: 0.13744, Variance: 0.00657

Test Epoch: 68 
task: sign, mean loss: 2.70074, accuracy: 0.45562, avg. loss over tasks: 2.70074
Diversity Loss - Mean: -0.13184, Variance: 0.01414
Semantic Loss - Mean: 2.43271, Variance: 0.04105

Train Epoch: 69 
task: sign, mean loss: 0.18154, accuracy: 0.94565, avg. loss over tasks: 0.18154, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.12817, Variance: 0.01314
Semantic Loss - Mean: 0.24781, Variance: 0.00664

Test Epoch: 69 
task: sign, mean loss: 2.70702, accuracy: 0.33728, avg. loss over tasks: 2.70702
Diversity Loss - Mean: -0.12981, Variance: 0.01414
Semantic Loss - Mean: 2.40961, Variance: 0.04112

Train Epoch: 70 
task: sign, mean loss: 0.12959, accuracy: 0.95109, avg. loss over tasks: 0.12959, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.12923, Variance: 0.01315
Semantic Loss - Mean: 0.16791, Variance: 0.00658

Test Epoch: 70 
task: sign, mean loss: 2.85056, accuracy: 0.34911, avg. loss over tasks: 2.85056
Diversity Loss - Mean: -0.13073, Variance: 0.01417
Semantic Loss - Mean: 2.45265, Variance: 0.04111

Train Epoch: 71 
task: sign, mean loss: 0.28158, accuracy: 0.92391, avg. loss over tasks: 0.28158, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.12983, Variance: 0.01317
Semantic Loss - Mean: 0.32605, Variance: 0.00662

Test Epoch: 71 
task: sign, mean loss: 3.59554, accuracy: 0.17751, avg. loss over tasks: 3.59554
Diversity Loss - Mean: -0.12664, Variance: 0.01418
Semantic Loss - Mean: 3.54558, Variance: 0.04342

Train Epoch: 72 
task: sign, mean loss: 0.15977, accuracy: 0.95109, avg. loss over tasks: 0.15977, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.12839, Variance: 0.01318
Semantic Loss - Mean: 0.22299, Variance: 0.00660

Test Epoch: 72 
task: sign, mean loss: 2.06170, accuracy: 0.50888, avg. loss over tasks: 2.06170
Diversity Loss - Mean: -0.13198, Variance: 0.01417
Semantic Loss - Mean: 1.83984, Variance: 0.04310

Train Epoch: 73 
task: sign, mean loss: 0.15089, accuracy: 0.93478, avg. loss over tasks: 0.15089, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.12892, Variance: 0.01320
Semantic Loss - Mean: 0.23330, Variance: 0.00663

Test Epoch: 73 
task: sign, mean loss: 2.11982, accuracy: 0.54438, avg. loss over tasks: 2.11982
Diversity Loss - Mean: -0.12996, Variance: 0.01418
Semantic Loss - Mean: 1.98255, Variance: 0.04300

Train Epoch: 74 
task: sign, mean loss: 0.13040, accuracy: 0.95109, avg. loss over tasks: 0.13040, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.12809, Variance: 0.01321
Semantic Loss - Mean: 0.17035, Variance: 0.00658

Test Epoch: 74 
task: sign, mean loss: 2.74088, accuracy: 0.51479, avg. loss over tasks: 2.74088
Diversity Loss - Mean: -0.12910, Variance: 0.01419
Semantic Loss - Mean: 2.50711, Variance: 0.04272

Train Epoch: 75 
task: sign, mean loss: 0.08851, accuracy: 0.95109, avg. loss over tasks: 0.08851, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.12776, Variance: 0.01322
Semantic Loss - Mean: 0.11408, Variance: 0.00654

Test Epoch: 75 
task: sign, mean loss: 3.13362, accuracy: 0.55030, avg. loss over tasks: 3.13362
Diversity Loss - Mean: -0.13704, Variance: 0.01426
Semantic Loss - Mean: 2.64677, Variance: 0.04241

Train Epoch: 76 
task: sign, mean loss: 0.04275, accuracy: 0.98913, avg. loss over tasks: 0.04275, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.12716, Variance: 0.01324
Semantic Loss - Mean: 0.08606, Variance: 0.00651

Test Epoch: 76 
task: sign, mean loss: 3.15959, accuracy: 0.51479, avg. loss over tasks: 3.15959
Diversity Loss - Mean: -0.13605, Variance: 0.01431
Semantic Loss - Mean: 2.72071, Variance: 0.04223

Train Epoch: 77 
task: sign, mean loss: 0.11333, accuracy: 0.96739, avg. loss over tasks: 0.11333, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.12863, Variance: 0.01326
Semantic Loss - Mean: 0.13695, Variance: 0.00647

Test Epoch: 77 
task: sign, mean loss: 3.21428, accuracy: 0.34320, avg. loss over tasks: 3.21428
Diversity Loss - Mean: -0.13081, Variance: 0.01433
Semantic Loss - Mean: 3.03100, Variance: 0.04209

Train Epoch: 78 
task: sign, mean loss: 0.07705, accuracy: 0.97283, avg. loss over tasks: 0.07705, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.12876, Variance: 0.01328
Semantic Loss - Mean: 0.09923, Variance: 0.00640

Test Epoch: 78 
task: sign, mean loss: 2.60491, accuracy: 0.36686, avg. loss over tasks: 2.60491
Diversity Loss - Mean: -0.13300, Variance: 0.01434
Semantic Loss - Mean: 2.38134, Variance: 0.04171

Train Epoch: 79 
task: sign, mean loss: 0.08313, accuracy: 0.96196, avg. loss over tasks: 0.08313, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.12953, Variance: 0.01329
Semantic Loss - Mean: 0.11725, Variance: 0.00638

Test Epoch: 79 
task: sign, mean loss: 2.43045, accuracy: 0.39053, avg. loss over tasks: 2.43045
Diversity Loss - Mean: -0.13363, Variance: 0.01436
Semantic Loss - Mean: 2.23633, Variance: 0.04143

Train Epoch: 80 
task: sign, mean loss: 0.07676, accuracy: 0.97283, avg. loss over tasks: 0.07676, lr: 0.00015015
Diversity Loss - Mean: -0.13054, Variance: 0.01331
Semantic Loss - Mean: 0.09317, Variance: 0.00632

Test Epoch: 80 
task: sign, mean loss: 2.50071, accuracy: 0.44970, avg. loss over tasks: 2.50071
Diversity Loss - Mean: -0.13380, Variance: 0.01437
Semantic Loss - Mean: 2.27197, Variance: 0.04129

Train Epoch: 81 
task: sign, mean loss: 0.02415, accuracy: 1.00000, avg. loss over tasks: 0.02415, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.13094, Variance: 0.01334
Semantic Loss - Mean: 0.04825, Variance: 0.00626

Test Epoch: 81 
task: sign, mean loss: 2.79256, accuracy: 0.45562, avg. loss over tasks: 2.79256
Diversity Loss - Mean: -0.13323, Variance: 0.01438
Semantic Loss - Mean: 2.49398, Variance: 0.04112

Train Epoch: 82 
task: sign, mean loss: 0.09823, accuracy: 0.96739, avg. loss over tasks: 0.09823, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.13032, Variance: 0.01336
Semantic Loss - Mean: 0.10538, Variance: 0.00628

Test Epoch: 82 
task: sign, mean loss: 2.80078, accuracy: 0.46154, avg. loss over tasks: 2.80078
Diversity Loss - Mean: -0.13221, Variance: 0.01439
Semantic Loss - Mean: 2.50159, Variance: 0.04127

Train Epoch: 83 
task: sign, mean loss: 0.03426, accuracy: 0.98913, avg. loss over tasks: 0.03426, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.13039, Variance: 0.01337
Semantic Loss - Mean: 0.07376, Variance: 0.00625

Test Epoch: 83 
task: sign, mean loss: 4.27577, accuracy: 0.21302, avg. loss over tasks: 4.27577
Diversity Loss - Mean: -0.12803, Variance: 0.01438
Semantic Loss - Mean: 3.91493, Variance: 0.04112

Train Epoch: 84 
task: sign, mean loss: 0.12143, accuracy: 0.95652, avg. loss over tasks: 0.12143, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.12935, Variance: 0.01337
Semantic Loss - Mean: 0.15829, Variance: 0.00622

Test Epoch: 84 
task: sign, mean loss: 3.29307, accuracy: 0.47337, avg. loss over tasks: 3.29307
Diversity Loss - Mean: -0.13606, Variance: 0.01439
Semantic Loss - Mean: 2.75401, Variance: 0.04079

Train Epoch: 85 
task: sign, mean loss: 0.02205, accuracy: 0.99457, avg. loss over tasks: 0.02205, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.13030, Variance: 0.01339
Semantic Loss - Mean: 0.03618, Variance: 0.00616

Test Epoch: 85 
task: sign, mean loss: 2.85568, accuracy: 0.49112, avg. loss over tasks: 2.85568
Diversity Loss - Mean: -0.13572, Variance: 0.01441
Semantic Loss - Mean: 2.44326, Variance: 0.04055

Train Epoch: 86 
task: sign, mean loss: 0.02733, accuracy: 0.98913, avg. loss over tasks: 0.02733, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.13104, Variance: 0.01342
Semantic Loss - Mean: 0.05850, Variance: 0.00613

Test Epoch: 86 
task: sign, mean loss: 3.08867, accuracy: 0.38462, avg. loss over tasks: 3.08867
Diversity Loss - Mean: -0.13269, Variance: 0.01441
Semantic Loss - Mean: 2.78443, Variance: 0.04044

Train Epoch: 87 
task: sign, mean loss: 0.04178, accuracy: 0.98913, avg. loss over tasks: 0.04178, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.13147, Variance: 0.01344
Semantic Loss - Mean: 0.04847, Variance: 0.00607

Test Epoch: 87 
task: sign, mean loss: 2.95939, accuracy: 0.40828, avg. loss over tasks: 2.95939
Diversity Loss - Mean: -0.13066, Variance: 0.01442
Semantic Loss - Mean: 2.78360, Variance: 0.04040

Train Epoch: 88 
task: sign, mean loss: 0.04187, accuracy: 0.98913, avg. loss over tasks: 0.04187, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.13204, Variance: 0.01346
Semantic Loss - Mean: 0.05348, Variance: 0.00601

Test Epoch: 88 
task: sign, mean loss: 2.64704, accuracy: 0.41420, avg. loss over tasks: 2.64704
Diversity Loss - Mean: -0.13163, Variance: 0.01443
Semantic Loss - Mean: 2.43409, Variance: 0.04024

Train Epoch: 89 
task: sign, mean loss: 0.04364, accuracy: 0.98370, avg. loss over tasks: 0.04364, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.13188, Variance: 0.01348
Semantic Loss - Mean: 0.06634, Variance: 0.00597

Test Epoch: 89 
task: sign, mean loss: 2.88415, accuracy: 0.43195, avg. loss over tasks: 2.88415
Diversity Loss - Mean: -0.12712, Variance: 0.01443
Semantic Loss - Mean: 2.59591, Variance: 0.04018

Train Epoch: 90 
task: sign, mean loss: 0.04006, accuracy: 0.98913, avg. loss over tasks: 0.04006, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.13205, Variance: 0.01350
Semantic Loss - Mean: 0.07761, Variance: 0.00595

Test Epoch: 90 
task: sign, mean loss: 3.07047, accuracy: 0.43787, avg. loss over tasks: 3.07047
Diversity Loss - Mean: -0.13326, Variance: 0.01443
Semantic Loss - Mean: 2.79528, Variance: 0.03998

Train Epoch: 91 
task: sign, mean loss: 0.02685, accuracy: 0.98913, avg. loss over tasks: 0.02685, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.13272, Variance: 0.01351
Semantic Loss - Mean: 0.04941, Variance: 0.00590

Test Epoch: 91 
task: sign, mean loss: 3.09250, accuracy: 0.40828, avg. loss over tasks: 3.09250
Diversity Loss - Mean: -0.13175, Variance: 0.01444
Semantic Loss - Mean: 2.77754, Variance: 0.03975

Train Epoch: 92 
task: sign, mean loss: 0.01407, accuracy: 0.99457, avg. loss over tasks: 0.01407, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.13234, Variance: 0.01353
Semantic Loss - Mean: 0.02581, Variance: 0.00584

Test Epoch: 92 
task: sign, mean loss: 3.12912, accuracy: 0.39053, avg. loss over tasks: 3.12912
Diversity Loss - Mean: -0.13177, Variance: 0.01444
Semantic Loss - Mean: 2.81354, Variance: 0.03959

Train Epoch: 93 
task: sign, mean loss: 0.04446, accuracy: 0.98370, avg. loss over tasks: 0.04446, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.13322, Variance: 0.01354
Semantic Loss - Mean: 0.05262, Variance: 0.00580

Test Epoch: 93 
task: sign, mean loss: 3.11036, accuracy: 0.40828, avg. loss over tasks: 3.11036
Diversity Loss - Mean: -0.13376, Variance: 0.01444
Semantic Loss - Mean: 2.72898, Variance: 0.03939

Train Epoch: 94 
task: sign, mean loss: 0.01742, accuracy: 0.99457, avg. loss over tasks: 0.01742, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.13281, Variance: 0.01356
Semantic Loss - Mean: 0.02845, Variance: 0.00574

Test Epoch: 94 
task: sign, mean loss: 3.25323, accuracy: 0.40237, avg. loss over tasks: 3.25323
Diversity Loss - Mean: -0.13415, Variance: 0.01444
Semantic Loss - Mean: 2.83092, Variance: 0.03930

Train Epoch: 95 
task: sign, mean loss: 0.01672, accuracy: 0.98913, avg. loss over tasks: 0.01672, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.13304, Variance: 0.01357
Semantic Loss - Mean: 0.03307, Variance: 0.00572

Test Epoch: 95 
task: sign, mean loss: 3.30754, accuracy: 0.42604, avg. loss over tasks: 3.30754
Diversity Loss - Mean: -0.13456, Variance: 0.01445
Semantic Loss - Mean: 2.86493, Variance: 0.03939

Train Epoch: 96 
task: sign, mean loss: 0.01106, accuracy: 1.00000, avg. loss over tasks: 0.01106, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.13405, Variance: 0.01358
Semantic Loss - Mean: 0.02435, Variance: 0.00566

Test Epoch: 96 
task: sign, mean loss: 3.18651, accuracy: 0.43787, avg. loss over tasks: 3.18651
Diversity Loss - Mean: -0.13514, Variance: 0.01445
Semantic Loss - Mean: 2.73869, Variance: 0.03956

Train Epoch: 97 
task: sign, mean loss: 0.01338, accuracy: 1.00000, avg. loss over tasks: 0.01338, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.13398, Variance: 0.01360
Semantic Loss - Mean: 0.03189, Variance: 0.00563

Test Epoch: 97 
task: sign, mean loss: 3.40755, accuracy: 0.43195, avg. loss over tasks: 3.40755
Diversity Loss - Mean: -0.13617, Variance: 0.01446
Semantic Loss - Mean: 2.94412, Variance: 0.03952

Train Epoch: 98 
task: sign, mean loss: 0.01727, accuracy: 0.99457, avg. loss over tasks: 0.01727, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.13437, Variance: 0.01362
Semantic Loss - Mean: 0.03181, Variance: 0.00558

Test Epoch: 98 
task: sign, mean loss: 3.30266, accuracy: 0.44379, avg. loss over tasks: 3.30266
Diversity Loss - Mean: -0.13674, Variance: 0.01447
Semantic Loss - Mean: 2.86881, Variance: 0.03941

Train Epoch: 99 
task: sign, mean loss: 0.01058, accuracy: 1.00000, avg. loss over tasks: 0.01058, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.13465, Variance: 0.01363
Semantic Loss - Mean: 0.02721, Variance: 0.00555

Test Epoch: 99 
task: sign, mean loss: 3.27750, accuracy: 0.42012, avg. loss over tasks: 3.27750
Diversity Loss - Mean: -0.13565, Variance: 0.01447
Semantic Loss - Mean: 2.89276, Variance: 0.03943

Train Epoch: 100 
task: sign, mean loss: 0.00519, accuracy: 1.00000, avg. loss over tasks: 0.00519, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.13493, Variance: 0.01365
Semantic Loss - Mean: 0.02322, Variance: 0.00551

Test Epoch: 100 
task: sign, mean loss: 3.18034, accuracy: 0.47337, avg. loss over tasks: 3.18034
Diversity Loss - Mean: -0.13538, Variance: 0.01447
Semantic Loss - Mean: 2.81597, Variance: 0.03944

Train Epoch: 101 
task: sign, mean loss: 0.02060, accuracy: 0.99457, avg. loss over tasks: 0.02060, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.13501, Variance: 0.01367
Semantic Loss - Mean: 0.03697, Variance: 0.00548

Test Epoch: 101 
task: sign, mean loss: 3.44494, accuracy: 0.40828, avg. loss over tasks: 3.44494
Diversity Loss - Mean: -0.13485, Variance: 0.01448
Semantic Loss - Mean: 3.04237, Variance: 0.03947

Train Epoch: 102 
task: sign, mean loss: 0.00518, accuracy: 1.00000, avg. loss over tasks: 0.00518, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.13522, Variance: 0.01369
Semantic Loss - Mean: 0.01408, Variance: 0.00543

Test Epoch: 102 
task: sign, mean loss: 3.48352, accuracy: 0.43787, avg. loss over tasks: 3.48352
Diversity Loss - Mean: -0.13274, Variance: 0.01447
Semantic Loss - Mean: 3.03241, Variance: 0.03942

Train Epoch: 103 
task: sign, mean loss: 0.04771, accuracy: 0.99457, avg. loss over tasks: 0.04771, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.13470, Variance: 0.01370
Semantic Loss - Mean: 0.05514, Variance: 0.00538

Test Epoch: 103 
task: sign, mean loss: 3.43589, accuracy: 0.43787, avg. loss over tasks: 3.43589
Diversity Loss - Mean: -0.13269, Variance: 0.01447
Semantic Loss - Mean: 2.97420, Variance: 0.03930

Train Epoch: 104 
task: sign, mean loss: 0.00375, accuracy: 1.00000, avg. loss over tasks: 0.00375, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.13520, Variance: 0.01372
Semantic Loss - Mean: 0.01615, Variance: 0.00534

Test Epoch: 104 
task: sign, mean loss: 3.39760, accuracy: 0.43787, avg. loss over tasks: 3.39760
Diversity Loss - Mean: -0.13367, Variance: 0.01447
Semantic Loss - Mean: 2.92867, Variance: 0.03916

Train Epoch: 105 
task: sign, mean loss: 0.01386, accuracy: 0.99457, avg. loss over tasks: 0.01386, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.13529, Variance: 0.01373
Semantic Loss - Mean: 0.01925, Variance: 0.00530

Test Epoch: 105 
task: sign, mean loss: 3.43501, accuracy: 0.41420, avg. loss over tasks: 3.43501
Diversity Loss - Mean: -0.13470, Variance: 0.01447
Semantic Loss - Mean: 2.99603, Variance: 0.03899

Train Epoch: 106 
task: sign, mean loss: 0.00264, accuracy: 1.00000, avg. loss over tasks: 0.00264, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.13556, Variance: 0.01375
Semantic Loss - Mean: 0.00986, Variance: 0.00525

Test Epoch: 106 
task: sign, mean loss: 3.49188, accuracy: 0.39645, avg. loss over tasks: 3.49188
Diversity Loss - Mean: -0.13497, Variance: 0.01446
Semantic Loss - Mean: 3.05441, Variance: 0.03886

Train Epoch: 107 
task: sign, mean loss: 0.00595, accuracy: 1.00000, avg. loss over tasks: 0.00595, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.13576, Variance: 0.01376
Semantic Loss - Mean: 0.02220, Variance: 0.00522

Test Epoch: 107 
task: sign, mean loss: 3.49194, accuracy: 0.40237, avg. loss over tasks: 3.49194
Diversity Loss - Mean: -0.13569, Variance: 0.01446
Semantic Loss - Mean: 3.03586, Variance: 0.03876

Train Epoch: 108 
task: sign, mean loss: 0.00439, accuracy: 1.00000, avg. loss over tasks: 0.00439, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.13538, Variance: 0.01377
Semantic Loss - Mean: 0.01519, Variance: 0.00518

Test Epoch: 108 
task: sign, mean loss: 3.25895, accuracy: 0.40828, avg. loss over tasks: 3.25895
Diversity Loss - Mean: -0.13584, Variance: 0.01445
Semantic Loss - Mean: 2.79257, Variance: 0.03855

Train Epoch: 109 
task: sign, mean loss: 0.00272, accuracy: 1.00000, avg. loss over tasks: 0.00272, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.13539, Variance: 0.01378
Semantic Loss - Mean: 0.01229, Variance: 0.00514

Test Epoch: 109 
task: sign, mean loss: 3.26004, accuracy: 0.43787, avg. loss over tasks: 3.26004
Diversity Loss - Mean: -0.13575, Variance: 0.01444
Semantic Loss - Mean: 2.76304, Variance: 0.03833

Train Epoch: 110 
task: sign, mean loss: 0.00291, accuracy: 1.00000, avg. loss over tasks: 0.00291, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.13587, Variance: 0.01379
Semantic Loss - Mean: 0.01227, Variance: 0.00509

Test Epoch: 110 
task: sign, mean loss: 3.16851, accuracy: 0.42604, avg. loss over tasks: 3.16851
Diversity Loss - Mean: -0.13614, Variance: 0.01443
Semantic Loss - Mean: 2.70238, Variance: 0.03815

Train Epoch: 111 
task: sign, mean loss: 0.00325, accuracy: 1.00000, avg. loss over tasks: 0.00325, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.13582, Variance: 0.01380
Semantic Loss - Mean: 0.02556, Variance: 0.00507

Test Epoch: 111 
task: sign, mean loss: 3.23343, accuracy: 0.41420, avg. loss over tasks: 3.23343
Diversity Loss - Mean: -0.13602, Variance: 0.01442
Semantic Loss - Mean: 2.79245, Variance: 0.03802

Train Epoch: 112 
task: sign, mean loss: 0.00437, accuracy: 1.00000, avg. loss over tasks: 0.00437, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.13606, Variance: 0.01381
Semantic Loss - Mean: 0.01075, Variance: 0.00503

Test Epoch: 112 
task: sign, mean loss: 3.30602, accuracy: 0.39645, avg. loss over tasks: 3.30602
Diversity Loss - Mean: -0.13566, Variance: 0.01442
Semantic Loss - Mean: 2.88737, Variance: 0.03802

Train Epoch: 113 
task: sign, mean loss: 0.02018, accuracy: 0.98913, avg. loss over tasks: 0.02018, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.13643, Variance: 0.01383
Semantic Loss - Mean: 0.03349, Variance: 0.00499

Test Epoch: 113 
task: sign, mean loss: 3.49838, accuracy: 0.37278, avg. loss over tasks: 3.49838
Diversity Loss - Mean: -0.13542, Variance: 0.01441
Semantic Loss - Mean: 3.07964, Variance: 0.03810

Train Epoch: 114 
task: sign, mean loss: 0.01132, accuracy: 0.99457, avg. loss over tasks: 0.01132, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.13569, Variance: 0.01384
Semantic Loss - Mean: 0.02215, Variance: 0.00496

Test Epoch: 114 
task: sign, mean loss: 3.74609, accuracy: 0.36095, avg. loss over tasks: 3.74609
Diversity Loss - Mean: -0.13509, Variance: 0.01440
Semantic Loss - Mean: 3.32423, Variance: 0.03807

Train Epoch: 115 
task: sign, mean loss: 0.01074, accuracy: 0.99457, avg. loss over tasks: 0.01074, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.13614, Variance: 0.01385
Semantic Loss - Mean: 0.01721, Variance: 0.00492

Test Epoch: 115 
task: sign, mean loss: 3.76391, accuracy: 0.36686, avg. loss over tasks: 3.76391
Diversity Loss - Mean: -0.13547, Variance: 0.01439
Semantic Loss - Mean: 3.31511, Variance: 0.03794

Train Epoch: 116 
task: sign, mean loss: 0.01096, accuracy: 0.99457, avg. loss over tasks: 0.01096, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.13609, Variance: 0.01385
Semantic Loss - Mean: 0.02032, Variance: 0.00488

Test Epoch: 116 
task: sign, mean loss: 3.42725, accuracy: 0.43787, avg. loss over tasks: 3.42725
Diversity Loss - Mean: -0.13716, Variance: 0.01439
Semantic Loss - Mean: 3.00265, Variance: 0.03779

Train Epoch: 117 
task: sign, mean loss: 0.00877, accuracy: 0.99457, avg. loss over tasks: 0.00877, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.13684, Variance: 0.01386
Semantic Loss - Mean: 0.01546, Variance: 0.00485

Test Epoch: 117 
task: sign, mean loss: 3.48769, accuracy: 0.38462, avg. loss over tasks: 3.48769
Diversity Loss - Mean: -0.13573, Variance: 0.01439
Semantic Loss - Mean: 2.99424, Variance: 0.03765

Train Epoch: 118 
task: sign, mean loss: 0.00424, accuracy: 1.00000, avg. loss over tasks: 0.00424, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.13656, Variance: 0.01387
Semantic Loss - Mean: 0.01345, Variance: 0.00482

Test Epoch: 118 
task: sign, mean loss: 3.55730, accuracy: 0.35503, avg. loss over tasks: 3.55730
Diversity Loss - Mean: -0.13599, Variance: 0.01438
Semantic Loss - Mean: 3.04498, Variance: 0.03752

Train Epoch: 119 
task: sign, mean loss: 0.00155, accuracy: 1.00000, avg. loss over tasks: 0.00155, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.13678, Variance: 0.01389
Semantic Loss - Mean: 0.00843, Variance: 0.00478

Test Epoch: 119 
task: sign, mean loss: 3.56895, accuracy: 0.36686, avg. loss over tasks: 3.56895
Diversity Loss - Mean: -0.13595, Variance: 0.01437
Semantic Loss - Mean: 3.06749, Variance: 0.03738

Train Epoch: 120 
task: sign, mean loss: 0.00366, accuracy: 1.00000, avg. loss over tasks: 0.00366, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.13682, Variance: 0.01390
Semantic Loss - Mean: 0.02749, Variance: 0.00478

Test Epoch: 120 
task: sign, mean loss: 3.51478, accuracy: 0.39053, avg. loss over tasks: 3.51478
Diversity Loss - Mean: -0.13652, Variance: 0.01437
Semantic Loss - Mean: 3.04659, Variance: 0.03723

Train Epoch: 121 
task: sign, mean loss: 0.00417, accuracy: 1.00000, avg. loss over tasks: 0.00417, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.13733, Variance: 0.01391
Semantic Loss - Mean: 0.01166, Variance: 0.00475

Test Epoch: 121 
task: sign, mean loss: 3.57869, accuracy: 0.36095, avg. loss over tasks: 3.57869
Diversity Loss - Mean: -0.13641, Variance: 0.01436
Semantic Loss - Mean: 3.10028, Variance: 0.03707

Train Epoch: 122 
task: sign, mean loss: 0.00495, accuracy: 1.00000, avg. loss over tasks: 0.00495, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.13683, Variance: 0.01393
Semantic Loss - Mean: 0.01233, Variance: 0.00471

Test Epoch: 122 
task: sign, mean loss: 3.68369, accuracy: 0.34911, avg. loss over tasks: 3.68369
Diversity Loss - Mean: -0.13715, Variance: 0.01436
Semantic Loss - Mean: 3.21583, Variance: 0.03693

Train Epoch: 123 
task: sign, mean loss: 0.01327, accuracy: 0.99457, avg. loss over tasks: 0.01327, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.13709, Variance: 0.01394
Semantic Loss - Mean: 0.02355, Variance: 0.00469

Test Epoch: 123 
task: sign, mean loss: 3.68568, accuracy: 0.36686, avg. loss over tasks: 3.68568
Diversity Loss - Mean: -0.13660, Variance: 0.01435
Semantic Loss - Mean: 3.19682, Variance: 0.03680

Train Epoch: 124 
task: sign, mean loss: 0.01720, accuracy: 0.99457, avg. loss over tasks: 0.01720, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.13732, Variance: 0.01395
Semantic Loss - Mean: 0.02261, Variance: 0.00466

Test Epoch: 124 
task: sign, mean loss: 3.50257, accuracy: 0.41420, avg. loss over tasks: 3.50257
Diversity Loss - Mean: -0.13633, Variance: 0.01435
Semantic Loss - Mean: 3.04718, Variance: 0.03665

Train Epoch: 125 
task: sign, mean loss: 0.00589, accuracy: 1.00000, avg. loss over tasks: 0.00589, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.13750, Variance: 0.01396
Semantic Loss - Mean: 0.00948, Variance: 0.00462

Test Epoch: 125 
task: sign, mean loss: 3.54155, accuracy: 0.38462, avg. loss over tasks: 3.54155
Diversity Loss - Mean: -0.13679, Variance: 0.01434
Semantic Loss - Mean: 3.10613, Variance: 0.03649

Train Epoch: 126 
task: sign, mean loss: 0.00346, accuracy: 1.00000, avg. loss over tasks: 0.00346, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.13730, Variance: 0.01397
Semantic Loss - Mean: 0.01222, Variance: 0.00458

Test Epoch: 126 
task: sign, mean loss: 3.44592, accuracy: 0.42012, avg. loss over tasks: 3.44592
Diversity Loss - Mean: -0.13706, Variance: 0.01434
Semantic Loss - Mean: 3.02255, Variance: 0.03632

Train Epoch: 127 
task: sign, mean loss: 0.00486, accuracy: 1.00000, avg. loss over tasks: 0.00486, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.13700, Variance: 0.01398
Semantic Loss - Mean: 0.01520, Variance: 0.00455

Test Epoch: 127 
task: sign, mean loss: 3.47971, accuracy: 0.40237, avg. loss over tasks: 3.47971
Diversity Loss - Mean: -0.13729, Variance: 0.01434
Semantic Loss - Mean: 3.05798, Variance: 0.03614

Train Epoch: 128 
task: sign, mean loss: 0.00505, accuracy: 1.00000, avg. loss over tasks: 0.00505, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.13726, Variance: 0.01399
Semantic Loss - Mean: 0.01590, Variance: 0.00453

Test Epoch: 128 
task: sign, mean loss: 3.74243, accuracy: 0.36686, avg. loss over tasks: 3.74243
Diversity Loss - Mean: -0.13704, Variance: 0.01434
Semantic Loss - Mean: 3.25860, Variance: 0.03596

Train Epoch: 129 
task: sign, mean loss: 0.00346, accuracy: 1.00000, avg. loss over tasks: 0.00346, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.13723, Variance: 0.01401
Semantic Loss - Mean: 0.01165, Variance: 0.00450

Test Epoch: 129 
task: sign, mean loss: 3.61598, accuracy: 0.37870, avg. loss over tasks: 3.61598
Diversity Loss - Mean: -0.13733, Variance: 0.01434
Semantic Loss - Mean: 3.16262, Variance: 0.03577

Train Epoch: 130 
task: sign, mean loss: 0.01779, accuracy: 0.99457, avg. loss over tasks: 0.01779, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.13711, Variance: 0.01402
Semantic Loss - Mean: 0.04123, Variance: 0.00449

Test Epoch: 130 
task: sign, mean loss: 3.60283, accuracy: 0.37278, avg. loss over tasks: 3.60283
Diversity Loss - Mean: -0.13705, Variance: 0.01433
Semantic Loss - Mean: 3.14440, Variance: 0.03560

Train Epoch: 131 
task: sign, mean loss: 0.00445, accuracy: 1.00000, avg. loss over tasks: 0.00445, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.13774, Variance: 0.01403
Semantic Loss - Mean: 0.01010, Variance: 0.00445

Test Epoch: 131 
task: sign, mean loss: 3.73787, accuracy: 0.37278, avg. loss over tasks: 3.73787
Diversity Loss - Mean: -0.13654, Variance: 0.01432
Semantic Loss - Mean: 3.22452, Variance: 0.03542

Train Epoch: 132 
task: sign, mean loss: 0.00124, accuracy: 1.00000, avg. loss over tasks: 0.00124, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.13709, Variance: 0.01404
Semantic Loss - Mean: 0.00774, Variance: 0.00443

Test Epoch: 132 
task: sign, mean loss: 3.66410, accuracy: 0.36095, avg. loss over tasks: 3.66410
Diversity Loss - Mean: -0.13714, Variance: 0.01432
Semantic Loss - Mean: 3.18328, Variance: 0.03526

Train Epoch: 133 
task: sign, mean loss: 0.00532, accuracy: 1.00000, avg. loss over tasks: 0.00532, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.13744, Variance: 0.01405
Semantic Loss - Mean: 0.01205, Variance: 0.00439

Test Epoch: 133 
task: sign, mean loss: 3.64974, accuracy: 0.36095, avg. loss over tasks: 3.64974
Diversity Loss - Mean: -0.13719, Variance: 0.01432
Semantic Loss - Mean: 3.17761, Variance: 0.03508

Train Epoch: 134 
task: sign, mean loss: 0.00810, accuracy: 0.99457, avg. loss over tasks: 0.00810, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13707, Variance: 0.01406
Semantic Loss - Mean: 0.01905, Variance: 0.00438

Test Epoch: 134 
task: sign, mean loss: 3.64566, accuracy: 0.37870, avg. loss over tasks: 3.64566
Diversity Loss - Mean: -0.13731, Variance: 0.01431
Semantic Loss - Mean: 3.18158, Variance: 0.03491

Train Epoch: 135 
task: sign, mean loss: 0.00820, accuracy: 0.99457, avg. loss over tasks: 0.00820, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.13730, Variance: 0.01407
Semantic Loss - Mean: 0.01508, Variance: 0.00435

Test Epoch: 135 
task: sign, mean loss: 3.57744, accuracy: 0.38462, avg. loss over tasks: 3.57744
Diversity Loss - Mean: -0.13758, Variance: 0.01431
Semantic Loss - Mean: 3.13178, Variance: 0.03474

Train Epoch: 136 
task: sign, mean loss: 0.00158, accuracy: 1.00000, avg. loss over tasks: 0.00158, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13758, Variance: 0.01408
Semantic Loss - Mean: 0.00959, Variance: 0.00433

Test Epoch: 136 
task: sign, mean loss: 3.48044, accuracy: 0.39645, avg. loss over tasks: 3.48044
Diversity Loss - Mean: -0.13783, Variance: 0.01431
Semantic Loss - Mean: 3.06895, Variance: 0.03456

Train Epoch: 137 
task: sign, mean loss: 0.00218, accuracy: 1.00000, avg. loss over tasks: 0.00218, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13773, Variance: 0.01409
Semantic Loss - Mean: 0.00641, Variance: 0.00430

Test Epoch: 137 
task: sign, mean loss: 3.56001, accuracy: 0.37870, avg. loss over tasks: 3.56001
Diversity Loss - Mean: -0.13713, Variance: 0.01430
Semantic Loss - Mean: 3.10831, Variance: 0.03440

Train Epoch: 138 
task: sign, mean loss: 0.00315, accuracy: 1.00000, avg. loss over tasks: 0.00315, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13740, Variance: 0.01410
Semantic Loss - Mean: 0.01298, Variance: 0.00427

Test Epoch: 138 
task: sign, mean loss: 3.65224, accuracy: 0.37278, avg. loss over tasks: 3.65224
Diversity Loss - Mean: -0.13708, Variance: 0.01430
Semantic Loss - Mean: 3.18044, Variance: 0.03423

Train Epoch: 139 
task: sign, mean loss: 0.00195, accuracy: 1.00000, avg. loss over tasks: 0.00195, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13737, Variance: 0.01411
Semantic Loss - Mean: 0.01397, Variance: 0.00425

Test Epoch: 139 
task: sign, mean loss: 3.62832, accuracy: 0.37278, avg. loss over tasks: 3.62832
Diversity Loss - Mean: -0.13729, Variance: 0.01429
Semantic Loss - Mean: 3.16534, Variance: 0.03407

Train Epoch: 140 
task: sign, mean loss: 0.00669, accuracy: 1.00000, avg. loss over tasks: 0.00669, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.13722, Variance: 0.01412
Semantic Loss - Mean: 0.01796, Variance: 0.00422

Test Epoch: 140 
task: sign, mean loss: 3.52965, accuracy: 0.39053, avg. loss over tasks: 3.52965
Diversity Loss - Mean: -0.13791, Variance: 0.01429
Semantic Loss - Mean: 3.10812, Variance: 0.03390

Train Epoch: 141 
task: sign, mean loss: 0.00299, accuracy: 1.00000, avg. loss over tasks: 0.00299, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.13710, Variance: 0.01413
Semantic Loss - Mean: 0.01313, Variance: 0.00421

Test Epoch: 141 
task: sign, mean loss: 3.48541, accuracy: 0.42012, avg. loss over tasks: 3.48541
Diversity Loss - Mean: -0.13759, Variance: 0.01429
Semantic Loss - Mean: 3.05292, Variance: 0.03374

Train Epoch: 142 
task: sign, mean loss: 0.02498, accuracy: 0.98370, avg. loss over tasks: 0.02498, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.13727, Variance: 0.01414
Semantic Loss - Mean: 0.05686, Variance: 0.00423

Test Epoch: 142 
task: sign, mean loss: 3.47433, accuracy: 0.43195, avg. loss over tasks: 3.47433
Diversity Loss - Mean: -0.13764, Variance: 0.01429
Semantic Loss - Mean: 3.05212, Variance: 0.03358

Train Epoch: 143 
task: sign, mean loss: 0.00249, accuracy: 1.00000, avg. loss over tasks: 0.00249, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13750, Variance: 0.01415
Semantic Loss - Mean: 0.00865, Variance: 0.00420

Test Epoch: 143 
task: sign, mean loss: 3.53771, accuracy: 0.40237, avg. loss over tasks: 3.53771
Diversity Loss - Mean: -0.13784, Variance: 0.01429
Semantic Loss - Mean: 3.12207, Variance: 0.03342

Train Epoch: 144 
task: sign, mean loss: 0.00610, accuracy: 1.00000, avg. loss over tasks: 0.00610, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.13719, Variance: 0.01415
Semantic Loss - Mean: 0.01620, Variance: 0.00418

Test Epoch: 144 
task: sign, mean loss: 3.59366, accuracy: 0.39053, avg. loss over tasks: 3.59366
Diversity Loss - Mean: -0.13783, Variance: 0.01429
Semantic Loss - Mean: 3.16566, Variance: 0.03327

Train Epoch: 145 
task: sign, mean loss: 0.00143, accuracy: 1.00000, avg. loss over tasks: 0.00143, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13748, Variance: 0.01416
Semantic Loss - Mean: 0.00622, Variance: 0.00415

Test Epoch: 145 
task: sign, mean loss: 3.58294, accuracy: 0.39053, avg. loss over tasks: 3.58294
Diversity Loss - Mean: -0.13788, Variance: 0.01429
Semantic Loss - Mean: 3.16329, Variance: 0.03311

Train Epoch: 146 
task: sign, mean loss: 0.02439, accuracy: 0.99457, avg. loss over tasks: 0.02439, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.13738, Variance: 0.01417
Semantic Loss - Mean: 0.04012, Variance: 0.00418

Test Epoch: 146 
task: sign, mean loss: 3.80219, accuracy: 0.36686, avg. loss over tasks: 3.80219
Diversity Loss - Mean: -0.13725, Variance: 0.01428
Semantic Loss - Mean: 3.29356, Variance: 0.03296

Train Epoch: 147 
task: sign, mean loss: 0.00145, accuracy: 1.00000, avg. loss over tasks: 0.00145, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13747, Variance: 0.01418
Semantic Loss - Mean: 0.00563, Variance: 0.00415

Test Epoch: 147 
task: sign, mean loss: 3.75245, accuracy: 0.36686, avg. loss over tasks: 3.75245
Diversity Loss - Mean: -0.13730, Variance: 0.01428
Semantic Loss - Mean: 3.26215, Variance: 0.03281

Train Epoch: 148 
task: sign, mean loss: 0.00661, accuracy: 1.00000, avg. loss over tasks: 0.00661, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.13701, Variance: 0.01419
Semantic Loss - Mean: 0.02902, Variance: 0.00414

Test Epoch: 148 
task: sign, mean loss: 3.69985, accuracy: 0.39053, avg. loss over tasks: 3.69985
Diversity Loss - Mean: -0.13733, Variance: 0.01428
Semantic Loss - Mean: 3.23487, Variance: 0.03266

Train Epoch: 149 
task: sign, mean loss: 0.00356, accuracy: 1.00000, avg. loss over tasks: 0.00356, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.13761, Variance: 0.01420
Semantic Loss - Mean: 0.00982, Variance: 0.00411

Test Epoch: 149 
task: sign, mean loss: 3.57317, accuracy: 0.39053, avg. loss over tasks: 3.57317
Diversity Loss - Mean: -0.13736, Variance: 0.01427
Semantic Loss - Mean: 3.13274, Variance: 0.03251

Train Epoch: 150 
task: sign, mean loss: 0.00121, accuracy: 1.00000, avg. loss over tasks: 0.00121, lr: 3e-07
Diversity Loss - Mean: -0.13735, Variance: 0.01420
Semantic Loss - Mean: 0.00712, Variance: 0.00409

Test Epoch: 150 
task: sign, mean loss: 3.60053, accuracy: 0.39053, avg. loss over tasks: 3.60053
Diversity Loss - Mean: -0.13764, Variance: 0.01427
Semantic Loss - Mean: 3.17040, Variance: 0.03237

