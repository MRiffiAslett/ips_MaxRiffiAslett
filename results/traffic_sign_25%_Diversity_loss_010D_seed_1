Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09168, accuracy: 0.63587, avg. loss over tasks: 1.09168, lr: 3e-05
Diversity Loss - Mean: -0.00929, Variance: 0.01049
Semantic Loss - Mean: 1.43090, Variance: 0.07263

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17650, accuracy: 0.66272, avg. loss over tasks: 1.17650
Diversity Loss - Mean: -0.02748, Variance: 0.01246
Semantic Loss - Mean: 1.16100, Variance: 0.05296

Train Epoch: 2 
task: sign, mean loss: 0.96423, accuracy: 0.67391, avg. loss over tasks: 0.96423, lr: 6e-05
Diversity Loss - Mean: -0.01097, Variance: 0.01041
Semantic Loss - Mean: 0.98357, Variance: 0.03922

Test Epoch: 2 
task: sign, mean loss: 1.11045, accuracy: 0.66272, avg. loss over tasks: 1.11045
Diversity Loss - Mean: -0.01775, Variance: 0.01195
Semantic Loss - Mean: 1.15192, Variance: 0.03222

Train Epoch: 3 
task: sign, mean loss: 0.79341, accuracy: 0.69022, avg. loss over tasks: 0.79341, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.01399, Variance: 0.01013
Semantic Loss - Mean: 0.99489, Variance: 0.02711

Test Epoch: 3 
task: sign, mean loss: 1.25885, accuracy: 0.59763, avg. loss over tasks: 1.25885
Diversity Loss - Mean: -0.03189, Variance: 0.01119
Semantic Loss - Mean: 1.11956, Variance: 0.02896

Train Epoch: 4 
task: sign, mean loss: 0.74631, accuracy: 0.70109, avg. loss over tasks: 0.74631, lr: 0.00012
Diversity Loss - Mean: -0.02376, Variance: 0.00972
Semantic Loss - Mean: 0.88433, Variance: 0.02080

Test Epoch: 4 
task: sign, mean loss: 1.61088, accuracy: 0.45562, avg. loss over tasks: 1.61088
Diversity Loss - Mean: -0.01464, Variance: 0.01050
Semantic Loss - Mean: 1.09156, Variance: 0.02341

Train Epoch: 5 
task: sign, mean loss: 0.72927, accuracy: 0.73913, avg. loss over tasks: 0.72927, lr: 0.00015
Diversity Loss - Mean: -0.00637, Variance: 0.00929
Semantic Loss - Mean: 0.78157, Variance: 0.01712

Test Epoch: 5 
task: sign, mean loss: 1.86260, accuracy: 0.60947, avg. loss over tasks: 1.86260
Diversity Loss - Mean: -0.00632, Variance: 0.01033
Semantic Loss - Mean: 1.33340, Variance: 0.02309

Train Epoch: 6 
task: sign, mean loss: 0.67780, accuracy: 0.76630, avg. loss over tasks: 0.67780, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.00465, Variance: 0.00901
Semantic Loss - Mean: 0.71960, Variance: 0.01468

Test Epoch: 6 
task: sign, mean loss: 1.99806, accuracy: 0.63905, avg. loss over tasks: 1.99806
Diversity Loss - Mean: 0.02565, Variance: 0.01059
Semantic Loss - Mean: 1.49042, Variance: 0.02132

Train Epoch: 7 
task: sign, mean loss: 0.52994, accuracy: 0.79891, avg. loss over tasks: 0.52994, lr: 0.00020999999999999998
Diversity Loss - Mean: 0.00713, Variance: 0.00881
Semantic Loss - Mean: 0.54354, Variance: 0.01289

Test Epoch: 7 
task: sign, mean loss: 1.87919, accuracy: 0.56213, avg. loss over tasks: 1.87919
Diversity Loss - Mean: 0.02300, Variance: 0.01066
Semantic Loss - Mean: 1.50153, Variance: 0.02221

Train Epoch: 8 
task: sign, mean loss: 0.58642, accuracy: 0.78261, avg. loss over tasks: 0.58642, lr: 0.00024
Diversity Loss - Mean: 0.02129, Variance: 0.00861
Semantic Loss - Mean: 0.59363, Variance: 0.01181

Test Epoch: 8 
task: sign, mean loss: 2.05780, accuracy: 0.63905, avg. loss over tasks: 2.05780
Diversity Loss - Mean: 0.09085, Variance: 0.01081
Semantic Loss - Mean: 1.79342, Variance: 0.02224

Train Epoch: 9 
task: sign, mean loss: 0.74494, accuracy: 0.75543, avg. loss over tasks: 0.74494, lr: 0.00027
Diversity Loss - Mean: -0.00155, Variance: 0.00855
Semantic Loss - Mean: 0.66441, Variance: 0.01094

Test Epoch: 9 
task: sign, mean loss: 2.58407, accuracy: 0.36095, avg. loss over tasks: 2.58407
Diversity Loss - Mean: 0.06484, Variance: 0.01080
Semantic Loss - Mean: 2.30077, Variance: 0.02649

Train Epoch: 10 
task: sign, mean loss: 0.71699, accuracy: 0.73370, avg. loss over tasks: 0.71699, lr: 0.0003
Diversity Loss - Mean: 0.00005, Variance: 0.00852
Semantic Loss - Mean: 0.67296, Variance: 0.01033

Test Epoch: 10 
task: sign, mean loss: 1.97860, accuracy: 0.50888, avg. loss over tasks: 1.97860
Diversity Loss - Mean: 0.01457, Variance: 0.01080
Semantic Loss - Mean: 1.59244, Variance: 0.02654

Train Epoch: 11 
task: sign, mean loss: 0.45684, accuracy: 0.82065, avg. loss over tasks: 0.45684, lr: 0.0002999622730061346
Diversity Loss - Mean: 0.02285, Variance: 0.00845
Semantic Loss - Mean: 0.44918, Variance: 0.00959

Test Epoch: 11 
task: sign, mean loss: 2.14079, accuracy: 0.50888, avg. loss over tasks: 2.14079
Diversity Loss - Mean: 0.04087, Variance: 0.01080
Semantic Loss - Mean: 1.82459, Variance: 0.02626

Train Epoch: 12 
task: sign, mean loss: 0.40704, accuracy: 0.88043, avg. loss over tasks: 0.40704, lr: 0.000299849111021216
Diversity Loss - Mean: 0.08126, Variance: 0.00832
Semantic Loss - Mean: 0.39057, Variance: 0.00901

Test Epoch: 12 
task: sign, mean loss: 3.84466, accuracy: 0.36095, avg. loss over tasks: 3.84466
Diversity Loss - Mean: 0.11243, Variance: 0.01084
Semantic Loss - Mean: 2.81752, Variance: 0.02879

Train Epoch: 13 
task: sign, mean loss: 0.38350, accuracy: 0.86957, avg. loss over tasks: 0.38350, lr: 0.0002996605710257114
Diversity Loss - Mean: 0.06099, Variance: 0.00821
Semantic Loss - Mean: 0.43880, Variance: 0.00910

Test Epoch: 13 
task: sign, mean loss: 1.71635, accuracy: 0.47337, avg. loss over tasks: 1.71635
Diversity Loss - Mean: 0.02144, Variance: 0.01085
Semantic Loss - Mean: 1.48751, Variance: 0.02850

Train Epoch: 14 
task: sign, mean loss: 0.34870, accuracy: 0.88587, avg. loss over tasks: 0.34870, lr: 0.00029939674795518656
Diversity Loss - Mean: 0.04151, Variance: 0.00818
Semantic Loss - Mean: 0.40949, Variance: 0.00902

Test Epoch: 14 
task: sign, mean loss: 1.77468, accuracy: 0.57988, avg. loss over tasks: 1.77468
Diversity Loss - Mean: 0.02053, Variance: 0.01078
Semantic Loss - Mean: 1.72556, Variance: 0.02971

Train Epoch: 15 
task: sign, mean loss: 0.27720, accuracy: 0.88587, avg. loss over tasks: 0.27720, lr: 0.0002990577746525024
Diversity Loss - Mean: 0.02829, Variance: 0.00820
Semantic Loss - Mean: 0.33197, Variance: 0.00865

Test Epoch: 15 
task: sign, mean loss: 2.08048, accuracy: 0.56805, avg. loss over tasks: 2.08048
Diversity Loss - Mean: 0.06936, Variance: 0.01073
Semantic Loss - Mean: 1.83172, Variance: 0.03071

Train Epoch: 16 
task: sign, mean loss: 0.20463, accuracy: 0.94565, avg. loss over tasks: 0.20463, lr: 0.000298643821800925
Diversity Loss - Mean: 0.05015, Variance: 0.00815
Semantic Loss - Mean: 0.24071, Variance: 0.00834

Test Epoch: 16 
task: sign, mean loss: 1.39184, accuracy: 0.55030, avg. loss over tasks: 1.39184
Diversity Loss - Mean: 0.04048, Variance: 0.01066
Semantic Loss - Mean: 1.26632, Variance: 0.03139

Train Epoch: 17 
task: sign, mean loss: 0.10597, accuracy: 0.95652, avg. loss over tasks: 0.10597, lr: 0.0002981550978381814
Diversity Loss - Mean: 0.07633, Variance: 0.00809
Semantic Loss - Mean: 0.13135, Variance: 0.00800

Test Epoch: 17 
task: sign, mean loss: 2.38427, accuracy: 0.52071, avg. loss over tasks: 2.38427
Diversity Loss - Mean: 0.07667, Variance: 0.01049
Semantic Loss - Mean: 2.06340, Variance: 0.03103

Train Epoch: 18 
task: sign, mean loss: 0.17527, accuracy: 0.94565, avg. loss over tasks: 0.17527, lr: 0.00029759184885150465
Diversity Loss - Mean: 0.07726, Variance: 0.00805
Semantic Loss - Mean: 0.19441, Variance: 0.00782

Test Epoch: 18 
task: sign, mean loss: 2.31626, accuracy: 0.53846, avg. loss over tasks: 2.31626
Diversity Loss - Mean: 0.04770, Variance: 0.01036
Semantic Loss - Mean: 1.98046, Variance: 0.03198

Train Epoch: 19 
task: sign, mean loss: 0.09737, accuracy: 0.95109, avg. loss over tasks: 0.09737, lr: 0.0002969543584537218
Diversity Loss - Mean: 0.06588, Variance: 0.00801
Semantic Loss - Mean: 0.16250, Variance: 0.00774

Test Epoch: 19 
task: sign, mean loss: 2.41764, accuracy: 0.57396, avg. loss over tasks: 2.41764
Diversity Loss - Mean: 0.10592, Variance: 0.01022
Semantic Loss - Mean: 2.08310, Variance: 0.03313

Train Epoch: 20 
task: sign, mean loss: 0.14190, accuracy: 0.94565, avg. loss over tasks: 0.14190, lr: 0.0002962429476404462
Diversity Loss - Mean: 0.06731, Variance: 0.00795
Semantic Loss - Mean: 0.19608, Variance: 0.00770

Test Epoch: 20 
task: sign, mean loss: 1.69240, accuracy: 0.40237, avg. loss over tasks: 1.69240
Diversity Loss - Mean: 0.10567, Variance: 0.01013
Semantic Loss - Mean: 1.43983, Variance: 0.03431

Train Epoch: 21 
task: sign, mean loss: 0.27361, accuracy: 0.91848, avg. loss over tasks: 0.27361, lr: 0.00029545797462844647
Diversity Loss - Mean: 0.06833, Variance: 0.00791
Semantic Loss - Mean: 0.27776, Variance: 0.00781

Test Epoch: 21 
task: sign, mean loss: 1.57506, accuracy: 0.68639, avg. loss over tasks: 1.57506
Diversity Loss - Mean: 0.05665, Variance: 0.01013
Semantic Loss - Mean: 1.36909, Variance: 0.03387

Train Epoch: 22 
task: sign, mean loss: 0.39307, accuracy: 0.85326, avg. loss over tasks: 0.39307, lr: 0.0002945998346752736
Diversity Loss - Mean: 0.04024, Variance: 0.00793
Semantic Loss - Mean: 0.44400, Variance: 0.00820

Test Epoch: 22 
task: sign, mean loss: 1.26060, accuracy: 0.68639, avg. loss over tasks: 1.26060
Diversity Loss - Mean: -0.00462, Variance: 0.01025
Semantic Loss - Mean: 1.15140, Variance: 0.03392

Train Epoch: 23 
task: sign, mean loss: 0.15808, accuracy: 0.95109, avg. loss over tasks: 0.15808, lr: 0.0002936689598802368
Diversity Loss - Mean: 0.02446, Variance: 0.00796
Semantic Loss - Mean: 0.19224, Variance: 0.00798

Test Epoch: 23 
task: sign, mean loss: 1.34192, accuracy: 0.64497, avg. loss over tasks: 1.34192
Diversity Loss - Mean: 0.01181, Variance: 0.01030
Semantic Loss - Mean: 1.22194, Variance: 0.03553

Train Epoch: 24 
task: sign, mean loss: 0.07331, accuracy: 0.97826, avg. loss over tasks: 0.07331, lr: 0.00029266581896682876
Diversity Loss - Mean: 0.03286, Variance: 0.00800
Semantic Loss - Mean: 0.11480, Variance: 0.00778

Test Epoch: 24 
task: sign, mean loss: 1.51497, accuracy: 0.66272, avg. loss over tasks: 1.51497
Diversity Loss - Mean: 0.03578, Variance: 0.01034
Semantic Loss - Mean: 1.28501, Variance: 0.03603

Train Epoch: 25 
task: sign, mean loss: 0.07454, accuracy: 0.98370, avg. loss over tasks: 0.07454, lr: 0.00029159091704670885
Diversity Loss - Mean: 0.04521, Variance: 0.00802
Semantic Loss - Mean: 0.12421, Variance: 0.00777

Test Epoch: 25 
task: sign, mean loss: 1.49872, accuracy: 0.65680, avg. loss over tasks: 1.49872
Diversity Loss - Mean: 0.08240, Variance: 0.01030
Semantic Loss - Mean: 1.43876, Variance: 0.03662

Train Epoch: 26 
task: sign, mean loss: 0.03619, accuracy: 0.99457, avg. loss over tasks: 0.03619, lr: 0.00029044479536536455
Diversity Loss - Mean: 0.05316, Variance: 0.00803
Semantic Loss - Mean: 0.06890, Variance: 0.00760

Test Epoch: 26 
task: sign, mean loss: 1.76961, accuracy: 0.52663, avg. loss over tasks: 1.76961
Diversity Loss - Mean: 0.09204, Variance: 0.01024
Semantic Loss - Mean: 1.76491, Variance: 0.04007

Train Epoch: 27 
task: sign, mean loss: 0.08542, accuracy: 0.97283, avg. loss over tasks: 0.08542, lr: 0.000289228031029578
Diversity Loss - Mean: 0.05940, Variance: 0.00801
Semantic Loss - Mean: 0.13911, Variance: 0.00785

Test Epoch: 27 
task: sign, mean loss: 1.18705, accuracy: 0.65680, avg. loss over tasks: 1.18705
Diversity Loss - Mean: 0.06661, Variance: 0.01022
Semantic Loss - Mean: 1.20037, Variance: 0.03977

Train Epoch: 28 
task: sign, mean loss: 0.10902, accuracy: 0.96196, avg. loss over tasks: 0.10902, lr: 0.0002879412367168349
Diversity Loss - Mean: 0.06124, Variance: 0.00799
Semantic Loss - Mean: 0.13551, Variance: 0.00768

Test Epoch: 28 
task: sign, mean loss: 2.07658, accuracy: 0.54438, avg. loss over tasks: 2.07658
Diversity Loss - Mean: 0.10144, Variance: 0.01018
Semantic Loss - Mean: 1.93655, Variance: 0.04253

Train Epoch: 29 
task: sign, mean loss: 0.12425, accuracy: 0.95652, avg. loss over tasks: 0.12425, lr: 0.00028658506036682353
Diversity Loss - Mean: 0.06436, Variance: 0.00801
Semantic Loss - Mean: 0.17883, Variance: 0.00774

Test Epoch: 29 
task: sign, mean loss: 2.24675, accuracy: 0.39645, avg. loss over tasks: 2.24675
Diversity Loss - Mean: 0.14594, Variance: 0.01008
Semantic Loss - Mean: 2.22268, Variance: 0.04698

Train Epoch: 30 
task: sign, mean loss: 0.21163, accuracy: 0.91304, avg. loss over tasks: 0.21163, lr: 0.00028516018485517746
Diversity Loss - Mean: 0.04216, Variance: 0.00805
Semantic Loss - Mean: 0.24114, Variance: 0.00780

Test Epoch: 30 
task: sign, mean loss: 2.90736, accuracy: 0.43195, avg. loss over tasks: 2.90736
Diversity Loss - Mean: 0.08478, Variance: 0.01004
Semantic Loss - Mean: 2.77156, Variance: 0.04892

Train Epoch: 31 
task: sign, mean loss: 0.35993, accuracy: 0.89130, avg. loss over tasks: 0.35993, lr: 0.00028366732764962686
Diversity Loss - Mean: 0.01345, Variance: 0.00806
Semantic Loss - Mean: 0.40791, Variance: 0.00802

Test Epoch: 31 
task: sign, mean loss: 1.36225, accuracy: 0.40828, avg. loss over tasks: 1.36225
Diversity Loss - Mean: 0.03475, Variance: 0.00998
Semantic Loss - Mean: 1.17112, Variance: 0.04999

Train Epoch: 32 
task: sign, mean loss: 0.93843, accuracy: 0.71196, avg. loss over tasks: 0.93843, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.03642, Variance: 0.00814
Semantic Loss - Mean: 0.95317, Variance: 0.00828

Test Epoch: 32 
task: sign, mean loss: 5.05783, accuracy: 0.14201, avg. loss over tasks: 5.05783
Diversity Loss - Mean: 0.23297, Variance: 0.01007
Semantic Loss - Mean: 3.30666, Variance: 0.05079

Train Epoch: 33 
task: sign, mean loss: 0.62193, accuracy: 0.75000, avg. loss over tasks: 0.62193, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.03969, Variance: 0.00824
Semantic Loss - Mean: 0.66630, Variance: 0.00835

Test Epoch: 33 
task: sign, mean loss: 2.06110, accuracy: 0.23669, avg. loss over tasks: 2.06110
Diversity Loss - Mean: 0.11280, Variance: 0.01013
Semantic Loss - Mean: 1.59598, Variance: 0.05003

Train Epoch: 34 
task: sign, mean loss: 0.44502, accuracy: 0.84783, avg. loss over tasks: 0.44502, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.02685, Variance: 0.00832
Semantic Loss - Mean: 0.48996, Variance: 0.00845

Test Epoch: 34 
task: sign, mean loss: 0.52008, accuracy: 0.82840, avg. loss over tasks: 0.52008
Diversity Loss - Mean: -0.00749, Variance: 0.01017
Semantic Loss - Mean: 0.53063, Variance: 0.04889

Train Epoch: 35 
task: sign, mean loss: 0.41086, accuracy: 0.83152, avg. loss over tasks: 0.41086, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.02959, Variance: 0.00841
Semantic Loss - Mean: 0.44720, Variance: 0.00849

Test Epoch: 35 
task: sign, mean loss: 0.60200, accuracy: 0.84024, avg. loss over tasks: 0.60200
Diversity Loss - Mean: 0.02823, Variance: 0.01022
Semantic Loss - Mean: 0.60759, Variance: 0.04772

Train Epoch: 36 
task: sign, mean loss: 0.27299, accuracy: 0.88043, avg. loss over tasks: 0.27299, lr: 0.00027521080207013716
Diversity Loss - Mean: 0.00941, Variance: 0.00846
Semantic Loss - Mean: 0.31380, Variance: 0.00847

Test Epoch: 36 
task: sign, mean loss: 0.41645, accuracy: 0.79882, avg. loss over tasks: 0.41645
Diversity Loss - Mean: 0.07044, Variance: 0.01027
Semantic Loss - Mean: 0.45198, Variance: 0.04646

Train Epoch: 37 
task: sign, mean loss: 0.41524, accuracy: 0.82609, avg. loss over tasks: 0.41524, lr: 0.0002733270110058693
Diversity Loss - Mean: 0.00311, Variance: 0.00851
Semantic Loss - Mean: 0.47783, Variance: 0.00891

Test Epoch: 37 
task: sign, mean loss: 0.52270, accuracy: 0.81657, avg. loss over tasks: 0.52270
Diversity Loss - Mean: 0.03018, Variance: 0.01036
Semantic Loss - Mean: 0.49305, Variance: 0.04527

Train Epoch: 38 
task: sign, mean loss: 0.27646, accuracy: 0.87500, avg. loss over tasks: 0.27646, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.00754, Variance: 0.00860
Semantic Loss - Mean: 0.33459, Variance: 0.00923

Test Epoch: 38 
task: sign, mean loss: 0.49164, accuracy: 0.77515, avg. loss over tasks: 0.49164
Diversity Loss - Mean: 0.03267, Variance: 0.01045
Semantic Loss - Mean: 0.49796, Variance: 0.04414

Train Epoch: 39 
task: sign, mean loss: 0.24961, accuracy: 0.87500, avg. loss over tasks: 0.24961, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.00126, Variance: 0.00867
Semantic Loss - Mean: 0.30293, Variance: 0.00929

Test Epoch: 39 
task: sign, mean loss: 0.79120, accuracy: 0.79290, avg. loss over tasks: 0.79120
Diversity Loss - Mean: 0.00968, Variance: 0.01051
Semantic Loss - Mean: 0.68800, Variance: 0.04308

Train Epoch: 40 
task: sign, mean loss: 0.28073, accuracy: 0.89130, avg. loss over tasks: 0.28073, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.00044, Variance: 0.00873
Semantic Loss - Mean: 0.33535, Variance: 0.00943

Test Epoch: 40 
task: sign, mean loss: 0.54346, accuracy: 0.76923, avg. loss over tasks: 0.54346
Diversity Loss - Mean: 0.00416, Variance: 0.01063
Semantic Loss - Mean: 0.54025, Variance: 0.04218

Train Epoch: 41 
task: sign, mean loss: 0.18344, accuracy: 0.92391, avg. loss over tasks: 0.18344, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.00882, Variance: 0.00881
Semantic Loss - Mean: 0.24907, Variance: 0.00957

Test Epoch: 41 
task: sign, mean loss: 0.64226, accuracy: 0.81657, avg. loss over tasks: 0.64226
Diversity Loss - Mean: 0.01359, Variance: 0.01072
Semantic Loss - Mean: 0.60597, Variance: 0.04128

Train Epoch: 42 
task: sign, mean loss: 0.27241, accuracy: 0.91848, avg. loss over tasks: 0.27241, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.01559, Variance: 0.00889
Semantic Loss - Mean: 0.31312, Variance: 0.00966

Test Epoch: 42 
task: sign, mean loss: 0.74167, accuracy: 0.78698, avg. loss over tasks: 0.74167
Diversity Loss - Mean: -0.00845, Variance: 0.01082
Semantic Loss - Mean: 0.68121, Variance: 0.04040

Train Epoch: 43 
task: sign, mean loss: 0.20267, accuracy: 0.93478, avg. loss over tasks: 0.20267, lr: 0.000260757131773478
Diversity Loss - Mean: -0.02061, Variance: 0.00896
Semantic Loss - Mean: 0.27471, Variance: 0.00967

Test Epoch: 43 
task: sign, mean loss: 0.41805, accuracy: 0.85207, avg. loss over tasks: 0.41805
Diversity Loss - Mean: 0.01718, Variance: 0.01090
Semantic Loss - Mean: 0.42854, Variance: 0.03959

Train Epoch: 44 
task: sign, mean loss: 0.10657, accuracy: 0.95652, avg. loss over tasks: 0.10657, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.01583, Variance: 0.00902
Semantic Loss - Mean: 0.19273, Variance: 0.00976

Test Epoch: 44 
task: sign, mean loss: 0.47978, accuracy: 0.86982, avg. loss over tasks: 0.47978
Diversity Loss - Mean: -0.01095, Variance: 0.01100
Semantic Loss - Mean: 0.46581, Variance: 0.03876

Train Epoch: 45 
task: sign, mean loss: 0.10099, accuracy: 0.96196, avg. loss over tasks: 0.10099, lr: 0.0002561099511608041
Diversity Loss - Mean: 0.00057, Variance: 0.00906
Semantic Loss - Mean: 0.15710, Variance: 0.00972

Test Epoch: 45 
task: sign, mean loss: 0.40330, accuracy: 0.87574, avg. loss over tasks: 0.40330
Diversity Loss - Mean: 0.01155, Variance: 0.01107
Semantic Loss - Mean: 0.39671, Variance: 0.03800

Train Epoch: 46 
task: sign, mean loss: 0.13807, accuracy: 0.94022, avg. loss over tasks: 0.13807, lr: 0.00025370573795068164
Diversity Loss - Mean: 0.00949, Variance: 0.00908
Semantic Loss - Mean: 0.20926, Variance: 0.00978

Test Epoch: 46 
task: sign, mean loss: 0.51681, accuracy: 0.86391, avg. loss over tasks: 0.51681
Diversity Loss - Mean: 0.01268, Variance: 0.01112
Semantic Loss - Mean: 0.49085, Variance: 0.03730

Train Epoch: 47 
task: sign, mean loss: 0.13936, accuracy: 0.96196, avg. loss over tasks: 0.13936, lr: 0.0002512493813079214
Diversity Loss - Mean: 0.00664, Variance: 0.00909
Semantic Loss - Mean: 0.21699, Variance: 0.00993

Test Epoch: 47 
task: sign, mean loss: 0.58540, accuracy: 0.84615, avg. loss over tasks: 0.58540
Diversity Loss - Mean: 0.07923, Variance: 0.01110
Semantic Loss - Mean: 0.61477, Variance: 0.03668

Train Epoch: 48 
task: sign, mean loss: 0.10777, accuracy: 0.95652, avg. loss over tasks: 0.10777, lr: 0.0002487421180820659
Diversity Loss - Mean: 0.00179, Variance: 0.00912
Semantic Loss - Mean: 0.16505, Variance: 0.00997

Test Epoch: 48 
task: sign, mean loss: 0.47097, accuracy: 0.87574, avg. loss over tasks: 0.47097
Diversity Loss - Mean: 0.08948, Variance: 0.01107
Semantic Loss - Mean: 0.52773, Variance: 0.03629

Train Epoch: 49 
task: sign, mean loss: 0.11608, accuracy: 0.97283, avg. loss over tasks: 0.11608, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.00216, Variance: 0.00915
Semantic Loss - Mean: 0.14711, Variance: 0.01002

Test Epoch: 49 
task: sign, mean loss: 0.81330, accuracy: 0.83432, avg. loss over tasks: 0.81330
Diversity Loss - Mean: 0.02203, Variance: 0.01111
Semantic Loss - Mean: 0.75577, Variance: 0.03572

Train Epoch: 50 
task: sign, mean loss: 0.09482, accuracy: 0.97826, avg. loss over tasks: 0.09482, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.00457, Variance: 0.00917
Semantic Loss - Mean: 0.17385, Variance: 0.01006

Test Epoch: 50 
task: sign, mean loss: 0.47220, accuracy: 0.88757, avg. loss over tasks: 0.47220
Diversity Loss - Mean: 0.03317, Variance: 0.01113
Semantic Loss - Mean: 0.45309, Variance: 0.03514

Train Epoch: 51 
task: sign, mean loss: 0.09726, accuracy: 0.95109, avg. loss over tasks: 0.09726, lr: 0.00024092763806954684
Diversity Loss - Mean: 0.00391, Variance: 0.00918
Semantic Loss - Mean: 0.17977, Variance: 0.01011

Test Epoch: 51 
task: sign, mean loss: 0.43223, accuracy: 0.86982, avg. loss over tasks: 0.43223
Diversity Loss - Mean: 0.02885, Variance: 0.01119
Semantic Loss - Mean: 0.44659, Variance: 0.03453

Train Epoch: 52 
task: sign, mean loss: 0.06730, accuracy: 0.98370, avg. loss over tasks: 0.06730, lr: 0.00023822962005602707
Diversity Loss - Mean: 0.00523, Variance: 0.00920
Semantic Loss - Mean: 0.16113, Variance: 0.01029

Test Epoch: 52 
task: sign, mean loss: 0.48121, accuracy: 0.86982, avg. loss over tasks: 0.48121
Diversity Loss - Mean: 0.04327, Variance: 0.01119
Semantic Loss - Mean: 0.64014, Variance: 0.03516

Train Epoch: 53 
task: sign, mean loss: 0.06582, accuracy: 0.97283, avg. loss over tasks: 0.06582, lr: 0.00023548725130129248
Diversity Loss - Mean: 0.00932, Variance: 0.00922
Semantic Loss - Mean: 0.16955, Variance: 0.01044

Test Epoch: 53 
task: sign, mean loss: 0.56578, accuracy: 0.86391, avg. loss over tasks: 0.56578
Diversity Loss - Mean: 0.05216, Variance: 0.01120
Semantic Loss - Mean: 0.51975, Variance: 0.03460

Train Epoch: 54 
task: sign, mean loss: 0.07360, accuracy: 0.99457, avg. loss over tasks: 0.07360, lr: 0.00023270191267059755
Diversity Loss - Mean: 0.01037, Variance: 0.00924
Semantic Loss - Mean: 0.15456, Variance: 0.01062

Test Epoch: 54 
task: sign, mean loss: 0.60622, accuracy: 0.82840, avg. loss over tasks: 0.60622
Diversity Loss - Mean: 0.07815, Variance: 0.01118
Semantic Loss - Mean: 0.68510, Variance: 0.03495

Train Epoch: 55 
task: sign, mean loss: 0.05830, accuracy: 0.98913, avg. loss over tasks: 0.05830, lr: 0.00022987500666582316
Diversity Loss - Mean: 0.01432, Variance: 0.00926
Semantic Loss - Mean: 0.13786, Variance: 0.01070

Test Epoch: 55 
task: sign, mean loss: 0.49506, accuracy: 0.86982, avg. loss over tasks: 0.49506
Diversity Loss - Mean: 0.02742, Variance: 0.01119
Semantic Loss - Mean: 0.47657, Variance: 0.03469

Train Epoch: 56 
task: sign, mean loss: 0.02954, accuracy: 0.99457, avg. loss over tasks: 0.02954, lr: 0.00022700795671927503
Diversity Loss - Mean: 0.01855, Variance: 0.00927
Semantic Loss - Mean: 0.13842, Variance: 0.01085

Test Epoch: 56 
task: sign, mean loss: 0.80966, accuracy: 0.86391, avg. loss over tasks: 0.80966
Diversity Loss - Mean: 0.07046, Variance: 0.01119
Semantic Loss - Mean: 0.80145, Variance: 0.03450

Train Epoch: 57 
task: sign, mean loss: 0.01842, accuracy: 0.99457, avg. loss over tasks: 0.01842, lr: 0.00022410220647694235
Diversity Loss - Mean: 0.00835, Variance: 0.00929
Semantic Loss - Mean: 0.09358, Variance: 0.01095

Test Epoch: 57 
task: sign, mean loss: 0.65005, accuracy: 0.88166, avg. loss over tasks: 0.65005
Diversity Loss - Mean: 0.08686, Variance: 0.01117
Semantic Loss - Mean: 0.65611, Variance: 0.03427

Train Epoch: 58 
task: sign, mean loss: 0.02758, accuracy: 0.99457, avg. loss over tasks: 0.02758, lr: 0.00022115921907157884
Diversity Loss - Mean: 0.00371, Variance: 0.00931
Semantic Loss - Mean: 0.09377, Variance: 0.01093

Test Epoch: 58 
task: sign, mean loss: 0.58949, accuracy: 0.89941, avg. loss over tasks: 0.58949
Diversity Loss - Mean: 0.06544, Variance: 0.01118
Semantic Loss - Mean: 0.53196, Variance: 0.03391

Train Epoch: 59 
task: sign, mean loss: 0.02938, accuracy: 0.98913, avg. loss over tasks: 0.02938, lr: 0.00021818047638597106
Diversity Loss - Mean: 0.00302, Variance: 0.00933
Semantic Loss - Mean: 0.10943, Variance: 0.01112

Test Epoch: 59 
task: sign, mean loss: 0.94953, accuracy: 0.86982, avg. loss over tasks: 0.94953
Diversity Loss - Mean: 0.03058, Variance: 0.01120
Semantic Loss - Mean: 0.71938, Variance: 0.03360

Train Epoch: 60 
task: sign, mean loss: 0.04139, accuracy: 0.98913, avg. loss over tasks: 0.04139, lr: 0.00021516747830676604
Diversity Loss - Mean: 0.00086, Variance: 0.00934
Semantic Loss - Mean: 0.09284, Variance: 0.01105

Test Epoch: 60 
task: sign, mean loss: 0.54779, accuracy: 0.89349, avg. loss over tasks: 0.54779
Diversity Loss - Mean: 0.06180, Variance: 0.01122
Semantic Loss - Mean: 0.69912, Variance: 0.03355

Train Epoch: 61 
task: sign, mean loss: 0.04813, accuracy: 0.98370, avg. loss over tasks: 0.04813, lr: 0.0002121217419692331
Diversity Loss - Mean: 0.00158, Variance: 0.00936
Semantic Loss - Mean: 0.09436, Variance: 0.01117

Test Epoch: 61 
task: sign, mean loss: 0.41034, accuracy: 0.89349, avg. loss over tasks: 0.41034
Diversity Loss - Mean: 0.05118, Variance: 0.01123
Semantic Loss - Mean: 0.45275, Variance: 0.03338

Train Epoch: 62 
task: sign, mean loss: 0.08430, accuracy: 0.97283, avg. loss over tasks: 0.08430, lr: 0.00020904480099334042
Diversity Loss - Mean: 0.01273, Variance: 0.00938
Semantic Loss - Mean: 0.14681, Variance: 0.01112

Test Epoch: 62 
task: sign, mean loss: 0.48078, accuracy: 0.88166, avg. loss over tasks: 0.48078
Diversity Loss - Mean: 0.03996, Variance: 0.01125
Semantic Loss - Mean: 0.48740, Variance: 0.03294

Train Epoch: 63 
task: sign, mean loss: 0.03905, accuracy: 0.98913, avg. loss over tasks: 0.03905, lr: 0.00020593820471153146
Diversity Loss - Mean: 0.01232, Variance: 0.00939
Semantic Loss - Mean: 0.08660, Variance: 0.01103

Test Epoch: 63 
task: sign, mean loss: 0.44407, accuracy: 0.91716, avg. loss over tasks: 0.44407
Diversity Loss - Mean: 0.04183, Variance: 0.01126
Semantic Loss - Mean: 0.43791, Variance: 0.03254

Train Epoch: 64 
task: sign, mean loss: 0.02281, accuracy: 0.99457, avg. loss over tasks: 0.02281, lr: 0.0002028035173885892
Diversity Loss - Mean: 0.00348, Variance: 0.00940
Semantic Loss - Mean: 0.06079, Variance: 0.01101

Test Epoch: 64 
task: sign, mean loss: 0.53238, accuracy: 0.89349, avg. loss over tasks: 0.53238
Diversity Loss - Mean: 0.02449, Variance: 0.01128
Semantic Loss - Mean: 0.48548, Variance: 0.03214

Train Epoch: 65 
task: sign, mean loss: 0.00949, accuracy: 1.00000, avg. loss over tasks: 0.00949, lr: 0.00019964231743398178
Diversity Loss - Mean: 0.00101, Variance: 0.00941
Semantic Loss - Mean: 0.07797, Variance: 0.01106

Test Epoch: 65 
task: sign, mean loss: 0.56230, accuracy: 0.88166, avg. loss over tasks: 0.56230
Diversity Loss - Mean: 0.02854, Variance: 0.01130
Semantic Loss - Mean: 0.53503, Variance: 0.03187

Train Epoch: 66 
task: sign, mean loss: 0.01661, accuracy: 0.99457, avg. loss over tasks: 0.01661, lr: 0.00019645619660708585
Diversity Loss - Mean: 0.00631, Variance: 0.00941
Semantic Loss - Mean: 0.09331, Variance: 0.01110

Test Epoch: 66 
task: sign, mean loss: 0.45897, accuracy: 0.89941, avg. loss over tasks: 0.45897
Diversity Loss - Mean: 0.02821, Variance: 0.01131
Semantic Loss - Mean: 0.40825, Variance: 0.03147

Train Epoch: 67 
task: sign, mean loss: 0.02683, accuracy: 0.99457, avg. loss over tasks: 0.02683, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.00034, Variance: 0.00942
Semantic Loss - Mean: 0.08169, Variance: 0.01104

Test Epoch: 67 
task: sign, mean loss: 0.50153, accuracy: 0.87574, avg. loss over tasks: 0.50153
Diversity Loss - Mean: 0.05843, Variance: 0.01130
Semantic Loss - Mean: 0.47299, Variance: 0.03123

Train Epoch: 68 
task: sign, mean loss: 0.18895, accuracy: 0.94565, avg. loss over tasks: 0.18895, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.00472, Variance: 0.00943
Semantic Loss - Mean: 0.23710, Variance: 0.01147

Test Epoch: 68 
task: sign, mean loss: 2.54664, accuracy: 0.47929, avg. loss over tasks: 2.54664
Diversity Loss - Mean: 0.05474, Variance: 0.01133
Semantic Loss - Mean: 1.88222, Variance: 0.03182

Train Epoch: 69 
task: sign, mean loss: 0.21371, accuracy: 0.95109, avg. loss over tasks: 0.21371, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.02672, Variance: 0.00946
Semantic Loss - Mean: 0.29220, Variance: 0.01162

Test Epoch: 69 
task: sign, mean loss: 0.45751, accuracy: 0.88166, avg. loss over tasks: 0.45751
Diversity Loss - Mean: 0.05065, Variance: 0.01132
Semantic Loss - Mean: 0.53781, Variance: 0.03172

Train Epoch: 70 
task: sign, mean loss: 0.08153, accuracy: 0.97826, avg. loss over tasks: 0.08153, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.01395, Variance: 0.00948
Semantic Loss - Mean: 0.12230, Variance: 0.01154

Test Epoch: 70 
task: sign, mean loss: 0.48162, accuracy: 0.85207, avg. loss over tasks: 0.48162
Diversity Loss - Mean: 0.03247, Variance: 0.01131
Semantic Loss - Mean: 0.46092, Variance: 0.03147

Train Epoch: 71 
task: sign, mean loss: 0.11548, accuracy: 0.97283, avg. loss over tasks: 0.11548, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.01385, Variance: 0.00949
Semantic Loss - Mean: 0.18380, Variance: 0.01163

Test Epoch: 71 
task: sign, mean loss: 0.55882, accuracy: 0.86982, avg. loss over tasks: 0.55882
Diversity Loss - Mean: 0.03120, Variance: 0.01131
Semantic Loss - Mean: 0.63749, Variance: 0.03150

Train Epoch: 72 
task: sign, mean loss: 0.09522, accuracy: 0.96739, avg. loss over tasks: 0.09522, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.01640, Variance: 0.00950
Semantic Loss - Mean: 0.15296, Variance: 0.01155

Test Epoch: 72 
task: sign, mean loss: 0.76789, accuracy: 0.81065, avg. loss over tasks: 0.76789
Diversity Loss - Mean: 0.03919, Variance: 0.01131
Semantic Loss - Mean: 0.76457, Variance: 0.03138

Train Epoch: 73 
task: sign, mean loss: 0.05248, accuracy: 0.97826, avg. loss over tasks: 0.05248, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.00929, Variance: 0.00951
Semantic Loss - Mean: 0.12028, Variance: 0.01159

Test Epoch: 73 
task: sign, mean loss: 0.50918, accuracy: 0.88757, avg. loss over tasks: 0.50918
Diversity Loss - Mean: -0.01279, Variance: 0.01136
Semantic Loss - Mean: 0.44484, Variance: 0.03102

Train Epoch: 74 
task: sign, mean loss: 0.03137, accuracy: 0.98913, avg. loss over tasks: 0.03137, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.01390, Variance: 0.00952
Semantic Loss - Mean: 0.08015, Variance: 0.01151

Test Epoch: 74 
task: sign, mean loss: 0.50086, accuracy: 0.88757, avg. loss over tasks: 0.50086
Diversity Loss - Mean: 0.00457, Variance: 0.01139
Semantic Loss - Mean: 0.47231, Variance: 0.03074

Train Epoch: 75 
task: sign, mean loss: 0.01199, accuracy: 1.00000, avg. loss over tasks: 0.01199, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.01356, Variance: 0.00954
Semantic Loss - Mean: 0.08361, Variance: 0.01152

Test Epoch: 75 
task: sign, mean loss: 0.47475, accuracy: 0.88757, avg. loss over tasks: 0.47475
Diversity Loss - Mean: 0.00481, Variance: 0.01143
Semantic Loss - Mean: 0.45297, Variance: 0.03051

Train Epoch: 76 
task: sign, mean loss: 0.01468, accuracy: 0.99457, avg. loss over tasks: 0.01468, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.01633, Variance: 0.00956
Semantic Loss - Mean: 0.06320, Variance: 0.01153

Test Epoch: 76 
task: sign, mean loss: 0.43336, accuracy: 0.91124, avg. loss over tasks: 0.43336
Diversity Loss - Mean: -0.00464, Variance: 0.01148
Semantic Loss - Mean: 0.41560, Variance: 0.03019

Train Epoch: 77 
task: sign, mean loss: 0.00849, accuracy: 1.00000, avg. loss over tasks: 0.00849, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.01747, Variance: 0.00957
Semantic Loss - Mean: 0.05446, Variance: 0.01147

Test Epoch: 77 
task: sign, mean loss: 0.44790, accuracy: 0.91716, avg. loss over tasks: 0.44790
Diversity Loss - Mean: 0.01005, Variance: 0.01150
Semantic Loss - Mean: 0.51487, Variance: 0.03005

Train Epoch: 78 
task: sign, mean loss: 0.02671, accuracy: 0.99457, avg. loss over tasks: 0.02671, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.01304, Variance: 0.00959
Semantic Loss - Mean: 0.06057, Variance: 0.01139

Test Epoch: 78 
task: sign, mean loss: 0.43226, accuracy: 0.92308, avg. loss over tasks: 0.43226
Diversity Loss - Mean: 0.00457, Variance: 0.01153
Semantic Loss - Mean: 0.48951, Variance: 0.02984

Train Epoch: 79 
task: sign, mean loss: 0.05082, accuracy: 0.98913, avg. loss over tasks: 0.05082, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.01533, Variance: 0.00960
Semantic Loss - Mean: 0.07870, Variance: 0.01131

Test Epoch: 79 
task: sign, mean loss: 0.38590, accuracy: 0.92899, avg. loss over tasks: 0.38590
Diversity Loss - Mean: -0.01693, Variance: 0.01157
Semantic Loss - Mean: 0.37345, Variance: 0.02955

Train Epoch: 80 
task: sign, mean loss: 0.08653, accuracy: 0.97283, avg. loss over tasks: 0.08653, lr: 0.00015015
Diversity Loss - Mean: -0.01873, Variance: 0.00961
Semantic Loss - Mean: 0.11303, Variance: 0.01123

Test Epoch: 80 
task: sign, mean loss: 0.41552, accuracy: 0.91716, avg. loss over tasks: 0.41552
Diversity Loss - Mean: -0.00249, Variance: 0.01160
Semantic Loss - Mean: 0.39789, Variance: 0.02924

Train Epoch: 81 
task: sign, mean loss: 0.19971, accuracy: 0.97283, avg. loss over tasks: 0.19971, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.02911, Variance: 0.00963
Semantic Loss - Mean: 0.20034, Variance: 0.01122

Test Epoch: 81 
task: sign, mean loss: 0.50919, accuracy: 0.87574, avg. loss over tasks: 0.50919
Diversity Loss - Mean: 0.01319, Variance: 0.01161
Semantic Loss - Mean: 0.67648, Variance: 0.02912

Train Epoch: 82 
task: sign, mean loss: 0.17780, accuracy: 0.94565, avg. loss over tasks: 0.17780, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.05116, Variance: 0.00967
Semantic Loss - Mean: 0.23830, Variance: 0.01132

Test Epoch: 82 
task: sign, mean loss: 0.56941, accuracy: 0.91716, avg. loss over tasks: 0.56941
Diversity Loss - Mean: 0.01269, Variance: 0.01159
Semantic Loss - Mean: 0.60192, Variance: 0.02891

Train Epoch: 83 
task: sign, mean loss: 0.08738, accuracy: 0.97283, avg. loss over tasks: 0.08738, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.04470, Variance: 0.00968
Semantic Loss - Mean: 0.17184, Variance: 0.01139

Test Epoch: 83 
task: sign, mean loss: 0.56600, accuracy: 0.88166, avg. loss over tasks: 0.56600
Diversity Loss - Mean: -0.02121, Variance: 0.01161
Semantic Loss - Mean: 0.53442, Variance: 0.02868

Train Epoch: 84 
task: sign, mean loss: 0.04691, accuracy: 0.98913, avg. loss over tasks: 0.04691, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.04484, Variance: 0.00970
Semantic Loss - Mean: 0.12246, Variance: 0.01146

Test Epoch: 84 
task: sign, mean loss: 0.32557, accuracy: 0.91124, avg. loss over tasks: 0.32557
Diversity Loss - Mean: -0.00775, Variance: 0.01162
Semantic Loss - Mean: 0.49552, Variance: 0.02881

Train Epoch: 85 
task: sign, mean loss: 0.03081, accuracy: 0.98913, avg. loss over tasks: 0.03081, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.04255, Variance: 0.00972
Semantic Loss - Mean: 0.06751, Variance: 0.01137

Test Epoch: 85 
task: sign, mean loss: 0.47234, accuracy: 0.85799, avg. loss over tasks: 0.47234
Diversity Loss - Mean: 0.00229, Variance: 0.01162
Semantic Loss - Mean: 0.83125, Variance: 0.02919

Train Epoch: 86 
task: sign, mean loss: 0.01658, accuracy: 1.00000, avg. loss over tasks: 0.01658, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.04126, Variance: 0.00974
Semantic Loss - Mean: 0.07126, Variance: 0.01131

Test Epoch: 86 
task: sign, mean loss: 0.27791, accuracy: 0.91716, avg. loss over tasks: 0.27791
Diversity Loss - Mean: -0.01164, Variance: 0.01164
Semantic Loss - Mean: 0.47053, Variance: 0.02919

Train Epoch: 87 
task: sign, mean loss: 0.01096, accuracy: 1.00000, avg. loss over tasks: 0.01096, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.03554, Variance: 0.00976
Semantic Loss - Mean: 0.05686, Variance: 0.01122

Test Epoch: 87 
task: sign, mean loss: 0.26316, accuracy: 0.91716, avg. loss over tasks: 0.26316
Diversity Loss - Mean: -0.01793, Variance: 0.01166
Semantic Loss - Mean: 0.32826, Variance: 0.02895

Train Epoch: 88 
task: sign, mean loss: 0.00654, accuracy: 1.00000, avg. loss over tasks: 0.00654, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.03882, Variance: 0.00977
Semantic Loss - Mean: 0.04858, Variance: 0.01116

Test Epoch: 88 
task: sign, mean loss: 0.29626, accuracy: 0.90533, avg. loss over tasks: 0.29626
Diversity Loss - Mean: -0.01401, Variance: 0.01168
Semantic Loss - Mean: 0.37887, Variance: 0.02874

Train Epoch: 89 
task: sign, mean loss: 0.01394, accuracy: 0.99457, avg. loss over tasks: 0.01394, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.03476, Variance: 0.00979
Semantic Loss - Mean: 0.05652, Variance: 0.01108

Test Epoch: 89 
task: sign, mean loss: 0.29777, accuracy: 0.91124, avg. loss over tasks: 0.29777
Diversity Loss - Mean: -0.01559, Variance: 0.01170
Semantic Loss - Mean: 0.34773, Variance: 0.02849

Train Epoch: 90 
task: sign, mean loss: 0.00501, accuracy: 1.00000, avg. loss over tasks: 0.00501, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.03769, Variance: 0.00981
Semantic Loss - Mean: 0.04530, Variance: 0.01103

Test Epoch: 90 
task: sign, mean loss: 0.30807, accuracy: 0.91124, avg. loss over tasks: 0.30807
Diversity Loss - Mean: -0.01529, Variance: 0.01172
Semantic Loss - Mean: 0.36531, Variance: 0.02828

Train Epoch: 91 
task: sign, mean loss: 0.02765, accuracy: 0.99457, avg. loss over tasks: 0.02765, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.03380, Variance: 0.00982
Semantic Loss - Mean: 0.05931, Variance: 0.01099

Test Epoch: 91 
task: sign, mean loss: 0.28304, accuracy: 0.92899, avg. loss over tasks: 0.28304
Diversity Loss - Mean: -0.01383, Variance: 0.01173
Semantic Loss - Mean: 0.33785, Variance: 0.02802

Train Epoch: 92 
task: sign, mean loss: 0.00450, accuracy: 1.00000, avg. loss over tasks: 0.00450, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.03587, Variance: 0.00983
Semantic Loss - Mean: 0.04192, Variance: 0.01095

Test Epoch: 92 
task: sign, mean loss: 0.29403, accuracy: 0.92899, avg. loss over tasks: 0.29403
Diversity Loss - Mean: -0.01258, Variance: 0.01175
Semantic Loss - Mean: 0.35363, Variance: 0.02781

Train Epoch: 93 
task: sign, mean loss: 0.00375, accuracy: 1.00000, avg. loss over tasks: 0.00375, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.03644, Variance: 0.00985
Semantic Loss - Mean: 0.03365, Variance: 0.01090

Test Epoch: 93 
task: sign, mean loss: 0.30841, accuracy: 0.93491, avg. loss over tasks: 0.30841
Diversity Loss - Mean: -0.01536, Variance: 0.01177
Semantic Loss - Mean: 0.36966, Variance: 0.02762

Train Epoch: 94 
task: sign, mean loss: 0.00852, accuracy: 0.99457, avg. loss over tasks: 0.00852, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.03374, Variance: 0.00986
Semantic Loss - Mean: 0.02952, Variance: 0.01081

Test Epoch: 94 
task: sign, mean loss: 0.27053, accuracy: 0.93491, avg. loss over tasks: 0.27053
Diversity Loss - Mean: -0.00943, Variance: 0.01178
Semantic Loss - Mean: 0.37486, Variance: 0.02740

Train Epoch: 95 
task: sign, mean loss: 0.01243, accuracy: 0.99457, avg. loss over tasks: 0.01243, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.03428, Variance: 0.00987
Semantic Loss - Mean: 0.05215, Variance: 0.01077

Test Epoch: 95 
task: sign, mean loss: 0.52260, accuracy: 0.88757, avg. loss over tasks: 0.52260
Diversity Loss - Mean: 0.00886, Variance: 0.01178
Semantic Loss - Mean: 0.59991, Variance: 0.02724

Train Epoch: 96 
task: sign, mean loss: 0.00626, accuracy: 1.00000, avg. loss over tasks: 0.00626, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.03376, Variance: 0.00988
Semantic Loss - Mean: 0.04259, Variance: 0.01072

Test Epoch: 96 
task: sign, mean loss: 0.49077, accuracy: 0.89941, avg. loss over tasks: 0.49077
Diversity Loss - Mean: -0.00131, Variance: 0.01179
Semantic Loss - Mean: 0.55587, Variance: 0.02707

Train Epoch: 97 
task: sign, mean loss: 0.01932, accuracy: 0.99457, avg. loss over tasks: 0.01932, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.03264, Variance: 0.00989
Semantic Loss - Mean: 0.05395, Variance: 0.01067

Test Epoch: 97 
task: sign, mean loss: 0.46888, accuracy: 0.91716, avg. loss over tasks: 0.46888
Diversity Loss - Mean: 0.00264, Variance: 0.01179
Semantic Loss - Mean: 0.57613, Variance: 0.02692

Train Epoch: 98 
task: sign, mean loss: 0.02252, accuracy: 0.99457, avg. loss over tasks: 0.02252, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.03536, Variance: 0.00990
Semantic Loss - Mean: 0.04112, Variance: 0.01060

Test Epoch: 98 
task: sign, mean loss: 0.55671, accuracy: 0.89349, avg. loss over tasks: 0.55671
Diversity Loss - Mean: 0.00525, Variance: 0.01179
Semantic Loss - Mean: 0.68790, Variance: 0.02679

Train Epoch: 99 
task: sign, mean loss: 0.02203, accuracy: 0.98913, avg. loss over tasks: 0.02203, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.03593, Variance: 0.00991
Semantic Loss - Mean: 0.04616, Variance: 0.01053

Test Epoch: 99 
task: sign, mean loss: 0.48805, accuracy: 0.88166, avg. loss over tasks: 0.48805
Diversity Loss - Mean: -0.00274, Variance: 0.01179
Semantic Loss - Mean: 0.55009, Variance: 0.02663

Train Epoch: 100 
task: sign, mean loss: 0.01137, accuracy: 0.99457, avg. loss over tasks: 0.01137, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.03924, Variance: 0.00992
Semantic Loss - Mean: 0.05748, Variance: 0.01049

Test Epoch: 100 
task: sign, mean loss: 0.42531, accuracy: 0.91716, avg. loss over tasks: 0.42531
Diversity Loss - Mean: -0.01113, Variance: 0.01179
Semantic Loss - Mean: 0.44935, Variance: 0.02641

Train Epoch: 101 
task: sign, mean loss: 0.00472, accuracy: 1.00000, avg. loss over tasks: 0.00472, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.03627, Variance: 0.00992
Semantic Loss - Mean: 0.03761, Variance: 0.01043

Test Epoch: 101 
task: sign, mean loss: 0.46971, accuracy: 0.92308, avg. loss over tasks: 0.46971
Diversity Loss - Mean: -0.02524, Variance: 0.01180
Semantic Loss - Mean: 0.50530, Variance: 0.02624

Train Epoch: 102 
task: sign, mean loss: 0.00305, accuracy: 1.00000, avg. loss over tasks: 0.00305, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.03832, Variance: 0.00993
Semantic Loss - Mean: 0.03729, Variance: 0.01041

Test Epoch: 102 
task: sign, mean loss: 0.46348, accuracy: 0.92308, avg. loss over tasks: 0.46348
Diversity Loss - Mean: -0.01746, Variance: 0.01181
Semantic Loss - Mean: 0.50054, Variance: 0.02608

Train Epoch: 103 
task: sign, mean loss: 0.00630, accuracy: 1.00000, avg. loss over tasks: 0.00630, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.03601, Variance: 0.00994
Semantic Loss - Mean: 0.02525, Variance: 0.01032

Test Epoch: 103 
task: sign, mean loss: 0.46453, accuracy: 0.92308, avg. loss over tasks: 0.46453
Diversity Loss - Mean: -0.00944, Variance: 0.01182
Semantic Loss - Mean: 0.51580, Variance: 0.02590

Train Epoch: 104 
task: sign, mean loss: 0.00238, accuracy: 1.00000, avg. loss over tasks: 0.00238, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.03495, Variance: 0.00994
Semantic Loss - Mean: 0.02568, Variance: 0.01026

Test Epoch: 104 
task: sign, mean loss: 0.42281, accuracy: 0.93491, avg. loss over tasks: 0.42281
Diversity Loss - Mean: -0.00510, Variance: 0.01182
Semantic Loss - Mean: 0.47272, Variance: 0.02573

Train Epoch: 105 
task: sign, mean loss: 0.00136, accuracy: 1.00000, avg. loss over tasks: 0.00136, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.03741, Variance: 0.00995
Semantic Loss - Mean: 0.02093, Variance: 0.01018

Test Epoch: 105 
task: sign, mean loss: 0.45555, accuracy: 0.92899, avg. loss over tasks: 0.45555
Diversity Loss - Mean: -0.01444, Variance: 0.01183
Semantic Loss - Mean: 0.50581, Variance: 0.02560

Train Epoch: 106 
task: sign, mean loss: 0.00293, accuracy: 1.00000, avg. loss over tasks: 0.00293, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.04103, Variance: 0.00996
Semantic Loss - Mean: 0.02616, Variance: 0.01010

Test Epoch: 106 
task: sign, mean loss: 0.38059, accuracy: 0.92308, avg. loss over tasks: 0.38059
Diversity Loss - Mean: -0.01232, Variance: 0.01183
Semantic Loss - Mean: 0.45284, Variance: 0.02545

Train Epoch: 107 
task: sign, mean loss: 0.01949, accuracy: 0.99457, avg. loss over tasks: 0.01949, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.03722, Variance: 0.00996
Semantic Loss - Mean: 0.05856, Variance: 0.01009

Test Epoch: 107 
task: sign, mean loss: 0.46840, accuracy: 0.92899, avg. loss over tasks: 0.46840
Diversity Loss - Mean: -0.01677, Variance: 0.01184
Semantic Loss - Mean: 0.49252, Variance: 0.02534

Train Epoch: 108 
task: sign, mean loss: 0.00407, accuracy: 1.00000, avg. loss over tasks: 0.00407, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.03843, Variance: 0.00997
Semantic Loss - Mean: 0.02775, Variance: 0.01002

Test Epoch: 108 
task: sign, mean loss: 0.48294, accuracy: 0.91716, avg. loss over tasks: 0.48294
Diversity Loss - Mean: -0.01726, Variance: 0.01185
Semantic Loss - Mean: 0.51256, Variance: 0.02527

Train Epoch: 109 
task: sign, mean loss: 0.00347, accuracy: 1.00000, avg. loss over tasks: 0.00347, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.04215, Variance: 0.00998
Semantic Loss - Mean: 0.03495, Variance: 0.00996

Test Epoch: 109 
task: sign, mean loss: 0.47070, accuracy: 0.92899, avg. loss over tasks: 0.47070
Diversity Loss - Mean: -0.01323, Variance: 0.01186
Semantic Loss - Mean: 0.53599, Variance: 0.02518

Train Epoch: 110 
task: sign, mean loss: 0.00145, accuracy: 1.00000, avg. loss over tasks: 0.00145, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.04141, Variance: 0.00998
Semantic Loss - Mean: 0.01711, Variance: 0.00989

Test Epoch: 110 
task: sign, mean loss: 0.46687, accuracy: 0.92308, avg. loss over tasks: 0.46687
Diversity Loss - Mean: -0.01259, Variance: 0.01186
Semantic Loss - Mean: 0.56331, Variance: 0.02508

Train Epoch: 111 
task: sign, mean loss: 0.00213, accuracy: 1.00000, avg. loss over tasks: 0.00213, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.04107, Variance: 0.00999
Semantic Loss - Mean: 0.03703, Variance: 0.00985

Test Epoch: 111 
task: sign, mean loss: 0.43100, accuracy: 0.91716, avg. loss over tasks: 0.43100
Diversity Loss - Mean: -0.01405, Variance: 0.01187
Semantic Loss - Mean: 0.49482, Variance: 0.02491

Train Epoch: 112 
task: sign, mean loss: 0.00164, accuracy: 1.00000, avg. loss over tasks: 0.00164, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.04372, Variance: 0.00999
Semantic Loss - Mean: 0.02153, Variance: 0.00978

Test Epoch: 112 
task: sign, mean loss: 0.43867, accuracy: 0.92899, avg. loss over tasks: 0.43867
Diversity Loss - Mean: -0.01557, Variance: 0.01188
Semantic Loss - Mean: 0.50275, Variance: 0.02477

Train Epoch: 113 
task: sign, mean loss: 0.00189, accuracy: 1.00000, avg. loss over tasks: 0.00189, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.04204, Variance: 0.01000
Semantic Loss - Mean: 0.01458, Variance: 0.00970

Test Epoch: 113 
task: sign, mean loss: 0.45388, accuracy: 0.92899, avg. loss over tasks: 0.45388
Diversity Loss - Mean: -0.01530, Variance: 0.01189
Semantic Loss - Mean: 0.51062, Variance: 0.02463

Train Epoch: 114 
task: sign, mean loss: 0.00193, accuracy: 1.00000, avg. loss over tasks: 0.00193, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.04152, Variance: 0.01001
Semantic Loss - Mean: 0.03154, Variance: 0.00966

Test Epoch: 114 
task: sign, mean loss: 0.42263, accuracy: 0.92308, avg. loss over tasks: 0.42263
Diversity Loss - Mean: -0.01522, Variance: 0.01190
Semantic Loss - Mean: 0.52651, Variance: 0.02452

Train Epoch: 115 
task: sign, mean loss: 0.00396, accuracy: 1.00000, avg. loss over tasks: 0.00396, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.04157, Variance: 0.01002
Semantic Loss - Mean: 0.02555, Variance: 0.00960

Test Epoch: 115 
task: sign, mean loss: 0.47838, accuracy: 0.92899, avg. loss over tasks: 0.47838
Diversity Loss - Mean: -0.01061, Variance: 0.01190
Semantic Loss - Mean: 0.66518, Variance: 0.02472

Train Epoch: 116 
task: sign, mean loss: 0.00246, accuracy: 1.00000, avg. loss over tasks: 0.00246, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.04291, Variance: 0.01002
Semantic Loss - Mean: 0.03051, Variance: 0.00953

Test Epoch: 116 
task: sign, mean loss: 0.43999, accuracy: 0.93491, avg. loss over tasks: 0.43999
Diversity Loss - Mean: -0.02103, Variance: 0.01191
Semantic Loss - Mean: 0.48689, Variance: 0.02455

Train Epoch: 117 
task: sign, mean loss: 0.00130, accuracy: 1.00000, avg. loss over tasks: 0.00130, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.04462, Variance: 0.01003
Semantic Loss - Mean: 0.02570, Variance: 0.00949

Test Epoch: 117 
task: sign, mean loss: 0.46465, accuracy: 0.92899, avg. loss over tasks: 0.46465
Diversity Loss - Mean: -0.02260, Variance: 0.01192
Semantic Loss - Mean: 0.49177, Variance: 0.02440

Train Epoch: 118 
task: sign, mean loss: 0.00336, accuracy: 1.00000, avg. loss over tasks: 0.00336, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.04205, Variance: 0.01003
Semantic Loss - Mean: 0.02773, Variance: 0.00942

Test Epoch: 118 
task: sign, mean loss: 0.42242, accuracy: 0.92899, avg. loss over tasks: 0.42242
Diversity Loss - Mean: -0.02213, Variance: 0.01193
Semantic Loss - Mean: 0.47852, Variance: 0.02426

Train Epoch: 119 
task: sign, mean loss: 0.00137, accuracy: 1.00000, avg. loss over tasks: 0.00137, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.04345, Variance: 0.01004
Semantic Loss - Mean: 0.01642, Variance: 0.00935

Test Epoch: 119 
task: sign, mean loss: 0.46700, accuracy: 0.92899, avg. loss over tasks: 0.46700
Diversity Loss - Mean: -0.02149, Variance: 0.01194
Semantic Loss - Mean: 0.52486, Variance: 0.02411

Train Epoch: 120 
task: sign, mean loss: 0.00114, accuracy: 1.00000, avg. loss over tasks: 0.00114, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.04448, Variance: 0.01004
Semantic Loss - Mean: 0.02555, Variance: 0.00928

Test Epoch: 120 
task: sign, mean loss: 0.48333, accuracy: 0.91716, avg. loss over tasks: 0.48333
Diversity Loss - Mean: -0.02551, Variance: 0.01195
Semantic Loss - Mean: 0.56169, Variance: 0.02400

Train Epoch: 121 
task: sign, mean loss: 0.00180, accuracy: 1.00000, avg. loss over tasks: 0.00180, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.04706, Variance: 0.01005
Semantic Loss - Mean: 0.03276, Variance: 0.00923

Test Epoch: 121 
task: sign, mean loss: 0.46304, accuracy: 0.92899, avg. loss over tasks: 0.46304
Diversity Loss - Mean: -0.01738, Variance: 0.01195
Semantic Loss - Mean: 0.55899, Variance: 0.02387

Train Epoch: 122 
task: sign, mean loss: 0.00199, accuracy: 1.00000, avg. loss over tasks: 0.00199, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.04392, Variance: 0.01005
Semantic Loss - Mean: 0.02979, Variance: 0.00919

Test Epoch: 122 
task: sign, mean loss: 0.44277, accuracy: 0.92308, avg. loss over tasks: 0.44277
Diversity Loss - Mean: -0.02078, Variance: 0.01196
Semantic Loss - Mean: 0.51306, Variance: 0.02375

Train Epoch: 123 
task: sign, mean loss: 0.00120, accuracy: 1.00000, avg. loss over tasks: 0.00120, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.04576, Variance: 0.01006
Semantic Loss - Mean: 0.01727, Variance: 0.00912

Test Epoch: 123 
task: sign, mean loss: 0.43085, accuracy: 0.92899, avg. loss over tasks: 0.43085
Diversity Loss - Mean: -0.02371, Variance: 0.01197
Semantic Loss - Mean: 0.50044, Variance: 0.02364

Train Epoch: 124 
task: sign, mean loss: 0.00823, accuracy: 1.00000, avg. loss over tasks: 0.00823, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.04651, Variance: 0.01007
Semantic Loss - Mean: 0.03883, Variance: 0.00909

Test Epoch: 124 
task: sign, mean loss: 0.45076, accuracy: 0.92899, avg. loss over tasks: 0.45076
Diversity Loss - Mean: -0.01804, Variance: 0.01197
Semantic Loss - Mean: 0.51516, Variance: 0.02349

Train Epoch: 125 
task: sign, mean loss: 0.00217, accuracy: 1.00000, avg. loss over tasks: 0.00217, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.04586, Variance: 0.01007
Semantic Loss - Mean: 0.01231, Variance: 0.00902

Test Epoch: 125 
task: sign, mean loss: 0.44961, accuracy: 0.92308, avg. loss over tasks: 0.44961
Diversity Loss - Mean: -0.01639, Variance: 0.01198
Semantic Loss - Mean: 0.50569, Variance: 0.02335

Train Epoch: 126 
task: sign, mean loss: 0.00262, accuracy: 1.00000, avg. loss over tasks: 0.00262, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.04447, Variance: 0.01007
Semantic Loss - Mean: 0.02451, Variance: 0.00897

Test Epoch: 126 
task: sign, mean loss: 0.46089, accuracy: 0.91716, avg. loss over tasks: 0.46089
Diversity Loss - Mean: -0.02272, Variance: 0.01199
Semantic Loss - Mean: 0.46780, Variance: 0.02320

Train Epoch: 127 
task: sign, mean loss: 0.00438, accuracy: 1.00000, avg. loss over tasks: 0.00438, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.04352, Variance: 0.01008
Semantic Loss - Mean: 0.01481, Variance: 0.00891

Test Epoch: 127 
task: sign, mean loss: 0.43433, accuracy: 0.92899, avg. loss over tasks: 0.43433
Diversity Loss - Mean: -0.02756, Variance: 0.01200
Semantic Loss - Mean: 0.46652, Variance: 0.02306

Train Epoch: 128 
task: sign, mean loss: 0.00454, accuracy: 1.00000, avg. loss over tasks: 0.00454, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.04443, Variance: 0.01009
Semantic Loss - Mean: 0.01307, Variance: 0.00884

Test Epoch: 128 
task: sign, mean loss: 0.50363, accuracy: 0.92308, avg. loss over tasks: 0.50363
Diversity Loss - Mean: -0.01437, Variance: 0.01200
Semantic Loss - Mean: 0.56234, Variance: 0.02294

Train Epoch: 129 
task: sign, mean loss: 0.00352, accuracy: 1.00000, avg. loss over tasks: 0.00352, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.04708, Variance: 0.01010
Semantic Loss - Mean: 0.01937, Variance: 0.00878

Test Epoch: 129 
task: sign, mean loss: 0.50679, accuracy: 0.92308, avg. loss over tasks: 0.50679
Diversity Loss - Mean: -0.02624, Variance: 0.01201
Semantic Loss - Mean: 0.52922, Variance: 0.02281

Train Epoch: 130 
task: sign, mean loss: 0.00114, accuracy: 1.00000, avg. loss over tasks: 0.00114, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.04457, Variance: 0.01010
Semantic Loss - Mean: 0.01779, Variance: 0.00873

Test Epoch: 130 
task: sign, mean loss: 0.48086, accuracy: 0.92308, avg. loss over tasks: 0.48086
Diversity Loss - Mean: -0.02452, Variance: 0.01201
Semantic Loss - Mean: 0.53162, Variance: 0.02270

Train Epoch: 131 
task: sign, mean loss: 0.00391, accuracy: 1.00000, avg. loss over tasks: 0.00391, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.04214, Variance: 0.01011
Semantic Loss - Mean: 0.01637, Variance: 0.00868

Test Epoch: 131 
task: sign, mean loss: 0.52946, accuracy: 0.91716, avg. loss over tasks: 0.52946
Diversity Loss - Mean: -0.01314, Variance: 0.01201
Semantic Loss - Mean: 0.59230, Variance: 0.02259

Train Epoch: 132 
task: sign, mean loss: 0.00080, accuracy: 1.00000, avg. loss over tasks: 0.00080, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.04582, Variance: 0.01011
Semantic Loss - Mean: 0.01399, Variance: 0.00862

Test Epoch: 132 
task: sign, mean loss: 0.49848, accuracy: 0.91716, avg. loss over tasks: 0.49848
Diversity Loss - Mean: -0.02282, Variance: 0.01202
Semantic Loss - Mean: 0.56184, Variance: 0.02249

Train Epoch: 133 
task: sign, mean loss: 0.00123, accuracy: 1.00000, avg. loss over tasks: 0.00123, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.04837, Variance: 0.01012
Semantic Loss - Mean: 0.01620, Variance: 0.00857

Test Epoch: 133 
task: sign, mean loss: 0.47510, accuracy: 0.92308, avg. loss over tasks: 0.47510
Diversity Loss - Mean: -0.02042, Variance: 0.01202
Semantic Loss - Mean: 0.53368, Variance: 0.02238

Train Epoch: 134 
task: sign, mean loss: 0.00163, accuracy: 1.00000, avg. loss over tasks: 0.00163, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.04664, Variance: 0.01012
Semantic Loss - Mean: 0.01008, Variance: 0.00851

Test Epoch: 134 
task: sign, mean loss: 0.49054, accuracy: 0.92308, avg. loss over tasks: 0.49054
Diversity Loss - Mean: -0.02278, Variance: 0.01203
Semantic Loss - Mean: 0.54297, Variance: 0.02228

Train Epoch: 135 
task: sign, mean loss: 0.00277, accuracy: 1.00000, avg. loss over tasks: 0.00277, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.04921, Variance: 0.01013
Semantic Loss - Mean: 0.01496, Variance: 0.00845

Test Epoch: 135 
task: sign, mean loss: 0.53055, accuracy: 0.92308, avg. loss over tasks: 0.53055
Diversity Loss - Mean: -0.02498, Variance: 0.01204
Semantic Loss - Mean: 0.56063, Variance: 0.02218

Train Epoch: 136 
task: sign, mean loss: 0.00065, accuracy: 1.00000, avg. loss over tasks: 0.00065, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.05024, Variance: 0.01013
Semantic Loss - Mean: 0.01586, Variance: 0.00840

Test Epoch: 136 
task: sign, mean loss: 0.46259, accuracy: 0.93491, avg. loss over tasks: 0.46259
Diversity Loss - Mean: -0.02354, Variance: 0.01204
Semantic Loss - Mean: 0.49524, Variance: 0.02205

Train Epoch: 137 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.04796, Variance: 0.01014
Semantic Loss - Mean: 0.02185, Variance: 0.00836

Test Epoch: 137 
task: sign, mean loss: 0.44047, accuracy: 0.93491, avg. loss over tasks: 0.44047
Diversity Loss - Mean: -0.02527, Variance: 0.01205
Semantic Loss - Mean: 0.48300, Variance: 0.02193

Train Epoch: 138 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.05049, Variance: 0.01015
Semantic Loss - Mean: 0.02456, Variance: 0.00832

Test Epoch: 138 
task: sign, mean loss: 0.45831, accuracy: 0.92308, avg. loss over tasks: 0.45831
Diversity Loss - Mean: -0.01772, Variance: 0.01205
Semantic Loss - Mean: 0.53005, Variance: 0.02184

Train Epoch: 139 
task: sign, mean loss: 0.00083, accuracy: 1.00000, avg. loss over tasks: 0.00083, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.04980, Variance: 0.01015
Semantic Loss - Mean: 0.01435, Variance: 0.00827

Test Epoch: 139 
task: sign, mean loss: 0.44157, accuracy: 0.92899, avg. loss over tasks: 0.44157
Diversity Loss - Mean: -0.01980, Variance: 0.01206
Semantic Loss - Mean: 0.50308, Variance: 0.02173

Train Epoch: 140 
task: sign, mean loss: 0.04747, accuracy: 0.98913, avg. loss over tasks: 0.04747, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.04713, Variance: 0.01016
Semantic Loss - Mean: 0.04726, Variance: 0.00823

Test Epoch: 140 
task: sign, mean loss: 0.42106, accuracy: 0.93491, avg. loss over tasks: 0.42106
Diversity Loss - Mean: -0.02733, Variance: 0.01207
Semantic Loss - Mean: 0.46501, Variance: 0.02163

Train Epoch: 141 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.04799, Variance: 0.01016
Semantic Loss - Mean: 0.02647, Variance: 0.00820

Test Epoch: 141 
task: sign, mean loss: 0.46704, accuracy: 0.92899, avg. loss over tasks: 0.46704
Diversity Loss - Mean: -0.02042, Variance: 0.01207
Semantic Loss - Mean: 0.50651, Variance: 0.02152

Train Epoch: 142 
task: sign, mean loss: 0.00144, accuracy: 1.00000, avg. loss over tasks: 0.00144, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.04718, Variance: 0.01017
Semantic Loss - Mean: 0.03949, Variance: 0.00820

Test Epoch: 142 
task: sign, mean loss: 0.47465, accuracy: 0.92308, avg. loss over tasks: 0.47465
Diversity Loss - Mean: -0.02592, Variance: 0.01208
Semantic Loss - Mean: 0.49812, Variance: 0.02140

Train Epoch: 143 
task: sign, mean loss: 0.00116, accuracy: 1.00000, avg. loss over tasks: 0.00116, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.04754, Variance: 0.01017
Semantic Loss - Mean: 0.01264, Variance: 0.00815

Test Epoch: 143 
task: sign, mean loss: 0.42680, accuracy: 0.92899, avg. loss over tasks: 0.42680
Diversity Loss - Mean: -0.02338, Variance: 0.01208
Semantic Loss - Mean: 0.48874, Variance: 0.02130

Train Epoch: 144 
task: sign, mean loss: 0.00087, accuracy: 1.00000, avg. loss over tasks: 0.00087, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.04874, Variance: 0.01018
Semantic Loss - Mean: 0.01099, Variance: 0.00809

Test Epoch: 144 
task: sign, mean loss: 0.47082, accuracy: 0.92899, avg. loss over tasks: 0.47082
Diversity Loss - Mean: -0.02126, Variance: 0.01209
Semantic Loss - Mean: 0.52954, Variance: 0.02120

Train Epoch: 145 
task: sign, mean loss: 0.00320, accuracy: 1.00000, avg. loss over tasks: 0.00320, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.04684, Variance: 0.01018
Semantic Loss - Mean: 0.01680, Variance: 0.00805

Test Epoch: 145 
task: sign, mean loss: 0.45495, accuracy: 0.92899, avg. loss over tasks: 0.45495
Diversity Loss - Mean: -0.02579, Variance: 0.01209
Semantic Loss - Mean: 0.50606, Variance: 0.02110

Train Epoch: 146 
task: sign, mean loss: 0.00211, accuracy: 1.00000, avg. loss over tasks: 0.00211, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.04530, Variance: 0.01019
Semantic Loss - Mean: 0.02534, Variance: 0.00802

Test Epoch: 146 
task: sign, mean loss: 0.48686, accuracy: 0.92308, avg. loss over tasks: 0.48686
Diversity Loss - Mean: -0.01293, Variance: 0.01209
Semantic Loss - Mean: 0.58058, Variance: 0.02103

Train Epoch: 147 
task: sign, mean loss: 0.00104, accuracy: 1.00000, avg. loss over tasks: 0.00104, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.04732, Variance: 0.01019
Semantic Loss - Mean: 0.01764, Variance: 0.00797

Test Epoch: 147 
task: sign, mean loss: 0.49604, accuracy: 0.91716, avg. loss over tasks: 0.49604
Diversity Loss - Mean: -0.01728, Variance: 0.01209
Semantic Loss - Mean: 0.58342, Variance: 0.02096

Train Epoch: 148 
task: sign, mean loss: 0.00174, accuracy: 1.00000, avg. loss over tasks: 0.00174, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.04676, Variance: 0.01019
Semantic Loss - Mean: 0.02912, Variance: 0.00796

Test Epoch: 148 
task: sign, mean loss: 0.48562, accuracy: 0.92308, avg. loss over tasks: 0.48562
Diversity Loss - Mean: -0.02122, Variance: 0.01210
Semantic Loss - Mean: 0.53043, Variance: 0.02087

Train Epoch: 149 
task: sign, mean loss: 0.00197, accuracy: 1.00000, avg. loss over tasks: 0.00197, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.04698, Variance: 0.01020
Semantic Loss - Mean: 0.03087, Variance: 0.00793

Test Epoch: 149 
task: sign, mean loss: 0.47691, accuracy: 0.92899, avg. loss over tasks: 0.47691
Diversity Loss - Mean: -0.02278, Variance: 0.01210
Semantic Loss - Mean: 0.50599, Variance: 0.02076

Train Epoch: 150 
task: sign, mean loss: 0.00108, accuracy: 1.00000, avg. loss over tasks: 0.00108, lr: 3e-07
Diversity Loss - Mean: -0.04648, Variance: 0.01020
Semantic Loss - Mean: 0.01995, Variance: 0.00790

Test Epoch: 150 
task: sign, mean loss: 0.47150, accuracy: 0.92899, avg. loss over tasks: 0.47150
Diversity Loss - Mean: -0.02340, Variance: 0.01211
Semantic Loss - Mean: 0.50699, Variance: 0.02066

