Used config:
{'B': 16,
 'B_seq': 16,
 'D': 128,
 'D_inner': 512,
 'D_k': 16,
 'D_v': 16,
 'H': 8,
 'I': 100,
 'M': 100,
 'N': 3600,
 'attn_dropout': 0.1,
 'data_dir': 'data/megapixel_mnist/dsets/megapixel_mnist_1500',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.001,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 1,
 'n_class': 10,
 'n_epoch': 100,
 'n_epoch_warmup': 10,
 'n_res_blocks': 2,
 'n_token': 4,
 'n_worker': 2,
 'patch_size': [50, 50],
 'patch_stride': [50, 50],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': False,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'majority'},
           'task1': {'act_fn': 'softmax',
                     'id': 1,
                     'metric': 'accuracy',
                     'name': 'max'},
           'task2': {'act_fn': 'softmax',
                     'id': 2,
                     'metric': 'accuracy',
                     'name': 'top'},
           'task3': {'act_fn': 'sigmoid',
                     'id': 3,
                     'metric': 'multilabel_accuracy',
                     'name': 'multi'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': True,
 'wd': 0.1}
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: majority, mean loss: 2.39123, accuracy: 0.10700, task: max, mean loss: 2.26932, accuracy: 0.20100, task: top, mean loss: 2.35482, accuracy: 0.11000, task: multi, mean loss: 0.69199, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.92684, lr: 0.0001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 1 
task: majority, mean loss: 2.30932, accuracy: 0.10800, task: max, mean loss: 1.92637, accuracy: 0.27400, task: top, mean loss: 2.30897, accuracy: 0.09100, task: multi, mean loss: 0.62137, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.79151
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 2 
task: majority, mean loss: 2.32161, accuracy: 0.09700, task: max, mean loss: 1.86850, accuracy: 0.24700, task: top, mean loss: 2.32530, accuracy: 0.10200, task: multi, mean loss: 0.61049, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78147, lr: 0.0002
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 2 
task: majority, mean loss: 2.32095, accuracy: 0.10000, task: max, mean loss: 1.89106, accuracy: 0.21300, task: top, mean loss: 2.32429, accuracy: 0.09900, task: multi, mean loss: 0.60127, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78439
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 3 
task: majority, mean loss: 2.34126, accuracy: 0.07600, task: max, mean loss: 1.84473, accuracy: 0.24500, task: top, mean loss: 2.32450, accuracy: 0.11300, task: multi, mean loss: 0.60441, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77872, lr: 0.00030000000000000003
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 3 
task: majority, mean loss: 2.33115, accuracy: 0.11000, task: max, mean loss: 1.87171, accuracy: 0.27400, task: top, mean loss: 2.31323, accuracy: 0.09700, task: multi, mean loss: 0.60165, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77943
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 4 
task: majority, mean loss: 2.34857, accuracy: 0.08300, task: max, mean loss: 1.85209, accuracy: 0.24100, task: top, mean loss: 2.33180, accuracy: 0.10600, task: multi, mean loss: 0.60632, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78469, lr: 0.0004
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 4 
task: majority, mean loss: 2.32496, accuracy: 0.10000, task: max, mean loss: 1.86588, accuracy: 0.21300, task: top, mean loss: 2.33004, accuracy: 0.10200, task: multi, mean loss: 0.60370, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78114
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 5 
task: majority, mean loss: 2.33574, accuracy: 0.09400, task: max, mean loss: 1.84069, accuracy: 0.22700, task: top, mean loss: 2.33103, accuracy: 0.09300, task: multi, mean loss: 0.60577, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77831, lr: 0.0005
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 5 
task: majority, mean loss: 2.31563, accuracy: 0.08100, task: max, mean loss: 1.86435, accuracy: 0.27400, task: top, mean loss: 2.31531, accuracy: 0.10000, task: multi, mean loss: 0.60182, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77428
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 6 
task: majority, mean loss: 2.32287, accuracy: 0.09400, task: max, mean loss: 1.84332, accuracy: 0.23800, task: top, mean loss: 2.33513, accuracy: 0.08700, task: multi, mean loss: 0.60609, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77685, lr: 0.0006000000000000001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 6 
task: majority, mean loss: 2.33016, accuracy: 0.09000, task: max, mean loss: 1.86171, accuracy: 0.26600, task: top, mean loss: 2.32480, accuracy: 0.09900, task: multi, mean loss: 0.60185, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77963
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 7 
task: majority, mean loss: 2.31755, accuracy: 0.12600, task: max, mean loss: 1.85354, accuracy: 0.24600, task: top, mean loss: 2.31979, accuracy: 0.10100, task: multi, mean loss: 0.60528, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77404, lr: 0.0007
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 7 
task: majority, mean loss: 2.32304, accuracy: 0.09400, task: max, mean loss: 1.91548, accuracy: 0.16500, task: top, mean loss: 2.34306, accuracy: 0.10300, task: multi, mean loss: 0.60392, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.79638
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 8 
task: majority, mean loss: 2.33149, accuracy: 0.09800, task: max, mean loss: 1.85488, accuracy: 0.23500, task: top, mean loss: 2.33532, accuracy: 0.09100, task: multi, mean loss: 0.60679, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78212, lr: 0.0008
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 8 
task: majority, mean loss: 2.33697, accuracy: 0.08900, task: max, mean loss: 1.87577, accuracy: 0.21300, task: top, mean loss: 2.34540, accuracy: 0.09900, task: multi, mean loss: 0.60089, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78976
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 9 
task: majority, mean loss: 2.32926, accuracy: 0.09600, task: max, mean loss: 1.84454, accuracy: 0.23400, task: top, mean loss: 2.32450, accuracy: 0.11100, task: multi, mean loss: 0.60699, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77632, lr: 0.0009000000000000001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 9 
task: majority, mean loss: 2.34577, accuracy: 0.08900, task: max, mean loss: 1.87482, accuracy: 0.27200, task: top, mean loss: 2.33715, accuracy: 0.09700, task: multi, mean loss: 0.60388, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.79041
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 10 
task: majority, mean loss: 2.33560, accuracy: 0.07600, task: max, mean loss: 1.84831, accuracy: 0.23200, task: top, mean loss: 2.32596, accuracy: 0.09800, task: multi, mean loss: 0.60613, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77900, lr: 0.001
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 10 
task: majority, mean loss: 2.32426, accuracy: 0.09400, task: max, mean loss: 1.87005, accuracy: 0.27400, task: top, mean loss: 2.32856, accuracy: 0.08900, task: multi, mean loss: 0.60278, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78141
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 11 
task: majority, mean loss: 2.32773, accuracy: 0.10800, task: max, mean loss: 1.84424, accuracy: 0.24900, task: top, mean loss: 2.31925, accuracy: 0.11300, task: multi, mean loss: 0.60767, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77472, lr: 0.0009996957180960382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 11 
task: majority, mean loss: 2.31106, accuracy: 0.10900, task: max, mean loss: 1.86767, accuracy: 0.27400, task: top, mean loss: 2.31513, accuracy: 0.10300, task: multi, mean loss: 0.60180, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77391
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 12 
task: majority, mean loss: 2.32356, accuracy: 0.10200, task: max, mean loss: 1.82956, accuracy: 0.26100, task: top, mean loss: 2.31974, accuracy: 0.09200, task: multi, mean loss: 0.60559, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76961, lr: 0.0009987832431047822
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 12 
task: majority, mean loss: 2.34312, accuracy: 0.09900, task: max, mean loss: 1.89182, accuracy: 0.21300, task: top, mean loss: 2.33106, accuracy: 0.09900, task: multi, mean loss: 0.60299, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.79225
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 13 
task: majority, mean loss: 2.31809, accuracy: 0.09800, task: max, mean loss: 1.83064, accuracy: 0.25000, task: top, mean loss: 2.31593, accuracy: 0.10200, task: multi, mean loss: 0.60478, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76736, lr: 0.0009972636867364526
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 13 
task: majority, mean loss: 2.33529, accuracy: 0.08900, task: max, mean loss: 1.88677, accuracy: 0.21300, task: top, mean loss: 2.33501, accuracy: 0.08900, task: multi, mean loss: 0.60259, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78992
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 14 
task: majority, mean loss: 2.33161, accuracy: 0.09900, task: max, mean loss: 1.82870, accuracy: 0.24700, task: top, mean loss: 2.31986, accuracy: 0.09800, task: multi, mean loss: 0.60531, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77137, lr: 0.0009951389003364144
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 14 
task: majority, mean loss: 2.32129, accuracy: 0.10900, task: max, mean loss: 1.86352, accuracy: 0.27400, task: top, mean loss: 2.33780, accuracy: 0.08900, task: multi, mean loss: 0.60221, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.78121
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 15 
task: majority, mean loss: 2.31568, accuracy: 0.10600, task: max, mean loss: 1.83465, accuracy: 0.25600, task: top, mean loss: 2.31310, accuracy: 0.11700, task: multi, mean loss: 0.60526, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76717, lr: 0.000992411472629598
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 15 
task: majority, mean loss: 2.31530, accuracy: 0.10000, task: max, mean loss: 1.86167, accuracy: 0.27400, task: top, mean loss: 2.32399, accuracy: 0.09900, task: multi, mean loss: 0.60296, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.77598
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 16 
task: majority, mean loss: 2.31849, accuracy: 0.11400, task: max, mean loss: 1.82687, accuracy: 0.24900, task: top, mean loss: 2.31312, accuracy: 0.09700, task: multi, mean loss: 0.60406, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76563, lr: 0.000989084726566536
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 16 
task: majority, mean loss: 2.43420, accuracy: 0.09400, task: max, mean loss: 1.86143, accuracy: 0.27400, task: top, mean loss: 2.37275, accuracy: 0.10000, task: multi, mean loss: 0.60238, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.81769
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 17 
task: majority, mean loss: 2.27371, accuracy: 0.13900, task: max, mean loss: 1.82748, accuracy: 0.25400, task: top, mean loss: 2.30185, accuracy: 0.11800, task: multi, mean loss: 0.60442, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.75186, lr: 0.00098516271527486
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 17 
task: majority, mean loss: 2.60894, accuracy: 0.09400, task: max, mean loss: 1.88378, accuracy: 0.27400, task: top, mean loss: 2.45546, accuracy: 0.10000, task: multi, mean loss: 0.60573, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.88848
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 18 
task: majority, mean loss: 2.21662, accuracy: 0.17600, task: max, mean loss: 1.81937, accuracy: 0.26200, task: top, mean loss: 2.25094, accuracy: 0.13800, task: multi, mean loss: 0.59832, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.72131, lr: 0.0009806502171211902
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 18 
task: majority, mean loss: 2.72126, accuracy: 0.09400, task: max, mean loss: 1.93289, accuracy: 0.14100, task: top, mean loss: 2.80073, accuracy: 0.10000, task: multi, mean loss: 0.63542, multilabel_accuracy: 0.00100, avg. loss over tasks: 2.02257
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 19 
task: majority, mean loss: 2.14851, accuracy: 0.19100, task: max, mean loss: 1.79304, accuracy: 0.26900, task: top, mean loss: 2.21644, accuracy: 0.14800, task: multi, mean loss: 0.58771, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.68643, lr: 0.0009755527298894293
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 19 
task: majority, mean loss: 2.45723, accuracy: 0.10600, task: max, mean loss: 1.94879, accuracy: 0.17400, task: top, mean loss: 2.39430, accuracy: 0.09900, task: multi, mean loss: 0.62367, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.85600
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 20 
task: majority, mean loss: 2.15013, accuracy: 0.18200, task: max, mean loss: 1.79376, accuracy: 0.26100, task: top, mean loss: 2.20327, accuracy: 0.16500, task: multi, mean loss: 0.58731, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.68362, lr: 0.0009698764640825613
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 20 
task: majority, mean loss: 2.29261, accuracy: 0.13800, task: max, mean loss: 1.84719, accuracy: 0.26600, task: top, mean loss: 2.33487, accuracy: 0.11800, task: multi, mean loss: 0.59064, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.76632
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 21 
task: majority, mean loss: 2.15484, accuracy: 0.16800, task: max, mean loss: 1.78826, accuracy: 0.26400, task: top, mean loss: 2.20110, accuracy: 0.16200, task: multi, mean loss: 0.58721, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.68285, lr: 0.0009636283353561103
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 21 
task: majority, mean loss: 2.61052, accuracy: 0.08400, task: max, mean loss: 1.98207, accuracy: 0.27400, task: top, mean loss: 2.53066, accuracy: 0.10900, task: multi, mean loss: 0.65111, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.94359
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 22 
task: majority, mean loss: 2.19427, accuracy: 0.17100, task: max, mean loss: 1.79253, accuracy: 0.29000, task: top, mean loss: 2.21770, accuracy: 0.14100, task: multi, mean loss: 0.59019, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.69867, lr: 0.0009568159560924791
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 22 
task: majority, mean loss: 2.83070, accuracy: 0.08900, task: max, mean loss: 1.91609, accuracy: 0.15900, task: top, mean loss: 2.62187, accuracy: 0.09800, task: multi, mean loss: 0.64191, multilabel_accuracy: 0.00100, avg. loss over tasks: 2.00264
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 23 
task: majority, mean loss: 2.09150, accuracy: 0.18900, task: max, mean loss: 1.76040, accuracy: 0.27600, task: top, mean loss: 2.14773, accuracy: 0.18100, task: multi, mean loss: 0.57455, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.64355, lr: 0.000949447626126434
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 23 
task: majority, mean loss: 2.51162, accuracy: 0.08800, task: max, mean loss: 1.96965, accuracy: 0.27400, task: top, mean loss: 2.39001, accuracy: 0.10600, task: multi, mean loss: 0.63148, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.87569
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 24 
task: majority, mean loss: 2.05080, accuracy: 0.21500, task: max, mean loss: 1.75361, accuracy: 0.29700, task: top, mean loss: 2.12072, accuracy: 0.17500, task: multi, mean loss: 0.57245, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.62439, lr: 0.000941532322633034
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 24 
task: majority, mean loss: 2.16851, accuracy: 0.15500, task: max, mean loss: 1.80540, accuracy: 0.29400, task: top, mean loss: 2.21536, accuracy: 0.14400, task: multi, mean loss: 0.57466, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.69098
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 25 
task: majority, mean loss: 2.02084, accuracy: 0.21200, task: max, mean loss: 1.74018, accuracy: 0.29000, task: top, mean loss: 2.07575, accuracy: 0.18600, task: multi, mean loss: 0.57184, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.60215, lr: 0.0009330796891903273
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 25 
task: majority, mean loss: 2.69148, accuracy: 0.10600, task: max, mean loss: 1.96855, accuracy: 0.26400, task: top, mean loss: 2.56596, accuracy: 0.11400, task: multi, mean loss: 0.65287, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.96972
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 26 
task: majority, mean loss: 1.99455, accuracy: 0.23200, task: max, mean loss: 1.73015, accuracy: 0.31300, task: top, mean loss: 2.07945, accuracy: 0.17200, task: multi, mean loss: 0.57281, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.59424, lr: 0.0009241000240301347
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 26 
task: majority, mean loss: 2.22845, accuracy: 0.18400, task: max, mean loss: 1.81601, accuracy: 0.29100, task: top, mean loss: 2.25218, accuracy: 0.17500, task: multi, mean loss: 0.57253, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.71729
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 27 
task: majority, mean loss: 1.98003, accuracy: 0.24800, task: max, mean loss: 1.73695, accuracy: 0.31500, task: top, mean loss: 2.04154, accuracy: 0.19600, task: multi, mean loss: 0.57333, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.58296, lr: 0.0009146042674912433
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 27 
task: majority, mean loss: 2.13333, accuracy: 0.21100, task: max, mean loss: 1.80223, accuracy: 0.27600, task: top, mean loss: 2.21951, accuracy: 0.16500, task: multi, mean loss: 0.56998, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.68126
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 28 
task: majority, mean loss: 1.94636, accuracy: 0.26200, task: max, mean loss: 1.72747, accuracy: 0.32700, task: top, mean loss: 2.01179, accuracy: 0.22400, task: multi, mean loss: 0.57007, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.56392, lr: 0.0009046039886902862
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 28 
task: majority, mean loss: 2.22514, accuracy: 0.18900, task: max, mean loss: 1.84782, accuracy: 0.25700, task: top, mean loss: 2.31013, accuracy: 0.15700, task: multi, mean loss: 0.57729, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.74010
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 29 
task: majority, mean loss: 1.88783, accuracy: 0.29600, task: max, mean loss: 1.71803, accuracy: 0.32100, task: top, mean loss: 1.98018, accuracy: 0.26000, task: multi, mean loss: 0.56274, multilabel_accuracy: 0.00000, avg. loss over tasks: 1.53719, lr: 0.0008941113714265576
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 29 
task: majority, mean loss: 2.08899, accuracy: 0.23500, task: max, mean loss: 1.81970, accuracy: 0.28200, task: top, mean loss: 2.16683, accuracy: 0.21900, task: multi, mean loss: 0.55970, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.65881
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 30 
task: majority, mean loss: 1.77103, accuracy: 0.35400, task: max, mean loss: 1.67263, accuracy: 0.37100, task: top, mean loss: 1.89031, accuracy: 0.29200, task: multi, mean loss: 0.55600, multilabel_accuracy: 0.00100, avg. loss over tasks: 1.47249, lr: 0.0008831391993379295
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 30 
task: majority, mean loss: 2.21766, accuracy: 0.20800, task: max, mean loss: 1.88145, accuracy: 0.27200, task: top, mean loss: 2.34236, accuracy: 0.17100, task: multi, mean loss: 0.57722, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.75467
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 31 
task: majority, mean loss: 1.75776, accuracy: 0.38000, task: max, mean loss: 1.68462, accuracy: 0.34000, task: top, mean loss: 1.85604, accuracy: 0.32500, task: multi, mean loss: 0.55253, multilabel_accuracy: 0.00500, avg. loss over tasks: 1.46274, lr: 0.0008717008403259585
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 31 
task: majority, mean loss: 2.21253, accuracy: 0.22200, task: max, mean loss: 1.82980, accuracy: 0.30900, task: top, mean loss: 2.31211, accuracy: 0.19800, task: multi, mean loss: 0.57581, multilabel_accuracy: 0.00900, avg. loss over tasks: 1.73257
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 32 
task: majority, mean loss: 1.65444, accuracy: 0.38900, task: max, mean loss: 1.64905, accuracy: 0.36200, task: top, mean loss: 1.76920, accuracy: 0.37000, task: multi, mean loss: 0.54880, multilabel_accuracy: 0.00400, avg. loss over tasks: 1.40537, lr: 0.0008598102302691562
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 32 
task: majority, mean loss: 2.09447, accuracy: 0.24100, task: max, mean loss: 1.87208, accuracy: 0.28300, task: top, mean loss: 2.18272, accuracy: 0.24000, task: multi, mean loss: 0.55775, multilabel_accuracy: 0.00700, avg. loss over tasks: 1.67676
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 33 
task: majority, mean loss: 1.56941, accuracy: 0.43600, task: max, mean loss: 1.60036, accuracy: 0.39700, task: top, mean loss: 1.71045, accuracy: 0.37900, task: multi, mean loss: 0.54195, multilabel_accuracy: 0.00700, avg. loss over tasks: 1.35554, lr: 0.0008474818560442692
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 33 
task: majority, mean loss: 2.26125, accuracy: 0.21800, task: max, mean loss: 1.95669, accuracy: 0.28900, task: top, mean loss: 2.30743, accuracy: 0.21400, task: multi, mean loss: 0.57595, multilabel_accuracy: 0.00600, avg. loss over tasks: 1.77533
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 34 
task: majority, mean loss: 1.50142, accuracy: 0.45800, task: max, mean loss: 1.56360, accuracy: 0.40700, task: top, mean loss: 1.63796, accuracy: 0.41700, task: multi, mean loss: 0.53582, multilabel_accuracy: 0.01100, avg. loss over tasks: 1.30970, lr: 0.0008347307378762497
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 34 
task: majority, mean loss: 2.23585, accuracy: 0.27600, task: max, mean loss: 1.80876, accuracy: 0.30200, task: top, mean loss: 2.27710, accuracy: 0.24200, task: multi, mean loss: 0.55798, multilabel_accuracy: 0.00300, avg. loss over tasks: 1.71992
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 35 
task: majority, mean loss: 1.40580, accuracy: 0.51400, task: max, mean loss: 1.52873, accuracy: 0.42000, task: top, mean loss: 1.52165, accuracy: 0.47500, task: multi, mean loss: 0.52329, multilabel_accuracy: 0.01100, avg. loss over tasks: 1.24487, lr: 0.0008215724110384264
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 35 
task: majority, mean loss: 2.23710, accuracy: 0.24000, task: max, mean loss: 1.87377, accuracy: 0.28600, task: top, mean loss: 2.34835, accuracy: 0.22400, task: multi, mean loss: 0.56604, multilabel_accuracy: 0.00200, avg. loss over tasks: 1.75632
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 36 
task: majority, mean loss: 1.34707, accuracy: 0.53000, task: max, mean loss: 1.48035, accuracy: 0.43300, task: top, mean loss: 1.48986, accuracy: 0.48000, task: multi, mean loss: 0.52270, multilabel_accuracy: 0.00800, avg. loss over tasks: 1.20999, lr: 0.0008080229069251663
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 36 
task: majority, mean loss: 2.61498, accuracy: 0.22100, task: max, mean loss: 1.95984, accuracy: 0.29700, task: top, mean loss: 2.55058, accuracy: 0.21600, task: multi, mean loss: 0.59828, multilabel_accuracy: 0.01200, avg. loss over tasks: 1.93092
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 37 
task: majority, mean loss: 1.24492, accuracy: 0.58000, task: max, mean loss: 1.43269, accuracy: 0.44700, task: top, mean loss: 1.39102, accuracy: 0.50300, task: multi, mean loss: 0.51302, multilabel_accuracy: 0.00600, avg. loss over tasks: 1.14541, lr: 0.0007940987335200903
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 37 
task: majority, mean loss: 2.26790, accuracy: 0.28900, task: max, mean loss: 1.88942, accuracy: 0.31900, task: top, mean loss: 2.30449, accuracy: 0.25300, task: multi, mean loss: 0.55483, multilabel_accuracy: 0.01300, avg. loss over tasks: 1.75416
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 38 
task: majority, mean loss: 1.09497, accuracy: 0.62400, task: max, mean loss: 1.34608, accuracy: 0.47600, task: top, mean loss: 1.26978, accuracy: 0.56500, task: multi, mean loss: 0.50152, multilabel_accuracy: 0.01700, avg. loss over tasks: 1.05309, lr: 0.0007798168552836382
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 38 
task: majority, mean loss: 2.45431, accuracy: 0.29800, task: max, mean loss: 1.89394, accuracy: 0.31400, task: top, mean loss: 2.49487, accuracy: 0.26300, task: multi, mean loss: 0.55467, multilabel_accuracy: 0.01100, avg. loss over tasks: 1.84945
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 39 
task: majority, mean loss: 1.07871, accuracy: 0.63100, task: max, mean loss: 1.31732, accuracy: 0.49300, task: top, mean loss: 1.22970, accuracy: 0.58200, task: multi, mean loss: 0.49822, multilabel_accuracy: 0.01500, avg. loss over tasks: 1.03099, lr: 0.000765194672484486
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 39 
task: majority, mean loss: 2.39724, accuracy: 0.28200, task: max, mean loss: 1.92995, accuracy: 0.33300, task: top, mean loss: 2.46798, accuracy: 0.25300, task: multi, mean loss: 0.55762, multilabel_accuracy: 0.00900, avg. loss over tasks: 1.83820
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 40 
task: majority, mean loss: 0.92225, accuracy: 0.68300, task: max, mean loss: 1.19530, accuracy: 0.54800, task: top, mean loss: 1.08609, accuracy: 0.64400, task: multi, mean loss: 0.48574, multilabel_accuracy: 0.02200, avg. loss over tasks: 0.92235, lr: 0.00075025
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 40 
task: majority, mean loss: 2.84355, accuracy: 0.21800, task: max, mean loss: 2.06642, accuracy: 0.28600, task: top, mean loss: 2.87803, accuracy: 0.21000, task: multi, mean loss: 0.60665, multilabel_accuracy: 0.01100, avg. loss over tasks: 2.09867
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 41 
task: majority, mean loss: 0.89923, accuracy: 0.69700, task: max, mean loss: 1.12654, accuracy: 0.56900, task: top, mean loss: 1.06279, accuracy: 0.63300, task: multi, mean loss: 0.47856, multilabel_accuracy: 0.02700, avg. loss over tasks: 0.89178, lr: 0.0007350010456115524
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 41 
task: majority, mean loss: 2.62197, accuracy: 0.27200, task: max, mean loss: 2.14071, accuracy: 0.31000, task: top, mean loss: 2.76528, accuracy: 0.23500, task: multi, mean loss: 0.58031, multilabel_accuracy: 0.01400, avg. loss over tasks: 2.02707
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 42 
task: majority, mean loss: 0.84880, accuracy: 0.72200, task: max, mean loss: 1.04489, accuracy: 0.60300, task: top, mean loss: 0.95236, accuracy: 0.69000, task: multi, mean loss: 0.46767, multilabel_accuracy: 0.02600, avg. loss over tasks: 0.82843, lr: 0.0007194663878211441
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 42 
task: majority, mean loss: 2.70506, accuracy: 0.27000, task: max, mean loss: 2.19647, accuracy: 0.29400, task: top, mean loss: 2.77320, accuracy: 0.23000, task: multi, mean loss: 0.58078, multilabel_accuracy: 0.01100, avg. loss over tasks: 2.06387
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 43 
task: majority, mean loss: 0.79794, accuracy: 0.72400, task: max, mean loss: 0.99222, accuracy: 0.63500, task: top, mean loss: 0.91382, accuracy: 0.70100, task: multi, mean loss: 0.46383, multilabel_accuracy: 0.04000, avg. loss over tasks: 0.79195, lr: 0.0007036649532163622
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 43 
task: majority, mean loss: 2.70892, accuracy: 0.26900, task: max, mean loss: 2.20549, accuracy: 0.31200, task: top, mean loss: 2.78991, accuracy: 0.23500, task: multi, mean loss: 0.58302, multilabel_accuracy: 0.01500, avg. loss over tasks: 2.07184
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 44 
task: majority, mean loss: 0.72531, accuracy: 0.74800, task: max, mean loss: 0.92262, accuracy: 0.65600, task: top, mean loss: 0.87029, accuracy: 0.72700, task: multi, mean loss: 0.45776, multilabel_accuracy: 0.04500, avg. loss over tasks: 0.74399, lr: 0.000687615993411248
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 44 
task: majority, mean loss: 2.89360, accuracy: 0.24600, task: max, mean loss: 2.43905, accuracy: 0.29600, task: top, mean loss: 2.96557, accuracy: 0.23000, task: multi, mean loss: 0.59712, multilabel_accuracy: 0.01100, avg. loss over tasks: 2.22383
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 45 
task: majority, mean loss: 0.60326, accuracy: 0.80600, task: max, mean loss: 0.85467, accuracy: 0.68500, task: top, mean loss: 0.70186, accuracy: 0.77100, task: multi, mean loss: 0.44334, multilabel_accuracy: 0.05500, avg. loss over tasks: 0.65078, lr: 0.0006713390615911716
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 45 
task: majority, mean loss: 2.94786, accuracy: 0.28200, task: max, mean loss: 2.40976, accuracy: 0.28000, task: top, mean loss: 2.96945, accuracy: 0.24500, task: multi, mean loss: 0.59238, multilabel_accuracy: 0.00900, avg. loss over tasks: 2.22986
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 46 
task: majority, mean loss: 0.53440, accuracy: 0.83400, task: max, mean loss: 0.77445, accuracy: 0.72500, task: top, mean loss: 0.69006, accuracy: 0.76800, task: multi, mean loss: 0.43425, multilabel_accuracy: 0.04800, avg. loss over tasks: 0.60829, lr: 0.0006548539886902863
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 46 
task: majority, mean loss: 3.50861, accuracy: 0.20000, task: max, mean loss: 2.76315, accuracy: 0.29700, task: top, mean loss: 3.42320, accuracy: 0.20300, task: multi, mean loss: 0.65580, multilabel_accuracy: 0.01500, avg. loss over tasks: 2.58769
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 47 
task: majority, mean loss: 0.50407, accuracy: 0.84300, task: max, mean loss: 0.70165, accuracy: 0.75600, task: top, mean loss: 0.60219, accuracy: 0.80700, task: multi, mean loss: 0.42633, multilabel_accuracy: 0.06500, avg. loss over tasks: 0.55856, lr: 0.0006381808592305911
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 47 
task: majority, mean loss: 3.14350, accuracy: 0.26400, task: max, mean loss: 2.59347, accuracy: 0.28800, task: top, mean loss: 3.17766, accuracy: 0.23400, task: multi, mean loss: 0.62124, multilabel_accuracy: 0.01300, avg. loss over tasks: 2.38397
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 48 
task: majority, mean loss: 0.45138, accuracy: 0.84900, task: max, mean loss: 0.64928, accuracy: 0.77800, task: top, mean loss: 0.53949, accuracy: 0.81700, task: multi, mean loss: 0.42200, multilabel_accuracy: 0.06200, avg. loss over tasks: 0.51554, lr: 0.0006213399868520341
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 48 
task: majority, mean loss: 3.17142, accuracy: 0.28100, task: max, mean loss: 2.60149, accuracy: 0.27200, task: top, mean loss: 3.39078, accuracy: 0.22700, task: multi, mean loss: 0.61449, multilabel_accuracy: 0.00700, avg. loss over tasks: 2.44454
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 49 
task: majority, mean loss: 0.41547, accuracy: 0.86000, task: max, mean loss: 0.60620, accuracy: 0.78700, task: top, mean loss: 0.50815, accuracy: 0.82500, task: multi, mean loss: 0.40732, multilabel_accuracy: 0.07300, avg. loss over tasks: 0.48429, lr: 0.0006043518895634709
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 49 
task: majority, mean loss: 3.15196, accuracy: 0.29400, task: max, mean loss: 2.69747, accuracy: 0.28200, task: top, mean loss: 3.17267, accuracy: 0.24100, task: multi, mean loss: 0.61125, multilabel_accuracy: 0.00800, avg. loss over tasks: 2.40834
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Train Epoch: 50 
task: majority, mean loss: 0.42484, accuracy: 0.86200, task: max, mean loss: 0.57503, accuracy: 0.79500, task: top, mean loss: 0.41160, accuracy: 0.87600, task: multi, mean loss: 0.40630, multilabel_accuracy: 0.07900, avg. loss over tasks: 0.45444, lr: 0.0005872372647446318
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000

Test Epoch: 50 
task: majority, mean loss: 3.28496, accuracy: 0.27400, task: max, mean loss: 2.87682, accuracy: 0.30400, task: top, mean loss: 3.41503, accuracy: 0.25700, task: multi, mean loss: 0.63654, multilabel_accuracy: 0.01100, avg. loss over tasks: 2.55334
Diversity Loss - Mean: 0.00000, Variance: 0.00000
Semantic Loss - Mean: 0.00000, Variance: 0.00000


