Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09094, accuracy: 0.63587, avg. loss over tasks: 1.09094, lr: 3e-05
Diversity Loss - Mean: -0.01155, Variance: 0.01050
Semantic Loss - Mean: 1.43142, Variance: 0.07308

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17392, accuracy: 0.66272, avg. loss over tasks: 1.17392
Diversity Loss - Mean: -0.03392, Variance: 0.01255
Semantic Loss - Mean: 1.16146, Variance: 0.05360

Train Epoch: 2 
task: sign, mean loss: 0.96256, accuracy: 0.66848, avg. loss over tasks: 0.96256, lr: 6e-05
Diversity Loss - Mean: -0.02926, Variance: 0.01054
Semantic Loss - Mean: 0.98128, Variance: 0.03952

Test Epoch: 2 
task: sign, mean loss: 1.10787, accuracy: 0.65680, avg. loss over tasks: 1.10787
Diversity Loss - Mean: -0.04911, Variance: 0.01237
Semantic Loss - Mean: 1.13944, Variance: 0.03266

Train Epoch: 3 
task: sign, mean loss: 0.80770, accuracy: 0.70652, avg. loss over tasks: 0.80770, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.06101, Variance: 0.01041
Semantic Loss - Mean: 0.98929, Variance: 0.02762

Test Epoch: 3 
task: sign, mean loss: 1.29633, accuracy: 0.61538, avg. loss over tasks: 1.29633
Diversity Loss - Mean: -0.07962, Variance: 0.01162
Semantic Loss - Mean: 1.11356, Variance: 0.03126

Train Epoch: 4 
task: sign, mean loss: 0.76131, accuracy: 0.71196, avg. loss over tasks: 0.76131, lr: 0.00012
Diversity Loss - Mean: -0.08840, Variance: 0.01024
Semantic Loss - Mean: 0.89536, Variance: 0.02132

Test Epoch: 4 
task: sign, mean loss: 1.40671, accuracy: 0.56213, avg. loss over tasks: 1.40671
Diversity Loss - Mean: -0.09226, Variance: 0.01104
Semantic Loss - Mean: 1.07158, Variance: 0.02565

Train Epoch: 5 
task: sign, mean loss: 0.75007, accuracy: 0.71739, avg. loss over tasks: 0.75007, lr: 0.00015
Diversity Loss - Mean: -0.08322, Variance: 0.01001
Semantic Loss - Mean: 0.81054, Variance: 0.01745

Test Epoch: 5 
task: sign, mean loss: 2.21242, accuracy: 0.28994, avg. loss over tasks: 2.21242
Diversity Loss - Mean: -0.08350, Variance: 0.01059
Semantic Loss - Mean: 1.30380, Variance: 0.02412

Train Epoch: 6 
task: sign, mean loss: 0.74108, accuracy: 0.76087, avg. loss over tasks: 0.74108, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.07737, Variance: 0.00991
Semantic Loss - Mean: 0.76269, Variance: 0.01508

Test Epoch: 6 
task: sign, mean loss: 1.75697, accuracy: 0.53254, avg. loss over tasks: 1.75697
Diversity Loss - Mean: -0.08220, Variance: 0.01099
Semantic Loss - Mean: 1.29544, Variance: 0.02305

Train Epoch: 7 
task: sign, mean loss: 0.62191, accuracy: 0.76087, avg. loss over tasks: 0.62191, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.07724, Variance: 0.01003
Semantic Loss - Mean: 0.63413, Variance: 0.01322

Test Epoch: 7 
task: sign, mean loss: 2.03916, accuracy: 0.34320, avg. loss over tasks: 2.03916
Diversity Loss - Mean: -0.04469, Variance: 0.01079
Semantic Loss - Mean: 1.66131, Variance: 0.02473

Train Epoch: 8 
task: sign, mean loss: 0.66407, accuracy: 0.78261, avg. loss over tasks: 0.66407, lr: 0.00024
Diversity Loss - Mean: -0.06412, Variance: 0.01010
Semantic Loss - Mean: 0.63719, Variance: 0.01203

Test Epoch: 8 
task: sign, mean loss: 2.14222, accuracy: 0.65680, avg. loss over tasks: 2.14222
Diversity Loss - Mean: -0.05308, Variance: 0.01165
Semantic Loss - Mean: 1.70266, Variance: 0.02649

Train Epoch: 9 
task: sign, mean loss: 0.78258, accuracy: 0.74457, avg. loss over tasks: 0.78258, lr: 0.00027
Diversity Loss - Mean: -0.08267, Variance: 0.01032
Semantic Loss - Mean: 0.75309, Variance: 0.01110

Test Epoch: 9 
task: sign, mean loss: 1.31457, accuracy: 0.62130, avg. loss over tasks: 1.31457
Diversity Loss - Mean: -0.08118, Variance: 0.01180
Semantic Loss - Mean: 1.15784, Variance: 0.02505

Train Epoch: 10 
task: sign, mean loss: 0.67569, accuracy: 0.73913, avg. loss over tasks: 0.67569, lr: 0.0003
Diversity Loss - Mean: -0.07126, Variance: 0.01030
Semantic Loss - Mean: 0.66596, Variance: 0.01032

Test Epoch: 10 
task: sign, mean loss: 2.35490, accuracy: 0.47929, avg. loss over tasks: 2.35490
Diversity Loss - Mean: -0.05886, Variance: 0.01174
Semantic Loss - Mean: 1.78794, Variance: 0.02567

Train Epoch: 11 
task: sign, mean loss: 0.66718, accuracy: 0.72826, avg. loss over tasks: 0.66718, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.05996, Variance: 0.01024
Semantic Loss - Mean: 0.64587, Variance: 0.00975

Test Epoch: 11 
task: sign, mean loss: 2.34528, accuracy: 0.58580, avg. loss over tasks: 2.34528
Diversity Loss - Mean: -0.04140, Variance: 0.01194
Semantic Loss - Mean: 1.89267, Variance: 0.02551

Train Epoch: 12 
task: sign, mean loss: 0.83786, accuracy: 0.75543, avg. loss over tasks: 0.83786, lr: 0.000299849111021216
Diversity Loss - Mean: -0.08655, Variance: 0.01056
Semantic Loss - Mean: 0.85836, Variance: 0.00931

Test Epoch: 12 
task: sign, mean loss: 3.68592, accuracy: 0.13609, avg. loss over tasks: 3.68592
Diversity Loss - Mean: 0.02659, Variance: 0.01213
Semantic Loss - Mean: 2.57903, Variance: 0.03948

Train Epoch: 13 
task: sign, mean loss: 0.95116, accuracy: 0.63587, avg. loss over tasks: 0.95116, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.09256, Variance: 0.01084
Semantic Loss - Mean: 0.93315, Variance: 0.00884

Test Epoch: 13 
task: sign, mean loss: 2.10959, accuracy: 0.25444, avg. loss over tasks: 2.10959
Diversity Loss - Mean: -0.02915, Variance: 0.01248
Semantic Loss - Mean: 1.61788, Variance: 0.04013

Train Epoch: 14 
task: sign, mean loss: 0.77042, accuracy: 0.75543, avg. loss over tasks: 0.77042, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.10442, Variance: 0.01124
Semantic Loss - Mean: 0.77829, Variance: 0.00830

Test Epoch: 14 
task: sign, mean loss: 2.18270, accuracy: 0.25444, avg. loss over tasks: 2.18270
Diversity Loss - Mean: -0.04054, Variance: 0.01260
Semantic Loss - Mean: 1.79117, Variance: 0.03979

Train Epoch: 15 
task: sign, mean loss: 0.77421, accuracy: 0.74457, avg. loss over tasks: 0.77421, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.09822, Variance: 0.01143
Semantic Loss - Mean: 0.79939, Variance: 0.00789

Test Epoch: 15 
task: sign, mean loss: 2.04951, accuracy: 0.36095, avg. loss over tasks: 2.04951
Diversity Loss - Mean: -0.04859, Variance: 0.01262
Semantic Loss - Mean: 1.62480, Variance: 0.03881

Train Epoch: 16 
task: sign, mean loss: 0.74381, accuracy: 0.70652, avg. loss over tasks: 0.74381, lr: 0.000298643821800925
Diversity Loss - Mean: -0.10010, Variance: 0.01160
Semantic Loss - Mean: 0.77055, Variance: 0.00752

Test Epoch: 16 
task: sign, mean loss: 1.55624, accuracy: 0.55030, avg. loss over tasks: 1.55624
Diversity Loss - Mean: -0.08426, Variance: 0.01286
Semantic Loss - Mean: 1.36593, Variance: 0.03726

Train Epoch: 17 
task: sign, mean loss: 0.57815, accuracy: 0.76630, avg. loss over tasks: 0.57815, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.09835, Variance: 0.01173
Semantic Loss - Mean: 0.60476, Variance: 0.00718

Test Epoch: 17 
task: sign, mean loss: 1.46368, accuracy: 0.47337, avg. loss over tasks: 1.46368
Diversity Loss - Mean: -0.09700, Variance: 0.01310
Semantic Loss - Mean: 1.29076, Variance: 0.03607

Train Epoch: 18 
task: sign, mean loss: 0.57055, accuracy: 0.83696, avg. loss over tasks: 0.57055, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.10064, Variance: 0.01187
Semantic Loss - Mean: 0.59103, Variance: 0.00690

Test Epoch: 18 
task: sign, mean loss: 1.70219, accuracy: 0.49704, avg. loss over tasks: 1.70219
Diversity Loss - Mean: -0.09140, Variance: 0.01330
Semantic Loss - Mean: 1.42982, Variance: 0.03524

Train Epoch: 19 
task: sign, mean loss: 0.68636, accuracy: 0.76630, avg. loss over tasks: 0.68636, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.10612, Variance: 0.01208
Semantic Loss - Mean: 0.70037, Variance: 0.00680

Test Epoch: 19 
task: sign, mean loss: 1.93257, accuracy: 0.39053, avg. loss over tasks: 1.93257
Diversity Loss - Mean: -0.04148, Variance: 0.01350
Semantic Loss - Mean: 1.45631, Variance: 0.03727

Train Epoch: 20 
task: sign, mean loss: 0.48094, accuracy: 0.80978, avg. loss over tasks: 0.48094, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.10450, Variance: 0.01227
Semantic Loss - Mean: 0.55310, Variance: 0.00676

Test Epoch: 20 
task: sign, mean loss: 1.16297, accuracy: 0.57396, avg. loss over tasks: 1.16297
Diversity Loss - Mean: -0.05518, Variance: 0.01363
Semantic Loss - Mean: 1.03153, Variance: 0.03719

Train Epoch: 21 
task: sign, mean loss: 0.34208, accuracy: 0.89674, avg. loss over tasks: 0.34208, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.08857, Variance: 0.01233
Semantic Loss - Mean: 0.39552, Variance: 0.00661

Test Epoch: 21 
task: sign, mean loss: 0.79565, accuracy: 0.76331, avg. loss over tasks: 0.79565
Diversity Loss - Mean: -0.08681, Variance: 0.01376
Semantic Loss - Mean: 0.72574, Variance: 0.03613

Train Epoch: 22 
task: sign, mean loss: 0.28746, accuracy: 0.88587, avg. loss over tasks: 0.28746, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.07446, Variance: 0.01230
Semantic Loss - Mean: 0.38637, Variance: 0.00676

Test Epoch: 22 
task: sign, mean loss: 0.99105, accuracy: 0.66272, avg. loss over tasks: 0.99105
Diversity Loss - Mean: -0.04950, Variance: 0.01381
Semantic Loss - Mean: 0.77645, Variance: 0.03533

Train Epoch: 23 
task: sign, mean loss: 0.31069, accuracy: 0.89130, avg. loss over tasks: 0.31069, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.07513, Variance: 0.01226
Semantic Loss - Mean: 0.37617, Variance: 0.00697

Test Epoch: 23 
task: sign, mean loss: 1.52276, accuracy: 0.69231, avg. loss over tasks: 1.52276
Diversity Loss - Mean: -0.07311, Variance: 0.01381
Semantic Loss - Mean: 1.29381, Variance: 0.03550

Train Epoch: 24 
task: sign, mean loss: 0.22670, accuracy: 0.92391, avg. loss over tasks: 0.22670, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.08307, Variance: 0.01224
Semantic Loss - Mean: 0.25835, Variance: 0.00701

Test Epoch: 24 
task: sign, mean loss: 3.28499, accuracy: 0.39053, avg. loss over tasks: 3.28499
Diversity Loss - Mean: -0.02111, Variance: 0.01386
Semantic Loss - Mean: 2.25443, Variance: 0.03952

Train Epoch: 25 
task: sign, mean loss: 0.12585, accuracy: 0.96196, avg. loss over tasks: 0.12585, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.08432, Variance: 0.01226
Semantic Loss - Mean: 0.20311, Variance: 0.00694

Test Epoch: 25 
task: sign, mean loss: 2.51291, accuracy: 0.48521, avg. loss over tasks: 2.51291
Diversity Loss - Mean: -0.06782, Variance: 0.01387
Semantic Loss - Mean: 1.81156, Variance: 0.04273

Train Epoch: 26 
task: sign, mean loss: 0.28480, accuracy: 0.86957, avg. loss over tasks: 0.28480, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.08859, Variance: 0.01230
Semantic Loss - Mean: 0.33546, Variance: 0.00712

Test Epoch: 26 
task: sign, mean loss: 3.37850, accuracy: 0.44379, avg. loss over tasks: 3.37850
Diversity Loss - Mean: -0.03730, Variance: 0.01380
Semantic Loss - Mean: 2.72124, Variance: 0.04463

Train Epoch: 27 
task: sign, mean loss: 0.56048, accuracy: 0.85326, avg. loss over tasks: 0.56048, lr: 0.000289228031029578
Diversity Loss - Mean: -0.09761, Variance: 0.01240
Semantic Loss - Mean: 0.64017, Variance: 0.00787

Test Epoch: 27 
task: sign, mean loss: 0.92059, accuracy: 0.74556, avg. loss over tasks: 0.92059
Diversity Loss - Mean: -0.07858, Variance: 0.01395
Semantic Loss - Mean: 0.90644, Variance: 0.04393

Train Epoch: 28 
task: sign, mean loss: 0.33031, accuracy: 0.89130, avg. loss over tasks: 0.33031, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.10121, Variance: 0.01257
Semantic Loss - Mean: 0.39060, Variance: 0.00790

Test Epoch: 28 
task: sign, mean loss: 0.70306, accuracy: 0.82249, avg. loss over tasks: 0.70306
Diversity Loss - Mean: -0.08256, Variance: 0.01405
Semantic Loss - Mean: 0.63949, Variance: 0.04323

Train Epoch: 29 
task: sign, mean loss: 0.32290, accuracy: 0.88043, avg. loss over tasks: 0.32290, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.10113, Variance: 0.01270
Semantic Loss - Mean: 0.40487, Variance: 0.00797

Test Epoch: 29 
task: sign, mean loss: 0.31791, accuracy: 0.88166, avg. loss over tasks: 0.31791
Diversity Loss - Mean: -0.09812, Variance: 0.01415
Semantic Loss - Mean: 0.37182, Variance: 0.04195

Train Epoch: 30 
task: sign, mean loss: 0.19544, accuracy: 0.91304, avg. loss over tasks: 0.19544, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.10183, Variance: 0.01282
Semantic Loss - Mean: 0.26528, Variance: 0.00797

Test Epoch: 30 
task: sign, mean loss: 1.05939, accuracy: 0.76923, avg. loss over tasks: 1.05939
Diversity Loss - Mean: -0.08553, Variance: 0.01410
Semantic Loss - Mean: 0.96542, Variance: 0.04093

Train Epoch: 31 
task: sign, mean loss: 0.31324, accuracy: 0.85870, avg. loss over tasks: 0.31324, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.10374, Variance: 0.01290
Semantic Loss - Mean: 0.35439, Variance: 0.00812

Test Epoch: 31 
task: sign, mean loss: 0.44231, accuracy: 0.84615, avg. loss over tasks: 0.44231
Diversity Loss - Mean: -0.09817, Variance: 0.01418
Semantic Loss - Mean: 0.43717, Variance: 0.03990

Train Epoch: 32 
task: sign, mean loss: 0.22315, accuracy: 0.91304, avg. loss over tasks: 0.22315, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.10238, Variance: 0.01297
Semantic Loss - Mean: 0.33833, Variance: 0.00833

Test Epoch: 32 
task: sign, mean loss: 1.05776, accuracy: 0.62130, avg. loss over tasks: 1.05776
Diversity Loss - Mean: -0.08122, Variance: 0.01424
Semantic Loss - Mean: 1.09753, Variance: 0.04099

Train Epoch: 33 
task: sign, mean loss: 0.16559, accuracy: 0.94565, avg. loss over tasks: 0.16559, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.10146, Variance: 0.01300
Semantic Loss - Mean: 0.21542, Variance: 0.00828

Test Epoch: 33 
task: sign, mean loss: 3.36685, accuracy: 0.40828, avg. loss over tasks: 3.36685
Diversity Loss - Mean: -0.05963, Variance: 0.01422
Semantic Loss - Mean: 2.25679, Variance: 0.04129

Train Epoch: 34 
task: sign, mean loss: 0.49307, accuracy: 0.85326, avg. loss over tasks: 0.49307, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.10013, Variance: 0.01305
Semantic Loss - Mean: 0.46779, Variance: 0.00871

Test Epoch: 34 
task: sign, mean loss: 3.30680, accuracy: 0.68047, avg. loss over tasks: 3.30680
Diversity Loss - Mean: -0.06818, Variance: 0.01436
Semantic Loss - Mean: 2.54147, Variance: 0.04392

Train Epoch: 35 
task: sign, mean loss: 0.92482, accuracy: 0.72826, avg. loss over tasks: 0.92482, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.11614, Variance: 0.01319
Semantic Loss - Mean: 0.95749, Variance: 0.00881

Test Epoch: 35 
task: sign, mean loss: 1.89371, accuracy: 0.66272, avg. loss over tasks: 1.89371
Diversity Loss - Mean: -0.12159, Variance: 0.01447
Semantic Loss - Mean: 1.59934, Variance: 0.04326

Train Epoch: 36 
task: sign, mean loss: 0.57414, accuracy: 0.79348, avg. loss over tasks: 0.57414, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.11759, Variance: 0.01334
Semantic Loss - Mean: 0.61934, Variance: 0.00865

Test Epoch: 36 
task: sign, mean loss: 1.73915, accuracy: 0.66272, avg. loss over tasks: 1.73915
Diversity Loss - Mean: -0.11302, Variance: 0.01458
Semantic Loss - Mean: 1.54430, Variance: 0.04234

Train Epoch: 37 
task: sign, mean loss: 0.43232, accuracy: 0.82609, avg. loss over tasks: 0.43232, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.11393, Variance: 0.01343
Semantic Loss - Mean: 0.47427, Variance: 0.00850

Test Epoch: 37 
task: sign, mean loss: 1.54506, accuracy: 0.61538, avg. loss over tasks: 1.54506
Diversity Loss - Mean: -0.09427, Variance: 0.01459
Semantic Loss - Mean: 1.38404, Variance: 0.04207

Train Epoch: 38 
task: sign, mean loss: 0.52460, accuracy: 0.80435, avg. loss over tasks: 0.52460, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.11172, Variance: 0.01350
Semantic Loss - Mean: 0.55647, Variance: 0.00843

Test Epoch: 38 
task: sign, mean loss: 2.07226, accuracy: 0.30769, avg. loss over tasks: 2.07226
Diversity Loss - Mean: -0.09399, Variance: 0.01466
Semantic Loss - Mean: 1.75627, Variance: 0.04163

Train Epoch: 39 
task: sign, mean loss: 0.35003, accuracy: 0.84239, avg. loss over tasks: 0.35003, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.11078, Variance: 0.01357
Semantic Loss - Mean: 0.38460, Variance: 0.00827

Test Epoch: 39 
task: sign, mean loss: 2.14497, accuracy: 0.39053, avg. loss over tasks: 2.14497
Diversity Loss - Mean: -0.09817, Variance: 0.01471
Semantic Loss - Mean: 1.84740, Variance: 0.04104

Train Epoch: 40 
task: sign, mean loss: 0.40365, accuracy: 0.85870, avg. loss over tasks: 0.40365, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.11062, Variance: 0.01363
Semantic Loss - Mean: 0.43794, Variance: 0.00812

Test Epoch: 40 
task: sign, mean loss: 2.17086, accuracy: 0.52071, avg. loss over tasks: 2.17086
Diversity Loss - Mean: -0.11488, Variance: 0.01475
Semantic Loss - Mean: 1.91520, Variance: 0.04024

Train Epoch: 41 
task: sign, mean loss: 0.30228, accuracy: 0.88587, avg. loss over tasks: 0.30228, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.10935, Variance: 0.01368
Semantic Loss - Mean: 0.33923, Variance: 0.00802

Test Epoch: 41 
task: sign, mean loss: 2.03090, accuracy: 0.63905, avg. loss over tasks: 2.03090
Diversity Loss - Mean: -0.11989, Variance: 0.01477
Semantic Loss - Mean: 1.91267, Variance: 0.03958

Train Epoch: 42 
task: sign, mean loss: 0.43821, accuracy: 0.81522, avg. loss over tasks: 0.43821, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.11426, Variance: 0.01372
Semantic Loss - Mean: 0.47917, Variance: 0.00799

Test Epoch: 42 
task: sign, mean loss: 2.10323, accuracy: 0.57396, avg. loss over tasks: 2.10323
Diversity Loss - Mean: -0.12438, Variance: 0.01479
Semantic Loss - Mean: 1.84377, Variance: 0.03898

Train Epoch: 43 
task: sign, mean loss: 0.26095, accuracy: 0.90217, avg. loss over tasks: 0.26095, lr: 0.000260757131773478
Diversity Loss - Mean: -0.10901, Variance: 0.01373
Semantic Loss - Mean: 0.33534, Variance: 0.00787

Test Epoch: 43 
task: sign, mean loss: 1.80073, accuracy: 0.62130, avg. loss over tasks: 1.80073
Diversity Loss - Mean: -0.12399, Variance: 0.01479
Semantic Loss - Mean: 1.57211, Variance: 0.03828

Train Epoch: 44 
task: sign, mean loss: 0.38989, accuracy: 0.85326, avg. loss over tasks: 0.38989, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.11361, Variance: 0.01376
Semantic Loss - Mean: 0.43537, Variance: 0.00781

Test Epoch: 44 
task: sign, mean loss: 3.03159, accuracy: 0.33728, avg. loss over tasks: 3.03159
Diversity Loss - Mean: -0.08675, Variance: 0.01479
Semantic Loss - Mean: 2.40342, Variance: 0.03784

Train Epoch: 45 
task: sign, mean loss: 0.31350, accuracy: 0.88043, avg. loss over tasks: 0.31350, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.11204, Variance: 0.01378
Semantic Loss - Mean: 0.39452, Variance: 0.00771

Test Epoch: 45 
task: sign, mean loss: 2.02337, accuracy: 0.46746, avg. loss over tasks: 2.02337
Diversity Loss - Mean: -0.10944, Variance: 0.01476
Semantic Loss - Mean: 1.75727, Variance: 0.03724

Train Epoch: 46 
task: sign, mean loss: 0.36111, accuracy: 0.86413, avg. loss over tasks: 0.36111, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.11332, Variance: 0.01378
Semantic Loss - Mean: 0.38521, Variance: 0.00760

Test Epoch: 46 
task: sign, mean loss: 2.15039, accuracy: 0.29586, avg. loss over tasks: 2.15039
Diversity Loss - Mean: -0.09849, Variance: 0.01476
Semantic Loss - Mean: 1.87507, Variance: 0.03675

Train Epoch: 47 
task: sign, mean loss: 0.19534, accuracy: 0.94022, avg. loss over tasks: 0.19534, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.11646, Variance: 0.01379
Semantic Loss - Mean: 0.23674, Variance: 0.00748

Test Epoch: 47 
task: sign, mean loss: 2.31820, accuracy: 0.44379, avg. loss over tasks: 2.31820
Diversity Loss - Mean: -0.11479, Variance: 0.01474
Semantic Loss - Mean: 2.07442, Variance: 0.03622

Train Epoch: 48 
task: sign, mean loss: 0.15986, accuracy: 0.94565, avg. loss over tasks: 0.15986, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.11309, Variance: 0.01379
Semantic Loss - Mean: 0.20381, Variance: 0.00740

Test Epoch: 48 
task: sign, mean loss: 2.99828, accuracy: 0.28994, avg. loss over tasks: 2.99828
Diversity Loss - Mean: -0.11139, Variance: 0.01473
Semantic Loss - Mean: 2.51753, Variance: 0.03578

Train Epoch: 49 
task: sign, mean loss: 0.23572, accuracy: 0.93478, avg. loss over tasks: 0.23572, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.11367, Variance: 0.01377
Semantic Loss - Mean: 0.26161, Variance: 0.00731

Test Epoch: 49 
task: sign, mean loss: 2.17257, accuracy: 0.59763, avg. loss over tasks: 2.17257
Diversity Loss - Mean: -0.12345, Variance: 0.01470
Semantic Loss - Mean: 1.96013, Variance: 0.03531

Train Epoch: 50 
task: sign, mean loss: 0.23817, accuracy: 0.94022, avg. loss over tasks: 0.23817, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.11445, Variance: 0.01376
Semantic Loss - Mean: 0.28704, Variance: 0.00721

Test Epoch: 50 
task: sign, mean loss: 2.12959, accuracy: 0.56213, avg. loss over tasks: 2.12959
Diversity Loss - Mean: -0.12616, Variance: 0.01469
Semantic Loss - Mean: 1.93787, Variance: 0.03486

Train Epoch: 51 
task: sign, mean loss: 0.18298, accuracy: 0.94022, avg. loss over tasks: 0.18298, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.11704, Variance: 0.01374
Semantic Loss - Mean: 0.21556, Variance: 0.00711

Test Epoch: 51 
task: sign, mean loss: 2.35592, accuracy: 0.49704, avg. loss over tasks: 2.35592
Diversity Loss - Mean: -0.12763, Variance: 0.01471
Semantic Loss - Mean: 1.98218, Variance: 0.03448

Train Epoch: 52 
task: sign, mean loss: 0.14055, accuracy: 0.94565, avg. loss over tasks: 0.14055, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.11741, Variance: 0.01371
Semantic Loss - Mean: 0.16078, Variance: 0.00701

Test Epoch: 52 
task: sign, mean loss: 2.48914, accuracy: 0.44970, avg. loss over tasks: 2.48914
Diversity Loss - Mean: -0.12400, Variance: 0.01471
Semantic Loss - Mean: 2.27518, Variance: 0.03414

Train Epoch: 53 
task: sign, mean loss: 0.26024, accuracy: 0.92935, avg. loss over tasks: 0.26024, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.12006, Variance: 0.01370
Semantic Loss - Mean: 0.27593, Variance: 0.00693

Test Epoch: 53 
task: sign, mean loss: 2.04948, accuracy: 0.49112, avg. loss over tasks: 2.04948
Diversity Loss - Mean: -0.12497, Variance: 0.01471
Semantic Loss - Mean: 1.83687, Variance: 0.03381

Train Epoch: 54 
task: sign, mean loss: 0.20570, accuracy: 0.92935, avg. loss over tasks: 0.20570, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.11890, Variance: 0.01368
Semantic Loss - Mean: 0.22826, Variance: 0.00684

Test Epoch: 54 
task: sign, mean loss: 2.42560, accuracy: 0.40828, avg. loss over tasks: 2.42560
Diversity Loss - Mean: -0.11741, Variance: 0.01471
Semantic Loss - Mean: 2.12227, Variance: 0.03345

Train Epoch: 55 
task: sign, mean loss: 0.18324, accuracy: 0.92391, avg. loss over tasks: 0.18324, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.12031, Variance: 0.01368
Semantic Loss - Mean: 0.22057, Variance: 0.00676

Test Epoch: 55 
task: sign, mean loss: 2.43763, accuracy: 0.41420, avg. loss over tasks: 2.43763
Diversity Loss - Mean: -0.11749, Variance: 0.01469
Semantic Loss - Mean: 2.28374, Variance: 0.03324

Train Epoch: 56 
task: sign, mean loss: 0.08127, accuracy: 0.98370, avg. loss over tasks: 0.08127, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.11735, Variance: 0.01366
Semantic Loss - Mean: 0.13114, Variance: 0.00669

Test Epoch: 56 
task: sign, mean loss: 2.32883, accuracy: 0.39645, avg. loss over tasks: 2.32883
Diversity Loss - Mean: -0.12026, Variance: 0.01473
Semantic Loss - Mean: 2.00903, Variance: 0.03306

Train Epoch: 57 
task: sign, mean loss: 0.08564, accuracy: 0.96739, avg. loss over tasks: 0.08564, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.11988, Variance: 0.01365
Semantic Loss - Mean: 0.10376, Variance: 0.00661

Test Epoch: 57 
task: sign, mean loss: 2.74074, accuracy: 0.43787, avg. loss over tasks: 2.74074
Diversity Loss - Mean: -0.12292, Variance: 0.01474
Semantic Loss - Mean: 2.46130, Variance: 0.03287

Train Epoch: 58 
task: sign, mean loss: 0.06000, accuracy: 0.97826, avg. loss over tasks: 0.06000, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.11821, Variance: 0.01363
Semantic Loss - Mean: 0.08064, Variance: 0.00651

Test Epoch: 58 
task: sign, mean loss: 2.69321, accuracy: 0.47929, avg. loss over tasks: 2.69321
Diversity Loss - Mean: -0.12755, Variance: 0.01472
Semantic Loss - Mean: 2.39316, Variance: 0.03254

Train Epoch: 59 
task: sign, mean loss: 0.07464, accuracy: 0.97283, avg. loss over tasks: 0.07464, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.11874, Variance: 0.01361
Semantic Loss - Mean: 0.08233, Variance: 0.00642

Test Epoch: 59 
task: sign, mean loss: 2.93074, accuracy: 0.37870, avg. loss over tasks: 2.93074
Diversity Loss - Mean: -0.12240, Variance: 0.01471
Semantic Loss - Mean: 2.60593, Variance: 0.03240

Train Epoch: 60 
task: sign, mean loss: 0.09450, accuracy: 0.96196, avg. loss over tasks: 0.09450, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.11813, Variance: 0.01358
Semantic Loss - Mean: 0.10735, Variance: 0.00634

Test Epoch: 60 
task: sign, mean loss: 2.40133, accuracy: 0.38462, avg. loss over tasks: 2.40133
Diversity Loss - Mean: -0.12340, Variance: 0.01471
Semantic Loss - Mean: 2.03506, Variance: 0.03214

Train Epoch: 61 
task: sign, mean loss: 0.08760, accuracy: 0.97826, avg. loss over tasks: 0.08760, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.12198, Variance: 0.01356
Semantic Loss - Mean: 0.14173, Variance: 0.00631

Test Epoch: 61 
task: sign, mean loss: 2.18515, accuracy: 0.44970, avg. loss over tasks: 2.18515
Diversity Loss - Mean: -0.12422, Variance: 0.01469
Semantic Loss - Mean: 2.12042, Variance: 0.03194

Train Epoch: 62 
task: sign, mean loss: 0.17542, accuracy: 0.95652, avg. loss over tasks: 0.17542, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.11938, Variance: 0.01354
Semantic Loss - Mean: 0.19533, Variance: 0.00635

Test Epoch: 62 
task: sign, mean loss: 2.99023, accuracy: 0.44379, avg. loss over tasks: 2.99023
Diversity Loss - Mean: -0.12212, Variance: 0.01471
Semantic Loss - Mean: 2.68221, Variance: 0.03195

Train Epoch: 63 
task: sign, mean loss: 0.13004, accuracy: 0.94022, avg. loss over tasks: 0.13004, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.12222, Variance: 0.01352
Semantic Loss - Mean: 0.16717, Variance: 0.00635

Test Epoch: 63 
task: sign, mean loss: 3.46879, accuracy: 0.28402, avg. loss over tasks: 3.46879
Diversity Loss - Mean: -0.12145, Variance: 0.01474
Semantic Loss - Mean: 2.71942, Variance: 0.03188

Train Epoch: 64 
task: sign, mean loss: 0.12409, accuracy: 0.95109, avg. loss over tasks: 0.12409, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.12388, Variance: 0.01351
Semantic Loss - Mean: 0.15488, Variance: 0.00630

Test Epoch: 64 
task: sign, mean loss: 2.90370, accuracy: 0.34320, avg. loss over tasks: 2.90370
Diversity Loss - Mean: -0.12747, Variance: 0.01474
Semantic Loss - Mean: 2.41577, Variance: 0.03192

Train Epoch: 65 
task: sign, mean loss: 0.08686, accuracy: 0.97826, avg. loss over tasks: 0.08686, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.12540, Variance: 0.01350
Semantic Loss - Mean: 0.10944, Variance: 0.00626

Test Epoch: 65 
task: sign, mean loss: 2.56554, accuracy: 0.48521, avg. loss over tasks: 2.56554
Diversity Loss - Mean: -0.12930, Variance: 0.01470
Semantic Loss - Mean: 2.21659, Variance: 0.03199

Train Epoch: 66 
task: sign, mean loss: 0.10574, accuracy: 0.96739, avg. loss over tasks: 0.10574, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.12411, Variance: 0.01350
Semantic Loss - Mean: 0.13974, Variance: 0.00620

Test Epoch: 66 
task: sign, mean loss: 2.58791, accuracy: 0.48521, avg. loss over tasks: 2.58791
Diversity Loss - Mean: -0.12997, Variance: 0.01471
Semantic Loss - Mean: 2.24361, Variance: 0.03195

Train Epoch: 67 
task: sign, mean loss: 0.10130, accuracy: 0.93478, avg. loss over tasks: 0.10130, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.12622, Variance: 0.01349
Semantic Loss - Mean: 0.12901, Variance: 0.00616

Test Epoch: 67 
task: sign, mean loss: 2.70572, accuracy: 0.42604, avg. loss over tasks: 2.70572
Diversity Loss - Mean: -0.12642, Variance: 0.01476
Semantic Loss - Mean: 2.32904, Variance: 0.03186

Train Epoch: 68 
task: sign, mean loss: 0.11369, accuracy: 0.96196, avg. loss over tasks: 0.11369, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.12590, Variance: 0.01348
Semantic Loss - Mean: 0.15592, Variance: 0.00616

Test Epoch: 68 
task: sign, mean loss: 2.89586, accuracy: 0.47337, avg. loss over tasks: 2.89586
Diversity Loss - Mean: -0.12931, Variance: 0.01477
Semantic Loss - Mean: 2.49239, Variance: 0.03178

Train Epoch: 69 
task: sign, mean loss: 0.06891, accuracy: 0.97826, avg. loss over tasks: 0.06891, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.12601, Variance: 0.01347
Semantic Loss - Mean: 0.10894, Variance: 0.00612

Test Epoch: 69 
task: sign, mean loss: 3.37370, accuracy: 0.40237, avg. loss over tasks: 3.37370
Diversity Loss - Mean: -0.12707, Variance: 0.01473
Semantic Loss - Mean: 2.78979, Variance: 0.03203

Train Epoch: 70 
task: sign, mean loss: 0.06483, accuracy: 0.97826, avg. loss over tasks: 0.06483, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.12460, Variance: 0.01345
Semantic Loss - Mean: 0.10656, Variance: 0.00609

Test Epoch: 70 
task: sign, mean loss: 3.34671, accuracy: 0.35503, avg. loss over tasks: 3.34671
Diversity Loss - Mean: -0.13043, Variance: 0.01472
Semantic Loss - Mean: 2.83033, Variance: 0.03247

Train Epoch: 71 
task: sign, mean loss: 0.12002, accuracy: 0.95652, avg. loss over tasks: 0.12002, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.12618, Variance: 0.01343
Semantic Loss - Mean: 0.15157, Variance: 0.00607

Test Epoch: 71 
task: sign, mean loss: 3.35479, accuracy: 0.40237, avg. loss over tasks: 3.35479
Diversity Loss - Mean: -0.12945, Variance: 0.01470
Semantic Loss - Mean: 2.85008, Variance: 0.03336

Train Epoch: 72 
task: sign, mean loss: 0.11751, accuracy: 0.94022, avg. loss over tasks: 0.11751, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.12630, Variance: 0.01341
Semantic Loss - Mean: 0.14436, Variance: 0.00603

Test Epoch: 72 
task: sign, mean loss: 3.68744, accuracy: 0.23669, avg. loss over tasks: 3.68744
Diversity Loss - Mean: -0.11647, Variance: 0.01469
Semantic Loss - Mean: 3.26427, Variance: 0.03446

Train Epoch: 73 
task: sign, mean loss: 0.30013, accuracy: 0.92391, avg. loss over tasks: 0.30013, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.12648, Variance: 0.01340
Semantic Loss - Mean: 0.35590, Variance: 0.00608

Test Epoch: 73 
task: sign, mean loss: 2.74168, accuracy: 0.46746, avg. loss over tasks: 2.74168
Diversity Loss - Mean: -0.12804, Variance: 0.01466
Semantic Loss - Mean: 2.35947, Variance: 0.03441

Train Epoch: 74 
task: sign, mean loss: 0.11372, accuracy: 0.95109, avg. loss over tasks: 0.11372, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.12683, Variance: 0.01338
Semantic Loss - Mean: 0.15312, Variance: 0.00603

Test Epoch: 74 
task: sign, mean loss: 2.41526, accuracy: 0.51479, avg. loss over tasks: 2.41526
Diversity Loss - Mean: -0.13179, Variance: 0.01465
Semantic Loss - Mean: 2.16715, Variance: 0.03437

Train Epoch: 75 
task: sign, mean loss: 0.06867, accuracy: 0.97283, avg. loss over tasks: 0.06867, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.12842, Variance: 0.01337
Semantic Loss - Mean: 0.09055, Variance: 0.00598

Test Epoch: 75 
task: sign, mean loss: 2.37918, accuracy: 0.55030, avg. loss over tasks: 2.37918
Diversity Loss - Mean: -0.13459, Variance: 0.01464
Semantic Loss - Mean: 2.01532, Variance: 0.03419

Train Epoch: 76 
task: sign, mean loss: 0.06155, accuracy: 0.97283, avg. loss over tasks: 0.06155, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.12717, Variance: 0.01336
Semantic Loss - Mean: 0.10334, Variance: 0.00592

Test Epoch: 76 
task: sign, mean loss: 2.84339, accuracy: 0.43195, avg. loss over tasks: 2.84339
Diversity Loss - Mean: -0.13026, Variance: 0.01463
Semantic Loss - Mean: 2.50963, Variance: 0.03408

Train Epoch: 77 
task: sign, mean loss: 0.07547, accuracy: 0.96739, avg. loss over tasks: 0.07547, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.12944, Variance: 0.01335
Semantic Loss - Mean: 0.08323, Variance: 0.00586

Test Epoch: 77 
task: sign, mean loss: 3.38808, accuracy: 0.28402, avg. loss over tasks: 3.38808
Diversity Loss - Mean: -0.12770, Variance: 0.01464
Semantic Loss - Mean: 3.08249, Variance: 0.03416

Train Epoch: 78 
task: sign, mean loss: 0.06750, accuracy: 0.97826, avg. loss over tasks: 0.06750, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.12993, Variance: 0.01334
Semantic Loss - Mean: 0.10855, Variance: 0.00591

Test Epoch: 78 
task: sign, mean loss: 2.69292, accuracy: 0.43787, avg. loss over tasks: 2.69292
Diversity Loss - Mean: -0.13309, Variance: 0.01466
Semantic Loss - Mean: 2.37325, Variance: 0.03421

Train Epoch: 79 
task: sign, mean loss: 0.06173, accuracy: 0.97283, avg. loss over tasks: 0.06173, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.12978, Variance: 0.01333
Semantic Loss - Mean: 0.07637, Variance: 0.00586

Test Epoch: 79 
task: sign, mean loss: 3.11718, accuracy: 0.34911, avg. loss over tasks: 3.11718
Diversity Loss - Mean: -0.12630, Variance: 0.01467
Semantic Loss - Mean: 2.88665, Variance: 0.03461

Train Epoch: 80 
task: sign, mean loss: 0.09185, accuracy: 0.96739, avg. loss over tasks: 0.09185, lr: 0.00015015
Diversity Loss - Mean: -0.12667, Variance: 0.01332
Semantic Loss - Mean: 0.13471, Variance: 0.00587

Test Epoch: 80 
task: sign, mean loss: 3.14764, accuracy: 0.28994, avg. loss over tasks: 3.14764
Diversity Loss - Mean: -0.12802, Variance: 0.01467
Semantic Loss - Mean: 2.74602, Variance: 0.03467

Train Epoch: 81 
task: sign, mean loss: 0.07096, accuracy: 0.97283, avg. loss over tasks: 0.07096, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.12992, Variance: 0.01331
Semantic Loss - Mean: 0.09859, Variance: 0.00584

Test Epoch: 81 
task: sign, mean loss: 3.06961, accuracy: 0.33136, avg. loss over tasks: 3.06961
Diversity Loss - Mean: -0.13035, Variance: 0.01469
Semantic Loss - Mean: 2.74422, Variance: 0.03471

Train Epoch: 82 
task: sign, mean loss: 0.15566, accuracy: 0.97283, avg. loss over tasks: 0.15566, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.13113, Variance: 0.01331
Semantic Loss - Mean: 0.20572, Variance: 0.00580

Test Epoch: 82 
task: sign, mean loss: 2.35276, accuracy: 0.53254, avg. loss over tasks: 2.35276
Diversity Loss - Mean: -0.13500, Variance: 0.01469
Semantic Loss - Mean: 2.17776, Variance: 0.03471

Train Epoch: 83 
task: sign, mean loss: 0.05570, accuracy: 0.98913, avg. loss over tasks: 0.05570, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12980, Variance: 0.01330
Semantic Loss - Mean: 0.09845, Variance: 0.00578

Test Epoch: 83 
task: sign, mean loss: 2.69672, accuracy: 0.52071, avg. loss over tasks: 2.69672
Diversity Loss - Mean: -0.13464, Variance: 0.01467
Semantic Loss - Mean: 2.45123, Variance: 0.03459

Train Epoch: 84 
task: sign, mean loss: 0.11355, accuracy: 0.96196, avg. loss over tasks: 0.11355, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.13005, Variance: 0.01330
Semantic Loss - Mean: 0.16673, Variance: 0.00578

Test Epoch: 84 
task: sign, mean loss: 2.55939, accuracy: 0.43787, avg. loss over tasks: 2.55939
Diversity Loss - Mean: -0.13563, Variance: 0.01465
Semantic Loss - Mean: 2.25946, Variance: 0.03445

Train Epoch: 85 
task: sign, mean loss: 0.05376, accuracy: 0.98370, avg. loss over tasks: 0.05376, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.13146, Variance: 0.01330
Semantic Loss - Mean: 0.06996, Variance: 0.00574

Test Epoch: 85 
task: sign, mean loss: 2.38777, accuracy: 0.44970, avg. loss over tasks: 2.38777
Diversity Loss - Mean: -0.13596, Variance: 0.01464
Semantic Loss - Mean: 2.07347, Variance: 0.03422

Train Epoch: 86 
task: sign, mean loss: 0.10959, accuracy: 0.97283, avg. loss over tasks: 0.10959, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.13139, Variance: 0.01330
Semantic Loss - Mean: 0.14908, Variance: 0.00572

Test Epoch: 86 
task: sign, mean loss: 2.35587, accuracy: 0.50296, avg. loss over tasks: 2.35587
Diversity Loss - Mean: -0.13333, Variance: 0.01463
Semantic Loss - Mean: 2.26459, Variance: 0.03405

Train Epoch: 87 
task: sign, mean loss: 0.07098, accuracy: 0.97283, avg. loss over tasks: 0.07098, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.12980, Variance: 0.01330
Semantic Loss - Mean: 0.09290, Variance: 0.00568

Test Epoch: 87 
task: sign, mean loss: 2.88182, accuracy: 0.37870, avg. loss over tasks: 2.88182
Diversity Loss - Mean: -0.13182, Variance: 0.01465
Semantic Loss - Mean: 2.69868, Variance: 0.03389

Train Epoch: 88 
task: sign, mean loss: 0.05794, accuracy: 0.98370, avg. loss over tasks: 0.05794, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.13048, Variance: 0.01331
Semantic Loss - Mean: 0.09114, Variance: 0.00566

Test Epoch: 88 
task: sign, mean loss: 2.93288, accuracy: 0.40237, avg. loss over tasks: 2.93288
Diversity Loss - Mean: -0.13251, Variance: 0.01465
Semantic Loss - Mean: 2.70419, Variance: 0.03401

Train Epoch: 89 
task: sign, mean loss: 0.06864, accuracy: 0.97283, avg. loss over tasks: 0.06864, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.13033, Variance: 0.01331
Semantic Loss - Mean: 0.09226, Variance: 0.00564

Test Epoch: 89 
task: sign, mean loss: 3.02922, accuracy: 0.35503, avg. loss over tasks: 3.02922
Diversity Loss - Mean: -0.13173, Variance: 0.01469
Semantic Loss - Mean: 2.58784, Variance: 0.03431

Train Epoch: 90 
task: sign, mean loss: 0.09135, accuracy: 0.96739, avg. loss over tasks: 0.09135, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.13229, Variance: 0.01331
Semantic Loss - Mean: 0.12570, Variance: 0.00562

Test Epoch: 90 
task: sign, mean loss: 2.87507, accuracy: 0.37278, avg. loss over tasks: 2.87507
Diversity Loss - Mean: -0.13298, Variance: 0.01469
Semantic Loss - Mean: 2.40528, Variance: 0.03421

Train Epoch: 91 
task: sign, mean loss: 0.07299, accuracy: 0.96196, avg. loss over tasks: 0.07299, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.13079, Variance: 0.01331
Semantic Loss - Mean: 0.10402, Variance: 0.00561

Test Epoch: 91 
task: sign, mean loss: 2.91836, accuracy: 0.40237, avg. loss over tasks: 2.91836
Diversity Loss - Mean: -0.13187, Variance: 0.01470
Semantic Loss - Mean: 2.58024, Variance: 0.03427

Train Epoch: 92 
task: sign, mean loss: 0.02655, accuracy: 0.99457, avg. loss over tasks: 0.02655, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.13137, Variance: 0.01330
Semantic Loss - Mean: 0.05431, Variance: 0.00557

Test Epoch: 92 
task: sign, mean loss: 3.27344, accuracy: 0.38462, avg. loss over tasks: 3.27344
Diversity Loss - Mean: -0.12949, Variance: 0.01472
Semantic Loss - Mean: 2.92075, Variance: 0.03429

Train Epoch: 93 
task: sign, mean loss: 0.03160, accuracy: 0.99457, avg. loss over tasks: 0.03160, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.13176, Variance: 0.01329
Semantic Loss - Mean: 0.04611, Variance: 0.00552

Test Epoch: 93 
task: sign, mean loss: 3.06754, accuracy: 0.43787, avg. loss over tasks: 3.06754
Diversity Loss - Mean: -0.13214, Variance: 0.01472
Semantic Loss - Mean: 2.70782, Variance: 0.03431

Train Epoch: 94 
task: sign, mean loss: 0.03036, accuracy: 0.98913, avg. loss over tasks: 0.03036, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.13169, Variance: 0.01328
Semantic Loss - Mean: 0.04213, Variance: 0.00547

Test Epoch: 94 
task: sign, mean loss: 3.01456, accuracy: 0.42604, avg. loss over tasks: 3.01456
Diversity Loss - Mean: -0.13279, Variance: 0.01471
Semantic Loss - Mean: 2.60059, Variance: 0.03426

Train Epoch: 95 
task: sign, mean loss: 0.01106, accuracy: 1.00000, avg. loss over tasks: 0.01106, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.13244, Variance: 0.01327
Semantic Loss - Mean: 0.02690, Variance: 0.00542

Test Epoch: 95 
task: sign, mean loss: 3.01129, accuracy: 0.43195, avg. loss over tasks: 3.01129
Diversity Loss - Mean: -0.13262, Variance: 0.01470
Semantic Loss - Mean: 2.60474, Variance: 0.03426

Train Epoch: 96 
task: sign, mean loss: 0.00871, accuracy: 1.00000, avg. loss over tasks: 0.00871, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.13207, Variance: 0.01326
Semantic Loss - Mean: 0.03073, Variance: 0.00538

Test Epoch: 96 
task: sign, mean loss: 3.16158, accuracy: 0.40237, avg. loss over tasks: 3.16158
Diversity Loss - Mean: -0.13302, Variance: 0.01470
Semantic Loss - Mean: 2.75402, Variance: 0.03421

Train Epoch: 97 
task: sign, mean loss: 0.01357, accuracy: 0.99457, avg. loss over tasks: 0.01357, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.13251, Variance: 0.01326
Semantic Loss - Mean: 0.02693, Variance: 0.00534

Test Epoch: 97 
task: sign, mean loss: 3.27657, accuracy: 0.41420, avg. loss over tasks: 3.27657
Diversity Loss - Mean: -0.13264, Variance: 0.01468
Semantic Loss - Mean: 2.88298, Variance: 0.03412

Train Epoch: 98 
task: sign, mean loss: 0.00489, accuracy: 1.00000, avg. loss over tasks: 0.00489, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.13328, Variance: 0.01325
Semantic Loss - Mean: 0.01764, Variance: 0.00529

Test Epoch: 98 
task: sign, mean loss: 3.29164, accuracy: 0.40828, avg. loss over tasks: 3.29164
Diversity Loss - Mean: -0.13293, Variance: 0.01467
Semantic Loss - Mean: 2.90263, Variance: 0.03400

Train Epoch: 99 
task: sign, mean loss: 0.01671, accuracy: 0.99457, avg. loss over tasks: 0.01671, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.13319, Variance: 0.01324
Semantic Loss - Mean: 0.03239, Variance: 0.00525

Test Epoch: 99 
task: sign, mean loss: 3.46675, accuracy: 0.37870, avg. loss over tasks: 3.46675
Diversity Loss - Mean: -0.13224, Variance: 0.01467
Semantic Loss - Mean: 2.99391, Variance: 0.03406

Train Epoch: 100 
task: sign, mean loss: 0.02214, accuracy: 0.99457, avg. loss over tasks: 0.02214, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.13318, Variance: 0.01324
Semantic Loss - Mean: 0.04234, Variance: 0.00523

Test Epoch: 100 
task: sign, mean loss: 3.35530, accuracy: 0.37278, avg. loss over tasks: 3.35530
Diversity Loss - Mean: -0.13419, Variance: 0.01466
Semantic Loss - Mean: 2.83038, Variance: 0.03416

Train Epoch: 101 
task: sign, mean loss: 0.01021, accuracy: 1.00000, avg. loss over tasks: 0.01021, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.13392, Variance: 0.01324
Semantic Loss - Mean: 0.03157, Variance: 0.00520

Test Epoch: 101 
task: sign, mean loss: 3.21853, accuracy: 0.35503, avg. loss over tasks: 3.21853
Diversity Loss - Mean: -0.13546, Variance: 0.01467
Semantic Loss - Mean: 2.68875, Variance: 0.03424

Train Epoch: 102 
task: sign, mean loss: 0.04131, accuracy: 0.98370, avg. loss over tasks: 0.04131, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.13346, Variance: 0.01323
Semantic Loss - Mean: 0.05924, Variance: 0.00516

Test Epoch: 102 
task: sign, mean loss: 3.44151, accuracy: 0.36095, avg. loss over tasks: 3.44151
Diversity Loss - Mean: -0.13292, Variance: 0.01469
Semantic Loss - Mean: 2.87096, Variance: 0.03428

Train Epoch: 103 
task: sign, mean loss: 0.05079, accuracy: 0.99457, avg. loss over tasks: 0.05079, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.13310, Variance: 0.01323
Semantic Loss - Mean: 0.06250, Variance: 0.00512

Test Epoch: 103 
task: sign, mean loss: 3.56710, accuracy: 0.39053, avg. loss over tasks: 3.56710
Diversity Loss - Mean: -0.13206, Variance: 0.01469
Semantic Loss - Mean: 3.05588, Variance: 0.03429

Train Epoch: 104 
task: sign, mean loss: 0.03580, accuracy: 0.98913, avg. loss over tasks: 0.03580, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.13300, Variance: 0.01322
Semantic Loss - Mean: 0.04087, Variance: 0.00509

Test Epoch: 104 
task: sign, mean loss: 3.33037, accuracy: 0.39053, avg. loss over tasks: 3.33037
Diversity Loss - Mean: -0.13458, Variance: 0.01470
Semantic Loss - Mean: 2.83762, Variance: 0.03438

Train Epoch: 105 
task: sign, mean loss: 0.01697, accuracy: 0.98913, avg. loss over tasks: 0.01697, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.13341, Variance: 0.01321
Semantic Loss - Mean: 0.04042, Variance: 0.00506

Test Epoch: 105 
task: sign, mean loss: 3.45511, accuracy: 0.33728, avg. loss over tasks: 3.45511
Diversity Loss - Mean: -0.13420, Variance: 0.01471
Semantic Loss - Mean: 3.00005, Variance: 0.03453

Train Epoch: 106 
task: sign, mean loss: 0.01252, accuracy: 0.99457, avg. loss over tasks: 0.01252, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.13371, Variance: 0.01321
Semantic Loss - Mean: 0.02280, Variance: 0.00503

Test Epoch: 106 
task: sign, mean loss: 3.40047, accuracy: 0.34911, avg. loss over tasks: 3.40047
Diversity Loss - Mean: -0.13335, Variance: 0.01471
Semantic Loss - Mean: 2.96580, Variance: 0.03454

Train Epoch: 107 
task: sign, mean loss: 0.04452, accuracy: 0.98913, avg. loss over tasks: 0.04452, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.13418, Variance: 0.01320
Semantic Loss - Mean: 0.04684, Variance: 0.00503

Test Epoch: 107 
task: sign, mean loss: 3.16144, accuracy: 0.40828, avg. loss over tasks: 3.16144
Diversity Loss - Mean: -0.13406, Variance: 0.01470
Semantic Loss - Mean: 2.78820, Variance: 0.03454

Train Epoch: 108 
task: sign, mean loss: 0.01630, accuracy: 0.99457, avg. loss over tasks: 0.01630, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.13384, Variance: 0.01319
Semantic Loss - Mean: 0.02496, Variance: 0.00500

Test Epoch: 108 
task: sign, mean loss: 3.16886, accuracy: 0.42604, avg. loss over tasks: 3.16886
Diversity Loss - Mean: -0.13087, Variance: 0.01470
Semantic Loss - Mean: 2.76766, Variance: 0.03459

Train Epoch: 109 
task: sign, mean loss: 0.00881, accuracy: 1.00000, avg. loss over tasks: 0.00881, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.13359, Variance: 0.01319
Semantic Loss - Mean: 0.02146, Variance: 0.00496

Test Epoch: 109 
task: sign, mean loss: 3.14507, accuracy: 0.43195, avg. loss over tasks: 3.14507
Diversity Loss - Mean: -0.13249, Variance: 0.01469
Semantic Loss - Mean: 2.72148, Variance: 0.03454

Train Epoch: 110 
task: sign, mean loss: 0.01956, accuracy: 0.99457, avg. loss over tasks: 0.01956, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.13381, Variance: 0.01318
Semantic Loss - Mean: 0.02967, Variance: 0.00493

Test Epoch: 110 
task: sign, mean loss: 3.12363, accuracy: 0.43195, avg. loss over tasks: 3.12363
Diversity Loss - Mean: -0.13475, Variance: 0.01469
Semantic Loss - Mean: 2.67601, Variance: 0.03444

Train Epoch: 111 
task: sign, mean loss: 0.00336, accuracy: 1.00000, avg. loss over tasks: 0.00336, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.13425, Variance: 0.01318
Semantic Loss - Mean: 0.01603, Variance: 0.00489

Test Epoch: 111 
task: sign, mean loss: 3.34804, accuracy: 0.40237, avg. loss over tasks: 3.34804
Diversity Loss - Mean: -0.13485, Variance: 0.01468
Semantic Loss - Mean: 2.83742, Variance: 0.03433

Train Epoch: 112 
task: sign, mean loss: 0.03110, accuracy: 0.99457, avg. loss over tasks: 0.03110, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.13424, Variance: 0.01317
Semantic Loss - Mean: 0.03238, Variance: 0.00485

Test Epoch: 112 
task: sign, mean loss: 3.19874, accuracy: 0.41420, avg. loss over tasks: 3.19874
Diversity Loss - Mean: -0.13517, Variance: 0.01468
Semantic Loss - Mean: 2.70911, Variance: 0.03429

Train Epoch: 113 
task: sign, mean loss: 0.03599, accuracy: 0.99457, avg. loss over tasks: 0.03599, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.13444, Variance: 0.01317
Semantic Loss - Mean: 0.05071, Variance: 0.00481

Test Epoch: 113 
task: sign, mean loss: 3.22246, accuracy: 0.41420, avg. loss over tasks: 3.22246
Diversity Loss - Mean: -0.13554, Variance: 0.01468
Semantic Loss - Mean: 2.72207, Variance: 0.03428

Train Epoch: 114 
task: sign, mean loss: 0.00796, accuracy: 1.00000, avg. loss over tasks: 0.00796, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.13468, Variance: 0.01316
Semantic Loss - Mean: 0.01470, Variance: 0.00477

Test Epoch: 114 
task: sign, mean loss: 3.32713, accuracy: 0.39053, avg. loss over tasks: 3.32713
Diversity Loss - Mean: -0.13540, Variance: 0.01468
Semantic Loss - Mean: 2.87424, Variance: 0.03422

Train Epoch: 115 
task: sign, mean loss: 0.02115, accuracy: 0.99457, avg. loss over tasks: 0.02115, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.13468, Variance: 0.01316
Semantic Loss - Mean: 0.02675, Variance: 0.00473

Test Epoch: 115 
task: sign, mean loss: 3.62503, accuracy: 0.37278, avg. loss over tasks: 3.62503
Diversity Loss - Mean: -0.13515, Variance: 0.01467
Semantic Loss - Mean: 3.10379, Variance: 0.03412

Train Epoch: 116 
task: sign, mean loss: 0.01034, accuracy: 0.99457, avg. loss over tasks: 0.01034, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.13436, Variance: 0.01315
Semantic Loss - Mean: 0.02083, Variance: 0.00470

Test Epoch: 116 
task: sign, mean loss: 3.51267, accuracy: 0.38462, avg. loss over tasks: 3.51267
Diversity Loss - Mean: -0.13535, Variance: 0.01466
Semantic Loss - Mean: 2.97728, Variance: 0.03406

Train Epoch: 117 
task: sign, mean loss: 0.00761, accuracy: 0.99457, avg. loss over tasks: 0.00761, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.13473, Variance: 0.01315
Semantic Loss - Mean: 0.01755, Variance: 0.00467

Test Epoch: 117 
task: sign, mean loss: 3.51797, accuracy: 0.37278, avg. loss over tasks: 3.51797
Diversity Loss - Mean: -0.13553, Variance: 0.01466
Semantic Loss - Mean: 2.95668, Variance: 0.03409

Train Epoch: 118 
task: sign, mean loss: 0.00652, accuracy: 1.00000, avg. loss over tasks: 0.00652, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.13466, Variance: 0.01314
Semantic Loss - Mean: 0.01298, Variance: 0.00463

Test Epoch: 118 
task: sign, mean loss: 3.68938, accuracy: 0.36686, avg. loss over tasks: 3.68938
Diversity Loss - Mean: -0.13566, Variance: 0.01465
Semantic Loss - Mean: 3.07756, Variance: 0.03409

Train Epoch: 119 
task: sign, mean loss: 0.00533, accuracy: 1.00000, avg. loss over tasks: 0.00533, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.13480, Variance: 0.01314
Semantic Loss - Mean: 0.01446, Variance: 0.00460

Test Epoch: 119 
task: sign, mean loss: 3.57921, accuracy: 0.37278, avg. loss over tasks: 3.57921
Diversity Loss - Mean: -0.13597, Variance: 0.01465
Semantic Loss - Mean: 2.99684, Variance: 0.03408

Train Epoch: 120 
task: sign, mean loss: 0.02229, accuracy: 0.98370, avg. loss over tasks: 0.02229, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.13512, Variance: 0.01313
Semantic Loss - Mean: 0.02676, Variance: 0.00458

Test Epoch: 120 
task: sign, mean loss: 3.36203, accuracy: 0.39053, avg. loss over tasks: 3.36203
Diversity Loss - Mean: -0.13632, Variance: 0.01465
Semantic Loss - Mean: 2.83204, Variance: 0.03403

Train Epoch: 121 
task: sign, mean loss: 0.00414, accuracy: 1.00000, avg. loss over tasks: 0.00414, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.13547, Variance: 0.01313
Semantic Loss - Mean: 0.01368, Variance: 0.00454

Test Epoch: 121 
task: sign, mean loss: 3.49415, accuracy: 0.39053, avg. loss over tasks: 3.49415
Diversity Loss - Mean: -0.13600, Variance: 0.01465
Semantic Loss - Mean: 2.93336, Variance: 0.03399

Train Epoch: 122 
task: sign, mean loss: 0.00477, accuracy: 1.00000, avg. loss over tasks: 0.00477, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.13494, Variance: 0.01313
Semantic Loss - Mean: 0.02578, Variance: 0.00454

Test Epoch: 122 
task: sign, mean loss: 3.54493, accuracy: 0.39053, avg. loss over tasks: 3.54493
Diversity Loss - Mean: -0.13624, Variance: 0.01464
Semantic Loss - Mean: 2.96735, Variance: 0.03395

Train Epoch: 123 
task: sign, mean loss: 0.00542, accuracy: 1.00000, avg. loss over tasks: 0.00542, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.13561, Variance: 0.01312
Semantic Loss - Mean: 0.01674, Variance: 0.00451

Test Epoch: 123 
task: sign, mean loss: 3.52886, accuracy: 0.37870, avg. loss over tasks: 3.52886
Diversity Loss - Mean: -0.13637, Variance: 0.01464
Semantic Loss - Mean: 2.93750, Variance: 0.03389

Train Epoch: 124 
task: sign, mean loss: 0.02152, accuracy: 0.99457, avg. loss over tasks: 0.02152, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.13576, Variance: 0.01312
Semantic Loss - Mean: 0.03380, Variance: 0.00449

Test Epoch: 124 
task: sign, mean loss: 3.54068, accuracy: 0.36686, avg. loss over tasks: 3.54068
Diversity Loss - Mean: -0.13585, Variance: 0.01464
Semantic Loss - Mean: 2.91712, Variance: 0.03381

Train Epoch: 125 
task: sign, mean loss: 0.00362, accuracy: 1.00000, avg. loss over tasks: 0.00362, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.13572, Variance: 0.01312
Semantic Loss - Mean: 0.01213, Variance: 0.00445

Test Epoch: 125 
task: sign, mean loss: 3.56282, accuracy: 0.36095, avg. loss over tasks: 3.56282
Diversity Loss - Mean: -0.13635, Variance: 0.01465
Semantic Loss - Mean: 2.92223, Variance: 0.03379

Train Epoch: 126 
task: sign, mean loss: 0.00732, accuracy: 0.99457, avg. loss over tasks: 0.00732, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.13552, Variance: 0.01311
Semantic Loss - Mean: 0.01358, Variance: 0.00443

Test Epoch: 126 
task: sign, mean loss: 3.27437, accuracy: 0.40237, avg. loss over tasks: 3.27437
Diversity Loss - Mean: -0.13660, Variance: 0.01464
Semantic Loss - Mean: 2.73267, Variance: 0.03372

Train Epoch: 127 
task: sign, mean loss: 0.03140, accuracy: 0.98913, avg. loss over tasks: 0.03140, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.13556, Variance: 0.01311
Semantic Loss - Mean: 0.03708, Variance: 0.00441

Test Epoch: 127 
task: sign, mean loss: 3.44415, accuracy: 0.39053, avg. loss over tasks: 3.44415
Diversity Loss - Mean: -0.13700, Variance: 0.01464
Semantic Loss - Mean: 2.87243, Variance: 0.03366

Train Epoch: 128 
task: sign, mean loss: 0.00388, accuracy: 1.00000, avg. loss over tasks: 0.00388, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.13592, Variance: 0.01311
Semantic Loss - Mean: 0.00655, Variance: 0.00438

Test Epoch: 128 
task: sign, mean loss: 3.55804, accuracy: 0.36686, avg. loss over tasks: 3.55804
Diversity Loss - Mean: -0.13681, Variance: 0.01464
Semantic Loss - Mean: 2.95806, Variance: 0.03359

Train Epoch: 129 
task: sign, mean loss: 0.01057, accuracy: 0.99457, avg. loss over tasks: 0.01057, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.13586, Variance: 0.01311
Semantic Loss - Mean: 0.01641, Variance: 0.00436

Test Epoch: 129 
task: sign, mean loss: 3.41365, accuracy: 0.39645, avg. loss over tasks: 3.41365
Diversity Loss - Mean: -0.13710, Variance: 0.01463
Semantic Loss - Mean: 2.85840, Variance: 0.03351

Train Epoch: 130 
task: sign, mean loss: 0.00794, accuracy: 0.99457, avg. loss over tasks: 0.00794, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.13596, Variance: 0.01311
Semantic Loss - Mean: 0.01745, Variance: 0.00433

Test Epoch: 130 
task: sign, mean loss: 3.50265, accuracy: 0.38462, avg. loss over tasks: 3.50265
Diversity Loss - Mean: -0.13681, Variance: 0.01463
Semantic Loss - Mean: 2.90634, Variance: 0.03344

Train Epoch: 131 
task: sign, mean loss: 0.00170, accuracy: 1.00000, avg. loss over tasks: 0.00170, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.13596, Variance: 0.01310
Semantic Loss - Mean: 0.00658, Variance: 0.00430

Test Epoch: 131 
task: sign, mean loss: 3.65277, accuracy: 0.36095, avg. loss over tasks: 3.65277
Diversity Loss - Mean: -0.13667, Variance: 0.01463
Semantic Loss - Mean: 3.02991, Variance: 0.03339

Train Epoch: 132 
task: sign, mean loss: 0.00760, accuracy: 1.00000, avg. loss over tasks: 0.00760, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.13623, Variance: 0.01310
Semantic Loss - Mean: 0.01613, Variance: 0.00427

Test Epoch: 132 
task: sign, mean loss: 3.58854, accuracy: 0.37278, avg. loss over tasks: 3.58854
Diversity Loss - Mean: -0.13683, Variance: 0.01463
Semantic Loss - Mean: 2.98215, Variance: 0.03334

Train Epoch: 133 
task: sign, mean loss: 0.00413, accuracy: 1.00000, avg. loss over tasks: 0.00413, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.13590, Variance: 0.01310
Semantic Loss - Mean: 0.00736, Variance: 0.00424

Test Epoch: 133 
task: sign, mean loss: 3.69243, accuracy: 0.35503, avg. loss over tasks: 3.69243
Diversity Loss - Mean: -0.13662, Variance: 0.01463
Semantic Loss - Mean: 3.05215, Variance: 0.03328

Train Epoch: 134 
task: sign, mean loss: 0.00165, accuracy: 1.00000, avg. loss over tasks: 0.00165, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13583, Variance: 0.01309
Semantic Loss - Mean: 0.01051, Variance: 0.00422

Test Epoch: 134 
task: sign, mean loss: 3.60539, accuracy: 0.36095, avg. loss over tasks: 3.60539
Diversity Loss - Mean: -0.13666, Variance: 0.01463
Semantic Loss - Mean: 3.00255, Variance: 0.03322

Train Epoch: 135 
task: sign, mean loss: 0.00474, accuracy: 1.00000, avg. loss over tasks: 0.00474, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.13601, Variance: 0.01309
Semantic Loss - Mean: 0.01440, Variance: 0.00419

Test Epoch: 135 
task: sign, mean loss: 3.51452, accuracy: 0.40237, avg. loss over tasks: 3.51452
Diversity Loss - Mean: -0.13690, Variance: 0.01462
Semantic Loss - Mean: 2.94430, Variance: 0.03316

Train Epoch: 136 
task: sign, mean loss: 0.00228, accuracy: 1.00000, avg. loss over tasks: 0.00228, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13633, Variance: 0.01309
Semantic Loss - Mean: 0.00785, Variance: 0.00416

Test Epoch: 136 
task: sign, mean loss: 3.47193, accuracy: 0.39645, avg. loss over tasks: 3.47193
Diversity Loss - Mean: -0.13676, Variance: 0.01462
Semantic Loss - Mean: 2.90183, Variance: 0.03310

Train Epoch: 137 
task: sign, mean loss: 0.00497, accuracy: 1.00000, avg. loss over tasks: 0.00497, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13632, Variance: 0.01308
Semantic Loss - Mean: 0.01199, Variance: 0.00414

Test Epoch: 137 
task: sign, mean loss: 3.57813, accuracy: 0.37278, avg. loss over tasks: 3.57813
Diversity Loss - Mean: -0.13666, Variance: 0.01462
Semantic Loss - Mean: 2.97137, Variance: 0.03305

Train Epoch: 138 
task: sign, mean loss: 0.00186, accuracy: 1.00000, avg. loss over tasks: 0.00186, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13616, Variance: 0.01308
Semantic Loss - Mean: 0.01116, Variance: 0.00412

Test Epoch: 138 
task: sign, mean loss: 3.48812, accuracy: 0.39053, avg. loss over tasks: 3.48812
Diversity Loss - Mean: -0.13690, Variance: 0.01462
Semantic Loss - Mean: 2.92141, Variance: 0.03301

Train Epoch: 139 
task: sign, mean loss: 0.01242, accuracy: 0.99457, avg. loss over tasks: 0.01242, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13637, Variance: 0.01308
Semantic Loss - Mean: 0.02352, Variance: 0.00410

Test Epoch: 139 
task: sign, mean loss: 3.59923, accuracy: 0.36686, avg. loss over tasks: 3.59923
Diversity Loss - Mean: -0.13654, Variance: 0.01462
Semantic Loss - Mean: 2.99104, Variance: 0.03296

Train Epoch: 140 
task: sign, mean loss: 0.00355, accuracy: 1.00000, avg. loss over tasks: 0.00355, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.13592, Variance: 0.01308
Semantic Loss - Mean: 0.01184, Variance: 0.00407

Test Epoch: 140 
task: sign, mean loss: 3.47850, accuracy: 0.39645, avg. loss over tasks: 3.47850
Diversity Loss - Mean: -0.13710, Variance: 0.01461
Semantic Loss - Mean: 2.91654, Variance: 0.03290

Train Epoch: 141 
task: sign, mean loss: 0.00807, accuracy: 0.99457, avg. loss over tasks: 0.00807, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.13621, Variance: 0.01308
Semantic Loss - Mean: 0.01211, Variance: 0.00404

Test Epoch: 141 
task: sign, mean loss: 3.37346, accuracy: 0.40828, avg. loss over tasks: 3.37346
Diversity Loss - Mean: -0.13708, Variance: 0.01461
Semantic Loss - Mean: 2.84125, Variance: 0.03282

Train Epoch: 142 
task: sign, mean loss: 0.02380, accuracy: 0.99457, avg. loss over tasks: 0.02380, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.13596, Variance: 0.01308
Semantic Loss - Mean: 0.03776, Variance: 0.00402

Test Epoch: 142 
task: sign, mean loss: 3.24579, accuracy: 0.43195, avg. loss over tasks: 3.24579
Diversity Loss - Mean: -0.13696, Variance: 0.01461
Semantic Loss - Mean: 2.74586, Variance: 0.03274

Train Epoch: 143 
task: sign, mean loss: 0.00328, accuracy: 1.00000, avg. loss over tasks: 0.00328, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13622, Variance: 0.01308
Semantic Loss - Mean: 0.01499, Variance: 0.00400

Test Epoch: 143 
task: sign, mean loss: 3.45224, accuracy: 0.39645, avg. loss over tasks: 3.45224
Diversity Loss - Mean: -0.13681, Variance: 0.01460
Semantic Loss - Mean: 2.89518, Variance: 0.03268

Train Epoch: 144 
task: sign, mean loss: 0.00232, accuracy: 1.00000, avg. loss over tasks: 0.00232, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.13621, Variance: 0.01307
Semantic Loss - Mean: 0.01058, Variance: 0.00398

Test Epoch: 144 
task: sign, mean loss: 3.46008, accuracy: 0.39053, avg. loss over tasks: 3.46008
Diversity Loss - Mean: -0.13717, Variance: 0.01460
Semantic Loss - Mean: 2.91025, Variance: 0.03260

Train Epoch: 145 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13588, Variance: 0.01307
Semantic Loss - Mean: 0.00613, Variance: 0.00395

Test Epoch: 145 
task: sign, mean loss: 3.43963, accuracy: 0.39053, avg. loss over tasks: 3.43963
Diversity Loss - Mean: -0.13722, Variance: 0.01460
Semantic Loss - Mean: 2.89767, Variance: 0.03252

Train Epoch: 146 
task: sign, mean loss: 0.01924, accuracy: 0.99457, avg. loss over tasks: 0.01924, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.13597, Variance: 0.01307
Semantic Loss - Mean: 0.02600, Variance: 0.00393

Test Epoch: 146 
task: sign, mean loss: 3.65824, accuracy: 0.37278, avg. loss over tasks: 3.65824
Diversity Loss - Mean: -0.13661, Variance: 0.01460
Semantic Loss - Mean: 3.04027, Variance: 0.03245

Train Epoch: 147 
task: sign, mean loss: 0.00574, accuracy: 1.00000, avg. loss over tasks: 0.00574, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13622, Variance: 0.01307
Semantic Loss - Mean: 0.01016, Variance: 0.00391

Test Epoch: 147 
task: sign, mean loss: 3.57928, accuracy: 0.38462, avg. loss over tasks: 3.57928
Diversity Loss - Mean: -0.13693, Variance: 0.01459
Semantic Loss - Mean: 2.98402, Variance: 0.03237

Train Epoch: 148 
task: sign, mean loss: 0.01236, accuracy: 0.99457, avg. loss over tasks: 0.01236, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.13556, Variance: 0.01307
Semantic Loss - Mean: 0.02151, Variance: 0.00389

Test Epoch: 148 
task: sign, mean loss: 3.54798, accuracy: 0.38462, avg. loss over tasks: 3.54798
Diversity Loss - Mean: -0.13669, Variance: 0.01459
Semantic Loss - Mean: 2.95901, Variance: 0.03230

Train Epoch: 149 
task: sign, mean loss: 0.00172, accuracy: 1.00000, avg. loss over tasks: 0.00172, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.13608, Variance: 0.01306
Semantic Loss - Mean: 0.01144, Variance: 0.00387

Test Epoch: 149 
task: sign, mean loss: 3.44492, accuracy: 0.40237, avg. loss over tasks: 3.44492
Diversity Loss - Mean: -0.13689, Variance: 0.01458
Semantic Loss - Mean: 2.89828, Variance: 0.03223

Train Epoch: 150 
task: sign, mean loss: 0.00707, accuracy: 0.99457, avg. loss over tasks: 0.00707, lr: 3e-07
Diversity Loss - Mean: -0.13583, Variance: 0.01306
Semantic Loss - Mean: 0.02195, Variance: 0.00386

Test Epoch: 150 
task: sign, mean loss: 3.46469, accuracy: 0.40828, avg. loss over tasks: 3.46469
Diversity Loss - Mean: -0.13680, Variance: 0.01458
Semantic Loss - Mean: 2.90658, Variance: 0.03216

