Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09321, accuracy: 0.63587, avg. loss over tasks: 1.09321, lr: 3e-05
Diversity Loss - Mean: -0.01173, Variance: 0.01047
Semantic Loss - Mean: 1.43093, Variance: 0.07326

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.18123, accuracy: 0.66272, avg. loss over tasks: 1.18123
Diversity Loss - Mean: -0.03410, Variance: 0.01259
Semantic Loss - Mean: 1.16162, Variance: 0.05343

Train Epoch: 2 
task: sign, mean loss: 0.96473, accuracy: 0.65217, avg. loss over tasks: 0.96473, lr: 6e-05
Diversity Loss - Mean: -0.02890, Variance: 0.01043
Semantic Loss - Mean: 0.98434, Variance: 0.03948

Test Epoch: 2 
task: sign, mean loss: 1.10895, accuracy: 0.66272, avg. loss over tasks: 1.10895
Diversity Loss - Mean: -0.04962, Variance: 0.01244
Semantic Loss - Mean: 1.13979, Variance: 0.03188

Train Epoch: 3 
task: sign, mean loss: 0.79596, accuracy: 0.71196, avg. loss over tasks: 0.79596, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.06116, Variance: 0.01038
Semantic Loss - Mean: 0.98885, Variance: 0.02743

Test Epoch: 3 
task: sign, mean loss: 1.29500, accuracy: 0.56805, avg. loss over tasks: 1.29500
Diversity Loss - Mean: -0.08255, Variance: 0.01158
Semantic Loss - Mean: 1.11096, Variance: 0.02898

Train Epoch: 4 
task: sign, mean loss: 0.76603, accuracy: 0.71196, avg. loss over tasks: 0.76603, lr: 0.00012
Diversity Loss - Mean: -0.08787, Variance: 0.01023
Semantic Loss - Mean: 0.89280, Variance: 0.02118

Test Epoch: 4 
task: sign, mean loss: 1.35822, accuracy: 0.66272, avg. loss over tasks: 1.35822
Diversity Loss - Mean: -0.09455, Variance: 0.01087
Semantic Loss - Mean: 1.05205, Variance: 0.02344

Train Epoch: 5 
task: sign, mean loss: 0.74189, accuracy: 0.71196, avg. loss over tasks: 0.74189, lr: 0.00015
Diversity Loss - Mean: -0.08129, Variance: 0.00994
Semantic Loss - Mean: 0.78949, Variance: 0.01727

Test Epoch: 5 
task: sign, mean loss: 1.87788, accuracy: 0.44379, avg. loss over tasks: 1.87788
Diversity Loss - Mean: -0.08123, Variance: 0.01037
Semantic Loss - Mean: 1.28653, Variance: 0.02211

Train Epoch: 6 
task: sign, mean loss: 0.67623, accuracy: 0.76087, avg. loss over tasks: 0.67623, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.07386, Variance: 0.00976
Semantic Loss - Mean: 0.72096, Variance: 0.01478

Test Epoch: 6 
task: sign, mean loss: 1.73502, accuracy: 0.57988, avg. loss over tasks: 1.73502
Diversity Loss - Mean: -0.08633, Variance: 0.01044
Semantic Loss - Mean: 1.36478, Variance: 0.02199

Train Epoch: 7 
task: sign, mean loss: 0.58764, accuracy: 0.77717, avg. loss over tasks: 0.58764, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.07061, Variance: 0.00977
Semantic Loss - Mean: 0.60444, Variance: 0.01290

Test Epoch: 7 
task: sign, mean loss: 2.17089, accuracy: 0.30178, avg. loss over tasks: 2.17089
Diversity Loss - Mean: -0.03047, Variance: 0.01021
Semantic Loss - Mean: 1.85087, Variance: 0.02245

Train Epoch: 8 
task: sign, mean loss: 0.59037, accuracy: 0.80978, avg. loss over tasks: 0.59037, lr: 0.00024
Diversity Loss - Mean: -0.06070, Variance: 0.00975
Semantic Loss - Mean: 0.61394, Variance: 0.01164

Test Epoch: 8 
task: sign, mean loss: 2.25069, accuracy: 0.37870, avg. loss over tasks: 2.25069
Diversity Loss - Mean: -0.04859, Variance: 0.01017
Semantic Loss - Mean: 1.61531, Variance: 0.02662

Train Epoch: 9 
task: sign, mean loss: 0.67814, accuracy: 0.77717, avg. loss over tasks: 0.67814, lr: 0.00027
Diversity Loss - Mean: -0.07428, Variance: 0.00976
Semantic Loss - Mean: 0.66102, Variance: 0.01073

Test Epoch: 9 
task: sign, mean loss: 1.82101, accuracy: 0.63905, avg. loss over tasks: 1.82101
Diversity Loss - Mean: -0.08451, Variance: 0.01036
Semantic Loss - Mean: 1.51158, Variance: 0.02710

Train Epoch: 10 
task: sign, mean loss: 0.63006, accuracy: 0.73913, avg. loss over tasks: 0.63006, lr: 0.0003
Diversity Loss - Mean: -0.07160, Variance: 0.00970
Semantic Loss - Mean: 0.61688, Variance: 0.01005

Test Epoch: 10 
task: sign, mean loss: 3.69966, accuracy: 0.26036, avg. loss over tasks: 3.69966
Diversity Loss - Mean: -0.01317, Variance: 0.01045
Semantic Loss - Mean: 2.54410, Variance: 0.03515

Train Epoch: 11 
task: sign, mean loss: 0.64116, accuracy: 0.77174, avg. loss over tasks: 0.64116, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.06000, Variance: 0.00972
Semantic Loss - Mean: 0.61438, Variance: 0.00950

Test Epoch: 11 
task: sign, mean loss: 2.67426, accuracy: 0.65680, avg. loss over tasks: 2.67426
Diversity Loss - Mean: -0.03988, Variance: 0.01089
Semantic Loss - Mean: 2.33039, Variance: 0.03811

Train Epoch: 12 
task: sign, mean loss: 0.77390, accuracy: 0.77717, avg. loss over tasks: 0.77390, lr: 0.000299849111021216
Diversity Loss - Mean: -0.07172, Variance: 0.00990
Semantic Loss - Mean: 0.78510, Variance: 0.00916

Test Epoch: 12 
task: sign, mean loss: 2.15204, accuracy: 0.40828, avg. loss over tasks: 2.15204
Diversity Loss - Mean: -0.06692, Variance: 0.01127
Semantic Loss - Mean: 1.65322, Variance: 0.03748

Train Epoch: 13 
task: sign, mean loss: 0.66046, accuracy: 0.77174, avg. loss over tasks: 0.66046, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.08364, Variance: 0.01011
Semantic Loss - Mean: 0.70168, Variance: 0.00869

Test Epoch: 13 
task: sign, mean loss: 2.27976, accuracy: 0.27219, avg. loss over tasks: 2.27976
Diversity Loss - Mean: -0.00088, Variance: 0.01125
Semantic Loss - Mean: 1.68842, Variance: 0.03875

Train Epoch: 14 
task: sign, mean loss: 0.59580, accuracy: 0.75000, avg. loss over tasks: 0.59580, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.09008, Variance: 0.01021
Semantic Loss - Mean: 0.64856, Variance: 0.00824

Test Epoch: 14 
task: sign, mean loss: 1.61967, accuracy: 0.51479, avg. loss over tasks: 1.61967
Diversity Loss - Mean: -0.07732, Variance: 0.01127
Semantic Loss - Mean: 1.39456, Variance: 0.03773

Train Epoch: 15 
task: sign, mean loss: 0.68863, accuracy: 0.71196, avg. loss over tasks: 0.68863, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.09209, Variance: 0.01030
Semantic Loss - Mean: 0.71504, Variance: 0.00786

Test Epoch: 15 
task: sign, mean loss: 1.71849, accuracy: 0.65089, avg. loss over tasks: 1.71849
Diversity Loss - Mean: -0.08903, Variance: 0.01143
Semantic Loss - Mean: 1.55449, Variance: 0.03605

Train Epoch: 16 
task: sign, mean loss: 0.72717, accuracy: 0.72826, avg. loss over tasks: 0.72717, lr: 0.000298643821800925
Diversity Loss - Mean: -0.09520, Variance: 0.01047
Semantic Loss - Mean: 0.70839, Variance: 0.00754

Test Epoch: 16 
task: sign, mean loss: 2.09711, accuracy: 0.66272, avg. loss over tasks: 2.09711
Diversity Loss - Mean: -0.09562, Variance: 0.01162
Semantic Loss - Mean: 1.80754, Variance: 0.03444

Train Epoch: 17 
task: sign, mean loss: 0.78494, accuracy: 0.66848, avg. loss over tasks: 0.78494, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.10681, Variance: 0.01071
Semantic Loss - Mean: 0.78796, Variance: 0.00722

Test Epoch: 17 
task: sign, mean loss: 1.70308, accuracy: 0.66272, avg. loss over tasks: 1.70308
Diversity Loss - Mean: -0.09750, Variance: 0.01192
Semantic Loss - Mean: 1.53170, Variance: 0.03302

Train Epoch: 18 
task: sign, mean loss: 0.55040, accuracy: 0.80435, avg. loss over tasks: 0.55040, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.10337, Variance: 0.01091
Semantic Loss - Mean: 0.59046, Variance: 0.00697

Test Epoch: 18 
task: sign, mean loss: 2.18846, accuracy: 0.66272, avg. loss over tasks: 2.18846
Diversity Loss - Mean: -0.09599, Variance: 0.01225
Semantic Loss - Mean: 1.87688, Variance: 0.03194

Train Epoch: 19 
task: sign, mean loss: 0.26896, accuracy: 0.89130, avg. loss over tasks: 0.26896, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.08685, Variance: 0.01096
Semantic Loss - Mean: 0.31916, Variance: 0.00673

Test Epoch: 19 
task: sign, mean loss: 3.00517, accuracy: 0.29586, avg. loss over tasks: 3.00517
Diversity Loss - Mean: -0.07550, Variance: 0.01237
Semantic Loss - Mean: 2.36792, Variance: 0.03178

Train Epoch: 20 
task: sign, mean loss: 0.31613, accuracy: 0.88043, avg. loss over tasks: 0.31613, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.07987, Variance: 0.01100
Semantic Loss - Mean: 0.35398, Variance: 0.00654

Test Epoch: 20 
task: sign, mean loss: 2.30637, accuracy: 0.45562, avg. loss over tasks: 2.30637
Diversity Loss - Mean: -0.09805, Variance: 0.01251
Semantic Loss - Mean: 2.03122, Variance: 0.03162

Train Epoch: 21 
task: sign, mean loss: 0.32888, accuracy: 0.85870, avg. loss over tasks: 0.32888, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.08654, Variance: 0.01104
Semantic Loss - Mean: 0.38491, Variance: 0.00640

Test Epoch: 21 
task: sign, mean loss: 2.76666, accuracy: 0.62722, avg. loss over tasks: 2.76666
Diversity Loss - Mean: -0.08980, Variance: 0.01269
Semantic Loss - Mean: 2.42996, Variance: 0.03086

Train Epoch: 22 
task: sign, mean loss: 0.35621, accuracy: 0.85870, avg. loss over tasks: 0.35621, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.09397, Variance: 0.01109
Semantic Loss - Mean: 0.43874, Variance: 0.00639

Test Epoch: 22 
task: sign, mean loss: 2.85873, accuracy: 0.43195, avg. loss over tasks: 2.85873
Diversity Loss - Mean: -0.08456, Variance: 0.01284
Semantic Loss - Mean: 2.39771, Variance: 0.03063

Train Epoch: 23 
task: sign, mean loss: 0.27351, accuracy: 0.91304, avg. loss over tasks: 0.27351, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.09189, Variance: 0.01113
Semantic Loss - Mean: 0.30806, Variance: 0.00631

Test Epoch: 23 
task: sign, mean loss: 2.27107, accuracy: 0.53846, avg. loss over tasks: 2.27107
Diversity Loss - Mean: -0.11027, Variance: 0.01297
Semantic Loss - Mean: 1.90810, Variance: 0.02999

Train Epoch: 24 
task: sign, mean loss: 0.38063, accuracy: 0.85326, avg. loss over tasks: 0.38063, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.09667, Variance: 0.01119
Semantic Loss - Mean: 0.41714, Variance: 0.00625

Test Epoch: 24 
task: sign, mean loss: 4.04952, accuracy: 0.14201, avg. loss over tasks: 4.04952
Diversity Loss - Mean: -0.03076, Variance: 0.01314
Semantic Loss - Mean: 3.27065, Variance: 0.03097

Train Epoch: 25 
task: sign, mean loss: 0.30267, accuracy: 0.89130, avg. loss over tasks: 0.30267, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.09624, Variance: 0.01120
Semantic Loss - Mean: 0.37250, Variance: 0.00613

Test Epoch: 25 
task: sign, mean loss: 1.85679, accuracy: 0.50296, avg. loss over tasks: 1.85679
Diversity Loss - Mean: -0.09512, Variance: 0.01312
Semantic Loss - Mean: 1.71940, Variance: 0.03048

Train Epoch: 26 
task: sign, mean loss: 0.29623, accuracy: 0.88587, avg. loss over tasks: 0.29623, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.10216, Variance: 0.01123
Semantic Loss - Mean: 0.30709, Variance: 0.00598

Test Epoch: 26 
task: sign, mean loss: 2.21849, accuracy: 0.57396, avg. loss over tasks: 2.21849
Diversity Loss - Mean: -0.11219, Variance: 0.01317
Semantic Loss - Mean: 1.95469, Variance: 0.02967

Train Epoch: 27 
task: sign, mean loss: 0.29502, accuracy: 0.87500, avg. loss over tasks: 0.29502, lr: 0.000289228031029578
Diversity Loss - Mean: -0.09775, Variance: 0.01125
Semantic Loss - Mean: 0.33238, Variance: 0.00590

Test Epoch: 27 
task: sign, mean loss: 2.89461, accuracy: 0.42012, avg. loss over tasks: 2.89461
Diversity Loss - Mean: -0.10188, Variance: 0.01320
Semantic Loss - Mean: 2.43934, Variance: 0.02924

Train Epoch: 28 
task: sign, mean loss: 0.31366, accuracy: 0.88043, avg. loss over tasks: 0.31366, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.10153, Variance: 0.01130
Semantic Loss - Mean: 0.33992, Variance: 0.00588

Test Epoch: 28 
task: sign, mean loss: 2.93994, accuracy: 0.44379, avg. loss over tasks: 2.93994
Diversity Loss - Mean: -0.09565, Variance: 0.01325
Semantic Loss - Mean: 2.50225, Variance: 0.02876

Train Epoch: 29 
task: sign, mean loss: 0.39831, accuracy: 0.84239, avg. loss over tasks: 0.39831, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.10396, Variance: 0.01136
Semantic Loss - Mean: 0.42060, Variance: 0.00587

Test Epoch: 29 
task: sign, mean loss: 2.19322, accuracy: 0.52071, avg. loss over tasks: 2.19322
Diversity Loss - Mean: -0.11406, Variance: 0.01334
Semantic Loss - Mean: 1.88776, Variance: 0.02908

Train Epoch: 30 
task: sign, mean loss: 0.38144, accuracy: 0.84783, avg. loss over tasks: 0.38144, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.10795, Variance: 0.01143
Semantic Loss - Mean: 0.41999, Variance: 0.00583

Test Epoch: 30 
task: sign, mean loss: 3.72290, accuracy: 0.34320, avg. loss over tasks: 3.72290
Diversity Loss - Mean: -0.07831, Variance: 0.01343
Semantic Loss - Mean: 3.06313, Variance: 0.02869

Train Epoch: 31 
task: sign, mean loss: 0.39611, accuracy: 0.85870, avg. loss over tasks: 0.39611, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.10811, Variance: 0.01149
Semantic Loss - Mean: 0.44357, Variance: 0.00580

Test Epoch: 31 
task: sign, mean loss: 2.30522, accuracy: 0.62130, avg. loss over tasks: 2.30522
Diversity Loss - Mean: -0.12182, Variance: 0.01358
Semantic Loss - Mean: 1.99079, Variance: 0.02872

Train Epoch: 32 
task: sign, mean loss: 0.26900, accuracy: 0.89130, avg. loss over tasks: 0.26900, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.10670, Variance: 0.01153
Semantic Loss - Mean: 0.33013, Variance: 0.00573

Test Epoch: 32 
task: sign, mean loss: 2.27770, accuracy: 0.56213, avg. loss over tasks: 2.27770
Diversity Loss - Mean: -0.11348, Variance: 0.01367
Semantic Loss - Mean: 2.06540, Variance: 0.02836

Train Epoch: 33 
task: sign, mean loss: 0.16704, accuracy: 0.94565, avg. loss over tasks: 0.16704, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.10138, Variance: 0.01154
Semantic Loss - Mean: 0.18765, Variance: 0.00561

Test Epoch: 33 
task: sign, mean loss: 2.49721, accuracy: 0.65089, avg. loss over tasks: 2.49721
Diversity Loss - Mean: -0.11561, Variance: 0.01379
Semantic Loss - Mean: 2.13255, Variance: 0.02779

Train Epoch: 34 
task: sign, mean loss: 0.10314, accuracy: 0.97283, avg. loss over tasks: 0.10314, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.10291, Variance: 0.01155
Semantic Loss - Mean: 0.13702, Variance: 0.00552

Test Epoch: 34 
task: sign, mean loss: 2.60760, accuracy: 0.53254, avg. loss over tasks: 2.60760
Diversity Loss - Mean: -0.12027, Variance: 0.01391
Semantic Loss - Mean: 2.27281, Variance: 0.02796

Train Epoch: 35 
task: sign, mean loss: 0.12745, accuracy: 0.96739, avg. loss over tasks: 0.12745, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.10394, Variance: 0.01156
Semantic Loss - Mean: 0.15300, Variance: 0.00551

Test Epoch: 35 
task: sign, mean loss: 3.11851, accuracy: 0.66272, avg. loss over tasks: 3.11851
Diversity Loss - Mean: -0.12290, Variance: 0.01411
Semantic Loss - Mean: 2.62236, Variance: 0.02799

Train Epoch: 36 
task: sign, mean loss: 0.12756, accuracy: 0.95652, avg. loss over tasks: 0.12756, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.10499, Variance: 0.01156
Semantic Loss - Mean: 0.16159, Variance: 0.00549

Test Epoch: 36 
task: sign, mean loss: 3.46717, accuracy: 0.40237, avg. loss over tasks: 3.46717
Diversity Loss - Mean: -0.10340, Variance: 0.01414
Semantic Loss - Mean: 2.98343, Variance: 0.02811

Train Epoch: 37 
task: sign, mean loss: 0.11612, accuracy: 0.96196, avg. loss over tasks: 0.11612, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.11037, Variance: 0.01157
Semantic Loss - Mean: 0.16273, Variance: 0.00539

Test Epoch: 37 
task: sign, mean loss: 2.37681, accuracy: 0.54438, avg. loss over tasks: 2.37681
Diversity Loss - Mean: -0.12037, Variance: 0.01420
Semantic Loss - Mean: 2.04842, Variance: 0.02803

Train Epoch: 38 
task: sign, mean loss: 0.13742, accuracy: 0.93478, avg. loss over tasks: 0.13742, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.11153, Variance: 0.01157
Semantic Loss - Mean: 0.19779, Variance: 0.00541

Test Epoch: 38 
task: sign, mean loss: 4.44003, accuracy: 0.29586, avg. loss over tasks: 4.44003
Diversity Loss - Mean: -0.09497, Variance: 0.01428
Semantic Loss - Mean: 3.25343, Variance: 0.02876

Train Epoch: 39 
task: sign, mean loss: 0.10660, accuracy: 0.96196, avg. loss over tasks: 0.10660, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.11280, Variance: 0.01159
Semantic Loss - Mean: 0.12983, Variance: 0.00538

Test Epoch: 39 
task: sign, mean loss: 3.46341, accuracy: 0.37278, avg. loss over tasks: 3.46341
Diversity Loss - Mean: -0.11186, Variance: 0.01432
Semantic Loss - Mean: 2.71402, Variance: 0.02921

Train Epoch: 40 
task: sign, mean loss: 0.16184, accuracy: 0.94022, avg. loss over tasks: 0.16184, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.11449, Variance: 0.01161
Semantic Loss - Mean: 0.19271, Variance: 0.00537

Test Epoch: 40 
task: sign, mean loss: 3.38844, accuracy: 0.45562, avg. loss over tasks: 3.38844
Diversity Loss - Mean: -0.11683, Variance: 0.01434
Semantic Loss - Mean: 2.93864, Variance: 0.02902

Train Epoch: 41 
task: sign, mean loss: 0.16099, accuracy: 0.95652, avg. loss over tasks: 0.16099, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.11689, Variance: 0.01164
Semantic Loss - Mean: 0.16474, Variance: 0.00528

Test Epoch: 41 
task: sign, mean loss: 2.51609, accuracy: 0.36686, avg. loss over tasks: 2.51609
Diversity Loss - Mean: -0.11834, Variance: 0.01445
Semantic Loss - Mean: 2.18316, Variance: 0.02927

Train Epoch: 42 
task: sign, mean loss: 0.45567, accuracy: 0.84239, avg. loss over tasks: 0.45567, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.11917, Variance: 0.01169
Semantic Loss - Mean: 0.49102, Variance: 0.00535

Test Epoch: 42 
task: sign, mean loss: 2.29028, accuracy: 0.30178, avg. loss over tasks: 2.29028
Diversity Loss - Mean: -0.10786, Variance: 0.01463
Semantic Loss - Mean: 1.88270, Variance: 0.02944

Train Epoch: 43 
task: sign, mean loss: 1.45522, accuracy: 0.59239, avg. loss over tasks: 1.45522, lr: 0.000260757131773478
Diversity Loss - Mean: -0.12516, Variance: 0.01181
Semantic Loss - Mean: 1.40644, Variance: 0.00537

Test Epoch: 43 
task: sign, mean loss: 1.38280, accuracy: 0.47929, avg. loss over tasks: 1.38280
Diversity Loss - Mean: -0.12848, Variance: 0.01492
Semantic Loss - Mean: 1.26014, Variance: 0.02917

Train Epoch: 44 
task: sign, mean loss: 0.97546, accuracy: 0.64674, avg. loss over tasks: 0.97546, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.13092, Variance: 0.01202
Semantic Loss - Mean: 0.97917, Variance: 0.00528

Test Epoch: 44 
task: sign, mean loss: 1.02524, accuracy: 0.64497, avg. loss over tasks: 1.02524
Diversity Loss - Mean: -0.12406, Variance: 0.01511
Semantic Loss - Mean: 1.03116, Variance: 0.02877

Train Epoch: 45 
task: sign, mean loss: 0.98041, accuracy: 0.65217, avg. loss over tasks: 0.98041, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.13250, Variance: 0.01220
Semantic Loss - Mean: 1.00223, Variance: 0.00518

Test Epoch: 45 
task: sign, mean loss: 1.12902, accuracy: 0.58580, avg. loss over tasks: 1.12902
Diversity Loss - Mean: -0.12450, Variance: 0.01532
Semantic Loss - Mean: 1.13307, Variance: 0.02832

Train Epoch: 46 
task: sign, mean loss: 0.90581, accuracy: 0.66848, avg. loss over tasks: 0.90581, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.13343, Variance: 0.01236
Semantic Loss - Mean: 0.91280, Variance: 0.00509

Test Epoch: 46 
task: sign, mean loss: 1.18940, accuracy: 0.66272, avg. loss over tasks: 1.18940
Diversity Loss - Mean: -0.13241, Variance: 0.01553
Semantic Loss - Mean: 1.26243, Variance: 0.02782

Train Epoch: 47 
task: sign, mean loss: 0.86473, accuracy: 0.68478, avg. loss over tasks: 0.86473, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.13371, Variance: 0.01249
Semantic Loss - Mean: 0.87173, Variance: 0.00499

Test Epoch: 47 
task: sign, mean loss: 1.31280, accuracy: 0.64497, avg. loss over tasks: 1.31280
Diversity Loss - Mean: -0.13283, Variance: 0.01570
Semantic Loss - Mean: 1.26686, Variance: 0.02731

Train Epoch: 48 
task: sign, mean loss: 0.76601, accuracy: 0.66848, avg. loss over tasks: 0.76601, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.13225, Variance: 0.01259
Semantic Loss - Mean: 0.77443, Variance: 0.00490

Test Epoch: 48 
task: sign, mean loss: 1.15258, accuracy: 0.63905, avg. loss over tasks: 1.15258
Diversity Loss - Mean: -0.13037, Variance: 0.01584
Semantic Loss - Mean: 1.15902, Variance: 0.02682

Train Epoch: 49 
task: sign, mean loss: 0.80367, accuracy: 0.69565, avg. loss over tasks: 0.80367, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.13195, Variance: 0.01269
Semantic Loss - Mean: 0.81977, Variance: 0.00482

Test Epoch: 49 
task: sign, mean loss: 1.14927, accuracy: 0.54438, avg. loss over tasks: 1.14927
Diversity Loss - Mean: -0.12874, Variance: 0.01592
Semantic Loss - Mean: 1.13443, Variance: 0.02634

Train Epoch: 50 
task: sign, mean loss: 0.73975, accuracy: 0.71739, avg. loss over tasks: 0.73975, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.12989, Variance: 0.01276
Semantic Loss - Mean: 0.75068, Variance: 0.00475

Test Epoch: 50 
task: sign, mean loss: 1.63073, accuracy: 0.52663, avg. loss over tasks: 1.63073
Diversity Loss - Mean: -0.12568, Variance: 0.01598
Semantic Loss - Mean: 1.56662, Variance: 0.02590

Train Epoch: 51 
task: sign, mean loss: 0.68091, accuracy: 0.73370, avg. loss over tasks: 0.68091, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.12900, Variance: 0.01283
Semantic Loss - Mean: 0.68814, Variance: 0.00467

Test Epoch: 51 
task: sign, mean loss: 1.39321, accuracy: 0.44379, avg. loss over tasks: 1.39321
Diversity Loss - Mean: -0.12615, Variance: 0.01606
Semantic Loss - Mean: 1.39498, Variance: 0.02551

Train Epoch: 52 
task: sign, mean loss: 0.64457, accuracy: 0.72283, avg. loss over tasks: 0.64457, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.12924, Variance: 0.01289
Semantic Loss - Mean: 0.65418, Variance: 0.00460

Test Epoch: 52 
task: sign, mean loss: 1.73000, accuracy: 0.28402, avg. loss over tasks: 1.73000
Diversity Loss - Mean: -0.12288, Variance: 0.01613
Semantic Loss - Mean: 1.68834, Variance: 0.02511

Train Epoch: 53 
task: sign, mean loss: 0.65917, accuracy: 0.73370, avg. loss over tasks: 0.65917, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.12897, Variance: 0.01294
Semantic Loss - Mean: 0.68247, Variance: 0.00453

Test Epoch: 53 
task: sign, mean loss: 2.58880, accuracy: 0.26627, avg. loss over tasks: 2.58880
Diversity Loss - Mean: -0.10510, Variance: 0.01615
Semantic Loss - Mean: 2.26690, Variance: 0.02528

Train Epoch: 54 
task: sign, mean loss: 0.59315, accuracy: 0.77174, avg. loss over tasks: 0.59315, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.12654, Variance: 0.01299
Semantic Loss - Mean: 0.60959, Variance: 0.00447

Test Epoch: 54 
task: sign, mean loss: 1.68625, accuracy: 0.48521, avg. loss over tasks: 1.68625
Diversity Loss - Mean: -0.12485, Variance: 0.01615
Semantic Loss - Mean: 1.52856, Variance: 0.02490

Train Epoch: 55 
task: sign, mean loss: 0.57317, accuracy: 0.76630, avg. loss over tasks: 0.57317, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.12650, Variance: 0.01303
Semantic Loss - Mean: 0.60986, Variance: 0.00442

Test Epoch: 55 
task: sign, mean loss: 1.73867, accuracy: 0.47929, avg. loss over tasks: 1.73867
Diversity Loss - Mean: -0.12326, Variance: 0.01616
Semantic Loss - Mean: 1.69483, Variance: 0.02466

Train Epoch: 56 
task: sign, mean loss: 0.71859, accuracy: 0.71739, avg. loss over tasks: 0.71859, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.12689, Variance: 0.01308
Semantic Loss - Mean: 0.73085, Variance: 0.00438

Test Epoch: 56 
task: sign, mean loss: 1.35767, accuracy: 0.48521, avg. loss over tasks: 1.35767
Diversity Loss - Mean: -0.12378, Variance: 0.01619
Semantic Loss - Mean: 1.26125, Variance: 0.02437

Train Epoch: 57 
task: sign, mean loss: 0.80332, accuracy: 0.67391, avg. loss over tasks: 0.80332, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.12993, Variance: 0.01315
Semantic Loss - Mean: 0.80213, Variance: 0.00433

Test Epoch: 57 
task: sign, mean loss: 1.84156, accuracy: 0.26036, avg. loss over tasks: 1.84156
Diversity Loss - Mean: -0.10989, Variance: 0.01626
Semantic Loss - Mean: 1.58686, Variance: 0.02407

Train Epoch: 58 
task: sign, mean loss: 0.82677, accuracy: 0.66848, avg. loss over tasks: 0.82677, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.13091, Variance: 0.01324
Semantic Loss - Mean: 0.82464, Variance: 0.00428

Test Epoch: 58 
task: sign, mean loss: 1.21280, accuracy: 0.43787, avg. loss over tasks: 1.21280
Diversity Loss - Mean: -0.12780, Variance: 0.01637
Semantic Loss - Mean: 1.10991, Variance: 0.02369

Train Epoch: 59 
task: sign, mean loss: 0.76125, accuracy: 0.76087, avg. loss over tasks: 0.76125, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.13188, Variance: 0.01334
Semantic Loss - Mean: 0.78896, Variance: 0.00422

Test Epoch: 59 
task: sign, mean loss: 1.77022, accuracy: 0.40237, avg. loss over tasks: 1.77022
Diversity Loss - Mean: -0.11979, Variance: 0.01642
Semantic Loss - Mean: 1.56825, Variance: 0.02343

Train Epoch: 60 
task: sign, mean loss: 0.75257, accuracy: 0.69022, avg. loss over tasks: 0.75257, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.13069, Variance: 0.01341
Semantic Loss - Mean: 0.76821, Variance: 0.00416

Test Epoch: 60 
task: sign, mean loss: 1.48005, accuracy: 0.47929, avg. loss over tasks: 1.48005
Diversity Loss - Mean: -0.12597, Variance: 0.01645
Semantic Loss - Mean: 1.38229, Variance: 0.02310

Train Epoch: 61 
task: sign, mean loss: 0.62699, accuracy: 0.78261, avg. loss over tasks: 0.62699, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.13041, Variance: 0.01346
Semantic Loss - Mean: 0.64840, Variance: 0.00411

Test Epoch: 61 
task: sign, mean loss: 2.45905, accuracy: 0.26036, avg. loss over tasks: 2.45905
Diversity Loss - Mean: -0.09887, Variance: 0.01648
Semantic Loss - Mean: 2.04365, Variance: 0.02295

Train Epoch: 62 
task: sign, mean loss: 0.56231, accuracy: 0.79348, avg. loss over tasks: 0.56231, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.12724, Variance: 0.01349
Semantic Loss - Mean: 0.58564, Variance: 0.00406

Test Epoch: 62 
task: sign, mean loss: 2.02505, accuracy: 0.35503, avg. loss over tasks: 2.02505
Diversity Loss - Mean: -0.11831, Variance: 0.01652
Semantic Loss - Mean: 1.73423, Variance: 0.02274

Train Epoch: 63 
task: sign, mean loss: 0.49401, accuracy: 0.77717, avg. loss over tasks: 0.49401, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.12469, Variance: 0.01350
Semantic Loss - Mean: 0.51086, Variance: 0.00402

Test Epoch: 63 
task: sign, mean loss: 1.62082, accuracy: 0.50888, avg. loss over tasks: 1.62082
Diversity Loss - Mean: -0.12636, Variance: 0.01656
Semantic Loss - Mean: 1.56040, Variance: 0.02244

Train Epoch: 64 
task: sign, mean loss: 0.48332, accuracy: 0.84783, avg. loss over tasks: 0.48332, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.12435, Variance: 0.01351
Semantic Loss - Mean: 0.49250, Variance: 0.00398

Test Epoch: 64 
task: sign, mean loss: 1.79583, accuracy: 0.44970, avg. loss over tasks: 1.79583
Diversity Loss - Mean: -0.12001, Variance: 0.01655
Semantic Loss - Mean: 1.66329, Variance: 0.02218

Train Epoch: 65 
task: sign, mean loss: 0.53617, accuracy: 0.82065, avg. loss over tasks: 0.53617, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.12375, Variance: 0.01350
Semantic Loss - Mean: 0.56045, Variance: 0.00395

Test Epoch: 65 
task: sign, mean loss: 2.89238, accuracy: 0.24852, avg. loss over tasks: 2.89238
Diversity Loss - Mean: -0.10284, Variance: 0.01652
Semantic Loss - Mean: 2.34421, Variance: 0.02207

Train Epoch: 66 
task: sign, mean loss: 0.50386, accuracy: 0.80978, avg. loss over tasks: 0.50386, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.12583, Variance: 0.01350
Semantic Loss - Mean: 0.52354, Variance: 0.00392

Test Epoch: 66 
task: sign, mean loss: 2.32542, accuracy: 0.36686, avg. loss over tasks: 2.32542
Diversity Loss - Mean: -0.10963, Variance: 0.01649
Semantic Loss - Mean: 1.81490, Variance: 0.02185

Train Epoch: 67 
task: sign, mean loss: 0.41659, accuracy: 0.83696, avg. loss over tasks: 0.41659, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.12580, Variance: 0.01349
Semantic Loss - Mean: 0.43548, Variance: 0.00390

Test Epoch: 67 
task: sign, mean loss: 1.68345, accuracy: 0.57988, avg. loss over tasks: 1.68345
Diversity Loss - Mean: -0.12737, Variance: 0.01648
Semantic Loss - Mean: 1.54154, Variance: 0.02160

Train Epoch: 68 
task: sign, mean loss: 0.55967, accuracy: 0.75543, avg. loss over tasks: 0.55967, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.12789, Variance: 0.01350
Semantic Loss - Mean: 0.60465, Variance: 0.00389

Test Epoch: 68 
task: sign, mean loss: 2.14974, accuracy: 0.31361, avg. loss over tasks: 2.14974
Diversity Loss - Mean: -0.11858, Variance: 0.01646
Semantic Loss - Mean: 1.83607, Variance: 0.02138

Train Epoch: 69 
task: sign, mean loss: 0.43328, accuracy: 0.82065, avg. loss over tasks: 0.43328, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.12689, Variance: 0.01349
Semantic Loss - Mean: 0.48662, Variance: 0.00387

Test Epoch: 69 
task: sign, mean loss: 2.26239, accuracy: 0.37870, avg. loss over tasks: 2.26239
Diversity Loss - Mean: -0.11742, Variance: 0.01645
Semantic Loss - Mean: 1.68994, Variance: 0.02122

Train Epoch: 70 
task: sign, mean loss: 0.39676, accuracy: 0.86413, avg. loss over tasks: 0.39676, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.12833, Variance: 0.01349
Semantic Loss - Mean: 0.45299, Variance: 0.00385

Test Epoch: 70 
task: sign, mean loss: 3.03544, accuracy: 0.30769, avg. loss over tasks: 3.03544
Diversity Loss - Mean: -0.09953, Variance: 0.01640
Semantic Loss - Mean: 2.16198, Variance: 0.02113

Train Epoch: 71 
task: sign, mean loss: 0.41736, accuracy: 0.85326, avg. loss over tasks: 0.41736, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.12437, Variance: 0.01347
Semantic Loss - Mean: 0.43791, Variance: 0.00383

Test Epoch: 71 
task: sign, mean loss: 2.93672, accuracy: 0.31361, avg. loss over tasks: 2.93672
Diversity Loss - Mean: -0.10812, Variance: 0.01639
Semantic Loss - Mean: 2.19065, Variance: 0.02107

Train Epoch: 72 
task: sign, mean loss: 0.49393, accuracy: 0.80978, avg. loss over tasks: 0.49393, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.12442, Variance: 0.01345
Semantic Loss - Mean: 0.52518, Variance: 0.00380

Test Epoch: 72 
task: sign, mean loss: 2.59177, accuracy: 0.33728, avg. loss over tasks: 2.59177
Diversity Loss - Mean: -0.12198, Variance: 0.01640
Semantic Loss - Mean: 2.21309, Variance: 0.02086

Train Epoch: 73 
task: sign, mean loss: 0.69096, accuracy: 0.77174, avg. loss over tasks: 0.69096, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.12564, Variance: 0.01344
Semantic Loss - Mean: 0.73869, Variance: 0.00381

Test Epoch: 73 
task: sign, mean loss: 1.73015, accuracy: 0.49704, avg. loss over tasks: 1.73015
Diversity Loss - Mean: -0.12589, Variance: 0.01639
Semantic Loss - Mean: 1.60530, Variance: 0.02066

Train Epoch: 74 
task: sign, mean loss: 0.55172, accuracy: 0.77174, avg. loss over tasks: 0.55172, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.12708, Variance: 0.01343
Semantic Loss - Mean: 0.57525, Variance: 0.00379

Test Epoch: 74 
task: sign, mean loss: 2.36183, accuracy: 0.32544, avg. loss over tasks: 2.36183
Diversity Loss - Mean: -0.12099, Variance: 0.01640
Semantic Loss - Mean: 1.97408, Variance: 0.02046

Train Epoch: 75 
task: sign, mean loss: 0.57309, accuracy: 0.75543, avg. loss over tasks: 0.57309, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.13011, Variance: 0.01345
Semantic Loss - Mean: 0.59638, Variance: 0.00376

Test Epoch: 75 
task: sign, mean loss: 2.81765, accuracy: 0.24260, avg. loss over tasks: 2.81765
Diversity Loss - Mean: -0.10982, Variance: 0.01642
Semantic Loss - Mean: 2.18670, Variance: 0.02043

Train Epoch: 76 
task: sign, mean loss: 0.49623, accuracy: 0.81522, avg. loss over tasks: 0.49623, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.13030, Variance: 0.01346
Semantic Loss - Mean: 0.52293, Variance: 0.00373

Test Epoch: 76 
task: sign, mean loss: 1.93946, accuracy: 0.28994, avg. loss over tasks: 1.93946
Diversity Loss - Mean: -0.12523, Variance: 0.01646
Semantic Loss - Mean: 1.65761, Variance: 0.02021

Train Epoch: 77 
task: sign, mean loss: 0.42902, accuracy: 0.82065, avg. loss over tasks: 0.42902, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.12883, Variance: 0.01346
Semantic Loss - Mean: 0.44754, Variance: 0.00370

Test Epoch: 77 
task: sign, mean loss: 2.09348, accuracy: 0.44379, avg. loss over tasks: 2.09348
Diversity Loss - Mean: -0.12342, Variance: 0.01647
Semantic Loss - Mean: 1.86261, Variance: 0.02001

Train Epoch: 78 
task: sign, mean loss: 0.42760, accuracy: 0.87500, avg. loss over tasks: 0.42760, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.12774, Variance: 0.01345
Semantic Loss - Mean: 0.45775, Variance: 0.00368

Test Epoch: 78 
task: sign, mean loss: 2.19096, accuracy: 0.39645, avg. loss over tasks: 2.19096
Diversity Loss - Mean: -0.12447, Variance: 0.01648
Semantic Loss - Mean: 1.95769, Variance: 0.01983

Train Epoch: 79 
task: sign, mean loss: 0.39619, accuracy: 0.87500, avg. loss over tasks: 0.39619, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.12692, Variance: 0.01344
Semantic Loss - Mean: 0.42647, Variance: 0.00365

Test Epoch: 79 
task: sign, mean loss: 3.48853, accuracy: 0.25444, avg. loss over tasks: 3.48853
Diversity Loss - Mean: -0.10552, Variance: 0.01646
Semantic Loss - Mean: 2.61670, Variance: 0.01974

Train Epoch: 80 
task: sign, mean loss: 0.32192, accuracy: 0.87500, avg. loss over tasks: 0.32192, lr: 0.00015015
Diversity Loss - Mean: -0.12627, Variance: 0.01342
Semantic Loss - Mean: 0.36115, Variance: 0.00362

Test Epoch: 80 
task: sign, mean loss: 2.55002, accuracy: 0.44379, avg. loss over tasks: 2.55002
Diversity Loss - Mean: -0.12415, Variance: 0.01647
Semantic Loss - Mean: 2.24047, Variance: 0.01961

Train Epoch: 81 
task: sign, mean loss: 0.37216, accuracy: 0.86957, avg. loss over tasks: 0.37216, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.12735, Variance: 0.01341
Semantic Loss - Mean: 0.41118, Variance: 0.00360

Test Epoch: 81 
task: sign, mean loss: 3.80027, accuracy: 0.23077, avg. loss over tasks: 3.80027
Diversity Loss - Mean: -0.10254, Variance: 0.01645
Semantic Loss - Mean: 2.77004, Variance: 0.01974

Train Epoch: 82 
task: sign, mean loss: 0.28659, accuracy: 0.92391, avg. loss over tasks: 0.28659, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.12705, Variance: 0.01340
Semantic Loss - Mean: 0.30387, Variance: 0.00357

Test Epoch: 82 
task: sign, mean loss: 2.93833, accuracy: 0.28994, avg. loss over tasks: 2.93833
Diversity Loss - Mean: -0.11414, Variance: 0.01644
Semantic Loss - Mean: 2.18463, Variance: 0.01971

Train Epoch: 83 
task: sign, mean loss: 0.29936, accuracy: 0.90761, avg. loss over tasks: 0.29936, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12665, Variance: 0.01339
Semantic Loss - Mean: 0.32838, Variance: 0.00356

Test Epoch: 83 
task: sign, mean loss: 2.73157, accuracy: 0.37870, avg. loss over tasks: 2.73157
Diversity Loss - Mean: -0.11959, Variance: 0.01643
Semantic Loss - Mean: 2.19974, Variance: 0.01963

Train Epoch: 84 
task: sign, mean loss: 0.29483, accuracy: 0.88043, avg. loss over tasks: 0.29483, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.12607, Variance: 0.01337
Semantic Loss - Mean: 0.32645, Variance: 0.00355

Test Epoch: 84 
task: sign, mean loss: 2.73424, accuracy: 0.39053, avg. loss over tasks: 2.73424
Diversity Loss - Mean: -0.11962, Variance: 0.01641
Semantic Loss - Mean: 2.22508, Variance: 0.01957

Train Epoch: 85 
task: sign, mean loss: 0.20741, accuracy: 0.91304, avg. loss over tasks: 0.20741, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.12410, Variance: 0.01335
Semantic Loss - Mean: 0.24443, Variance: 0.00354

Test Epoch: 85 
task: sign, mean loss: 2.41860, accuracy: 0.45562, avg. loss over tasks: 2.41860
Diversity Loss - Mean: -0.12671, Variance: 0.01642
Semantic Loss - Mean: 2.17561, Variance: 0.01947

Train Epoch: 86 
task: sign, mean loss: 0.18679, accuracy: 0.94565, avg. loss over tasks: 0.18679, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.12433, Variance: 0.01333
Semantic Loss - Mean: 0.21646, Variance: 0.00353

Test Epoch: 86 
task: sign, mean loss: 3.12210, accuracy: 0.36686, avg. loss over tasks: 3.12210
Diversity Loss - Mean: -0.10732, Variance: 0.01640
Semantic Loss - Mean: 2.46783, Variance: 0.01940

Train Epoch: 87 
task: sign, mean loss: 0.19781, accuracy: 0.91304, avg. loss over tasks: 0.19781, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.12371, Variance: 0.01331
Semantic Loss - Mean: 0.25054, Variance: 0.00355

Test Epoch: 87 
task: sign, mean loss: 3.52729, accuracy: 0.33136, avg. loss over tasks: 3.52729
Diversity Loss - Mean: -0.11499, Variance: 0.01639
Semantic Loss - Mean: 2.51588, Variance: 0.01934

Train Epoch: 88 
task: sign, mean loss: 0.20441, accuracy: 0.95109, avg. loss over tasks: 0.20441, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.12512, Variance: 0.01329
Semantic Loss - Mean: 0.23338, Variance: 0.00354

Test Epoch: 88 
task: sign, mean loss: 3.45368, accuracy: 0.34911, avg. loss over tasks: 3.45368
Diversity Loss - Mean: -0.11453, Variance: 0.01638
Semantic Loss - Mean: 2.60157, Variance: 0.01937

Train Epoch: 89 
task: sign, mean loss: 0.15423, accuracy: 0.94022, avg. loss over tasks: 0.15423, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.12583, Variance: 0.01328
Semantic Loss - Mean: 0.15764, Variance: 0.00353

Test Epoch: 89 
task: sign, mean loss: 2.62666, accuracy: 0.42604, avg. loss over tasks: 2.62666
Diversity Loss - Mean: -0.12069, Variance: 0.01638
Semantic Loss - Mean: 2.19966, Variance: 0.01937

Train Epoch: 90 
task: sign, mean loss: 0.23140, accuracy: 0.92391, avg. loss over tasks: 0.23140, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.12625, Variance: 0.01326
Semantic Loss - Mean: 0.29044, Variance: 0.00353

Test Epoch: 90 
task: sign, mean loss: 3.23631, accuracy: 0.36686, avg. loss over tasks: 3.23631
Diversity Loss - Mean: -0.11916, Variance: 0.01636
Semantic Loss - Mean: 2.56466, Variance: 0.01964

Train Epoch: 91 
task: sign, mean loss: 0.15731, accuracy: 0.94565, avg. loss over tasks: 0.15731, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.12572, Variance: 0.01324
Semantic Loss - Mean: 0.20009, Variance: 0.00357

Test Epoch: 91 
task: sign, mean loss: 4.72667, accuracy: 0.26036, avg. loss over tasks: 4.72667
Diversity Loss - Mean: -0.10713, Variance: 0.01633
Semantic Loss - Mean: 2.99388, Variance: 0.01987

Train Epoch: 92 
task: sign, mean loss: 0.17160, accuracy: 0.95109, avg. loss over tasks: 0.17160, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.12438, Variance: 0.01322
Semantic Loss - Mean: 0.19964, Variance: 0.00356

Test Epoch: 92 
task: sign, mean loss: 4.22739, accuracy: 0.27811, avg. loss over tasks: 4.22739
Diversity Loss - Mean: -0.10630, Variance: 0.01631
Semantic Loss - Mean: 2.83910, Variance: 0.01989

Train Epoch: 93 
task: sign, mean loss: 0.11781, accuracy: 0.94565, avg. loss over tasks: 0.11781, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.12495, Variance: 0.01320
Semantic Loss - Mean: 0.15844, Variance: 0.00360

Test Epoch: 93 
task: sign, mean loss: 2.75361, accuracy: 0.39053, avg. loss over tasks: 2.75361
Diversity Loss - Mean: -0.12459, Variance: 0.01630
Semantic Loss - Mean: 2.23641, Variance: 0.01987

Train Epoch: 94 
task: sign, mean loss: 0.08746, accuracy: 0.97283, avg. loss over tasks: 0.08746, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.12465, Variance: 0.01318
Semantic Loss - Mean: 0.11154, Variance: 0.00360

Test Epoch: 94 
task: sign, mean loss: 2.79771, accuracy: 0.37870, avg. loss over tasks: 2.79771
Diversity Loss - Mean: -0.11921, Variance: 0.01628
Semantic Loss - Mean: 2.38202, Variance: 0.01995

Train Epoch: 95 
task: sign, mean loss: 0.10171, accuracy: 0.97283, avg. loss over tasks: 0.10171, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.12339, Variance: 0.01316
Semantic Loss - Mean: 0.14063, Variance: 0.00359

Test Epoch: 95 
task: sign, mean loss: 2.94156, accuracy: 0.40237, avg. loss over tasks: 2.94156
Diversity Loss - Mean: -0.12192, Variance: 0.01626
Semantic Loss - Mean: 2.46768, Variance: 0.02000

Train Epoch: 96 
task: sign, mean loss: 0.10037, accuracy: 0.97283, avg. loss over tasks: 0.10037, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.12249, Variance: 0.01313
Semantic Loss - Mean: 0.10106, Variance: 0.00356

Test Epoch: 96 
task: sign, mean loss: 3.02541, accuracy: 0.39645, avg. loss over tasks: 3.02541
Diversity Loss - Mean: -0.11863, Variance: 0.01623
Semantic Loss - Mean: 2.43552, Variance: 0.02002

Train Epoch: 97 
task: sign, mean loss: 0.11689, accuracy: 0.94565, avg. loss over tasks: 0.11689, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.12569, Variance: 0.01311
Semantic Loss - Mean: 0.16268, Variance: 0.00357

Test Epoch: 97 
task: sign, mean loss: 2.82749, accuracy: 0.39053, avg. loss over tasks: 2.82749
Diversity Loss - Mean: -0.11216, Variance: 0.01619
Semantic Loss - Mean: 2.31247, Variance: 0.02032

Train Epoch: 98 
task: sign, mean loss: 0.08451, accuracy: 0.97283, avg. loss over tasks: 0.08451, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.12411, Variance: 0.01308
Semantic Loss - Mean: 0.11037, Variance: 0.00355

Test Epoch: 98 
task: sign, mean loss: 2.62848, accuracy: 0.42604, avg. loss over tasks: 2.62848
Diversity Loss - Mean: -0.11552, Variance: 0.01617
Semantic Loss - Mean: 2.27801, Variance: 0.02043

Train Epoch: 99 
task: sign, mean loss: 0.13269, accuracy: 0.97283, avg. loss over tasks: 0.13269, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.12302, Variance: 0.01305
Semantic Loss - Mean: 0.14292, Variance: 0.00356

Test Epoch: 99 
task: sign, mean loss: 3.60830, accuracy: 0.32544, avg. loss over tasks: 3.60830
Diversity Loss - Mean: -0.11186, Variance: 0.01614
Semantic Loss - Mean: 2.94185, Variance: 0.02062

Train Epoch: 100 
task: sign, mean loss: 0.08460, accuracy: 0.97283, avg. loss over tasks: 0.08460, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.12586, Variance: 0.01303
Semantic Loss - Mean: 0.13062, Variance: 0.00358

Test Epoch: 100 
task: sign, mean loss: 2.86507, accuracy: 0.40237, avg. loss over tasks: 2.86507
Diversity Loss - Mean: -0.11796, Variance: 0.01612
Semantic Loss - Mean: 2.33072, Variance: 0.02077

Train Epoch: 101 
task: sign, mean loss: 0.06170, accuracy: 0.98913, avg. loss over tasks: 0.06170, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.12579, Variance: 0.01301
Semantic Loss - Mean: 0.10392, Variance: 0.00360

Test Epoch: 101 
task: sign, mean loss: 3.28160, accuracy: 0.38462, avg. loss over tasks: 3.28160
Diversity Loss - Mean: -0.11962, Variance: 0.01610
Semantic Loss - Mean: 2.74350, Variance: 0.02102

Train Epoch: 102 
task: sign, mean loss: 0.04613, accuracy: 0.98913, avg. loss over tasks: 0.04613, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.12596, Variance: 0.01298
Semantic Loss - Mean: 0.08003, Variance: 0.00361

Test Epoch: 102 
task: sign, mean loss: 2.67644, accuracy: 0.46746, avg. loss over tasks: 2.67644
Diversity Loss - Mean: -0.12179, Variance: 0.01608
Semantic Loss - Mean: 2.23214, Variance: 0.02124

Train Epoch: 103 
task: sign, mean loss: 0.08472, accuracy: 0.96196, avg. loss over tasks: 0.08472, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.12711, Variance: 0.01296
Semantic Loss - Mean: 0.12719, Variance: 0.00360

Test Epoch: 103 
task: sign, mean loss: 3.13961, accuracy: 0.43195, avg. loss over tasks: 3.13961
Diversity Loss - Mean: -0.12238, Variance: 0.01606
Semantic Loss - Mean: 2.51685, Variance: 0.02146

Train Epoch: 104 
task: sign, mean loss: 0.05580, accuracy: 0.98370, avg. loss over tasks: 0.05580, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.12686, Variance: 0.01294
Semantic Loss - Mean: 0.07534, Variance: 0.00360

Test Epoch: 104 
task: sign, mean loss: 3.23555, accuracy: 0.42604, avg. loss over tasks: 3.23555
Diversity Loss - Mean: -0.11912, Variance: 0.01604
Semantic Loss - Mean: 2.55857, Variance: 0.02181

Train Epoch: 105 
task: sign, mean loss: 0.02721, accuracy: 0.98913, avg. loss over tasks: 0.02721, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.12630, Variance: 0.01291
Semantic Loss - Mean: 0.05015, Variance: 0.00357

Test Epoch: 105 
task: sign, mean loss: 3.52797, accuracy: 0.39053, avg. loss over tasks: 3.52797
Diversity Loss - Mean: -0.11808, Variance: 0.01602
Semantic Loss - Mean: 2.84557, Variance: 0.02215

Train Epoch: 106 
task: sign, mean loss: 0.04764, accuracy: 0.98913, avg. loss over tasks: 0.04764, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.12663, Variance: 0.01289
Semantic Loss - Mean: 0.06402, Variance: 0.00355

Test Epoch: 106 
task: sign, mean loss: 3.33530, accuracy: 0.41420, avg. loss over tasks: 3.33530
Diversity Loss - Mean: -0.11973, Variance: 0.01600
Semantic Loss - Mean: 2.69109, Variance: 0.02235

Train Epoch: 107 
task: sign, mean loss: 0.02017, accuracy: 1.00000, avg. loss over tasks: 0.02017, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.12691, Variance: 0.01287
Semantic Loss - Mean: 0.04070, Variance: 0.00354

Test Epoch: 107 
task: sign, mean loss: 2.94944, accuracy: 0.44970, avg. loss over tasks: 2.94944
Diversity Loss - Mean: -0.12182, Variance: 0.01597
Semantic Loss - Mean: 2.40529, Variance: 0.02245

Train Epoch: 108 
task: sign, mean loss: 0.04050, accuracy: 0.97283, avg. loss over tasks: 0.04050, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.12684, Variance: 0.01285
Semantic Loss - Mean: 0.06137, Variance: 0.00353

Test Epoch: 108 
task: sign, mean loss: 3.14442, accuracy: 0.43787, avg. loss over tasks: 3.14442
Diversity Loss - Mean: -0.12175, Variance: 0.01596
Semantic Loss - Mean: 2.58425, Variance: 0.02263

Train Epoch: 109 
task: sign, mean loss: 0.04345, accuracy: 0.98370, avg. loss over tasks: 0.04345, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.12710, Variance: 0.01282
Semantic Loss - Mean: 0.05836, Variance: 0.00352

Test Epoch: 109 
task: sign, mean loss: 2.88876, accuracy: 0.43195, avg. loss over tasks: 2.88876
Diversity Loss - Mean: -0.12151, Variance: 0.01593
Semantic Loss - Mean: 2.42929, Variance: 0.02273

Train Epoch: 110 
task: sign, mean loss: 0.02984, accuracy: 0.98913, avg. loss over tasks: 0.02984, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.12787, Variance: 0.01280
Semantic Loss - Mean: 0.05217, Variance: 0.00352

Test Epoch: 110 
task: sign, mean loss: 3.83163, accuracy: 0.38462, avg. loss over tasks: 3.83163
Diversity Loss - Mean: -0.11767, Variance: 0.01591
Semantic Loss - Mean: 3.02745, Variance: 0.02299

Train Epoch: 111 
task: sign, mean loss: 0.04325, accuracy: 0.98913, avg. loss over tasks: 0.04325, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.12766, Variance: 0.01278
Semantic Loss - Mean: 0.07613, Variance: 0.00352

Test Epoch: 111 
task: sign, mean loss: 3.49702, accuracy: 0.41420, avg. loss over tasks: 3.49702
Diversity Loss - Mean: -0.12109, Variance: 0.01589
Semantic Loss - Mean: 2.63896, Variance: 0.02314

Train Epoch: 112 
task: sign, mean loss: 0.02546, accuracy: 0.98913, avg. loss over tasks: 0.02546, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.12813, Variance: 0.01276
Semantic Loss - Mean: 0.04083, Variance: 0.00351

Test Epoch: 112 
task: sign, mean loss: 3.36099, accuracy: 0.43195, avg. loss over tasks: 3.36099
Diversity Loss - Mean: -0.12229, Variance: 0.01587
Semantic Loss - Mean: 2.54910, Variance: 0.02321

Train Epoch: 113 
task: sign, mean loss: 0.06369, accuracy: 0.97283, avg. loss over tasks: 0.06369, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.12860, Variance: 0.01274
Semantic Loss - Mean: 0.07976, Variance: 0.00350

Test Epoch: 113 
task: sign, mean loss: 3.21589, accuracy: 0.44970, avg. loss over tasks: 3.21589
Diversity Loss - Mean: -0.11925, Variance: 0.01585
Semantic Loss - Mean: 2.52635, Variance: 0.02330

Train Epoch: 114 
task: sign, mean loss: 0.02814, accuracy: 0.99457, avg. loss over tasks: 0.02814, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.12840, Variance: 0.01272
Semantic Loss - Mean: 0.05804, Variance: 0.00352

Test Epoch: 114 
task: sign, mean loss: 3.44613, accuracy: 0.43787, avg. loss over tasks: 3.44613
Diversity Loss - Mean: -0.11715, Variance: 0.01583
Semantic Loss - Mean: 2.73117, Variance: 0.02352

Train Epoch: 115 
task: sign, mean loss: 0.03462, accuracy: 0.98913, avg. loss over tasks: 0.03462, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.12790, Variance: 0.01270
Semantic Loss - Mean: 0.06460, Variance: 0.00352

Test Epoch: 115 
task: sign, mean loss: 2.67251, accuracy: 0.49704, avg. loss over tasks: 2.67251
Diversity Loss - Mean: -0.12258, Variance: 0.01581
Semantic Loss - Mean: 2.16875, Variance: 0.02372

Train Epoch: 116 
task: sign, mean loss: 0.01560, accuracy: 1.00000, avg. loss over tasks: 0.01560, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.12871, Variance: 0.01268
Semantic Loss - Mean: 0.04097, Variance: 0.00350

Test Epoch: 116 
task: sign, mean loss: 2.26056, accuracy: 0.58580, avg. loss over tasks: 2.26056
Diversity Loss - Mean: -0.12466, Variance: 0.01579
Semantic Loss - Mean: 1.84324, Variance: 0.02378

Train Epoch: 117 
task: sign, mean loss: 0.01383, accuracy: 0.99457, avg. loss over tasks: 0.01383, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.12851, Variance: 0.01267
Semantic Loss - Mean: 0.04863, Variance: 0.00351

Test Epoch: 117 
task: sign, mean loss: 2.64134, accuracy: 0.52071, avg. loss over tasks: 2.64134
Diversity Loss - Mean: -0.12146, Variance: 0.01577
Semantic Loss - Mean: 2.18224, Variance: 0.02392

Train Epoch: 118 
task: sign, mean loss: 0.01661, accuracy: 0.99457, avg. loss over tasks: 0.01661, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.12842, Variance: 0.01265
Semantic Loss - Mean: 0.03823, Variance: 0.00349

Test Epoch: 118 
task: sign, mean loss: 2.84154, accuracy: 0.50888, avg. loss over tasks: 2.84154
Diversity Loss - Mean: -0.12074, Variance: 0.01576
Semantic Loss - Mean: 2.35296, Variance: 0.02399

Train Epoch: 119 
task: sign, mean loss: 0.00982, accuracy: 1.00000, avg. loss over tasks: 0.00982, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.12827, Variance: 0.01263
Semantic Loss - Mean: 0.02888, Variance: 0.00348

Test Epoch: 119 
task: sign, mean loss: 2.97613, accuracy: 0.50296, avg. loss over tasks: 2.97613
Diversity Loss - Mean: -0.12134, Variance: 0.01573
Semantic Loss - Mean: 2.43772, Variance: 0.02402

Train Epoch: 120 
task: sign, mean loss: 0.01271, accuracy: 1.00000, avg. loss over tasks: 0.01271, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.12877, Variance: 0.01261
Semantic Loss - Mean: 0.05133, Variance: 0.00347

Test Epoch: 120 
task: sign, mean loss: 3.05882, accuracy: 0.50296, avg. loss over tasks: 3.05882
Diversity Loss - Mean: -0.12237, Variance: 0.01571
Semantic Loss - Mean: 2.49776, Variance: 0.02401

Train Epoch: 121 
task: sign, mean loss: 0.01381, accuracy: 1.00000, avg. loss over tasks: 0.01381, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.12855, Variance: 0.01259
Semantic Loss - Mean: 0.04663, Variance: 0.00346

Test Epoch: 121 
task: sign, mean loss: 3.20630, accuracy: 0.46154, avg. loss over tasks: 3.20630
Diversity Loss - Mean: -0.12060, Variance: 0.01569
Semantic Loss - Mean: 2.54442, Variance: 0.02400

Train Epoch: 122 
task: sign, mean loss: 0.01937, accuracy: 0.99457, avg. loss over tasks: 0.01937, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.12895, Variance: 0.01258
Semantic Loss - Mean: 0.04725, Variance: 0.00348

Test Epoch: 122 
task: sign, mean loss: 3.24107, accuracy: 0.46154, avg. loss over tasks: 3.24107
Diversity Loss - Mean: -0.12248, Variance: 0.01567
Semantic Loss - Mean: 2.50735, Variance: 0.02394

Train Epoch: 123 
task: sign, mean loss: 0.01654, accuracy: 0.99457, avg. loss over tasks: 0.01654, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.12885, Variance: 0.01256
Semantic Loss - Mean: 0.03621, Variance: 0.00346

Test Epoch: 123 
task: sign, mean loss: 3.21458, accuracy: 0.46154, avg. loss over tasks: 3.21458
Diversity Loss - Mean: -0.12349, Variance: 0.01566
Semantic Loss - Mean: 2.43644, Variance: 0.02392

Train Epoch: 124 
task: sign, mean loss: 0.01061, accuracy: 1.00000, avg. loss over tasks: 0.01061, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.12899, Variance: 0.01254
Semantic Loss - Mean: 0.02903, Variance: 0.00343

Test Epoch: 124 
task: sign, mean loss: 3.16949, accuracy: 0.45562, avg. loss over tasks: 3.16949
Diversity Loss - Mean: -0.12270, Variance: 0.01564
Semantic Loss - Mean: 2.39151, Variance: 0.02394

Train Epoch: 125 
task: sign, mean loss: 0.00632, accuracy: 1.00000, avg. loss over tasks: 0.00632, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.12930, Variance: 0.01253
Semantic Loss - Mean: 0.02730, Variance: 0.00343

Test Epoch: 125 
task: sign, mean loss: 3.29521, accuracy: 0.42604, avg. loss over tasks: 3.29521
Diversity Loss - Mean: -0.12271, Variance: 0.01562
Semantic Loss - Mean: 2.49247, Variance: 0.02394

Train Epoch: 126 
task: sign, mean loss: 0.04081, accuracy: 0.98913, avg. loss over tasks: 0.04081, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.12945, Variance: 0.01251
Semantic Loss - Mean: 0.04516, Variance: 0.00342

Test Epoch: 126 
task: sign, mean loss: 3.27903, accuracy: 0.46154, avg. loss over tasks: 3.27903
Diversity Loss - Mean: -0.12332, Variance: 0.01560
Semantic Loss - Mean: 2.46340, Variance: 0.02395

Train Epoch: 127 
task: sign, mean loss: 0.01513, accuracy: 1.00000, avg. loss over tasks: 0.01513, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.12877, Variance: 0.01249
Semantic Loss - Mean: 0.03592, Variance: 0.00340

Test Epoch: 127 
task: sign, mean loss: 3.18526, accuracy: 0.47337, avg. loss over tasks: 3.18526
Diversity Loss - Mean: -0.12511, Variance: 0.01559
Semantic Loss - Mean: 2.40612, Variance: 0.02395

Train Epoch: 128 
task: sign, mean loss: 0.01003, accuracy: 1.00000, avg. loss over tasks: 0.01003, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.12949, Variance: 0.01248
Semantic Loss - Mean: 0.02963, Variance: 0.00339

Test Epoch: 128 
task: sign, mean loss: 3.43670, accuracy: 0.44379, avg. loss over tasks: 3.43670
Diversity Loss - Mean: -0.12426, Variance: 0.01557
Semantic Loss - Mean: 2.57741, Variance: 0.02396

Train Epoch: 129 
task: sign, mean loss: 0.00851, accuracy: 1.00000, avg. loss over tasks: 0.00851, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.12940, Variance: 0.01246
Semantic Loss - Mean: 0.01998, Variance: 0.00337

Test Epoch: 129 
task: sign, mean loss: 3.15299, accuracy: 0.47337, avg. loss over tasks: 3.15299
Diversity Loss - Mean: -0.12519, Variance: 0.01555
Semantic Loss - Mean: 2.42008, Variance: 0.02398

Train Epoch: 130 
task: sign, mean loss: 0.01481, accuracy: 0.99457, avg. loss over tasks: 0.01481, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.12933, Variance: 0.01245
Semantic Loss - Mean: 0.03440, Variance: 0.00337

Test Epoch: 130 
task: sign, mean loss: 2.99676, accuracy: 0.49704, avg. loss over tasks: 2.99676
Diversity Loss - Mean: -0.12549, Variance: 0.01554
Semantic Loss - Mean: 2.31884, Variance: 0.02398

Train Epoch: 131 
task: sign, mean loss: 0.00834, accuracy: 1.00000, avg. loss over tasks: 0.00834, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.12945, Variance: 0.01243
Semantic Loss - Mean: 0.04078, Variance: 0.00338

Test Epoch: 131 
task: sign, mean loss: 3.01039, accuracy: 0.49112, avg. loss over tasks: 3.01039
Diversity Loss - Mean: -0.12535, Variance: 0.01552
Semantic Loss - Mean: 2.33783, Variance: 0.02398

Train Epoch: 132 
task: sign, mean loss: 0.00437, accuracy: 1.00000, avg. loss over tasks: 0.00437, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.12995, Variance: 0.01242
Semantic Loss - Mean: 0.02065, Variance: 0.00337

Test Epoch: 132 
task: sign, mean loss: 3.10150, accuracy: 0.50296, avg. loss over tasks: 3.10150
Diversity Loss - Mean: -0.12542, Variance: 0.01551
Semantic Loss - Mean: 2.39769, Variance: 0.02398

Train Epoch: 133 
task: sign, mean loss: 0.00601, accuracy: 1.00000, avg. loss over tasks: 0.00601, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.12976, Variance: 0.01240
Semantic Loss - Mean: 0.01862, Variance: 0.00335

Test Epoch: 133 
task: sign, mean loss: 3.08725, accuracy: 0.49704, avg. loss over tasks: 3.08725
Diversity Loss - Mean: -0.12506, Variance: 0.01549
Semantic Loss - Mean: 2.38520, Variance: 0.02397

Train Epoch: 134 
task: sign, mean loss: 0.01027, accuracy: 1.00000, avg. loss over tasks: 0.01027, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.12989, Variance: 0.01239
Semantic Loss - Mean: 0.02315, Variance: 0.00333

Test Epoch: 134 
task: sign, mean loss: 3.12414, accuracy: 0.49704, avg. loss over tasks: 3.12414
Diversity Loss - Mean: -0.12540, Variance: 0.01548
Semantic Loss - Mean: 2.42037, Variance: 0.02397

Train Epoch: 135 
task: sign, mean loss: 0.00931, accuracy: 1.00000, avg. loss over tasks: 0.00931, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.12996, Variance: 0.01237
Semantic Loss - Mean: 0.02959, Variance: 0.00332

Test Epoch: 135 
task: sign, mean loss: 3.15630, accuracy: 0.50888, avg. loss over tasks: 3.15630
Diversity Loss - Mean: -0.12549, Variance: 0.01546
Semantic Loss - Mean: 2.44441, Variance: 0.02398

Train Epoch: 136 
task: sign, mean loss: 0.00319, accuracy: 1.00000, avg. loss over tasks: 0.00319, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13025, Variance: 0.01236
Semantic Loss - Mean: 0.01569, Variance: 0.00330

Test Epoch: 136 
task: sign, mean loss: 3.06191, accuracy: 0.49704, avg. loss over tasks: 3.06191
Diversity Loss - Mean: -0.12521, Variance: 0.01545
Semantic Loss - Mean: 2.38046, Variance: 0.02400

Train Epoch: 137 
task: sign, mean loss: 0.00542, accuracy: 1.00000, avg. loss over tasks: 0.00542, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13002, Variance: 0.01235
Semantic Loss - Mean: 0.02453, Variance: 0.00328

Test Epoch: 137 
task: sign, mean loss: 3.16780, accuracy: 0.48521, avg. loss over tasks: 3.16780
Diversity Loss - Mean: -0.12461, Variance: 0.01543
Semantic Loss - Mean: 2.45358, Variance: 0.02403

Train Epoch: 138 
task: sign, mean loss: 0.02022, accuracy: 0.99457, avg. loss over tasks: 0.02022, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13014, Variance: 0.01234
Semantic Loss - Mean: 0.04654, Variance: 0.00328

Test Epoch: 138 
task: sign, mean loss: 3.21850, accuracy: 0.47929, avg. loss over tasks: 3.21850
Diversity Loss - Mean: -0.12439, Variance: 0.01542
Semantic Loss - Mean: 2.50103, Variance: 0.02406

Train Epoch: 139 
task: sign, mean loss: 0.00924, accuracy: 0.99457, avg. loss over tasks: 0.00924, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13012, Variance: 0.01233
Semantic Loss - Mean: 0.02574, Variance: 0.00327

Test Epoch: 139 
task: sign, mean loss: 3.35776, accuracy: 0.46154, avg. loss over tasks: 3.35776
Diversity Loss - Mean: -0.12406, Variance: 0.01540
Semantic Loss - Mean: 2.59480, Variance: 0.02410

Train Epoch: 140 
task: sign, mean loss: 0.04677, accuracy: 0.98370, avg. loss over tasks: 0.04677, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.12932, Variance: 0.01231
Semantic Loss - Mean: 0.06290, Variance: 0.00327

Test Epoch: 140 
task: sign, mean loss: 3.04254, accuracy: 0.50296, avg. loss over tasks: 3.04254
Diversity Loss - Mean: -0.12531, Variance: 0.01539
Semantic Loss - Mean: 2.38940, Variance: 0.02413

Train Epoch: 141 
task: sign, mean loss: 0.01128, accuracy: 0.99457, avg. loss over tasks: 0.01128, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.12999, Variance: 0.01230
Semantic Loss - Mean: 0.03162, Variance: 0.00328

Test Epoch: 141 
task: sign, mean loss: 3.01724, accuracy: 0.51479, avg. loss over tasks: 3.01724
Diversity Loss - Mean: -0.12543, Variance: 0.01537
Semantic Loss - Mean: 2.37480, Variance: 0.02416

Train Epoch: 142 
task: sign, mean loss: 0.00966, accuracy: 1.00000, avg. loss over tasks: 0.00966, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.12975, Variance: 0.01229
Semantic Loss - Mean: 0.04888, Variance: 0.00331

Test Epoch: 142 
task: sign, mean loss: 2.95901, accuracy: 0.52071, avg. loss over tasks: 2.95901
Diversity Loss - Mean: -0.12566, Variance: 0.01536
Semantic Loss - Mean: 2.33597, Variance: 0.02420

Train Epoch: 143 
task: sign, mean loss: 0.00942, accuracy: 1.00000, avg. loss over tasks: 0.00942, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.12997, Variance: 0.01228
Semantic Loss - Mean: 0.02536, Variance: 0.00329

Test Epoch: 143 
task: sign, mean loss: 3.16986, accuracy: 0.47929, avg. loss over tasks: 3.16986
Diversity Loss - Mean: -0.12456, Variance: 0.01535
Semantic Loss - Mean: 2.47731, Variance: 0.02426

Train Epoch: 144 
task: sign, mean loss: 0.00650, accuracy: 1.00000, avg. loss over tasks: 0.00650, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.12994, Variance: 0.01226
Semantic Loss - Mean: 0.02074, Variance: 0.00327

Test Epoch: 144 
task: sign, mean loss: 3.20730, accuracy: 0.47337, avg. loss over tasks: 3.20730
Diversity Loss - Mean: -0.12497, Variance: 0.01534
Semantic Loss - Mean: 2.50293, Variance: 0.02432

Train Epoch: 145 
task: sign, mean loss: 0.00573, accuracy: 1.00000, avg. loss over tasks: 0.00573, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.12954, Variance: 0.01225
Semantic Loss - Mean: 0.01970, Variance: 0.00325

Test Epoch: 145 
task: sign, mean loss: 3.16857, accuracy: 0.48521, avg. loss over tasks: 3.16857
Diversity Loss - Mean: -0.12477, Variance: 0.01532
Semantic Loss - Mean: 2.46746, Variance: 0.02437

Train Epoch: 146 
task: sign, mean loss: 0.03240, accuracy: 0.98913, avg. loss over tasks: 0.03240, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.12979, Variance: 0.01224
Semantic Loss - Mean: 0.06596, Variance: 0.00325

Test Epoch: 146 
task: sign, mean loss: 3.44118, accuracy: 0.44379, avg. loss over tasks: 3.44118
Diversity Loss - Mean: -0.12402, Variance: 0.01531
Semantic Loss - Mean: 2.65307, Variance: 0.02442

Train Epoch: 147 
task: sign, mean loss: 0.00562, accuracy: 1.00000, avg. loss over tasks: 0.00562, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.12966, Variance: 0.01222
Semantic Loss - Mean: 0.01869, Variance: 0.00323

Test Epoch: 147 
task: sign, mean loss: 3.28090, accuracy: 0.46746, avg. loss over tasks: 3.28090
Diversity Loss - Mean: -0.12428, Variance: 0.01530
Semantic Loss - Mean: 2.55556, Variance: 0.02447

Train Epoch: 148 
task: sign, mean loss: 0.03965, accuracy: 0.98913, avg. loss over tasks: 0.03965, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.12917, Variance: 0.01221
Semantic Loss - Mean: 0.05604, Variance: 0.00324

Test Epoch: 148 
task: sign, mean loss: 3.30548, accuracy: 0.46746, avg. loss over tasks: 3.30548
Diversity Loss - Mean: -0.12458, Variance: 0.01529
Semantic Loss - Mean: 2.57859, Variance: 0.02452

Train Epoch: 149 
task: sign, mean loss: 0.00806, accuracy: 1.00000, avg. loss over tasks: 0.00806, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.12977, Variance: 0.01220
Semantic Loss - Mean: 0.02824, Variance: 0.00322

Test Epoch: 149 
task: sign, mean loss: 3.12092, accuracy: 0.49112, avg. loss over tasks: 3.12092
Diversity Loss - Mean: -0.12534, Variance: 0.01528
Semantic Loss - Mean: 2.44846, Variance: 0.02455

Train Epoch: 150 
task: sign, mean loss: 0.00294, accuracy: 1.00000, avg. loss over tasks: 0.00294, lr: 3e-07
Diversity Loss - Mean: -0.12960, Variance: 0.01218
Semantic Loss - Mean: 0.01711, Variance: 0.00321

Test Epoch: 150 
task: sign, mean loss: 3.13033, accuracy: 0.49704, avg. loss over tasks: 3.13033
Diversity Loss - Mean: -0.12528, Variance: 0.01526
Semantic Loss - Mean: 2.45466, Variance: 0.02457

