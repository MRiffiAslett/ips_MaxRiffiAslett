Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09373, accuracy: 0.63587, avg. loss over tasks: 1.09373, lr: 3e-05
Diversity Loss - Mean: -0.01053, Variance: 0.01051
Semantic Loss - Mean: 1.43160, Variance: 0.07250

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17429, accuracy: 0.66272, avg. loss over tasks: 1.17429
Diversity Loss - Mean: -0.03164, Variance: 0.01249
Semantic Loss - Mean: 1.16129, Variance: 0.05330

Train Epoch: 2 
task: sign, mean loss: 0.96048, accuracy: 0.66848, avg. loss over tasks: 0.96048, lr: 6e-05
Diversity Loss - Mean: -0.02302, Variance: 0.01053
Semantic Loss - Mean: 0.98060, Variance: 0.03918

Test Epoch: 2 
task: sign, mean loss: 1.10283, accuracy: 0.66864, avg. loss over tasks: 1.10283
Diversity Loss - Mean: -0.03764, Variance: 0.01223
Semantic Loss - Mean: 1.14490, Variance: 0.03221

Train Epoch: 3 
task: sign, mean loss: 0.78561, accuracy: 0.68478, avg. loss over tasks: 0.78561, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.04671, Variance: 0.01042
Semantic Loss - Mean: 0.98738, Variance: 0.02724

Test Epoch: 3 
task: sign, mean loss: 1.22775, accuracy: 0.59763, avg. loss over tasks: 1.22775
Diversity Loss - Mean: -0.06739, Variance: 0.01158
Semantic Loss - Mean: 1.10218, Variance: 0.02958

Train Epoch: 4 
task: sign, mean loss: 0.75735, accuracy: 0.70652, avg. loss over tasks: 0.75735, lr: 0.00012
Diversity Loss - Mean: -0.07246, Variance: 0.01016
Semantic Loss - Mean: 0.88286, Variance: 0.02101

Test Epoch: 4 
task: sign, mean loss: 1.56576, accuracy: 0.45562, avg. loss over tasks: 1.56576
Diversity Loss - Mean: -0.06974, Variance: 0.01101
Semantic Loss - Mean: 1.09520, Variance: 0.02524

Train Epoch: 5 
task: sign, mean loss: 0.70287, accuracy: 0.71739, avg. loss over tasks: 0.70287, lr: 0.00015
Diversity Loss - Mean: -0.06712, Variance: 0.00988
Semantic Loss - Mean: 0.76001, Variance: 0.01717

Test Epoch: 5 
task: sign, mean loss: 2.17571, accuracy: 0.34320, avg. loss over tasks: 2.17571
Diversity Loss - Mean: -0.06491, Variance: 0.01034
Semantic Loss - Mean: 1.29191, Variance: 0.02331

Train Epoch: 6 
task: sign, mean loss: 0.62081, accuracy: 0.77174, avg. loss over tasks: 0.62081, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.05492, Variance: 0.00962
Semantic Loss - Mean: 0.66522, Variance: 0.01482

Test Epoch: 6 
task: sign, mean loss: 2.29665, accuracy: 0.40828, avg. loss over tasks: 2.29665
Diversity Loss - Mean: -0.02861, Variance: 0.01001
Semantic Loss - Mean: 1.56514, Variance: 0.02574

Train Epoch: 7 
task: sign, mean loss: 0.65452, accuracy: 0.75543, avg. loss over tasks: 0.65452, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.07565, Variance: 0.00971
Semantic Loss - Mean: 0.67860, Variance: 0.01314

Test Epoch: 7 
task: sign, mean loss: 1.92699, accuracy: 0.59763, avg. loss over tasks: 1.92699
Diversity Loss - Mean: -0.06012, Variance: 0.01044
Semantic Loss - Mean: 1.46117, Variance: 0.02605

Train Epoch: 8 
task: sign, mean loss: 0.64045, accuracy: 0.77717, avg. loss over tasks: 0.64045, lr: 0.00024
Diversity Loss - Mean: -0.04824, Variance: 0.00968
Semantic Loss - Mean: 0.65790, Variance: 0.01219

Test Epoch: 8 
task: sign, mean loss: 3.22476, accuracy: 0.42604, avg. loss over tasks: 3.22476
Diversity Loss - Mean: 0.02571, Variance: 0.01054
Semantic Loss - Mean: 2.15939, Variance: 0.03073

Train Epoch: 9 
task: sign, mean loss: 0.74829, accuracy: 0.73913, avg. loss over tasks: 0.74829, lr: 0.00027
Diversity Loss - Mean: -0.04387, Variance: 0.00960
Semantic Loss - Mean: 0.66971, Variance: 0.01135

Test Epoch: 9 
task: sign, mean loss: 3.92819, accuracy: 0.12426, avg. loss over tasks: 3.92819
Diversity Loss - Mean: 0.03837, Variance: 0.01049
Semantic Loss - Mean: 2.82713, Variance: 0.03905

Train Epoch: 10 
task: sign, mean loss: 0.88843, accuracy: 0.66848, avg. loss over tasks: 0.88843, lr: 0.0003
Diversity Loss - Mean: -0.06065, Variance: 0.00958
Semantic Loss - Mean: 0.85364, Variance: 0.01060

Test Epoch: 10 
task: sign, mean loss: 3.38500, accuracy: 0.20118, avg. loss over tasks: 3.38500
Diversity Loss - Mean: 0.01581, Variance: 0.01058
Semantic Loss - Mean: 2.28239, Variance: 0.03869

Train Epoch: 11 
task: sign, mean loss: 0.77826, accuracy: 0.64674, avg. loss over tasks: 0.77826, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.06410, Variance: 0.00962
Semantic Loss - Mean: 0.75338, Variance: 0.00986

Test Epoch: 11 
task: sign, mean loss: 1.94712, accuracy: 0.65680, avg. loss over tasks: 1.94712
Diversity Loss - Mean: -0.06081, Variance: 0.01065
Semantic Loss - Mean: 1.50427, Variance: 0.03679

Train Epoch: 12 
task: sign, mean loss: 0.53841, accuracy: 0.76630, avg. loss over tasks: 0.53841, lr: 0.000299849111021216
Diversity Loss - Mean: -0.05713, Variance: 0.00967
Semantic Loss - Mean: 0.54892, Variance: 0.00920

Test Epoch: 12 
task: sign, mean loss: 2.38992, accuracy: 0.50296, avg. loss over tasks: 2.38992
Diversity Loss - Mean: -0.04094, Variance: 0.01073
Semantic Loss - Mean: 1.69027, Variance: 0.03573

Train Epoch: 13 
task: sign, mean loss: 0.48092, accuracy: 0.84239, avg. loss over tasks: 0.48092, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.05071, Variance: 0.00969
Semantic Loss - Mean: 0.49791, Variance: 0.00880

Test Epoch: 13 
task: sign, mean loss: 2.25831, accuracy: 0.62722, avg. loss over tasks: 2.25831
Diversity Loss - Mean: -0.07009, Variance: 0.01097
Semantic Loss - Mean: 1.82153, Variance: 0.03427

Train Epoch: 14 
task: sign, mean loss: 0.45595, accuracy: 0.83152, avg. loss over tasks: 0.45595, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.05654, Variance: 0.00973
Semantic Loss - Mean: 0.48735, Variance: 0.00900

Test Epoch: 14 
task: sign, mean loss: 2.41131, accuracy: 0.34320, avg. loss over tasks: 2.41131
Diversity Loss - Mean: -0.02094, Variance: 0.01110
Semantic Loss - Mean: 1.81824, Variance: 0.03565

Train Epoch: 15 
task: sign, mean loss: 0.47168, accuracy: 0.83152, avg. loss over tasks: 0.47168, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.07137, Variance: 0.00987
Semantic Loss - Mean: 0.48118, Variance: 0.00865

Test Epoch: 15 
task: sign, mean loss: 1.74922, accuracy: 0.67456, avg. loss over tasks: 1.74922
Diversity Loss - Mean: -0.09316, Variance: 0.01120
Semantic Loss - Mean: 1.55293, Variance: 0.03462

Train Epoch: 16 
task: sign, mean loss: 0.44760, accuracy: 0.84239, avg. loss over tasks: 0.44760, lr: 0.000298643821800925
Diversity Loss - Mean: -0.07657, Variance: 0.01007
Semantic Loss - Mean: 0.48913, Variance: 0.00827

Test Epoch: 16 
task: sign, mean loss: 2.32045, accuracy: 0.30769, avg. loss over tasks: 2.32045
Diversity Loss - Mean: -0.05073, Variance: 0.01133
Semantic Loss - Mean: 1.88740, Variance: 0.03436

Train Epoch: 17 
task: sign, mean loss: 0.33578, accuracy: 0.84239, avg. loss over tasks: 0.33578, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.06716, Variance: 0.01020
Semantic Loss - Mean: 0.36227, Variance: 0.00789

Test Epoch: 17 
task: sign, mean loss: 1.65515, accuracy: 0.56805, avg. loss over tasks: 1.65515
Diversity Loss - Mean: -0.06999, Variance: 0.01147
Semantic Loss - Mean: 1.53778, Variance: 0.03429

Train Epoch: 18 
task: sign, mean loss: 0.44594, accuracy: 0.88587, avg. loss over tasks: 0.44594, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.06340, Variance: 0.01023
Semantic Loss - Mean: 0.45183, Variance: 0.00801

Test Epoch: 18 
task: sign, mean loss: 1.64224, accuracy: 0.60947, avg. loss over tasks: 1.64224
Diversity Loss - Mean: -0.08732, Variance: 0.01159
Semantic Loss - Mean: 1.50757, Variance: 0.03324

Train Epoch: 19 
task: sign, mean loss: 0.29077, accuracy: 0.88587, avg. loss over tasks: 0.29077, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.07210, Variance: 0.01026
Semantic Loss - Mean: 0.34073, Variance: 0.00782

Test Epoch: 19 
task: sign, mean loss: 3.38671, accuracy: 0.34911, avg. loss over tasks: 3.38671
Diversity Loss - Mean: 0.01342, Variance: 0.01172
Semantic Loss - Mean: 2.47752, Variance: 0.03448

Train Epoch: 20 
task: sign, mean loss: 0.22676, accuracy: 0.88587, avg. loss over tasks: 0.22676, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.07593, Variance: 0.01033
Semantic Loss - Mean: 0.30453, Variance: 0.00764

Test Epoch: 20 
task: sign, mean loss: 1.99559, accuracy: 0.59763, avg. loss over tasks: 1.99559
Diversity Loss - Mean: -0.08951, Variance: 0.01169
Semantic Loss - Mean: 1.71591, Variance: 0.03420

Train Epoch: 21 
task: sign, mean loss: 0.23715, accuracy: 0.92391, avg. loss over tasks: 0.23715, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.06466, Variance: 0.01031
Semantic Loss - Mean: 0.29170, Variance: 0.00745

Test Epoch: 21 
task: sign, mean loss: 1.83032, accuracy: 0.65680, avg. loss over tasks: 1.83032
Diversity Loss - Mean: -0.09290, Variance: 0.01178
Semantic Loss - Mean: 1.71990, Variance: 0.03329

Train Epoch: 22 
task: sign, mean loss: 0.42095, accuracy: 0.86413, avg. loss over tasks: 0.42095, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.07661, Variance: 0.01036
Semantic Loss - Mean: 0.46803, Variance: 0.00756

Test Epoch: 22 
task: sign, mean loss: 2.45073, accuracy: 0.33728, avg. loss over tasks: 2.45073
Diversity Loss - Mean: -0.04804, Variance: 0.01177
Semantic Loss - Mean: 2.15152, Variance: 0.03451

Train Epoch: 23 
task: sign, mean loss: 0.19377, accuracy: 0.92391, avg. loss over tasks: 0.19377, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.07771, Variance: 0.01039
Semantic Loss - Mean: 0.25151, Variance: 0.00735

Test Epoch: 23 
task: sign, mean loss: 4.33374, accuracy: 0.17160, avg. loss over tasks: 4.33374
Diversity Loss - Mean: -0.00772, Variance: 0.01190
Semantic Loss - Mean: 3.19896, Variance: 0.03609

Train Epoch: 24 
task: sign, mean loss: 0.16870, accuracy: 0.95652, avg. loss over tasks: 0.16870, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.07902, Variance: 0.01042
Semantic Loss - Mean: 0.21474, Variance: 0.00712

Test Epoch: 24 
task: sign, mean loss: 2.49571, accuracy: 0.37278, avg. loss over tasks: 2.49571
Diversity Loss - Mean: -0.06263, Variance: 0.01195
Semantic Loss - Mean: 2.24511, Variance: 0.03651

Train Epoch: 25 
task: sign, mean loss: 0.16432, accuracy: 0.94565, avg. loss over tasks: 0.16432, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.07151, Variance: 0.01041
Semantic Loss - Mean: 0.19754, Variance: 0.00696

Test Epoch: 25 
task: sign, mean loss: 1.53685, accuracy: 0.60355, avg. loss over tasks: 1.53685
Diversity Loss - Mean: -0.09808, Variance: 0.01202
Semantic Loss - Mean: 1.41948, Variance: 0.03600

Train Epoch: 26 
task: sign, mean loss: 0.12688, accuracy: 0.95109, avg. loss over tasks: 0.12688, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.07938, Variance: 0.01042
Semantic Loss - Mean: 0.14910, Variance: 0.00679

Test Epoch: 26 
task: sign, mean loss: 1.72210, accuracy: 0.61538, avg. loss over tasks: 1.72210
Diversity Loss - Mean: -0.09895, Variance: 0.01208
Semantic Loss - Mean: 1.65666, Variance: 0.03512

Train Epoch: 27 
task: sign, mean loss: 0.17848, accuracy: 0.93478, avg. loss over tasks: 0.17848, lr: 0.000289228031029578
Diversity Loss - Mean: -0.08005, Variance: 0.01041
Semantic Loss - Mean: 0.23229, Variance: 0.00674

Test Epoch: 27 
task: sign, mean loss: 4.63590, accuracy: 0.14201, avg. loss over tasks: 4.63590
Diversity Loss - Mean: 0.00211, Variance: 0.01225
Semantic Loss - Mean: 3.24555, Variance: 0.03642

Train Epoch: 28 
task: sign, mean loss: 0.22408, accuracy: 0.90761, avg. loss over tasks: 0.22408, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.08742, Variance: 0.01040
Semantic Loss - Mean: 0.26519, Variance: 0.00682

Test Epoch: 28 
task: sign, mean loss: 3.15079, accuracy: 0.38462, avg. loss over tasks: 3.15079
Diversity Loss - Mean: -0.08561, Variance: 0.01228
Semantic Loss - Mean: 2.20245, Variance: 0.03741

Train Epoch: 29 
task: sign, mean loss: 0.17024, accuracy: 0.93478, avg. loss over tasks: 0.17024, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.09045, Variance: 0.01039
Semantic Loss - Mean: 0.19006, Variance: 0.00670

Test Epoch: 29 
task: sign, mean loss: 2.30677, accuracy: 0.57988, avg. loss over tasks: 2.30677
Diversity Loss - Mean: -0.08603, Variance: 0.01228
Semantic Loss - Mean: 1.76783, Variance: 0.03719

Train Epoch: 30 
task: sign, mean loss: 0.34073, accuracy: 0.88587, avg. loss over tasks: 0.34073, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.09747, Variance: 0.01042
Semantic Loss - Mean: 0.34965, Variance: 0.00671

Test Epoch: 30 
task: sign, mean loss: 1.39384, accuracy: 0.62722, avg. loss over tasks: 1.39384
Diversity Loss - Mean: -0.10582, Variance: 0.01238
Semantic Loss - Mean: 1.55220, Variance: 0.03733

Train Epoch: 31 
task: sign, mean loss: 0.27557, accuracy: 0.90761, avg. loss over tasks: 0.27557, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.09215, Variance: 0.01047
Semantic Loss - Mean: 0.31469, Variance: 0.00665

Test Epoch: 31 
task: sign, mean loss: 1.86925, accuracy: 0.38462, avg. loss over tasks: 1.86925
Diversity Loss - Mean: -0.08350, Variance: 0.01240
Semantic Loss - Mean: 1.57201, Variance: 0.03777

Train Epoch: 32 
task: sign, mean loss: 0.13073, accuracy: 0.94022, avg. loss over tasks: 0.13073, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.08535, Variance: 0.01049
Semantic Loss - Mean: 0.22910, Variance: 0.00660

Test Epoch: 32 
task: sign, mean loss: 1.51472, accuracy: 0.52071, avg. loss over tasks: 1.51472
Diversity Loss - Mean: -0.07782, Variance: 0.01243
Semantic Loss - Mean: 1.56254, Variance: 0.03790

Train Epoch: 33 
task: sign, mean loss: 0.06149, accuracy: 0.97826, avg. loss over tasks: 0.06149, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.07801, Variance: 0.01050
Semantic Loss - Mean: 0.09598, Variance: 0.00646

Test Epoch: 33 
task: sign, mean loss: 1.87942, accuracy: 0.59763, avg. loss over tasks: 1.87942
Diversity Loss - Mean: -0.07060, Variance: 0.01241
Semantic Loss - Mean: 1.82034, Variance: 0.03796

Train Epoch: 34 
task: sign, mean loss: 0.04781, accuracy: 0.98913, avg. loss over tasks: 0.04781, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.08298, Variance: 0.01053
Semantic Loss - Mean: 0.08192, Variance: 0.00635

Test Epoch: 34 
task: sign, mean loss: 2.17119, accuracy: 0.63314, avg. loss over tasks: 2.17119
Diversity Loss - Mean: -0.09754, Variance: 0.01242
Semantic Loss - Mean: 1.96652, Variance: 0.03783

Train Epoch: 35 
task: sign, mean loss: 0.09192, accuracy: 0.97283, avg. loss over tasks: 0.09192, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.08361, Variance: 0.01055
Semantic Loss - Mean: 0.12084, Variance: 0.00625

Test Epoch: 35 
task: sign, mean loss: 1.99237, accuracy: 0.58580, avg. loss over tasks: 1.99237
Diversity Loss - Mean: -0.09691, Variance: 0.01245
Semantic Loss - Mean: 1.77116, Variance: 0.03740

Train Epoch: 36 
task: sign, mean loss: 0.09562, accuracy: 0.96739, avg. loss over tasks: 0.09562, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.07918, Variance: 0.01054
Semantic Loss - Mean: 0.13261, Variance: 0.00628

Test Epoch: 36 
task: sign, mean loss: 2.55777, accuracy: 0.44379, avg. loss over tasks: 2.55777
Diversity Loss - Mean: -0.07349, Variance: 0.01246
Semantic Loss - Mean: 2.11861, Variance: 0.03844

Train Epoch: 37 
task: sign, mean loss: 0.22589, accuracy: 0.96196, avg. loss over tasks: 0.22589, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.09282, Variance: 0.01056
Semantic Loss - Mean: 0.25186, Variance: 0.00638

Test Epoch: 37 
task: sign, mean loss: 2.09620, accuracy: 0.69231, avg. loss over tasks: 2.09620
Diversity Loss - Mean: -0.11387, Variance: 0.01257
Semantic Loss - Mean: 1.78262, Variance: 0.03834

Train Epoch: 38 
task: sign, mean loss: 0.16991, accuracy: 0.92935, avg. loss over tasks: 0.16991, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.09695, Variance: 0.01058
Semantic Loss - Mean: 0.22323, Variance: 0.00641

Test Epoch: 38 
task: sign, mean loss: 2.33198, accuracy: 0.50888, avg. loss over tasks: 2.33198
Diversity Loss - Mean: -0.08905, Variance: 0.01255
Semantic Loss - Mean: 2.06769, Variance: 0.03913

Train Epoch: 39 
task: sign, mean loss: 0.12550, accuracy: 0.97283, avg. loss over tasks: 0.12550, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.10066, Variance: 0.01060
Semantic Loss - Mean: 0.19861, Variance: 0.00653

Test Epoch: 39 
task: sign, mean loss: 2.27741, accuracy: 0.56213, avg. loss over tasks: 2.27741
Diversity Loss - Mean: -0.08049, Variance: 0.01254
Semantic Loss - Mean: 1.75121, Variance: 0.04002

Train Epoch: 40 
task: sign, mean loss: 0.07376, accuracy: 0.97826, avg. loss over tasks: 0.07376, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.09381, Variance: 0.01061
Semantic Loss - Mean: 0.11439, Variance: 0.00644

Test Epoch: 40 
task: sign, mean loss: 2.33029, accuracy: 0.49112, avg. loss over tasks: 2.33029
Diversity Loss - Mean: -0.06208, Variance: 0.01257
Semantic Loss - Mean: 1.77252, Variance: 0.04003

Train Epoch: 41 
task: sign, mean loss: 0.06895, accuracy: 0.97826, avg. loss over tasks: 0.06895, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.10116, Variance: 0.01064
Semantic Loss - Mean: 0.10724, Variance: 0.00647

Test Epoch: 41 
task: sign, mean loss: 2.79181, accuracy: 0.40237, avg. loss over tasks: 2.79181
Diversity Loss - Mean: -0.07600, Variance: 0.01256
Semantic Loss - Mean: 2.28761, Variance: 0.04028

Train Epoch: 42 
task: sign, mean loss: 0.09898, accuracy: 0.97283, avg. loss over tasks: 0.09898, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.09676, Variance: 0.01064
Semantic Loss - Mean: 0.13835, Variance: 0.00642

Test Epoch: 42 
task: sign, mean loss: 2.05125, accuracy: 0.46746, avg. loss over tasks: 2.05125
Diversity Loss - Mean: -0.07620, Variance: 0.01258
Semantic Loss - Mean: 1.69569, Variance: 0.04092

Train Epoch: 43 
task: sign, mean loss: 0.33560, accuracy: 0.86957, avg. loss over tasks: 0.33560, lr: 0.000260757131773478
Diversity Loss - Mean: -0.09260, Variance: 0.01065
Semantic Loss - Mean: 0.34945, Variance: 0.00662

Test Epoch: 43 
task: sign, mean loss: 1.64638, accuracy: 0.42012, avg. loss over tasks: 1.64638
Diversity Loss - Mean: -0.04256, Variance: 0.01271
Semantic Loss - Mean: 1.35027, Variance: 0.04181

Train Epoch: 44 
task: sign, mean loss: 0.66511, accuracy: 0.79348, avg. loss over tasks: 0.66511, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.08698, Variance: 0.01064
Semantic Loss - Mean: 0.71029, Variance: 0.00708

Test Epoch: 44 
task: sign, mean loss: 0.72728, accuracy: 0.79882, avg. loss over tasks: 0.72728
Diversity Loss - Mean: -0.07288, Variance: 0.01273
Semantic Loss - Mean: 0.64519, Variance: 0.04133

Train Epoch: 45 
task: sign, mean loss: 0.58565, accuracy: 0.79348, avg. loss over tasks: 0.58565, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.10178, Variance: 0.01065
Semantic Loss - Mean: 0.71324, Variance: 0.00759

Test Epoch: 45 
task: sign, mean loss: 4.40494, accuracy: 0.14201, avg. loss over tasks: 4.40494
Diversity Loss - Mean: -0.04048, Variance: 0.01286
Semantic Loss - Mean: 3.25454, Variance: 0.04529

Train Epoch: 46 
task: sign, mean loss: 0.59613, accuracy: 0.79348, avg. loss over tasks: 0.59613, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.09958, Variance: 0.01069
Semantic Loss - Mean: 0.58526, Variance: 0.00754

Test Epoch: 46 
task: sign, mean loss: 1.05580, accuracy: 0.71006, avg. loss over tasks: 1.05580
Diversity Loss - Mean: -0.09843, Variance: 0.01290
Semantic Loss - Mean: 0.92291, Variance: 0.04452

Train Epoch: 47 
task: sign, mean loss: 0.34277, accuracy: 0.88043, avg. loss over tasks: 0.34277, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.10381, Variance: 0.01074
Semantic Loss - Mean: 0.42776, Variance: 0.00770

Test Epoch: 47 
task: sign, mean loss: 0.68215, accuracy: 0.76331, avg. loss over tasks: 0.68215
Diversity Loss - Mean: -0.08787, Variance: 0.01293
Semantic Loss - Mean: 0.73117, Variance: 0.04448

Train Epoch: 48 
task: sign, mean loss: 0.20944, accuracy: 0.92935, avg. loss over tasks: 0.20944, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.10430, Variance: 0.01078
Semantic Loss - Mean: 0.27828, Variance: 0.00768

Test Epoch: 48 
task: sign, mean loss: 1.06790, accuracy: 0.65680, avg. loss over tasks: 1.06790
Diversity Loss - Mean: -0.09743, Variance: 0.01293
Semantic Loss - Mean: 1.12612, Variance: 0.04404

Train Epoch: 49 
task: sign, mean loss: 0.17839, accuracy: 0.95109, avg. loss over tasks: 0.17839, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.09887, Variance: 0.01080
Semantic Loss - Mean: 0.23817, Variance: 0.00767

Test Epoch: 49 
task: sign, mean loss: 0.81537, accuracy: 0.74556, avg. loss over tasks: 0.81537
Diversity Loss - Mean: -0.10515, Variance: 0.01296
Semantic Loss - Mean: 0.73038, Variance: 0.04334

Train Epoch: 50 
task: sign, mean loss: 0.15421, accuracy: 0.94565, avg. loss over tasks: 0.15421, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.09159, Variance: 0.01079
Semantic Loss - Mean: 0.21125, Variance: 0.00776

Test Epoch: 50 
task: sign, mean loss: 0.61709, accuracy: 0.76331, avg. loss over tasks: 0.61709
Diversity Loss - Mean: -0.10329, Variance: 0.01300
Semantic Loss - Mean: 0.56550, Variance: 0.04282

Train Epoch: 51 
task: sign, mean loss: 0.15206, accuracy: 0.93478, avg. loss over tasks: 0.15206, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.09433, Variance: 0.01079
Semantic Loss - Mean: 0.21122, Variance: 0.00779

Test Epoch: 51 
task: sign, mean loss: 0.49951, accuracy: 0.81657, avg. loss over tasks: 0.49951
Diversity Loss - Mean: -0.09280, Variance: 0.01304
Semantic Loss - Mean: 0.47407, Variance: 0.04212

Train Epoch: 52 
task: sign, mean loss: 0.14925, accuracy: 0.92935, avg. loss over tasks: 0.14925, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.09330, Variance: 0.01078
Semantic Loss - Mean: 0.20539, Variance: 0.00779

Test Epoch: 52 
task: sign, mean loss: 0.89477, accuracy: 0.76331, avg. loss over tasks: 0.89477
Diversity Loss - Mean: -0.08888, Variance: 0.01303
Semantic Loss - Mean: 0.91007, Variance: 0.04192

Train Epoch: 53 
task: sign, mean loss: 0.27928, accuracy: 0.88043, avg. loss over tasks: 0.27928, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.09630, Variance: 0.01079
Semantic Loss - Mean: 0.36571, Variance: 0.00797

Test Epoch: 53 
task: sign, mean loss: 1.01043, accuracy: 0.78107, avg. loss over tasks: 1.01043
Diversity Loss - Mean: -0.09506, Variance: 0.01302
Semantic Loss - Mean: 0.99538, Variance: 0.04215

Train Epoch: 54 
task: sign, mean loss: 0.15293, accuracy: 0.93478, avg. loss over tasks: 0.15293, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.09326, Variance: 0.01079
Semantic Loss - Mean: 0.24890, Variance: 0.00813

Test Epoch: 54 
task: sign, mean loss: 1.23408, accuracy: 0.66864, avg. loss over tasks: 1.23408
Diversity Loss - Mean: -0.10332, Variance: 0.01307
Semantic Loss - Mean: 1.01413, Variance: 0.04186

Train Epoch: 55 
task: sign, mean loss: 0.19264, accuracy: 0.93478, avg. loss over tasks: 0.19264, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.10360, Variance: 0.01082
Semantic Loss - Mean: 0.27707, Variance: 0.00831

Test Epoch: 55 
task: sign, mean loss: 3.02329, accuracy: 0.38462, avg. loss over tasks: 3.02329
Diversity Loss - Mean: -0.06718, Variance: 0.01315
Semantic Loss - Mean: 2.17482, Variance: 0.04234

Train Epoch: 56 
task: sign, mean loss: 0.22323, accuracy: 0.92391, avg. loss over tasks: 0.22323, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.10002, Variance: 0.01084
Semantic Loss - Mean: 0.28173, Variance: 0.00828

Test Epoch: 56 
task: sign, mean loss: 1.89745, accuracy: 0.44970, avg. loss over tasks: 1.89745
Diversity Loss - Mean: -0.06784, Variance: 0.01322
Semantic Loss - Mean: 1.39410, Variance: 0.04242

Train Epoch: 57 
task: sign, mean loss: 0.14284, accuracy: 0.95652, avg. loss over tasks: 0.14284, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.10457, Variance: 0.01087
Semantic Loss - Mean: 0.22060, Variance: 0.00847

Test Epoch: 57 
task: sign, mean loss: 2.29389, accuracy: 0.43195, avg. loss over tasks: 2.29389
Diversity Loss - Mean: -0.07769, Variance: 0.01322
Semantic Loss - Mean: 1.91582, Variance: 0.04229

Train Epoch: 58 
task: sign, mean loss: 0.22316, accuracy: 0.91848, avg. loss over tasks: 0.22316, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.10492, Variance: 0.01089
Semantic Loss - Mean: 0.23986, Variance: 0.00850

Test Epoch: 58 
task: sign, mean loss: 1.62126, accuracy: 0.70414, avg. loss over tasks: 1.62126
Diversity Loss - Mean: -0.09216, Variance: 0.01324
Semantic Loss - Mean: 1.36664, Variance: 0.04246

Train Epoch: 59 
task: sign, mean loss: 0.18171, accuracy: 0.94565, avg. loss over tasks: 0.18171, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.10386, Variance: 0.01090
Semantic Loss - Mean: 0.24639, Variance: 0.00860

Test Epoch: 59 
task: sign, mean loss: 0.80870, accuracy: 0.78698, avg. loss over tasks: 0.80870
Diversity Loss - Mean: -0.10567, Variance: 0.01327
Semantic Loss - Mean: 0.74984, Variance: 0.04193

Train Epoch: 60 
task: sign, mean loss: 0.10208, accuracy: 0.95652, avg. loss over tasks: 0.10208, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.10404, Variance: 0.01091
Semantic Loss - Mean: 0.16332, Variance: 0.00865

Test Epoch: 60 
task: sign, mean loss: 0.72861, accuracy: 0.81657, avg. loss over tasks: 0.72861
Diversity Loss - Mean: -0.09177, Variance: 0.01327
Semantic Loss - Mean: 0.67082, Variance: 0.04137

Train Epoch: 61 
task: sign, mean loss: 0.12809, accuracy: 0.96739, avg. loss over tasks: 0.12809, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.10561, Variance: 0.01091
Semantic Loss - Mean: 0.21550, Variance: 0.00881

Test Epoch: 61 
task: sign, mean loss: 0.85011, accuracy: 0.82840, avg. loss over tasks: 0.85011
Diversity Loss - Mean: -0.08921, Variance: 0.01325
Semantic Loss - Mean: 0.79088, Variance: 0.04082

Train Epoch: 62 
task: sign, mean loss: 0.13263, accuracy: 0.96196, avg. loss over tasks: 0.13263, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.10362, Variance: 0.01090
Semantic Loss - Mean: 0.18852, Variance: 0.00885

Test Epoch: 62 
task: sign, mean loss: 0.95155, accuracy: 0.79290, avg. loss over tasks: 0.95155
Diversity Loss - Mean: -0.10748, Variance: 0.01328
Semantic Loss - Mean: 0.88611, Variance: 0.04033

Train Epoch: 63 
task: sign, mean loss: 0.10496, accuracy: 0.95109, avg. loss over tasks: 0.10496, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.10903, Variance: 0.01091
Semantic Loss - Mean: 0.15411, Variance: 0.00884

Test Epoch: 63 
task: sign, mean loss: 0.72601, accuracy: 0.74556, avg. loss over tasks: 0.72601
Diversity Loss - Mean: -0.08446, Variance: 0.01337
Semantic Loss - Mean: 0.64658, Variance: 0.03989

Train Epoch: 64 
task: sign, mean loss: 0.09888, accuracy: 0.95652, avg. loss over tasks: 0.09888, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.10722, Variance: 0.01092
Semantic Loss - Mean: 0.18030, Variance: 0.00895

Test Epoch: 64 
task: sign, mean loss: 0.79429, accuracy: 0.77515, avg. loss over tasks: 0.79429
Diversity Loss - Mean: -0.07865, Variance: 0.01346
Semantic Loss - Mean: 0.76473, Variance: 0.03962

Train Epoch: 65 
task: sign, mean loss: 0.05265, accuracy: 0.99457, avg. loss over tasks: 0.05265, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.10571, Variance: 0.01093
Semantic Loss - Mean: 0.13290, Variance: 0.00904

Test Epoch: 65 
task: sign, mean loss: 0.66182, accuracy: 0.82840, avg. loss over tasks: 0.66182
Diversity Loss - Mean: -0.10193, Variance: 0.01347
Semantic Loss - Mean: 0.63365, Variance: 0.03916

Train Epoch: 66 
task: sign, mean loss: 0.05088, accuracy: 0.98370, avg. loss over tasks: 0.05088, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.10661, Variance: 0.01093
Semantic Loss - Mean: 0.11945, Variance: 0.00913

Test Epoch: 66 
task: sign, mean loss: 0.72166, accuracy: 0.82249, avg. loss over tasks: 0.72166
Diversity Loss - Mean: -0.10722, Variance: 0.01347
Semantic Loss - Mean: 0.62824, Variance: 0.03871

Train Epoch: 67 
task: sign, mean loss: 0.04806, accuracy: 0.98370, avg. loss over tasks: 0.04806, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.10813, Variance: 0.01093
Semantic Loss - Mean: 0.12069, Variance: 0.00914

Test Epoch: 67 
task: sign, mean loss: 1.02425, accuracy: 0.79290, avg. loss over tasks: 1.02425
Diversity Loss - Mean: -0.10678, Variance: 0.01346
Semantic Loss - Mean: 0.96439, Variance: 0.03846

Train Epoch: 68 
task: sign, mean loss: 0.17187, accuracy: 0.96196, avg. loss over tasks: 0.17187, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.10871, Variance: 0.01093
Semantic Loss - Mean: 0.23142, Variance: 0.00929

Test Epoch: 68 
task: sign, mean loss: 1.39682, accuracy: 0.77515, avg. loss over tasks: 1.39682
Diversity Loss - Mean: -0.10988, Variance: 0.01345
Semantic Loss - Mean: 1.24352, Variance: 0.03857

Train Epoch: 69 
task: sign, mean loss: 0.09078, accuracy: 0.97826, avg. loss over tasks: 0.09078, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.10961, Variance: 0.01094
Semantic Loss - Mean: 0.17507, Variance: 0.00938

Test Epoch: 69 
task: sign, mean loss: 1.06106, accuracy: 0.80473, avg. loss over tasks: 1.06106
Diversity Loss - Mean: -0.11507, Variance: 0.01348
Semantic Loss - Mean: 1.02242, Variance: 0.03837

Train Epoch: 70 
task: sign, mean loss: 0.06437, accuracy: 0.97826, avg. loss over tasks: 0.06437, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.11045, Variance: 0.01094
Semantic Loss - Mean: 0.11525, Variance: 0.00930

Test Epoch: 70 
task: sign, mean loss: 0.56203, accuracy: 0.84024, avg. loss over tasks: 0.56203
Diversity Loss - Mean: -0.11109, Variance: 0.01350
Semantic Loss - Mean: 0.59982, Variance: 0.03802

Train Epoch: 71 
task: sign, mean loss: 0.04667, accuracy: 0.99457, avg. loss over tasks: 0.04667, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.11027, Variance: 0.01093
Semantic Loss - Mean: 0.10707, Variance: 0.00932

Test Epoch: 71 
task: sign, mean loss: 0.88020, accuracy: 0.82840, avg. loss over tasks: 0.88020
Diversity Loss - Mean: -0.11361, Variance: 0.01350
Semantic Loss - Mean: 0.90003, Variance: 0.03801

Train Epoch: 72 
task: sign, mean loss: 0.02906, accuracy: 0.99457, avg. loss over tasks: 0.02906, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.11208, Variance: 0.01093
Semantic Loss - Mean: 0.09764, Variance: 0.00936

Test Epoch: 72 
task: sign, mean loss: 1.43296, accuracy: 0.79290, avg. loss over tasks: 1.43296
Diversity Loss - Mean: -0.10252, Variance: 0.01351
Semantic Loss - Mean: 1.29812, Variance: 0.03791

Train Epoch: 73 
task: sign, mean loss: 0.08506, accuracy: 0.96739, avg. loss over tasks: 0.08506, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.10960, Variance: 0.01093
Semantic Loss - Mean: 0.15699, Variance: 0.00952

Test Epoch: 73 
task: sign, mean loss: 0.91196, accuracy: 0.82840, avg. loss over tasks: 0.91196
Diversity Loss - Mean: -0.10972, Variance: 0.01351
Semantic Loss - Mean: 0.75815, Variance: 0.03763

Train Epoch: 74 
task: sign, mean loss: 0.08262, accuracy: 0.98370, avg. loss over tasks: 0.08262, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.10546, Variance: 0.01092
Semantic Loss - Mean: 0.12769, Variance: 0.00947

Test Epoch: 74 
task: sign, mean loss: 0.72194, accuracy: 0.80473, avg. loss over tasks: 0.72194
Diversity Loss - Mean: -0.10033, Variance: 0.01354
Semantic Loss - Mean: 0.56590, Variance: 0.03722

Train Epoch: 75 
task: sign, mean loss: 0.09270, accuracy: 0.95652, avg. loss over tasks: 0.09270, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.10791, Variance: 0.01092
Semantic Loss - Mean: 0.17106, Variance: 0.00964

Test Epoch: 75 
task: sign, mean loss: 0.55581, accuracy: 0.82840, avg. loss over tasks: 0.55581
Diversity Loss - Mean: -0.10229, Variance: 0.01355
Semantic Loss - Mean: 0.48944, Variance: 0.03685

Train Epoch: 76 
task: sign, mean loss: 0.08487, accuracy: 0.96739, avg. loss over tasks: 0.08487, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.11276, Variance: 0.01092
Semantic Loss - Mean: 0.16606, Variance: 0.00983

Test Epoch: 76 
task: sign, mean loss: 0.63865, accuracy: 0.84615, avg. loss over tasks: 0.63865
Diversity Loss - Mean: -0.10309, Variance: 0.01355
Semantic Loss - Mean: 0.80033, Variance: 0.03723

Train Epoch: 77 
task: sign, mean loss: 0.04919, accuracy: 0.98370, avg. loss over tasks: 0.04919, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.11113, Variance: 0.01092
Semantic Loss - Mean: 0.11318, Variance: 0.00986

Test Epoch: 77 
task: sign, mean loss: 0.76327, accuracy: 0.85207, avg. loss over tasks: 0.76327
Diversity Loss - Mean: -0.10445, Variance: 0.01355
Semantic Loss - Mean: 1.08665, Variance: 0.03816

Train Epoch: 78 
task: sign, mean loss: 0.05053, accuracy: 0.98370, avg. loss over tasks: 0.05053, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.11117, Variance: 0.01091
Semantic Loss - Mean: 0.10602, Variance: 0.00985

Test Epoch: 78 
task: sign, mean loss: 0.80420, accuracy: 0.86391, avg. loss over tasks: 0.80420
Diversity Loss - Mean: -0.11004, Variance: 0.01354
Semantic Loss - Mean: 0.64747, Variance: 0.03797

Train Epoch: 79 
task: sign, mean loss: 0.02731, accuracy: 0.99457, avg. loss over tasks: 0.02731, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.11313, Variance: 0.01090
Semantic Loss - Mean: 0.07679, Variance: 0.00982

Test Epoch: 79 
task: sign, mean loss: 0.92752, accuracy: 0.86391, avg. loss over tasks: 0.92752
Diversity Loss - Mean: -0.10989, Variance: 0.01353
Semantic Loss - Mean: 0.71547, Variance: 0.03767

Train Epoch: 80 
task: sign, mean loss: 0.02896, accuracy: 0.98370, avg. loss over tasks: 0.02896, lr: 0.00015015
Diversity Loss - Mean: -0.11287, Variance: 0.01089
Semantic Loss - Mean: 0.07487, Variance: 0.00978

Test Epoch: 80 
task: sign, mean loss: 0.71914, accuracy: 0.84615, avg. loss over tasks: 0.71914
Diversity Loss - Mean: -0.10260, Variance: 0.01352
Semantic Loss - Mean: 0.66838, Variance: 0.03747

Train Epoch: 81 
task: sign, mean loss: 0.20192, accuracy: 0.97283, avg. loss over tasks: 0.20192, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.11318, Variance: 0.01088
Semantic Loss - Mean: 0.25070, Variance: 0.00989

Test Epoch: 81 
task: sign, mean loss: 0.42337, accuracy: 0.89349, avg. loss over tasks: 0.42337
Diversity Loss - Mean: -0.11356, Variance: 0.01353
Semantic Loss - Mean: 0.52654, Variance: 0.03730

Train Epoch: 82 
task: sign, mean loss: 0.41373, accuracy: 0.88587, avg. loss over tasks: 0.41373, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.11766, Variance: 0.01089
Semantic Loss - Mean: 0.44698, Variance: 0.01015

Test Epoch: 82 
task: sign, mean loss: 2.63037, accuracy: 0.43787, avg. loss over tasks: 2.63037
Diversity Loss - Mean: -0.10794, Variance: 0.01360
Semantic Loss - Mean: 1.88250, Variance: 0.03775

Train Epoch: 83 
task: sign, mean loss: 0.10173, accuracy: 0.95652, avg. loss over tasks: 0.10173, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.11869, Variance: 0.01090
Semantic Loss - Mean: 0.21296, Variance: 0.01022

Test Epoch: 83 
task: sign, mean loss: 1.15602, accuracy: 0.59763, avg. loss over tasks: 1.15602
Diversity Loss - Mean: -0.11199, Variance: 0.01363
Semantic Loss - Mean: 1.34200, Variance: 0.03819

Train Epoch: 84 
task: sign, mean loss: 0.11410, accuracy: 0.96739, avg. loss over tasks: 0.11410, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.11593, Variance: 0.01090
Semantic Loss - Mean: 0.22678, Variance: 0.01041

Test Epoch: 84 
task: sign, mean loss: 0.53530, accuracy: 0.84615, avg. loss over tasks: 0.53530
Diversity Loss - Mean: -0.12194, Variance: 0.01363
Semantic Loss - Mean: 0.46347, Variance: 0.03777

Train Epoch: 85 
task: sign, mean loss: 0.13239, accuracy: 0.95652, avg. loss over tasks: 0.13239, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.12060, Variance: 0.01091
Semantic Loss - Mean: 0.17593, Variance: 0.01038

Test Epoch: 85 
task: sign, mean loss: 0.93930, accuracy: 0.82249, avg. loss over tasks: 0.93930
Diversity Loss - Mean: -0.12462, Variance: 0.01362
Semantic Loss - Mean: 0.81873, Variance: 0.03767

Train Epoch: 86 
task: sign, mean loss: 0.04599, accuracy: 0.98370, avg. loss over tasks: 0.04599, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.11985, Variance: 0.01093
Semantic Loss - Mean: 0.12237, Variance: 0.01040

Test Epoch: 86 
task: sign, mean loss: 0.78099, accuracy: 0.84024, avg. loss over tasks: 0.78099
Diversity Loss - Mean: -0.12002, Variance: 0.01361
Semantic Loss - Mean: 0.79156, Variance: 0.03740

Train Epoch: 87 
task: sign, mean loss: 0.01785, accuracy: 1.00000, avg. loss over tasks: 0.01785, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.11779, Variance: 0.01093
Semantic Loss - Mean: 0.09225, Variance: 0.01036

Test Epoch: 87 
task: sign, mean loss: 0.63076, accuracy: 0.88757, avg. loss over tasks: 0.63076
Diversity Loss - Mean: -0.12030, Variance: 0.01362
Semantic Loss - Mean: 0.61389, Variance: 0.03707

Train Epoch: 88 
task: sign, mean loss: 0.02181, accuracy: 0.99457, avg. loss over tasks: 0.02181, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.11796, Variance: 0.01094
Semantic Loss - Mean: 0.07094, Variance: 0.01031

Test Epoch: 88 
task: sign, mean loss: 0.60759, accuracy: 0.86391, avg. loss over tasks: 0.60759
Diversity Loss - Mean: -0.12088, Variance: 0.01362
Semantic Loss - Mean: 0.56002, Variance: 0.03675

Train Epoch: 89 
task: sign, mean loss: 0.02125, accuracy: 1.00000, avg. loss over tasks: 0.02125, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.11802, Variance: 0.01094
Semantic Loss - Mean: 0.07878, Variance: 0.01027

Test Epoch: 89 
task: sign, mean loss: 0.71684, accuracy: 0.84024, avg. loss over tasks: 0.71684
Diversity Loss - Mean: -0.12065, Variance: 0.01363
Semantic Loss - Mean: 0.66158, Variance: 0.03640

Train Epoch: 90 
task: sign, mean loss: 0.01118, accuracy: 1.00000, avg. loss over tasks: 0.01118, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.11938, Variance: 0.01095
Semantic Loss - Mean: 0.06671, Variance: 0.01024

Test Epoch: 90 
task: sign, mean loss: 0.78704, accuracy: 0.85207, avg. loss over tasks: 0.78704
Diversity Loss - Mean: -0.12252, Variance: 0.01363
Semantic Loss - Mean: 0.75732, Variance: 0.03615

Train Epoch: 91 
task: sign, mean loss: 0.07210, accuracy: 0.98370, avg. loss over tasks: 0.07210, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.11879, Variance: 0.01095
Semantic Loss - Mean: 0.11249, Variance: 0.01023

Test Epoch: 91 
task: sign, mean loss: 0.78414, accuracy: 0.85799, avg. loss over tasks: 0.78414
Diversity Loss - Mean: -0.12246, Variance: 0.01363
Semantic Loss - Mean: 0.73471, Variance: 0.03590

Train Epoch: 92 
task: sign, mean loss: 0.01159, accuracy: 1.00000, avg. loss over tasks: 0.01159, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.11799, Variance: 0.01096
Semantic Loss - Mean: 0.07737, Variance: 0.01025

Test Epoch: 92 
task: sign, mean loss: 0.72900, accuracy: 0.86982, avg. loss over tasks: 0.72900
Diversity Loss - Mean: -0.12147, Variance: 0.01364
Semantic Loss - Mean: 0.66864, Variance: 0.03564

Train Epoch: 93 
task: sign, mean loss: 0.03318, accuracy: 0.98913, avg. loss over tasks: 0.03318, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.11902, Variance: 0.01096
Semantic Loss - Mean: 0.09712, Variance: 0.01048

Test Epoch: 93 
task: sign, mean loss: 0.64115, accuracy: 0.88757, avg. loss over tasks: 0.64115
Diversity Loss - Mean: -0.12400, Variance: 0.01365
Semantic Loss - Mean: 0.57778, Variance: 0.03533

Train Epoch: 94 
task: sign, mean loss: 0.01105, accuracy: 1.00000, avg. loss over tasks: 0.01105, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.11932, Variance: 0.01096
Semantic Loss - Mean: 0.04893, Variance: 0.01042

Test Epoch: 94 
task: sign, mean loss: 0.68791, accuracy: 0.84615, avg. loss over tasks: 0.68791
Diversity Loss - Mean: -0.12233, Variance: 0.01365
Semantic Loss - Mean: 0.68666, Variance: 0.03515

Train Epoch: 95 
task: sign, mean loss: 0.00880, accuracy: 1.00000, avg. loss over tasks: 0.00880, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.11884, Variance: 0.01096
Semantic Loss - Mean: 0.05138, Variance: 0.01036

Test Epoch: 95 
task: sign, mean loss: 0.70691, accuracy: 0.85207, avg. loss over tasks: 0.70691
Diversity Loss - Mean: -0.11699, Variance: 0.01365
Semantic Loss - Mean: 0.73348, Variance: 0.03492

Train Epoch: 96 
task: sign, mean loss: 0.01577, accuracy: 0.99457, avg. loss over tasks: 0.01577, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.11646, Variance: 0.01096
Semantic Loss - Mean: 0.07300, Variance: 0.01032

Test Epoch: 96 
task: sign, mean loss: 0.59527, accuracy: 0.86391, avg. loss over tasks: 0.59527
Diversity Loss - Mean: -0.11737, Variance: 0.01364
Semantic Loss - Mean: 0.58159, Variance: 0.03466

Train Epoch: 97 
task: sign, mean loss: 0.00880, accuracy: 1.00000, avg. loss over tasks: 0.00880, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.11853, Variance: 0.01096
Semantic Loss - Mean: 0.06409, Variance: 0.01033

Test Epoch: 97 
task: sign, mean loss: 0.67481, accuracy: 0.87574, avg. loss over tasks: 0.67481
Diversity Loss - Mean: -0.12085, Variance: 0.01364
Semantic Loss - Mean: 0.65107, Variance: 0.03436

Train Epoch: 98 
task: sign, mean loss: 0.01563, accuracy: 0.98913, avg. loss over tasks: 0.01563, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.12111, Variance: 0.01096
Semantic Loss - Mean: 0.05468, Variance: 0.01030

Test Epoch: 98 
task: sign, mean loss: 0.66826, accuracy: 0.88757, avg. loss over tasks: 0.66826
Diversity Loss - Mean: -0.12246, Variance: 0.01364
Semantic Loss - Mean: 0.64848, Variance: 0.03410

Train Epoch: 99 
task: sign, mean loss: 0.00604, accuracy: 1.00000, avg. loss over tasks: 0.00604, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.12087, Variance: 0.01096
Semantic Loss - Mean: 0.03339, Variance: 0.01023

Test Epoch: 99 
task: sign, mean loss: 0.62518, accuracy: 0.87574, avg. loss over tasks: 0.62518
Diversity Loss - Mean: -0.12228, Variance: 0.01363
Semantic Loss - Mean: 0.63309, Variance: 0.03388

Train Epoch: 100 
task: sign, mean loss: 0.04447, accuracy: 0.98913, avg. loss over tasks: 0.04447, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.11948, Variance: 0.01096
Semantic Loss - Mean: 0.09698, Variance: 0.01023

Test Epoch: 100 
task: sign, mean loss: 0.47290, accuracy: 0.89349, avg. loss over tasks: 0.47290
Diversity Loss - Mean: -0.12250, Variance: 0.01364
Semantic Loss - Mean: 0.49101, Variance: 0.03362

Train Epoch: 101 
task: sign, mean loss: 0.01388, accuracy: 1.00000, avg. loss over tasks: 0.01388, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.12124, Variance: 0.01096
Semantic Loss - Mean: 0.05764, Variance: 0.01021

Test Epoch: 101 
task: sign, mean loss: 0.57619, accuracy: 0.89349, avg. loss over tasks: 0.57619
Diversity Loss - Mean: -0.12226, Variance: 0.01363
Semantic Loss - Mean: 0.61737, Variance: 0.03342

Train Epoch: 102 
task: sign, mean loss: 0.00431, accuracy: 1.00000, avg. loss over tasks: 0.00431, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.12021, Variance: 0.01096
Semantic Loss - Mean: 0.04522, Variance: 0.01021

Test Epoch: 102 
task: sign, mean loss: 0.63904, accuracy: 0.87574, avg. loss over tasks: 0.63904
Diversity Loss - Mean: -0.12230, Variance: 0.01363
Semantic Loss - Mean: 0.69934, Variance: 0.03318

Train Epoch: 103 
task: sign, mean loss: 0.02630, accuracy: 0.99457, avg. loss over tasks: 0.02630, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.12045, Variance: 0.01096
Semantic Loss - Mean: 0.06052, Variance: 0.01015

Test Epoch: 103 
task: sign, mean loss: 0.65210, accuracy: 0.87574, avg. loss over tasks: 0.65210
Diversity Loss - Mean: -0.12341, Variance: 0.01363
Semantic Loss - Mean: 0.69452, Variance: 0.03291

Train Epoch: 104 
task: sign, mean loss: 0.00729, accuracy: 1.00000, avg. loss over tasks: 0.00729, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.12156, Variance: 0.01096
Semantic Loss - Mean: 0.04412, Variance: 0.01010

Test Epoch: 104 
task: sign, mean loss: 0.56705, accuracy: 0.87574, avg. loss over tasks: 0.56705
Diversity Loss - Mean: -0.12381, Variance: 0.01363
Semantic Loss - Mean: 0.60859, Variance: 0.03266

Train Epoch: 105 
task: sign, mean loss: 0.00659, accuracy: 1.00000, avg. loss over tasks: 0.00659, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.12131, Variance: 0.01096
Semantic Loss - Mean: 0.03567, Variance: 0.01003

Test Epoch: 105 
task: sign, mean loss: 0.61666, accuracy: 0.88166, avg. loss over tasks: 0.61666
Diversity Loss - Mean: -0.12348, Variance: 0.01363
Semantic Loss - Mean: 0.69168, Variance: 0.03246

Train Epoch: 106 
task: sign, mean loss: 0.00294, accuracy: 1.00000, avg. loss over tasks: 0.00294, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.12213, Variance: 0.01096
Semantic Loss - Mean: 0.04164, Variance: 0.01000

Test Epoch: 106 
task: sign, mean loss: 0.56432, accuracy: 0.87574, avg. loss over tasks: 0.56432
Diversity Loss - Mean: -0.12321, Variance: 0.01363
Semantic Loss - Mean: 0.65922, Variance: 0.03232

Train Epoch: 107 
task: sign, mean loss: 0.00428, accuracy: 1.00000, avg. loss over tasks: 0.00428, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.12168, Variance: 0.01096
Semantic Loss - Mean: 0.03784, Variance: 0.00997

Test Epoch: 107 
task: sign, mean loss: 0.63026, accuracy: 0.86391, avg. loss over tasks: 0.63026
Diversity Loss - Mean: -0.12369, Variance: 0.01363
Semantic Loss - Mean: 0.69779, Variance: 0.03217

Train Epoch: 108 
task: sign, mean loss: 0.00359, accuracy: 1.00000, avg. loss over tasks: 0.00359, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.12257, Variance: 0.01096
Semantic Loss - Mean: 0.03657, Variance: 0.00992

Test Epoch: 108 
task: sign, mean loss: 0.68011, accuracy: 0.86391, avg. loss over tasks: 0.68011
Diversity Loss - Mean: -0.12413, Variance: 0.01363
Semantic Loss - Mean: 0.71623, Variance: 0.03198

Train Epoch: 109 
task: sign, mean loss: 0.00794, accuracy: 0.99457, avg. loss over tasks: 0.00794, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.12216, Variance: 0.01096
Semantic Loss - Mean: 0.05501, Variance: 0.00990

Test Epoch: 109 
task: sign, mean loss: 0.69922, accuracy: 0.85207, avg. loss over tasks: 0.69922
Diversity Loss - Mean: -0.12423, Variance: 0.01363
Semantic Loss - Mean: 0.76502, Variance: 0.03179

Train Epoch: 110 
task: sign, mean loss: 0.00273, accuracy: 1.00000, avg. loss over tasks: 0.00273, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.12198, Variance: 0.01096
Semantic Loss - Mean: 0.02572, Variance: 0.00983

Test Epoch: 110 
task: sign, mean loss: 0.67913, accuracy: 0.84615, avg. loss over tasks: 0.67913
Diversity Loss - Mean: -0.12490, Variance: 0.01363
Semantic Loss - Mean: 0.78271, Variance: 0.03162

Train Epoch: 111 
task: sign, mean loss: 0.00811, accuracy: 1.00000, avg. loss over tasks: 0.00811, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.12286, Variance: 0.01096
Semantic Loss - Mean: 0.05729, Variance: 0.00981

Test Epoch: 111 
task: sign, mean loss: 0.63369, accuracy: 0.86982, avg. loss over tasks: 0.63369
Diversity Loss - Mean: -0.12614, Variance: 0.01363
Semantic Loss - Mean: 0.66496, Variance: 0.03138

Train Epoch: 112 
task: sign, mean loss: 0.00190, accuracy: 1.00000, avg. loss over tasks: 0.00190, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.12377, Variance: 0.01095
Semantic Loss - Mean: 0.04148, Variance: 0.00978

Test Epoch: 112 
task: sign, mean loss: 0.64079, accuracy: 0.86391, avg. loss over tasks: 0.64079
Diversity Loss - Mean: -0.12584, Variance: 0.01363
Semantic Loss - Mean: 0.68684, Variance: 0.03119

Train Epoch: 113 
task: sign, mean loss: 0.00930, accuracy: 0.99457, avg. loss over tasks: 0.00930, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.12355, Variance: 0.01095
Semantic Loss - Mean: 0.05308, Variance: 0.00976

Test Epoch: 113 
task: sign, mean loss: 0.65580, accuracy: 0.86391, avg. loss over tasks: 0.65580
Diversity Loss - Mean: -0.12564, Variance: 0.01363
Semantic Loss - Mean: 0.66985, Variance: 0.03098

Train Epoch: 114 
task: sign, mean loss: 0.00445, accuracy: 1.00000, avg. loss over tasks: 0.00445, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.12317, Variance: 0.01095
Semantic Loss - Mean: 0.05512, Variance: 0.00976

Test Epoch: 114 
task: sign, mean loss: 0.71883, accuracy: 0.85799, avg. loss over tasks: 0.71883
Diversity Loss - Mean: -0.12595, Variance: 0.01363
Semantic Loss - Mean: 0.74032, Variance: 0.03079

Train Epoch: 115 
task: sign, mean loss: 0.00931, accuracy: 1.00000, avg. loss over tasks: 0.00931, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.12344, Variance: 0.01095
Semantic Loss - Mean: 0.05261, Variance: 0.00974

Test Epoch: 115 
task: sign, mean loss: 0.71822, accuracy: 0.85207, avg. loss over tasks: 0.71822
Diversity Loss - Mean: -0.12606, Variance: 0.01363
Semantic Loss - Mean: 0.77524, Variance: 0.03064

Train Epoch: 116 
task: sign, mean loss: 0.00335, accuracy: 1.00000, avg. loss over tasks: 0.00335, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.12333, Variance: 0.01095
Semantic Loss - Mean: 0.04624, Variance: 0.00970

Test Epoch: 116 
task: sign, mean loss: 0.58974, accuracy: 0.87574, avg. loss over tasks: 0.58974
Diversity Loss - Mean: -0.12679, Variance: 0.01363
Semantic Loss - Mean: 0.60913, Variance: 0.03042

Train Epoch: 117 
task: sign, mean loss: 0.00240, accuracy: 1.00000, avg. loss over tasks: 0.00240, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.12448, Variance: 0.01096
Semantic Loss - Mean: 0.04157, Variance: 0.00967

Test Epoch: 117 
task: sign, mean loss: 0.63842, accuracy: 0.87574, avg. loss over tasks: 0.63842
Diversity Loss - Mean: -0.12689, Variance: 0.01363
Semantic Loss - Mean: 0.65298, Variance: 0.03022

Train Epoch: 118 
task: sign, mean loss: 0.00754, accuracy: 1.00000, avg. loss over tasks: 0.00754, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.12435, Variance: 0.01095
Semantic Loss - Mean: 0.04813, Variance: 0.00963

Test Epoch: 118 
task: sign, mean loss: 0.66964, accuracy: 0.85799, avg. loss over tasks: 0.66964
Diversity Loss - Mean: -0.12737, Variance: 0.01363
Semantic Loss - Mean: 0.67844, Variance: 0.03003

Train Epoch: 119 
task: sign, mean loss: 0.00190, accuracy: 1.00000, avg. loss over tasks: 0.00190, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.12442, Variance: 0.01095
Semantic Loss - Mean: 0.04096, Variance: 0.00961

Test Epoch: 119 
task: sign, mean loss: 0.68002, accuracy: 0.86982, avg. loss over tasks: 0.68002
Diversity Loss - Mean: -0.12587, Variance: 0.01363
Semantic Loss - Mean: 0.67176, Variance: 0.02989

Train Epoch: 120 
task: sign, mean loss: 0.00258, accuracy: 1.00000, avg. loss over tasks: 0.00258, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.12477, Variance: 0.01095
Semantic Loss - Mean: 0.04247, Variance: 0.00957

Test Epoch: 120 
task: sign, mean loss: 0.68651, accuracy: 0.85799, avg. loss over tasks: 0.68651
Diversity Loss - Mean: -0.12698, Variance: 0.01363
Semantic Loss - Mean: 0.69341, Variance: 0.02974

Train Epoch: 121 
task: sign, mean loss: 0.00552, accuracy: 1.00000, avg. loss over tasks: 0.00552, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.12571, Variance: 0.01096
Semantic Loss - Mean: 0.05232, Variance: 0.00954

Test Epoch: 121 
task: sign, mean loss: 0.65070, accuracy: 0.84615, avg. loss over tasks: 0.65070
Diversity Loss - Mean: -0.12586, Variance: 0.01363
Semantic Loss - Mean: 0.69048, Variance: 0.02957

Train Epoch: 122 
task: sign, mean loss: 0.00426, accuracy: 1.00000, avg. loss over tasks: 0.00426, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.12385, Variance: 0.01096
Semantic Loss - Mean: 0.04372, Variance: 0.00951

Test Epoch: 122 
task: sign, mean loss: 0.65832, accuracy: 0.84615, avg. loss over tasks: 0.65832
Diversity Loss - Mean: -0.12720, Variance: 0.01362
Semantic Loss - Mean: 0.68426, Variance: 0.02941

Train Epoch: 123 
task: sign, mean loss: 0.00952, accuracy: 0.99457, avg. loss over tasks: 0.00952, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.12484, Variance: 0.01096
Semantic Loss - Mean: 0.04419, Variance: 0.00947

Test Epoch: 123 
task: sign, mean loss: 0.63189, accuracy: 0.85207, avg. loss over tasks: 0.63189
Diversity Loss - Mean: -0.12716, Variance: 0.01362
Semantic Loss - Mean: 0.66446, Variance: 0.02921

Train Epoch: 124 
task: sign, mean loss: 0.00615, accuracy: 1.00000, avg. loss over tasks: 0.00615, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.12507, Variance: 0.01096
Semantic Loss - Mean: 0.04455, Variance: 0.00943

Test Epoch: 124 
task: sign, mean loss: 0.55401, accuracy: 0.88757, avg. loss over tasks: 0.55401
Diversity Loss - Mean: -0.12718, Variance: 0.01363
Semantic Loss - Mean: 0.56690, Variance: 0.02900

Train Epoch: 125 
task: sign, mean loss: 0.00637, accuracy: 0.99457, avg. loss over tasks: 0.00637, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.12560, Variance: 0.01096
Semantic Loss - Mean: 0.02311, Variance: 0.00937

Test Epoch: 125 
task: sign, mean loss: 0.62242, accuracy: 0.85207, avg. loss over tasks: 0.62242
Diversity Loss - Mean: -0.12702, Variance: 0.01363
Semantic Loss - Mean: 0.66971, Variance: 0.02882

Train Epoch: 126 
task: sign, mean loss: 0.00581, accuracy: 1.00000, avg. loss over tasks: 0.00581, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.12461, Variance: 0.01097
Semantic Loss - Mean: 0.03495, Variance: 0.00933

Test Epoch: 126 
task: sign, mean loss: 0.62715, accuracy: 0.85207, avg. loss over tasks: 0.62715
Diversity Loss - Mean: -0.12634, Variance: 0.01363
Semantic Loss - Mean: 0.62766, Variance: 0.02864

Train Epoch: 127 
task: sign, mean loss: 0.00365, accuracy: 1.00000, avg. loss over tasks: 0.00365, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.12524, Variance: 0.01097
Semantic Loss - Mean: 0.02627, Variance: 0.00927

Test Epoch: 127 
task: sign, mean loss: 0.61033, accuracy: 0.85799, avg. loss over tasks: 0.61033
Diversity Loss - Mean: -0.12800, Variance: 0.01363
Semantic Loss - Mean: 0.62959, Variance: 0.02845

Train Epoch: 128 
task: sign, mean loss: 0.00416, accuracy: 1.00000, avg. loss over tasks: 0.00416, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.12494, Variance: 0.01097
Semantic Loss - Mean: 0.03529, Variance: 0.00924

Test Epoch: 128 
task: sign, mean loss: 0.68601, accuracy: 0.84024, avg. loss over tasks: 0.68601
Diversity Loss - Mean: -0.12730, Variance: 0.01362
Semantic Loss - Mean: 0.71733, Variance: 0.02827

Train Epoch: 129 
task: sign, mean loss: 0.00783, accuracy: 0.99457, avg. loss over tasks: 0.00783, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.12525, Variance: 0.01097
Semantic Loss - Mean: 0.04351, Variance: 0.00921

Test Epoch: 129 
task: sign, mean loss: 0.63623, accuracy: 0.84024, avg. loss over tasks: 0.63623
Diversity Loss - Mean: -0.12778, Variance: 0.01363
Semantic Loss - Mean: 0.66085, Variance: 0.02809

Train Epoch: 130 
task: sign, mean loss: 0.00288, accuracy: 1.00000, avg. loss over tasks: 0.00288, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.12596, Variance: 0.01098
Semantic Loss - Mean: 0.02759, Variance: 0.00916

Test Epoch: 130 
task: sign, mean loss: 0.63206, accuracy: 0.85207, avg. loss over tasks: 0.63206
Diversity Loss - Mean: -0.12778, Variance: 0.01363
Semantic Loss - Mean: 0.68022, Variance: 0.02791

Train Epoch: 131 
task: sign, mean loss: 0.00401, accuracy: 1.00000, avg. loss over tasks: 0.00401, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.12566, Variance: 0.01098
Semantic Loss - Mean: 0.04402, Variance: 0.00913

Test Epoch: 131 
task: sign, mean loss: 0.71470, accuracy: 0.83432, avg. loss over tasks: 0.71470
Diversity Loss - Mean: -0.12725, Variance: 0.01363
Semantic Loss - Mean: 0.77103, Variance: 0.02774

Train Epoch: 132 
task: sign, mean loss: 0.00222, accuracy: 1.00000, avg. loss over tasks: 0.00222, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.12560, Variance: 0.01098
Semantic Loss - Mean: 0.02832, Variance: 0.00910

Test Epoch: 132 
task: sign, mean loss: 0.67862, accuracy: 0.84024, avg. loss over tasks: 0.67862
Diversity Loss - Mean: -0.12832, Variance: 0.01363
Semantic Loss - Mean: 0.72194, Variance: 0.02757

Train Epoch: 133 
task: sign, mean loss: 0.00130, accuracy: 1.00000, avg. loss over tasks: 0.00130, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.12641, Variance: 0.01098
Semantic Loss - Mean: 0.02542, Variance: 0.00905

Test Epoch: 133 
task: sign, mean loss: 0.67798, accuracy: 0.84024, avg. loss over tasks: 0.67798
Diversity Loss - Mean: -0.12789, Variance: 0.01363
Semantic Loss - Mean: 0.72022, Variance: 0.02739

Train Epoch: 134 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.12609, Variance: 0.01098
Semantic Loss - Mean: 0.02018, Variance: 0.00900

Test Epoch: 134 
task: sign, mean loss: 0.69742, accuracy: 0.84615, avg. loss over tasks: 0.69742
Diversity Loss - Mean: -0.12874, Variance: 0.01363
Semantic Loss - Mean: 0.73522, Variance: 0.02723

Train Epoch: 135 
task: sign, mean loss: 0.00169, accuracy: 1.00000, avg. loss over tasks: 0.00169, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.12581, Variance: 0.01098
Semantic Loss - Mean: 0.02471, Variance: 0.00895

Test Epoch: 135 
task: sign, mean loss: 0.71049, accuracy: 0.84024, avg. loss over tasks: 0.71049
Diversity Loss - Mean: -0.12857, Variance: 0.01363
Semantic Loss - Mean: 0.72905, Variance: 0.02707

Train Epoch: 136 
task: sign, mean loss: 0.00128, accuracy: 1.00000, avg. loss over tasks: 0.00128, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.12619, Variance: 0.01098
Semantic Loss - Mean: 0.03281, Variance: 0.00893

Test Epoch: 136 
task: sign, mean loss: 0.68877, accuracy: 0.84615, avg. loss over tasks: 0.68877
Diversity Loss - Mean: -0.12840, Variance: 0.01363
Semantic Loss - Mean: 0.70396, Variance: 0.02691

Train Epoch: 137 
task: sign, mean loss: 0.00408, accuracy: 1.00000, avg. loss over tasks: 0.00408, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.12641, Variance: 0.01099
Semantic Loss - Mean: 0.04039, Variance: 0.00890

Test Epoch: 137 
task: sign, mean loss: 0.66584, accuracy: 0.85207, avg. loss over tasks: 0.66584
Diversity Loss - Mean: -0.12805, Variance: 0.01363
Semantic Loss - Mean: 0.67697, Variance: 0.02675

Train Epoch: 138 
task: sign, mean loss: 0.01238, accuracy: 0.99457, avg. loss over tasks: 0.01238, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.12569, Variance: 0.01099
Semantic Loss - Mean: 0.05895, Variance: 0.00889

Test Epoch: 138 
task: sign, mean loss: 0.69567, accuracy: 0.84615, avg. loss over tasks: 0.69567
Diversity Loss - Mean: -0.12717, Variance: 0.01363
Semantic Loss - Mean: 0.71214, Variance: 0.02659

Train Epoch: 139 
task: sign, mean loss: 0.00250, accuracy: 1.00000, avg. loss over tasks: 0.00250, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.12573, Variance: 0.01099
Semantic Loss - Mean: 0.02740, Variance: 0.00883

Test Epoch: 139 
task: sign, mean loss: 0.70298, accuracy: 0.84024, avg. loss over tasks: 0.70298
Diversity Loss - Mean: -0.12812, Variance: 0.01363
Semantic Loss - Mean: 0.73739, Variance: 0.02643

Train Epoch: 140 
task: sign, mean loss: 0.02258, accuracy: 0.99457, avg. loss over tasks: 0.02258, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.12652, Variance: 0.01099
Semantic Loss - Mean: 0.04647, Variance: 0.00881

Test Epoch: 140 
task: sign, mean loss: 0.64969, accuracy: 0.85799, avg. loss over tasks: 0.64969
Diversity Loss - Mean: -0.12843, Variance: 0.01363
Semantic Loss - Mean: 0.68404, Variance: 0.02627

Train Epoch: 141 
task: sign, mean loss: 0.00331, accuracy: 1.00000, avg. loss over tasks: 0.00331, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.12589, Variance: 0.01100
Semantic Loss - Mean: 0.04125, Variance: 0.00882

Test Epoch: 141 
task: sign, mean loss: 0.62517, accuracy: 0.85207, avg. loss over tasks: 0.62517
Diversity Loss - Mean: -0.12839, Variance: 0.01364
Semantic Loss - Mean: 0.65181, Variance: 0.02612

Train Epoch: 142 
task: sign, mean loss: 0.00375, accuracy: 1.00000, avg. loss over tasks: 0.00375, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.12565, Variance: 0.01100
Semantic Loss - Mean: 0.06088, Variance: 0.00882

Test Epoch: 142 
task: sign, mean loss: 0.63051, accuracy: 0.85207, avg. loss over tasks: 0.63051
Diversity Loss - Mean: -0.12798, Variance: 0.01364
Semantic Loss - Mean: 0.64399, Variance: 0.02596

Train Epoch: 143 
task: sign, mean loss: 0.01161, accuracy: 0.99457, avg. loss over tasks: 0.01161, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.12645, Variance: 0.01100
Semantic Loss - Mean: 0.02550, Variance: 0.00877

Test Epoch: 143 
task: sign, mean loss: 0.62182, accuracy: 0.85207, avg. loss over tasks: 0.62182
Diversity Loss - Mean: -0.12846, Variance: 0.01364
Semantic Loss - Mean: 0.66409, Variance: 0.02581

Train Epoch: 144 
task: sign, mean loss: 0.00165, accuracy: 1.00000, avg. loss over tasks: 0.00165, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.12633, Variance: 0.01100
Semantic Loss - Mean: 0.02212, Variance: 0.00871

Test Epoch: 144 
task: sign, mean loss: 0.64779, accuracy: 0.84615, avg. loss over tasks: 0.64779
Diversity Loss - Mean: -0.12829, Variance: 0.01364
Semantic Loss - Mean: 0.69591, Variance: 0.02566

Train Epoch: 145 
task: sign, mean loss: 0.00101, accuracy: 1.00000, avg. loss over tasks: 0.00101, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.12639, Variance: 0.01100
Semantic Loss - Mean: 0.03265, Variance: 0.00869

Test Epoch: 145 
task: sign, mean loss: 0.64328, accuracy: 0.84615, avg. loss over tasks: 0.64328
Diversity Loss - Mean: -0.12826, Variance: 0.01364
Semantic Loss - Mean: 0.67670, Variance: 0.02551

Train Epoch: 146 
task: sign, mean loss: 0.00792, accuracy: 1.00000, avg. loss over tasks: 0.00792, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.12556, Variance: 0.01101
Semantic Loss - Mean: 0.03985, Variance: 0.00866

Test Epoch: 146 
task: sign, mean loss: 0.73892, accuracy: 0.82840, avg. loss over tasks: 0.73892
Diversity Loss - Mean: -0.12817, Variance: 0.01364
Semantic Loss - Mean: 0.79501, Variance: 0.02536

Train Epoch: 147 
task: sign, mean loss: 0.00252, accuracy: 1.00000, avg. loss over tasks: 0.00252, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.12649, Variance: 0.01101
Semantic Loss - Mean: 0.03255, Variance: 0.00864

Test Epoch: 147 
task: sign, mean loss: 0.71344, accuracy: 0.84024, avg. loss over tasks: 0.71344
Diversity Loss - Mean: -0.12854, Variance: 0.01364
Semantic Loss - Mean: 0.77109, Variance: 0.02522

Train Epoch: 148 
task: sign, mean loss: 0.00191, accuracy: 1.00000, avg. loss over tasks: 0.00191, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.12486, Variance: 0.01101
Semantic Loss - Mean: 0.04129, Variance: 0.00863

Test Epoch: 148 
task: sign, mean loss: 0.69004, accuracy: 0.83432, avg. loss over tasks: 0.69004
Diversity Loss - Mean: -0.12849, Variance: 0.01364
Semantic Loss - Mean: 0.72139, Variance: 0.02508

Train Epoch: 149 
task: sign, mean loss: 0.00234, accuracy: 1.00000, avg. loss over tasks: 0.00234, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.12578, Variance: 0.01101
Semantic Loss - Mean: 0.05937, Variance: 0.00865

Test Epoch: 149 
task: sign, mean loss: 0.66537, accuracy: 0.85207, avg. loss over tasks: 0.66537
Diversity Loss - Mean: -0.12913, Variance: 0.01364
Semantic Loss - Mean: 0.68281, Variance: 0.02494

Train Epoch: 150 
task: sign, mean loss: 0.00200, accuracy: 1.00000, avg. loss over tasks: 0.00200, lr: 3e-07
Diversity Loss - Mean: -0.12678, Variance: 0.01101
Semantic Loss - Mean: 0.01918, Variance: 0.00860

Test Epoch: 150 
task: sign, mean loss: 0.64633, accuracy: 0.84024, avg. loss over tasks: 0.64633
Diversity Loss - Mean: -0.12847, Variance: 0.01365
Semantic Loss - Mean: 0.67011, Variance: 0.02480

