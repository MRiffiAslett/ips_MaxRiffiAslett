Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09298, accuracy: 0.63587, avg. loss over tasks: 1.09298, lr: 3e-05
Diversity Loss - Mean: -0.01078, Variance: 0.01051
Semantic Loss - Mean: 1.43160, Variance: 0.07294

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17595, accuracy: 0.66272, avg. loss over tasks: 1.17595
Diversity Loss - Mean: -0.03161, Variance: 0.01246
Semantic Loss - Mean: 1.16116, Variance: 0.05333

Train Epoch: 2 
task: sign, mean loss: 0.96753, accuracy: 0.66848, avg. loss over tasks: 0.96753, lr: 6e-05
Diversity Loss - Mean: -0.02276, Variance: 0.01048
Semantic Loss - Mean: 0.98185, Variance: 0.03949

Test Epoch: 2 
task: sign, mean loss: 1.12162, accuracy: 0.66272, avg. loss over tasks: 1.12162
Diversity Loss - Mean: -0.03822, Variance: 0.01213
Semantic Loss - Mean: 1.14639, Variance: 0.03203

Train Epoch: 3 
task: sign, mean loss: 0.81067, accuracy: 0.69022, avg. loss over tasks: 0.81067, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.04630, Variance: 0.01032
Semantic Loss - Mean: 0.99250, Variance: 0.02738

Test Epoch: 3 
task: sign, mean loss: 1.28233, accuracy: 0.65089, avg. loss over tasks: 1.28233
Diversity Loss - Mean: -0.06941, Variance: 0.01139
Semantic Loss - Mean: 1.11048, Variance: 0.02841

Train Epoch: 4 
task: sign, mean loss: 0.78160, accuracy: 0.69565, avg. loss over tasks: 0.78160, lr: 0.00012
Diversity Loss - Mean: -0.07352, Variance: 0.01009
Semantic Loss - Mean: 0.89318, Variance: 0.02115

Test Epoch: 4 
task: sign, mean loss: 1.43196, accuracy: 0.53846, avg. loss over tasks: 1.43196
Diversity Loss - Mean: -0.08039, Variance: 0.01075
Semantic Loss - Mean: 1.04937, Variance: 0.02262

Train Epoch: 5 
task: sign, mean loss: 0.73944, accuracy: 0.69565, avg. loss over tasks: 0.73944, lr: 0.00015
Diversity Loss - Mean: -0.07138, Variance: 0.00988
Semantic Loss - Mean: 0.79356, Variance: 0.01721

Test Epoch: 5 
task: sign, mean loss: 2.39692, accuracy: 0.30769, avg. loss over tasks: 2.39692
Diversity Loss - Mean: -0.06203, Variance: 0.01028
Semantic Loss - Mean: 1.32987, Variance: 0.02167

Train Epoch: 6 
task: sign, mean loss: 0.71569, accuracy: 0.77174, avg. loss over tasks: 0.71569, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.06113, Variance: 0.00969
Semantic Loss - Mean: 0.74403, Variance: 0.01467

Test Epoch: 6 
task: sign, mean loss: 1.86718, accuracy: 0.47929, avg. loss over tasks: 1.86718
Diversity Loss - Mean: -0.06359, Variance: 0.01040
Semantic Loss - Mean: 1.43222, Variance: 0.02124

Train Epoch: 7 
task: sign, mean loss: 0.60199, accuracy: 0.76630, avg. loss over tasks: 0.60199, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.06863, Variance: 0.00973
Semantic Loss - Mean: 0.65083, Variance: 0.01292

Test Epoch: 7 
task: sign, mean loss: 2.46395, accuracy: 0.34911, avg. loss over tasks: 2.46395
Diversity Loss - Mean: -0.04499, Variance: 0.01038
Semantic Loss - Mean: 1.67689, Variance: 0.02324

Train Epoch: 8 
task: sign, mean loss: 0.56186, accuracy: 0.78261, avg. loss over tasks: 0.56186, lr: 0.00024
Diversity Loss - Mean: -0.04988, Variance: 0.00966
Semantic Loss - Mean: 0.59615, Variance: 0.01170

Test Epoch: 8 
task: sign, mean loss: 2.21743, accuracy: 0.43195, avg. loss over tasks: 2.21743
Diversity Loss - Mean: -0.04202, Variance: 0.01064
Semantic Loss - Mean: 1.56956, Variance: 0.02496

Train Epoch: 9 
task: sign, mean loss: 0.65022, accuracy: 0.80435, avg. loss over tasks: 0.65022, lr: 0.00027
Diversity Loss - Mean: -0.05002, Variance: 0.00958
Semantic Loss - Mean: 0.60625, Variance: 0.01087

Test Epoch: 9 
task: sign, mean loss: 3.06525, accuracy: 0.39053, avg. loss over tasks: 3.06525
Diversity Loss - Mean: 0.00090, Variance: 0.01058
Semantic Loss - Mean: 2.39661, Variance: 0.03285

Train Epoch: 10 
task: sign, mean loss: 0.86229, accuracy: 0.70652, avg. loss over tasks: 0.86229, lr: 0.0003
Diversity Loss - Mean: -0.05702, Variance: 0.00950
Semantic Loss - Mean: 0.81968, Variance: 0.01036

Test Epoch: 10 
task: sign, mean loss: 2.26774, accuracy: 0.37870, avg. loss over tasks: 2.26774
Diversity Loss - Mean: -0.03159, Variance: 0.01073
Semantic Loss - Mean: 1.75815, Variance: 0.03520

Train Epoch: 11 
task: sign, mean loss: 0.62892, accuracy: 0.74457, avg. loss over tasks: 0.62892, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.05393, Variance: 0.00948
Semantic Loss - Mean: 0.61652, Variance: 0.00966

Test Epoch: 11 
task: sign, mean loss: 2.14769, accuracy: 0.56805, avg. loss over tasks: 2.14769
Diversity Loss - Mean: -0.05499, Variance: 0.01103
Semantic Loss - Mean: 1.81525, Variance: 0.03337

Train Epoch: 12 
task: sign, mean loss: 0.43452, accuracy: 0.84239, avg. loss over tasks: 0.43452, lr: 0.000299849111021216
Diversity Loss - Mean: -0.04659, Variance: 0.00948
Semantic Loss - Mean: 0.46731, Variance: 0.00914

Test Epoch: 12 
task: sign, mean loss: 3.25785, accuracy: 0.33728, avg. loss over tasks: 3.25785
Diversity Loss - Mean: -0.01735, Variance: 0.01117
Semantic Loss - Mean: 2.27172, Variance: 0.03375

Train Epoch: 13 
task: sign, mean loss: 0.53708, accuracy: 0.80978, avg. loss over tasks: 0.53708, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.05520, Variance: 0.00949
Semantic Loss - Mean: 0.54407, Variance: 0.00891

Test Epoch: 13 
task: sign, mean loss: 2.38791, accuracy: 0.28994, avg. loss over tasks: 2.38791
Diversity Loss - Mean: -0.02999, Variance: 0.01111
Semantic Loss - Mean: 1.88332, Variance: 0.03369

Train Epoch: 14 
task: sign, mean loss: 0.37944, accuracy: 0.84783, avg. loss over tasks: 0.37944, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.06227, Variance: 0.00949
Semantic Loss - Mean: 0.43390, Variance: 0.00842

Test Epoch: 14 
task: sign, mean loss: 1.96636, accuracy: 0.31361, avg. loss over tasks: 1.96636
Diversity Loss - Mean: -0.05912, Variance: 0.01118
Semantic Loss - Mean: 1.68092, Variance: 0.03263

Train Epoch: 15 
task: sign, mean loss: 0.27611, accuracy: 0.91304, avg. loss over tasks: 0.27611, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.06019, Variance: 0.00950
Semantic Loss - Mean: 0.29768, Variance: 0.00795

Test Epoch: 15 
task: sign, mean loss: 2.05303, accuracy: 0.57396, avg. loss over tasks: 2.05303
Diversity Loss - Mean: -0.08537, Variance: 0.01134
Semantic Loss - Mean: 1.84631, Variance: 0.03208

Train Epoch: 16 
task: sign, mean loss: 0.28908, accuracy: 0.89130, avg. loss over tasks: 0.28908, lr: 0.000298643821800925
Diversity Loss - Mean: -0.05580, Variance: 0.00950
Semantic Loss - Mean: 0.31991, Variance: 0.00801

Test Epoch: 16 
task: sign, mean loss: 2.11386, accuracy: 0.62722, avg. loss over tasks: 2.11386
Diversity Loss - Mean: -0.06829, Variance: 0.01153
Semantic Loss - Mean: 1.84163, Variance: 0.03075

Train Epoch: 17 
task: sign, mean loss: 0.29712, accuracy: 0.87500, avg. loss over tasks: 0.29712, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.05936, Variance: 0.00948
Semantic Loss - Mean: 0.33238, Variance: 0.00788

Test Epoch: 17 
task: sign, mean loss: 3.15980, accuracy: 0.35503, avg. loss over tasks: 3.15980
Diversity Loss - Mean: -0.06019, Variance: 0.01156
Semantic Loss - Mean: 2.67239, Variance: 0.03247

Train Epoch: 18 
task: sign, mean loss: 0.45307, accuracy: 0.84783, avg. loss over tasks: 0.45307, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.07440, Variance: 0.00956
Semantic Loss - Mean: 0.44776, Variance: 0.00771

Test Epoch: 18 
task: sign, mean loss: 1.87227, accuracy: 0.68047, avg. loss over tasks: 1.87227
Diversity Loss - Mean: -0.09545, Variance: 0.01181
Semantic Loss - Mean: 1.69950, Variance: 0.03152

Train Epoch: 19 
task: sign, mean loss: 0.55301, accuracy: 0.84239, avg. loss over tasks: 0.55301, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.08831, Variance: 0.00967
Semantic Loss - Mean: 0.60348, Variance: 0.00769

Test Epoch: 19 
task: sign, mean loss: 1.92066, accuracy: 0.60355, avg. loss over tasks: 1.92066
Diversity Loss - Mean: -0.09700, Variance: 0.01190
Semantic Loss - Mean: 1.93181, Variance: 0.03093

Train Epoch: 20 
task: sign, mean loss: 0.39978, accuracy: 0.85326, avg. loss over tasks: 0.39978, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.09850, Variance: 0.00981
Semantic Loss - Mean: 0.44067, Variance: 0.00748

Test Epoch: 20 
task: sign, mean loss: 1.91123, accuracy: 0.63314, avg. loss over tasks: 1.91123
Diversity Loss - Mean: -0.10142, Variance: 0.01200
Semantic Loss - Mean: 1.75251, Variance: 0.02994

Train Epoch: 21 
task: sign, mean loss: 0.26081, accuracy: 0.89674, avg. loss over tasks: 0.26081, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.07804, Variance: 0.00985
Semantic Loss - Mean: 0.29425, Variance: 0.00731

Test Epoch: 21 
task: sign, mean loss: 2.94297, accuracy: 0.35503, avg. loss over tasks: 2.94297
Diversity Loss - Mean: -0.07029, Variance: 0.01201
Semantic Loss - Mean: 2.37594, Variance: 0.03049

Train Epoch: 22 
task: sign, mean loss: 0.30304, accuracy: 0.89130, avg. loss over tasks: 0.30304, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.07592, Variance: 0.00989
Semantic Loss - Mean: 0.35777, Variance: 0.00727

Test Epoch: 22 
task: sign, mean loss: 2.82698, accuracy: 0.30769, avg. loss over tasks: 2.82698
Diversity Loss - Mean: -0.06849, Variance: 0.01208
Semantic Loss - Mean: 2.42100, Variance: 0.03185

Train Epoch: 23 
task: sign, mean loss: 0.31740, accuracy: 0.88587, avg. loss over tasks: 0.31740, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.08008, Variance: 0.00993
Semantic Loss - Mean: 0.35824, Variance: 0.00717

Test Epoch: 23 
task: sign, mean loss: 4.15152, accuracy: 0.20710, avg. loss over tasks: 4.15152
Diversity Loss - Mean: -0.02754, Variance: 0.01216
Semantic Loss - Mean: 3.03655, Variance: 0.03297

Train Epoch: 24 
task: sign, mean loss: 0.28171, accuracy: 0.90217, avg. loss over tasks: 0.28171, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.08636, Variance: 0.00997
Semantic Loss - Mean: 0.31349, Variance: 0.00700

Test Epoch: 24 
task: sign, mean loss: 3.14053, accuracy: 0.36686, avg. loss over tasks: 3.14053
Diversity Loss - Mean: -0.06239, Variance: 0.01213
Semantic Loss - Mean: 2.67471, Variance: 0.03302

Train Epoch: 25 
task: sign, mean loss: 0.30763, accuracy: 0.88043, avg. loss over tasks: 0.30763, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.07762, Variance: 0.00999
Semantic Loss - Mean: 0.31751, Variance: 0.00684

Test Epoch: 25 
task: sign, mean loss: 2.39714, accuracy: 0.45562, avg. loss over tasks: 2.39714
Diversity Loss - Mean: -0.08321, Variance: 0.01210
Semantic Loss - Mean: 2.08982, Variance: 0.03291

Train Epoch: 26 
task: sign, mean loss: 0.33706, accuracy: 0.85326, avg. loss over tasks: 0.33706, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.09307, Variance: 0.01005
Semantic Loss - Mean: 0.35707, Variance: 0.00686

Test Epoch: 26 
task: sign, mean loss: 2.27529, accuracy: 0.39053, avg. loss over tasks: 2.27529
Diversity Loss - Mean: -0.09352, Variance: 0.01209
Semantic Loss - Mean: 2.10380, Variance: 0.03254

Train Epoch: 27 
task: sign, mean loss: 0.28608, accuracy: 0.90761, avg. loss over tasks: 0.28608, lr: 0.000289228031029578
Diversity Loss - Mean: -0.08998, Variance: 0.01008
Semantic Loss - Mean: 0.32695, Variance: 0.00679

Test Epoch: 27 
task: sign, mean loss: 2.45853, accuracy: 0.33728, avg. loss over tasks: 2.45853
Diversity Loss - Mean: -0.09845, Variance: 0.01208
Semantic Loss - Mean: 2.18811, Variance: 0.03189

Train Epoch: 28 
task: sign, mean loss: 0.19819, accuracy: 0.92391, avg. loss over tasks: 0.19819, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.09482, Variance: 0.01014
Semantic Loss - Mean: 0.24291, Variance: 0.00665

Test Epoch: 28 
task: sign, mean loss: 2.44977, accuracy: 0.37870, avg. loss over tasks: 2.44977
Diversity Loss - Mean: -0.09317, Variance: 0.01212
Semantic Loss - Mean: 2.07957, Variance: 0.03137

Train Epoch: 29 
task: sign, mean loss: 0.22662, accuracy: 0.92391, avg. loss over tasks: 0.22662, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.09436, Variance: 0.01020
Semantic Loss - Mean: 0.25185, Variance: 0.00653

Test Epoch: 29 
task: sign, mean loss: 2.53441, accuracy: 0.39053, avg. loss over tasks: 2.53441
Diversity Loss - Mean: -0.08801, Variance: 0.01213
Semantic Loss - Mean: 2.28705, Variance: 0.03103

Train Epoch: 30 
task: sign, mean loss: 0.19139, accuracy: 0.93478, avg. loss over tasks: 0.19139, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.09266, Variance: 0.01025
Semantic Loss - Mean: 0.22197, Variance: 0.00643

Test Epoch: 30 
task: sign, mean loss: 2.52245, accuracy: 0.35503, avg. loss over tasks: 2.52245
Diversity Loss - Mean: -0.09179, Variance: 0.01217
Semantic Loss - Mean: 2.29056, Variance: 0.03056

Train Epoch: 31 
task: sign, mean loss: 0.11494, accuracy: 0.95109, avg. loss over tasks: 0.11494, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.09701, Variance: 0.01031
Semantic Loss - Mean: 0.15600, Variance: 0.00629

Test Epoch: 31 
task: sign, mean loss: 3.21479, accuracy: 0.35503, avg. loss over tasks: 3.21479
Diversity Loss - Mean: -0.07309, Variance: 0.01219
Semantic Loss - Mean: 2.61451, Variance: 0.03063

Train Epoch: 32 
task: sign, mean loss: 0.09417, accuracy: 0.96196, avg. loss over tasks: 0.09417, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.09544, Variance: 0.01037
Semantic Loss - Mean: 0.10600, Variance: 0.00614

Test Epoch: 32 
task: sign, mean loss: 3.24234, accuracy: 0.27811, avg. loss over tasks: 3.24234
Diversity Loss - Mean: -0.08054, Variance: 0.01216
Semantic Loss - Mean: 2.68245, Variance: 0.03150

Train Epoch: 33 
task: sign, mean loss: 0.10463, accuracy: 0.97283, avg. loss over tasks: 0.10463, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.09326, Variance: 0.01042
Semantic Loss - Mean: 0.12222, Variance: 0.00604

Test Epoch: 33 
task: sign, mean loss: 2.90738, accuracy: 0.45562, avg. loss over tasks: 2.90738
Diversity Loss - Mean: -0.10337, Variance: 0.01220
Semantic Loss - Mean: 2.63481, Variance: 0.03157

Train Epoch: 34 
task: sign, mean loss: 0.20191, accuracy: 0.94565, avg. loss over tasks: 0.20191, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.09293, Variance: 0.01045
Semantic Loss - Mean: 0.22279, Variance: 0.00609

Test Epoch: 34 
task: sign, mean loss: 3.10698, accuracy: 0.59172, avg. loss over tasks: 3.10698
Diversity Loss - Mean: -0.11910, Variance: 0.01246
Semantic Loss - Mean: 2.75525, Variance: 0.03132

Train Epoch: 35 
task: sign, mean loss: 0.17805, accuracy: 0.94565, avg. loss over tasks: 0.17805, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.10715, Variance: 0.01054
Semantic Loss - Mean: 0.25584, Variance: 0.00625

Test Epoch: 35 
task: sign, mean loss: 2.49915, accuracy: 0.35503, avg. loss over tasks: 2.49915
Diversity Loss - Mean: -0.09390, Variance: 0.01249
Semantic Loss - Mean: 2.22452, Variance: 0.03202

Train Epoch: 36 
task: sign, mean loss: 0.11267, accuracy: 0.95109, avg. loss over tasks: 0.11267, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.10064, Variance: 0.01056
Semantic Loss - Mean: 0.14932, Variance: 0.00615

Test Epoch: 36 
task: sign, mean loss: 2.62808, accuracy: 0.39645, avg. loss over tasks: 2.62808
Diversity Loss - Mean: -0.11129, Variance: 0.01253
Semantic Loss - Mean: 2.48044, Variance: 0.03237

Train Epoch: 37 
task: sign, mean loss: 0.12124, accuracy: 0.95109, avg. loss over tasks: 0.12124, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.10520, Variance: 0.01062
Semantic Loss - Mean: 0.17273, Variance: 0.00603

Test Epoch: 37 
task: sign, mean loss: 3.41563, accuracy: 0.31953, avg. loss over tasks: 3.41563
Diversity Loss - Mean: -0.08562, Variance: 0.01252
Semantic Loss - Mean: 2.98119, Variance: 0.03270

Train Epoch: 38 
task: sign, mean loss: 0.11200, accuracy: 0.97283, avg. loss over tasks: 0.11200, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.10051, Variance: 0.01066
Semantic Loss - Mean: 0.12523, Variance: 0.00592

Test Epoch: 38 
task: sign, mean loss: 3.89901, accuracy: 0.31953, avg. loss over tasks: 3.89901
Diversity Loss - Mean: -0.09530, Variance: 0.01252
Semantic Loss - Mean: 3.19511, Variance: 0.03344

Train Epoch: 39 
task: sign, mean loss: 0.07172, accuracy: 0.96196, avg. loss over tasks: 0.07172, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.09956, Variance: 0.01071
Semantic Loss - Mean: 0.09189, Variance: 0.00584

Test Epoch: 39 
task: sign, mean loss: 3.52174, accuracy: 0.40828, avg. loss over tasks: 3.52174
Diversity Loss - Mean: -0.10006, Variance: 0.01261
Semantic Loss - Mean: 3.02815, Variance: 0.03323

Train Epoch: 40 
task: sign, mean loss: 0.08695, accuracy: 0.98370, avg. loss over tasks: 0.08695, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.10426, Variance: 0.01076
Semantic Loss - Mean: 0.12598, Variance: 0.00579

Test Epoch: 40 
task: sign, mean loss: 3.15517, accuracy: 0.39053, avg. loss over tasks: 3.15517
Diversity Loss - Mean: -0.09842, Variance: 0.01266
Semantic Loss - Mean: 2.55113, Variance: 0.03330

Train Epoch: 41 
task: sign, mean loss: 0.08563, accuracy: 0.97283, avg. loss over tasks: 0.08563, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.10281, Variance: 0.01080
Semantic Loss - Mean: 0.12126, Variance: 0.00583

Test Epoch: 41 
task: sign, mean loss: 3.54242, accuracy: 0.40237, avg. loss over tasks: 3.54242
Diversity Loss - Mean: -0.10201, Variance: 0.01271
Semantic Loss - Mean: 2.91558, Variance: 0.03453

Train Epoch: 42 
task: sign, mean loss: 0.23523, accuracy: 0.94565, avg. loss over tasks: 0.23523, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.10820, Variance: 0.01084
Semantic Loss - Mean: 0.25054, Variance: 0.00583

Test Epoch: 42 
task: sign, mean loss: 3.40058, accuracy: 0.38462, avg. loss over tasks: 3.40058
Diversity Loss - Mean: -0.10770, Variance: 0.01276
Semantic Loss - Mean: 3.16002, Variance: 0.03469

Train Epoch: 43 
task: sign, mean loss: 0.10959, accuracy: 0.96196, avg. loss over tasks: 0.10959, lr: 0.000260757131773478
Diversity Loss - Mean: -0.10817, Variance: 0.01088
Semantic Loss - Mean: 0.14704, Variance: 0.00573

Test Epoch: 43 
task: sign, mean loss: 2.50560, accuracy: 0.45562, avg. loss over tasks: 2.50560
Diversity Loss - Mean: -0.11256, Variance: 0.01284
Semantic Loss - Mean: 2.30891, Variance: 0.03514

Train Epoch: 44 
task: sign, mean loss: 0.11180, accuracy: 0.96739, avg. loss over tasks: 0.11180, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.11145, Variance: 0.01095
Semantic Loss - Mean: 0.14071, Variance: 0.00567

Test Epoch: 44 
task: sign, mean loss: 2.92170, accuracy: 0.33728, avg. loss over tasks: 2.92170
Diversity Loss - Mean: -0.10148, Variance: 0.01289
Semantic Loss - Mean: 2.73379, Variance: 0.03542

Train Epoch: 45 
task: sign, mean loss: 0.09929, accuracy: 0.95109, avg. loss over tasks: 0.09929, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.11014, Variance: 0.01100
Semantic Loss - Mean: 0.13263, Variance: 0.00563

Test Epoch: 45 
task: sign, mean loss: 2.61632, accuracy: 0.39645, avg. loss over tasks: 2.61632
Diversity Loss - Mean: -0.11484, Variance: 0.01293
Semantic Loss - Mean: 2.34761, Variance: 0.03631

Train Epoch: 46 
task: sign, mean loss: 0.08374, accuracy: 0.97826, avg. loss over tasks: 0.08374, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.10648, Variance: 0.01103
Semantic Loss - Mean: 0.10987, Variance: 0.00559

Test Epoch: 46 
task: sign, mean loss: 2.84595, accuracy: 0.47337, avg. loss over tasks: 2.84595
Diversity Loss - Mean: -0.11243, Variance: 0.01293
Semantic Loss - Mean: 2.45489, Variance: 0.03727

Train Epoch: 47 
task: sign, mean loss: 0.08372, accuracy: 0.97283, avg. loss over tasks: 0.08372, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.10885, Variance: 0.01107
Semantic Loss - Mean: 0.12811, Variance: 0.00555

Test Epoch: 47 
task: sign, mean loss: 3.88324, accuracy: 0.33136, avg. loss over tasks: 3.88324
Diversity Loss - Mean: -0.08702, Variance: 0.01295
Semantic Loss - Mean: 3.66283, Variance: 0.03770

Train Epoch: 48 
task: sign, mean loss: 0.07245, accuracy: 0.96739, avg. loss over tasks: 0.07245, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.10910, Variance: 0.01111
Semantic Loss - Mean: 0.11104, Variance: 0.00552

Test Epoch: 48 
task: sign, mean loss: 3.53301, accuracy: 0.35503, avg. loss over tasks: 3.53301
Diversity Loss - Mean: -0.10321, Variance: 0.01294
Semantic Loss - Mean: 3.04387, Variance: 0.03781

Train Epoch: 49 
task: sign, mean loss: 0.05807, accuracy: 0.98370, avg. loss over tasks: 0.05807, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.10839, Variance: 0.01115
Semantic Loss - Mean: 0.07985, Variance: 0.00546

Test Epoch: 49 
task: sign, mean loss: 3.66063, accuracy: 0.37870, avg. loss over tasks: 3.66063
Diversity Loss - Mean: -0.10797, Variance: 0.01296
Semantic Loss - Mean: 3.13567, Variance: 0.03764

Train Epoch: 50 
task: sign, mean loss: 0.04381, accuracy: 0.98913, avg. loss over tasks: 0.04381, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.10704, Variance: 0.01119
Semantic Loss - Mean: 0.07263, Variance: 0.00544

Test Epoch: 50 
task: sign, mean loss: 4.40877, accuracy: 0.30769, avg. loss over tasks: 4.40877
Diversity Loss - Mean: -0.09635, Variance: 0.01298
Semantic Loss - Mean: 3.73032, Variance: 0.03877

Train Epoch: 51 
task: sign, mean loss: 0.07872, accuracy: 0.96739, avg. loss over tasks: 0.07872, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.10997, Variance: 0.01125
Semantic Loss - Mean: 0.12083, Variance: 0.00541

Test Epoch: 51 
task: sign, mean loss: 2.64604, accuracy: 0.57396, avg. loss over tasks: 2.64604
Diversity Loss - Mean: -0.11391, Variance: 0.01302
Semantic Loss - Mean: 2.25619, Variance: 0.03884

Train Epoch: 52 
task: sign, mean loss: 0.15336, accuracy: 0.94565, avg. loss over tasks: 0.15336, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.11109, Variance: 0.01129
Semantic Loss - Mean: 0.17637, Variance: 0.00536

Test Epoch: 52 
task: sign, mean loss: 3.13059, accuracy: 0.52663, avg. loss over tasks: 3.13059
Diversity Loss - Mean: -0.12082, Variance: 0.01308
Semantic Loss - Mean: 2.75037, Variance: 0.03895

Train Epoch: 53 
task: sign, mean loss: 0.17960, accuracy: 0.94565, avg. loss over tasks: 0.17960, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.11477, Variance: 0.01133
Semantic Loss - Mean: 0.20791, Variance: 0.00540

Test Epoch: 53 
task: sign, mean loss: 2.89174, accuracy: 0.50296, avg. loss over tasks: 2.89174
Diversity Loss - Mean: -0.12339, Variance: 0.01311
Semantic Loss - Mean: 2.60757, Variance: 0.03942

Train Epoch: 54 
task: sign, mean loss: 0.15401, accuracy: 0.93478, avg. loss over tasks: 0.15401, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.11627, Variance: 0.01137
Semantic Loss - Mean: 0.17330, Variance: 0.00537

Test Epoch: 54 
task: sign, mean loss: 3.60412, accuracy: 0.29586, avg. loss over tasks: 3.60412
Diversity Loss - Mean: -0.10671, Variance: 0.01314
Semantic Loss - Mean: 3.14231, Variance: 0.03980

Train Epoch: 55 
task: sign, mean loss: 0.11888, accuracy: 0.96739, avg. loss over tasks: 0.11888, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.11294, Variance: 0.01141
Semantic Loss - Mean: 0.18948, Variance: 0.00541

Test Epoch: 55 
task: sign, mean loss: 2.71399, accuracy: 0.46746, avg. loss over tasks: 2.71399
Diversity Loss - Mean: -0.11740, Variance: 0.01322
Semantic Loss - Mean: 2.50087, Variance: 0.03982

Train Epoch: 56 
task: sign, mean loss: 0.07994, accuracy: 0.97283, avg. loss over tasks: 0.07994, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.11340, Variance: 0.01143
Semantic Loss - Mean: 0.11498, Variance: 0.00535

Test Epoch: 56 
task: sign, mean loss: 2.84089, accuracy: 0.43195, avg. loss over tasks: 2.84089
Diversity Loss - Mean: -0.11309, Variance: 0.01327
Semantic Loss - Mean: 2.55851, Variance: 0.03969

Train Epoch: 57 
task: sign, mean loss: 0.04641, accuracy: 0.97283, avg. loss over tasks: 0.04641, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.11370, Variance: 0.01147
Semantic Loss - Mean: 0.07254, Variance: 0.00531

Test Epoch: 57 
task: sign, mean loss: 3.23401, accuracy: 0.44379, avg. loss over tasks: 3.23401
Diversity Loss - Mean: -0.12400, Variance: 0.01332
Semantic Loss - Mean: 2.83085, Variance: 0.03980

Train Epoch: 58 
task: sign, mean loss: 0.01954, accuracy: 0.99457, avg. loss over tasks: 0.01954, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.11503, Variance: 0.01150
Semantic Loss - Mean: 0.03365, Variance: 0.00522

Test Epoch: 58 
task: sign, mean loss: 3.43992, accuracy: 0.40828, avg. loss over tasks: 3.43992
Diversity Loss - Mean: -0.12107, Variance: 0.01335
Semantic Loss - Mean: 3.03233, Variance: 0.03995

Train Epoch: 59 
task: sign, mean loss: 0.03644, accuracy: 0.98370, avg. loss over tasks: 0.03644, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.11549, Variance: 0.01153
Semantic Loss - Mean: 0.05548, Variance: 0.00515

Test Epoch: 59 
task: sign, mean loss: 3.60887, accuracy: 0.39645, avg. loss over tasks: 3.60887
Diversity Loss - Mean: -0.11457, Variance: 0.01337
Semantic Loss - Mean: 2.98273, Variance: 0.04002

Train Epoch: 60 
task: sign, mean loss: 0.03098, accuracy: 0.98370, avg. loss over tasks: 0.03098, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.11619, Variance: 0.01154
Semantic Loss - Mean: 0.04907, Variance: 0.00510

Test Epoch: 60 
task: sign, mean loss: 2.67523, accuracy: 0.47337, avg. loss over tasks: 2.67523
Diversity Loss - Mean: -0.12319, Variance: 0.01339
Semantic Loss - Mean: 2.34330, Variance: 0.04003

Train Epoch: 61 
task: sign, mean loss: 0.02201, accuracy: 0.98913, avg. loss over tasks: 0.02201, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.11903, Variance: 0.01156
Semantic Loss - Mean: 0.04478, Variance: 0.00505

Test Epoch: 61 
task: sign, mean loss: 2.69175, accuracy: 0.47337, avg. loss over tasks: 2.69175
Diversity Loss - Mean: -0.12190, Variance: 0.01339
Semantic Loss - Mean: 2.41157, Variance: 0.04051

Train Epoch: 62 
task: sign, mean loss: 0.03908, accuracy: 0.98913, avg. loss over tasks: 0.03908, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.11862, Variance: 0.01158
Semantic Loss - Mean: 0.08693, Variance: 0.00507

Test Epoch: 62 
task: sign, mean loss: 3.34574, accuracy: 0.37278, avg. loss over tasks: 3.34574
Diversity Loss - Mean: -0.11301, Variance: 0.01339
Semantic Loss - Mean: 2.73128, Variance: 0.04150

Train Epoch: 63 
task: sign, mean loss: 0.05279, accuracy: 0.98913, avg. loss over tasks: 0.05279, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.11866, Variance: 0.01160
Semantic Loss - Mean: 0.07881, Variance: 0.00505

Test Epoch: 63 
task: sign, mean loss: 3.69985, accuracy: 0.40828, avg. loss over tasks: 3.69985
Diversity Loss - Mean: -0.12075, Variance: 0.01338
Semantic Loss - Mean: 3.07392, Variance: 0.04204

Train Epoch: 64 
task: sign, mean loss: 0.04690, accuracy: 0.97826, avg. loss over tasks: 0.04690, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.12059, Variance: 0.01162
Semantic Loss - Mean: 0.08169, Variance: 0.00503

Test Epoch: 64 
task: sign, mean loss: 4.18110, accuracy: 0.37870, avg. loss over tasks: 4.18110
Diversity Loss - Mean: -0.12106, Variance: 0.01339
Semantic Loss - Mean: 3.36640, Variance: 0.04262

Train Epoch: 65 
task: sign, mean loss: 0.12332, accuracy: 0.96739, avg. loss over tasks: 0.12332, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.12072, Variance: 0.01163
Semantic Loss - Mean: 0.17036, Variance: 0.00507

Test Epoch: 65 
task: sign, mean loss: 5.18867, accuracy: 0.24852, avg. loss over tasks: 5.18867
Diversity Loss - Mean: -0.10893, Variance: 0.01342
Semantic Loss - Mean: 3.71002, Variance: 0.04321

Train Epoch: 66 
task: sign, mean loss: 0.13294, accuracy: 0.97283, avg. loss over tasks: 0.13294, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.12152, Variance: 0.01165
Semantic Loss - Mean: 0.17036, Variance: 0.00521

Test Epoch: 66 
task: sign, mean loss: 3.39860, accuracy: 0.36095, avg. loss over tasks: 3.39860
Diversity Loss - Mean: -0.12249, Variance: 0.01342
Semantic Loss - Mean: 2.65729, Variance: 0.04322

Train Epoch: 67 
task: sign, mean loss: 0.03891, accuracy: 0.98370, avg. loss over tasks: 0.03891, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.12219, Variance: 0.01167
Semantic Loss - Mean: 0.07061, Variance: 0.00516

Test Epoch: 67 
task: sign, mean loss: 3.50118, accuracy: 0.40828, avg. loss over tasks: 3.50118
Diversity Loss - Mean: -0.11205, Variance: 0.01341
Semantic Loss - Mean: 3.07545, Variance: 0.04372

Train Epoch: 68 
task: sign, mean loss: 0.04107, accuracy: 0.98913, avg. loss over tasks: 0.04107, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.12095, Variance: 0.01169
Semantic Loss - Mean: 0.06949, Variance: 0.00516

Test Epoch: 68 
task: sign, mean loss: 3.06451, accuracy: 0.39053, avg. loss over tasks: 3.06451
Diversity Loss - Mean: -0.11663, Variance: 0.01339
Semantic Loss - Mean: 2.75169, Variance: 0.04419

Train Epoch: 69 
task: sign, mean loss: 0.06824, accuracy: 0.97826, avg. loss over tasks: 0.06824, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.11819, Variance: 0.01170
Semantic Loss - Mean: 0.07784, Variance: 0.00517

Test Epoch: 69 
task: sign, mean loss: 2.94712, accuracy: 0.40828, avg. loss over tasks: 2.94712
Diversity Loss - Mean: -0.12298, Variance: 0.01338
Semantic Loss - Mean: 2.57892, Variance: 0.04489

Train Epoch: 70 
task: sign, mean loss: 0.06088, accuracy: 0.97826, avg. loss over tasks: 0.06088, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.12267, Variance: 0.01172
Semantic Loss - Mean: 0.10522, Variance: 0.00517

Test Epoch: 70 
task: sign, mean loss: 2.89272, accuracy: 0.55621, avg. loss over tasks: 2.89272
Diversity Loss - Mean: -0.13078, Variance: 0.01344
Semantic Loss - Mean: 2.61406, Variance: 0.04515

Train Epoch: 71 
task: sign, mean loss: 0.09989, accuracy: 0.98370, avg. loss over tasks: 0.09989, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.12483, Variance: 0.01175
Semantic Loss - Mean: 0.12322, Variance: 0.00515

Test Epoch: 71 
task: sign, mean loss: 2.92688, accuracy: 0.52663, avg. loss over tasks: 2.92688
Diversity Loss - Mean: -0.13235, Variance: 0.01348
Semantic Loss - Mean: 2.52174, Variance: 0.04523

Train Epoch: 72 
task: sign, mean loss: 0.03631, accuracy: 0.98913, avg. loss over tasks: 0.03631, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.12270, Variance: 0.01176
Semantic Loss - Mean: 0.06915, Variance: 0.00515

Test Epoch: 72 
task: sign, mean loss: 3.29440, accuracy: 0.43787, avg. loss over tasks: 3.29440
Diversity Loss - Mean: -0.12777, Variance: 0.01348
Semantic Loss - Mean: 2.82434, Variance: 0.04514

Train Epoch: 73 
task: sign, mean loss: 0.11947, accuracy: 0.97283, avg. loss over tasks: 0.11947, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.12506, Variance: 0.01177
Semantic Loss - Mean: 0.13580, Variance: 0.00514

Test Epoch: 73 
task: sign, mean loss: 3.19436, accuracy: 0.51479, avg. loss over tasks: 3.19436
Diversity Loss - Mean: -0.13106, Variance: 0.01349
Semantic Loss - Mean: 2.70025, Variance: 0.04505

Train Epoch: 74 
task: sign, mean loss: 0.05030, accuracy: 0.98913, avg. loss over tasks: 0.05030, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.12380, Variance: 0.01179
Semantic Loss - Mean: 0.05871, Variance: 0.00508

Test Epoch: 74 
task: sign, mean loss: 3.29255, accuracy: 0.37870, avg. loss over tasks: 3.29255
Diversity Loss - Mean: -0.12828, Variance: 0.01349
Semantic Loss - Mean: 2.99503, Variance: 0.04511

Train Epoch: 75 
task: sign, mean loss: 0.01216, accuracy: 1.00000, avg. loss over tasks: 0.01216, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.12516, Variance: 0.01180
Semantic Loss - Mean: 0.03090, Variance: 0.00503

Test Epoch: 75 
task: sign, mean loss: 3.25687, accuracy: 0.40828, avg. loss over tasks: 3.25687
Diversity Loss - Mean: -0.13241, Variance: 0.01350
Semantic Loss - Mean: 2.90639, Variance: 0.04496

Train Epoch: 76 
task: sign, mean loss: 0.00653, accuracy: 1.00000, avg. loss over tasks: 0.00653, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.12669, Variance: 0.01182
Semantic Loss - Mean: 0.02466, Variance: 0.00499

Test Epoch: 76 
task: sign, mean loss: 3.30251, accuracy: 0.43787, avg. loss over tasks: 3.30251
Diversity Loss - Mean: -0.13252, Variance: 0.01352
Semantic Loss - Mean: 2.99110, Variance: 0.04485

Train Epoch: 77 
task: sign, mean loss: 0.03073, accuracy: 0.98913, avg. loss over tasks: 0.03073, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.12743, Variance: 0.01184
Semantic Loss - Mean: 0.05184, Variance: 0.00495

Test Epoch: 77 
task: sign, mean loss: 3.80165, accuracy: 0.31953, avg. loss over tasks: 3.80165
Diversity Loss - Mean: -0.12686, Variance: 0.01352
Semantic Loss - Mean: 3.44003, Variance: 0.04509

Train Epoch: 78 
task: sign, mean loss: 0.02160, accuracy: 0.98913, avg. loss over tasks: 0.02160, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.12644, Variance: 0.01186
Semantic Loss - Mean: 0.03458, Variance: 0.00490

Test Epoch: 78 
task: sign, mean loss: 3.67276, accuracy: 0.33136, avg. loss over tasks: 3.67276
Diversity Loss - Mean: -0.12954, Variance: 0.01352
Semantic Loss - Mean: 3.32501, Variance: 0.04528

Train Epoch: 79 
task: sign, mean loss: 0.01127, accuracy: 0.99457, avg. loss over tasks: 0.01127, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.12772, Variance: 0.01187
Semantic Loss - Mean: 0.01808, Variance: 0.00485

Test Epoch: 79 
task: sign, mean loss: 3.22397, accuracy: 0.41420, avg. loss over tasks: 3.22397
Diversity Loss - Mean: -0.13286, Variance: 0.01353
Semantic Loss - Mean: 2.86412, Variance: 0.04527

Train Epoch: 80 
task: sign, mean loss: 0.00499, accuracy: 1.00000, avg. loss over tasks: 0.00499, lr: 0.00015015
Diversity Loss - Mean: -0.12838, Variance: 0.01189
Semantic Loss - Mean: 0.01697, Variance: 0.00482

Test Epoch: 80 
task: sign, mean loss: 3.14283, accuracy: 0.49704, avg. loss over tasks: 3.14283
Diversity Loss - Mean: -0.13484, Variance: 0.01355
Semantic Loss - Mean: 2.82675, Variance: 0.04515

Train Epoch: 81 
task: sign, mean loss: 0.03788, accuracy: 0.99457, avg. loss over tasks: 0.03788, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.12913, Variance: 0.01191
Semantic Loss - Mean: 0.03256, Variance: 0.00478

Test Epoch: 81 
task: sign, mean loss: 3.34048, accuracy: 0.40237, avg. loss over tasks: 3.34048
Diversity Loss - Mean: -0.13319, Variance: 0.01356
Semantic Loss - Mean: 2.96864, Variance: 0.04522

Train Epoch: 82 
task: sign, mean loss: 0.05042, accuracy: 0.98913, avg. loss over tasks: 0.05042, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.13020, Variance: 0.01193
Semantic Loss - Mean: 0.08261, Variance: 0.00478

Test Epoch: 82 
task: sign, mean loss: 3.65270, accuracy: 0.40828, avg. loss over tasks: 3.65270
Diversity Loss - Mean: -0.13276, Variance: 0.01358
Semantic Loss - Mean: 3.24463, Variance: 0.04522

Train Epoch: 83 
task: sign, mean loss: 0.14258, accuracy: 0.97826, avg. loss over tasks: 0.14258, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12931, Variance: 0.01195
Semantic Loss - Mean: 0.16161, Variance: 0.00479

Test Epoch: 83 
task: sign, mean loss: 3.74148, accuracy: 0.37870, avg. loss over tasks: 3.74148
Diversity Loss - Mean: -0.13004, Variance: 0.01358
Semantic Loss - Mean: 3.29090, Variance: 0.04520

Train Epoch: 84 
task: sign, mean loss: 0.05332, accuracy: 0.98913, avg. loss over tasks: 0.05332, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.12859, Variance: 0.01195
Semantic Loss - Mean: 0.11753, Variance: 0.00482

Test Epoch: 84 
task: sign, mean loss: 3.47022, accuracy: 0.41420, avg. loss over tasks: 3.47022
Diversity Loss - Mean: -0.13186, Variance: 0.01358
Semantic Loss - Mean: 2.92259, Variance: 0.04521

Train Epoch: 85 
task: sign, mean loss: 0.02034, accuracy: 0.99457, avg. loss over tasks: 0.02034, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.12998, Variance: 0.01195
Semantic Loss - Mean: 0.02894, Variance: 0.00477

Test Epoch: 85 
task: sign, mean loss: 3.34604, accuracy: 0.40237, avg. loss over tasks: 3.34604
Diversity Loss - Mean: -0.13245, Variance: 0.01357
Semantic Loss - Mean: 2.88636, Variance: 0.04502

Train Epoch: 86 
task: sign, mean loss: 0.03571, accuracy: 0.99457, avg. loss over tasks: 0.03571, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.13134, Variance: 0.01197
Semantic Loss - Mean: 0.05000, Variance: 0.00472

Test Epoch: 86 
task: sign, mean loss: 3.25119, accuracy: 0.39645, avg. loss over tasks: 3.25119
Diversity Loss - Mean: -0.13116, Variance: 0.01355
Semantic Loss - Mean: 2.85033, Variance: 0.04474

Train Epoch: 87 
task: sign, mean loss: 0.04022, accuracy: 0.97826, avg. loss over tasks: 0.04022, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.12992, Variance: 0.01198
Semantic Loss - Mean: 0.06048, Variance: 0.00470

Test Epoch: 87 
task: sign, mean loss: 3.23770, accuracy: 0.40828, avg. loss over tasks: 3.23770
Diversity Loss - Mean: -0.13311, Variance: 0.01355
Semantic Loss - Mean: 2.89187, Variance: 0.04454

Train Epoch: 88 
task: sign, mean loss: 0.02551, accuracy: 0.98370, avg. loss over tasks: 0.02551, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.13142, Variance: 0.01199
Semantic Loss - Mean: 0.02677, Variance: 0.00466

Test Epoch: 88 
task: sign, mean loss: 3.36513, accuracy: 0.42012, avg. loss over tasks: 3.36513
Diversity Loss - Mean: -0.13334, Variance: 0.01355
Semantic Loss - Mean: 3.05556, Variance: 0.04437

Train Epoch: 89 
task: sign, mean loss: 0.02635, accuracy: 0.98913, avg. loss over tasks: 0.02635, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.13060, Variance: 0.01200
Semantic Loss - Mean: 0.05083, Variance: 0.00465

Test Epoch: 89 
task: sign, mean loss: 3.48350, accuracy: 0.40828, avg. loss over tasks: 3.48350
Diversity Loss - Mean: -0.13277, Variance: 0.01355
Semantic Loss - Mean: 3.07221, Variance: 0.04421

Train Epoch: 90 
task: sign, mean loss: 0.00873, accuracy: 1.00000, avg. loss over tasks: 0.00873, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.13163, Variance: 0.01201
Semantic Loss - Mean: 0.02319, Variance: 0.00462

Test Epoch: 90 
task: sign, mean loss: 3.44251, accuracy: 0.43195, avg. loss over tasks: 3.44251
Diversity Loss - Mean: -0.13439, Variance: 0.01355
Semantic Loss - Mean: 2.93730, Variance: 0.04433

Train Epoch: 91 
task: sign, mean loss: 0.03097, accuracy: 0.98913, avg. loss over tasks: 0.03097, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.13213, Variance: 0.01203
Semantic Loss - Mean: 0.05723, Variance: 0.00459

Test Epoch: 91 
task: sign, mean loss: 3.90968, accuracy: 0.36095, avg. loss over tasks: 3.90968
Diversity Loss - Mean: -0.13074, Variance: 0.01355
Semantic Loss - Mean: 3.29589, Variance: 0.04432

Train Epoch: 92 
task: sign, mean loss: 0.01307, accuracy: 0.99457, avg. loss over tasks: 0.01307, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.13145, Variance: 0.01204
Semantic Loss - Mean: 0.02846, Variance: 0.00455

Test Epoch: 92 
task: sign, mean loss: 4.59838, accuracy: 0.31361, avg. loss over tasks: 4.59838
Diversity Loss - Mean: -0.12722, Variance: 0.01355
Semantic Loss - Mean: 3.94039, Variance: 0.04454

Train Epoch: 93 
task: sign, mean loss: 0.00649, accuracy: 1.00000, avg. loss over tasks: 0.00649, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.13116, Variance: 0.01204
Semantic Loss - Mean: 0.01556, Variance: 0.00451

Test Epoch: 93 
task: sign, mean loss: 3.96402, accuracy: 0.37278, avg. loss over tasks: 3.96402
Diversity Loss - Mean: -0.13161, Variance: 0.01356
Semantic Loss - Mean: 3.40312, Variance: 0.04500

Train Epoch: 94 
task: sign, mean loss: 0.01877, accuracy: 0.99457, avg. loss over tasks: 0.01877, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.13179, Variance: 0.01205
Semantic Loss - Mean: 0.02825, Variance: 0.00447

Test Epoch: 94 
task: sign, mean loss: 3.52839, accuracy: 0.39053, avg. loss over tasks: 3.52839
Diversity Loss - Mean: -0.13372, Variance: 0.01356
Semantic Loss - Mean: 3.00339, Variance: 0.04513

Train Epoch: 95 
task: sign, mean loss: 0.00190, accuracy: 1.00000, avg. loss over tasks: 0.00190, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.13198, Variance: 0.01206
Semantic Loss - Mean: 0.01291, Variance: 0.00444

Test Epoch: 95 
task: sign, mean loss: 3.39305, accuracy: 0.42604, avg. loss over tasks: 3.39305
Diversity Loss - Mean: -0.13459, Variance: 0.01355
Semantic Loss - Mean: 2.90568, Variance: 0.04520

Train Epoch: 96 
task: sign, mean loss: 0.05107, accuracy: 0.99457, avg. loss over tasks: 0.05107, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.13290, Variance: 0.01207
Semantic Loss - Mean: 0.06096, Variance: 0.00440

Test Epoch: 96 
task: sign, mean loss: 3.41761, accuracy: 0.39053, avg. loss over tasks: 3.41761
Diversity Loss - Mean: -0.13419, Variance: 0.01355
Semantic Loss - Mean: 2.92649, Variance: 0.04543

Train Epoch: 97 
task: sign, mean loss: 0.00310, accuracy: 1.00000, avg. loss over tasks: 0.00310, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.13320, Variance: 0.01209
Semantic Loss - Mean: 0.01054, Variance: 0.00436

Test Epoch: 97 
task: sign, mean loss: 3.97271, accuracy: 0.31953, avg. loss over tasks: 3.97271
Diversity Loss - Mean: -0.13225, Variance: 0.01354
Semantic Loss - Mean: 3.42999, Variance: 0.04551

Train Epoch: 98 
task: sign, mean loss: 0.00958, accuracy: 1.00000, avg. loss over tasks: 0.00958, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.13340, Variance: 0.01210
Semantic Loss - Mean: 0.01975, Variance: 0.00433

Test Epoch: 98 
task: sign, mean loss: 3.75490, accuracy: 0.36095, avg. loss over tasks: 3.75490
Diversity Loss - Mean: -0.13351, Variance: 0.01354
Semantic Loss - Mean: 3.23963, Variance: 0.04543

Train Epoch: 99 
task: sign, mean loss: 0.01625, accuracy: 0.98913, avg. loss over tasks: 0.01625, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.13328, Variance: 0.01210
Semantic Loss - Mean: 0.02294, Variance: 0.00430

Test Epoch: 99 
task: sign, mean loss: 3.84306, accuracy: 0.34911, avg. loss over tasks: 3.84306
Diversity Loss - Mean: -0.13377, Variance: 0.01353
Semantic Loss - Mean: 3.20164, Variance: 0.04537

Train Epoch: 100 
task: sign, mean loss: 0.03244, accuracy: 0.98370, avg. loss over tasks: 0.03244, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.13306, Variance: 0.01211
Semantic Loss - Mean: 0.05106, Variance: 0.00429

Test Epoch: 100 
task: sign, mean loss: 3.27522, accuracy: 0.42012, avg. loss over tasks: 3.27522
Diversity Loss - Mean: -0.13563, Variance: 0.01352
Semantic Loss - Mean: 2.77105, Variance: 0.04540

Train Epoch: 101 
task: sign, mean loss: 0.00350, accuracy: 1.00000, avg. loss over tasks: 0.00350, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.13342, Variance: 0.01212
Semantic Loss - Mean: 0.01707, Variance: 0.00427

Test Epoch: 101 
task: sign, mean loss: 3.14699, accuracy: 0.46154, avg. loss over tasks: 3.14699
Diversity Loss - Mean: -0.13631, Variance: 0.01352
Semantic Loss - Mean: 2.72831, Variance: 0.04545

Train Epoch: 102 
task: sign, mean loss: 0.00818, accuracy: 0.99457, avg. loss over tasks: 0.00818, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.13361, Variance: 0.01213
Semantic Loss - Mean: 0.01849, Variance: 0.00424

Test Epoch: 102 
task: sign, mean loss: 3.11792, accuracy: 0.44379, avg. loss over tasks: 3.11792
Diversity Loss - Mean: -0.13549, Variance: 0.01352
Semantic Loss - Mean: 2.65733, Variance: 0.04534

Train Epoch: 103 
task: sign, mean loss: 0.00355, accuracy: 1.00000, avg. loss over tasks: 0.00355, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.13330, Variance: 0.01214
Semantic Loss - Mean: 0.01344, Variance: 0.00421

Test Epoch: 103 
task: sign, mean loss: 3.42105, accuracy: 0.39645, avg. loss over tasks: 3.42105
Diversity Loss - Mean: -0.13453, Variance: 0.01352
Semantic Loss - Mean: 2.89099, Variance: 0.04526

Train Epoch: 104 
task: sign, mean loss: 0.01090, accuracy: 0.99457, avg. loss over tasks: 0.01090, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.13341, Variance: 0.01215
Semantic Loss - Mean: 0.02313, Variance: 0.00418

Test Epoch: 104 
task: sign, mean loss: 3.65306, accuracy: 0.40828, avg. loss over tasks: 3.65306
Diversity Loss - Mean: -0.13476, Variance: 0.01351
Semantic Loss - Mean: 3.05565, Variance: 0.04535

Train Epoch: 105 
task: sign, mean loss: 0.00205, accuracy: 1.00000, avg. loss over tasks: 0.00205, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.13400, Variance: 0.01216
Semantic Loss - Mean: 0.00480, Variance: 0.00414

Test Epoch: 105 
task: sign, mean loss: 3.52358, accuracy: 0.40828, avg. loss over tasks: 3.52358
Diversity Loss - Mean: -0.13563, Variance: 0.01352
Semantic Loss - Mean: 3.01407, Variance: 0.04548

Train Epoch: 106 
task: sign, mean loss: 0.00252, accuracy: 1.00000, avg. loss over tasks: 0.00252, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.13435, Variance: 0.01217
Semantic Loss - Mean: 0.00555, Variance: 0.00410

Test Epoch: 106 
task: sign, mean loss: 3.53881, accuracy: 0.39053, avg. loss over tasks: 3.53881
Diversity Loss - Mean: -0.13558, Variance: 0.01352
Semantic Loss - Mean: 3.03190, Variance: 0.04546

Train Epoch: 107 
task: sign, mean loss: 0.00294, accuracy: 1.00000, avg. loss over tasks: 0.00294, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.13450, Variance: 0.01218
Semantic Loss - Mean: 0.01068, Variance: 0.00407

Test Epoch: 107 
task: sign, mean loss: 3.46023, accuracy: 0.41420, avg. loss over tasks: 3.46023
Diversity Loss - Mean: -0.13597, Variance: 0.01352
Semantic Loss - Mean: 3.03034, Variance: 0.04530

Train Epoch: 108 
task: sign, mean loss: 0.00181, accuracy: 1.00000, avg. loss over tasks: 0.00181, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.13459, Variance: 0.01218
Semantic Loss - Mean: 0.00722, Variance: 0.00403

Test Epoch: 108 
task: sign, mean loss: 3.39822, accuracy: 0.42604, avg. loss over tasks: 3.39822
Diversity Loss - Mean: -0.13548, Variance: 0.01352
Semantic Loss - Mean: 2.95865, Variance: 0.04510

Train Epoch: 109 
task: sign, mean loss: 0.00181, accuracy: 1.00000, avg. loss over tasks: 0.00181, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.13462, Variance: 0.01219
Semantic Loss - Mean: 0.00545, Variance: 0.00399

Test Epoch: 109 
task: sign, mean loss: 3.37973, accuracy: 0.43787, avg. loss over tasks: 3.37973
Diversity Loss - Mean: -0.13566, Variance: 0.01352
Semantic Loss - Mean: 2.96538, Variance: 0.04494

Train Epoch: 110 
task: sign, mean loss: 0.00155, accuracy: 1.00000, avg. loss over tasks: 0.00155, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.13447, Variance: 0.01220
Semantic Loss - Mean: 0.00636, Variance: 0.00396

Test Epoch: 110 
task: sign, mean loss: 3.55539, accuracy: 0.40828, avg. loss over tasks: 3.55539
Diversity Loss - Mean: -0.13536, Variance: 0.01351
Semantic Loss - Mean: 3.11464, Variance: 0.04482

Train Epoch: 111 
task: sign, mean loss: 0.00303, accuracy: 1.00000, avg. loss over tasks: 0.00303, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.13472, Variance: 0.01221
Semantic Loss - Mean: 0.01628, Variance: 0.00394

Test Epoch: 111 
task: sign, mean loss: 3.43266, accuracy: 0.42012, avg. loss over tasks: 3.43266
Diversity Loss - Mean: -0.13574, Variance: 0.01351
Semantic Loss - Mean: 3.00915, Variance: 0.04477

Train Epoch: 112 
task: sign, mean loss: 0.00769, accuracy: 0.99457, avg. loss over tasks: 0.00769, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.13503, Variance: 0.01222
Semantic Loss - Mean: 0.00724, Variance: 0.00391

Test Epoch: 112 
task: sign, mean loss: 3.54988, accuracy: 0.41420, avg. loss over tasks: 3.54988
Diversity Loss - Mean: -0.13593, Variance: 0.01350
Semantic Loss - Mean: 3.08387, Variance: 0.04484

Train Epoch: 113 
task: sign, mean loss: 0.00540, accuracy: 0.99457, avg. loss over tasks: 0.00540, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.13562, Variance: 0.01223
Semantic Loss - Mean: 0.01448, Variance: 0.00389

Test Epoch: 113 
task: sign, mean loss: 3.77290, accuracy: 0.40237, avg. loss over tasks: 3.77290
Diversity Loss - Mean: -0.13590, Variance: 0.01350
Semantic Loss - Mean: 3.23755, Variance: 0.04487

Train Epoch: 114 
task: sign, mean loss: 0.00156, accuracy: 1.00000, avg. loss over tasks: 0.00156, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.13551, Variance: 0.01224
Semantic Loss - Mean: 0.00927, Variance: 0.00386

Test Epoch: 114 
task: sign, mean loss: 3.85596, accuracy: 0.39053, avg. loss over tasks: 3.85596
Diversity Loss - Mean: -0.13583, Variance: 0.01350
Semantic Loss - Mean: 3.30013, Variance: 0.04493

Train Epoch: 115 
task: sign, mean loss: 0.00237, accuracy: 1.00000, avg. loss over tasks: 0.00237, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.13534, Variance: 0.01225
Semantic Loss - Mean: 0.01086, Variance: 0.00384

Test Epoch: 115 
task: sign, mean loss: 3.93928, accuracy: 0.39053, avg. loss over tasks: 3.93928
Diversity Loss - Mean: -0.13575, Variance: 0.01350
Semantic Loss - Mean: 3.35412, Variance: 0.04493

Train Epoch: 116 
task: sign, mean loss: 0.01686, accuracy: 0.99457, avg. loss over tasks: 0.01686, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.13538, Variance: 0.01226
Semantic Loss - Mean: 0.02196, Variance: 0.00381

Test Epoch: 116 
task: sign, mean loss: 3.52144, accuracy: 0.41420, avg. loss over tasks: 3.52144
Diversity Loss - Mean: -0.13654, Variance: 0.01350
Semantic Loss - Mean: 3.05220, Variance: 0.04481

Train Epoch: 117 
task: sign, mean loss: 0.00677, accuracy: 0.99457, avg. loss over tasks: 0.00677, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.13572, Variance: 0.01227
Semantic Loss - Mean: 0.01551, Variance: 0.00378

Test Epoch: 117 
task: sign, mean loss: 3.75298, accuracy: 0.40237, avg. loss over tasks: 3.75298
Diversity Loss - Mean: -0.13623, Variance: 0.01350
Semantic Loss - Mean: 3.20867, Variance: 0.04471

Train Epoch: 118 
task: sign, mean loss: 0.00458, accuracy: 1.00000, avg. loss over tasks: 0.00458, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.13530, Variance: 0.01228
Semantic Loss - Mean: 0.02196, Variance: 0.00376

Test Epoch: 118 
task: sign, mean loss: 3.93678, accuracy: 0.39645, avg. loss over tasks: 3.93678
Diversity Loss - Mean: -0.13591, Variance: 0.01350
Semantic Loss - Mean: 3.37039, Variance: 0.04462

Train Epoch: 119 
task: sign, mean loss: 0.00103, accuracy: 1.00000, avg. loss over tasks: 0.00103, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.13559, Variance: 0.01228
Semantic Loss - Mean: 0.00607, Variance: 0.00373

Test Epoch: 119 
task: sign, mean loss: 3.81623, accuracy: 0.40237, avg. loss over tasks: 3.81623
Diversity Loss - Mean: -0.13588, Variance: 0.01350
Semantic Loss - Mean: 3.28860, Variance: 0.04454

Train Epoch: 120 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.13569, Variance: 0.01229
Semantic Loss - Mean: 0.00982, Variance: 0.00371

Test Epoch: 120 
task: sign, mean loss: 3.66440, accuracy: 0.41420, avg. loss over tasks: 3.66440
Diversity Loss - Mean: -0.13656, Variance: 0.01350
Semantic Loss - Mean: 3.20819, Variance: 0.04447

Train Epoch: 121 
task: sign, mean loss: 0.01862, accuracy: 0.98913, avg. loss over tasks: 0.01862, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.13592, Variance: 0.01230
Semantic Loss - Mean: 0.02126, Variance: 0.00369

Test Epoch: 121 
task: sign, mean loss: 3.86778, accuracy: 0.39053, avg. loss over tasks: 3.86778
Diversity Loss - Mean: -0.13614, Variance: 0.01350
Semantic Loss - Mean: 3.31211, Variance: 0.04450

Train Epoch: 122 
task: sign, mean loss: 0.00278, accuracy: 1.00000, avg. loss over tasks: 0.00278, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.13585, Variance: 0.01231
Semantic Loss - Mean: 0.01881, Variance: 0.00369

Test Epoch: 122 
task: sign, mean loss: 3.84542, accuracy: 0.41420, avg. loss over tasks: 3.84542
Diversity Loss - Mean: -0.13576, Variance: 0.01350
Semantic Loss - Mean: 3.27798, Variance: 0.04464

Train Epoch: 123 
task: sign, mean loss: 0.00142, accuracy: 1.00000, avg. loss over tasks: 0.00142, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.13589, Variance: 0.01232
Semantic Loss - Mean: 0.01389, Variance: 0.00368

Test Epoch: 123 
task: sign, mean loss: 3.91124, accuracy: 0.40237, avg. loss over tasks: 3.91124
Diversity Loss - Mean: -0.13563, Variance: 0.01350
Semantic Loss - Mean: 3.31415, Variance: 0.04483

Train Epoch: 124 
task: sign, mean loss: 0.00133, accuracy: 1.00000, avg. loss over tasks: 0.00133, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.13608, Variance: 0.01233
Semantic Loss - Mean: 0.00860, Variance: 0.00366

Test Epoch: 124 
task: sign, mean loss: 3.78761, accuracy: 0.39645, avg. loss over tasks: 3.78761
Diversity Loss - Mean: -0.13579, Variance: 0.01350
Semantic Loss - Mean: 3.19474, Variance: 0.04497

Train Epoch: 125 
task: sign, mean loss: 0.00088, accuracy: 1.00000, avg. loss over tasks: 0.00088, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.13616, Variance: 0.01234
Semantic Loss - Mean: 0.00433, Variance: 0.00363

Test Epoch: 125 
task: sign, mean loss: 3.79411, accuracy: 0.39645, avg. loss over tasks: 3.79411
Diversity Loss - Mean: -0.13619, Variance: 0.01350
Semantic Loss - Mean: 3.20750, Variance: 0.04510

Train Epoch: 126 
task: sign, mean loss: 0.00407, accuracy: 1.00000, avg. loss over tasks: 0.00407, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.13608, Variance: 0.01234
Semantic Loss - Mean: 0.00676, Variance: 0.00360

Test Epoch: 126 
task: sign, mean loss: 3.73430, accuracy: 0.39645, avg. loss over tasks: 3.73430
Diversity Loss - Mean: -0.13644, Variance: 0.01350
Semantic Loss - Mean: 3.17221, Variance: 0.04519

Train Epoch: 127 
task: sign, mean loss: 0.00151, accuracy: 1.00000, avg. loss over tasks: 0.00151, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.13602, Variance: 0.01235
Semantic Loss - Mean: 0.00755, Variance: 0.00358

Test Epoch: 127 
task: sign, mean loss: 3.80451, accuracy: 0.40237, avg. loss over tasks: 3.80451
Diversity Loss - Mean: -0.13667, Variance: 0.01350
Semantic Loss - Mean: 3.22912, Variance: 0.04525

Train Epoch: 128 
task: sign, mean loss: 0.00376, accuracy: 1.00000, avg. loss over tasks: 0.00376, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.13634, Variance: 0.01236
Semantic Loss - Mean: 0.00645, Variance: 0.00355

Test Epoch: 128 
task: sign, mean loss: 3.84444, accuracy: 0.40828, avg. loss over tasks: 3.84444
Diversity Loss - Mean: -0.13644, Variance: 0.01350
Semantic Loss - Mean: 3.23928, Variance: 0.04530

Train Epoch: 129 
task: sign, mean loss: 0.00229, accuracy: 1.00000, avg. loss over tasks: 0.00229, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.13624, Variance: 0.01237
Semantic Loss - Mean: 0.00410, Variance: 0.00352

Test Epoch: 129 
task: sign, mean loss: 3.58221, accuracy: 0.42012, avg. loss over tasks: 3.58221
Diversity Loss - Mean: -0.13704, Variance: 0.01350
Semantic Loss - Mean: 3.09754, Variance: 0.04533

Train Epoch: 130 
task: sign, mean loss: 0.00766, accuracy: 0.99457, avg. loss over tasks: 0.00766, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.13648, Variance: 0.01238
Semantic Loss - Mean: 0.01144, Variance: 0.00350

Test Epoch: 130 
task: sign, mean loss: 3.69406, accuracy: 0.40828, avg. loss over tasks: 3.69406
Diversity Loss - Mean: -0.13676, Variance: 0.01350
Semantic Loss - Mean: 3.15782, Variance: 0.04536

Train Epoch: 131 
task: sign, mean loss: 0.00226, accuracy: 1.00000, avg. loss over tasks: 0.00226, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.13661, Variance: 0.01239
Semantic Loss - Mean: 0.00816, Variance: 0.00347

Test Epoch: 131 
task: sign, mean loss: 3.86801, accuracy: 0.40237, avg. loss over tasks: 3.86801
Diversity Loss - Mean: -0.13634, Variance: 0.01350
Semantic Loss - Mean: 3.25297, Variance: 0.04536

Train Epoch: 132 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.13676, Variance: 0.01240
Semantic Loss - Mean: 0.00277, Variance: 0.00345

Test Epoch: 132 
task: sign, mean loss: 3.77566, accuracy: 0.40828, avg. loss over tasks: 3.77566
Diversity Loss - Mean: -0.13660, Variance: 0.01350
Semantic Loss - Mean: 3.21117, Variance: 0.04533

Train Epoch: 133 
task: sign, mean loss: 0.00239, accuracy: 1.00000, avg. loss over tasks: 0.00239, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.13657, Variance: 0.01241
Semantic Loss - Mean: 0.00479, Variance: 0.00342

Test Epoch: 133 
task: sign, mean loss: 3.92789, accuracy: 0.39645, avg. loss over tasks: 3.92789
Diversity Loss - Mean: -0.13643, Variance: 0.01350
Semantic Loss - Mean: 3.30252, Variance: 0.04529

Train Epoch: 134 
task: sign, mean loss: 0.00149, accuracy: 1.00000, avg. loss over tasks: 0.00149, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13638, Variance: 0.01241
Semantic Loss - Mean: 0.01217, Variance: 0.00342

Test Epoch: 134 
task: sign, mean loss: 3.70502, accuracy: 0.40237, avg. loss over tasks: 3.70502
Diversity Loss - Mean: -0.13666, Variance: 0.01350
Semantic Loss - Mean: 3.17209, Variance: 0.04524

Train Epoch: 135 
task: sign, mean loss: 0.00513, accuracy: 1.00000, avg. loss over tasks: 0.00513, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.13652, Variance: 0.01242
Semantic Loss - Mean: 0.01203, Variance: 0.00340

Test Epoch: 135 
task: sign, mean loss: 3.83415, accuracy: 0.40828, avg. loss over tasks: 3.83415
Diversity Loss - Mean: -0.13669, Variance: 0.01350
Semantic Loss - Mean: 3.26249, Variance: 0.04516

Train Epoch: 136 
task: sign, mean loss: 0.00070, accuracy: 1.00000, avg. loss over tasks: 0.00070, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13661, Variance: 0.01243
Semantic Loss - Mean: 0.00516, Variance: 0.00338

Test Epoch: 136 
task: sign, mean loss: 3.79823, accuracy: 0.40237, avg. loss over tasks: 3.79823
Diversity Loss - Mean: -0.13671, Variance: 0.01350
Semantic Loss - Mean: 3.23876, Variance: 0.04510

Train Epoch: 137 
task: sign, mean loss: 0.00107, accuracy: 1.00000, avg. loss over tasks: 0.00107, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13647, Variance: 0.01244
Semantic Loss - Mean: 0.00419, Variance: 0.00335

Test Epoch: 137 
task: sign, mean loss: 3.82102, accuracy: 0.39053, avg. loss over tasks: 3.82102
Diversity Loss - Mean: -0.13642, Variance: 0.01350
Semantic Loss - Mean: 3.23376, Variance: 0.04503

Train Epoch: 138 
task: sign, mean loss: 0.00419, accuracy: 1.00000, avg. loss over tasks: 0.00419, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13646, Variance: 0.01245
Semantic Loss - Mean: 0.00767, Variance: 0.00333

Test Epoch: 138 
task: sign, mean loss: 3.70405, accuracy: 0.41420, avg. loss over tasks: 3.70405
Diversity Loss - Mean: -0.13666, Variance: 0.01350
Semantic Loss - Mean: 3.16109, Variance: 0.04493

Train Epoch: 139 
task: sign, mean loss: 0.00122, accuracy: 1.00000, avg. loss over tasks: 0.00122, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13675, Variance: 0.01246
Semantic Loss - Mean: 0.00690, Variance: 0.00331

Test Epoch: 139 
task: sign, mean loss: 3.81374, accuracy: 0.38462, avg. loss over tasks: 3.81374
Diversity Loss - Mean: -0.13657, Variance: 0.01350
Semantic Loss - Mean: 3.21997, Variance: 0.04484

Train Epoch: 140 
task: sign, mean loss: 0.00335, accuracy: 1.00000, avg. loss over tasks: 0.00335, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.13637, Variance: 0.01247
Semantic Loss - Mean: 0.00989, Variance: 0.00329

Test Epoch: 140 
task: sign, mean loss: 3.60373, accuracy: 0.40237, avg. loss over tasks: 3.60373
Diversity Loss - Mean: -0.13711, Variance: 0.01350
Semantic Loss - Mean: 3.10822, Variance: 0.04473

Train Epoch: 141 
task: sign, mean loss: 0.00110, accuracy: 1.00000, avg. loss over tasks: 0.00110, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.13644, Variance: 0.01247
Semantic Loss - Mean: 0.00732, Variance: 0.00328

Test Epoch: 141 
task: sign, mean loss: 3.53012, accuracy: 0.42012, avg. loss over tasks: 3.53012
Diversity Loss - Mean: -0.13715, Variance: 0.01350
Semantic Loss - Mean: 3.05799, Variance: 0.04461

Train Epoch: 142 
task: sign, mean loss: 0.00689, accuracy: 1.00000, avg. loss over tasks: 0.00689, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.13664, Variance: 0.01248
Semantic Loss - Mean: 0.01639, Variance: 0.00326

Test Epoch: 142 
task: sign, mean loss: 3.43987, accuracy: 0.44379, avg. loss over tasks: 3.43987
Diversity Loss - Mean: -0.13736, Variance: 0.01350
Semantic Loss - Mean: 2.99568, Variance: 0.04450

Train Epoch: 143 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13677, Variance: 0.01249
Semantic Loss - Mean: 0.00488, Variance: 0.00324

Test Epoch: 143 
task: sign, mean loss: 3.92128, accuracy: 0.39053, avg. loss over tasks: 3.92128
Diversity Loss - Mean: -0.13656, Variance: 0.01350
Semantic Loss - Mean: 3.30289, Variance: 0.04440

Train Epoch: 144 
task: sign, mean loss: 0.00922, accuracy: 0.99457, avg. loss over tasks: 0.00922, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.13683, Variance: 0.01249
Semantic Loss - Mean: 0.01103, Variance: 0.00322

Test Epoch: 144 
task: sign, mean loss: 3.98211, accuracy: 0.40237, avg. loss over tasks: 3.98211
Diversity Loss - Mean: -0.13646, Variance: 0.01350
Semantic Loss - Mean: 3.34023, Variance: 0.04429

Train Epoch: 145 
task: sign, mean loss: 0.00056, accuracy: 1.00000, avg. loss over tasks: 0.00056, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13651, Variance: 0.01250
Semantic Loss - Mean: 0.00484, Variance: 0.00319

Test Epoch: 145 
task: sign, mean loss: 3.85461, accuracy: 0.40237, avg. loss over tasks: 3.85461
Diversity Loss - Mean: -0.13665, Variance: 0.01350
Semantic Loss - Mean: 3.27536, Variance: 0.04418

Train Epoch: 146 
task: sign, mean loss: 0.00198, accuracy: 1.00000, avg. loss over tasks: 0.00198, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.13660, Variance: 0.01251
Semantic Loss - Mean: 0.00891, Variance: 0.00317

Test Epoch: 146 
task: sign, mean loss: 4.04305, accuracy: 0.39645, avg. loss over tasks: 4.04305
Diversity Loss - Mean: -0.13625, Variance: 0.01350
Semantic Loss - Mean: 3.36795, Variance: 0.04408

Train Epoch: 147 
task: sign, mean loss: 0.00146, accuracy: 1.00000, avg. loss over tasks: 0.00146, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13654, Variance: 0.01252
Semantic Loss - Mean: 0.00491, Variance: 0.00315

Test Epoch: 147 
task: sign, mean loss: 3.87894, accuracy: 0.41420, avg. loss over tasks: 3.87894
Diversity Loss - Mean: -0.13649, Variance: 0.01350
Semantic Loss - Mean: 3.28791, Variance: 0.04397

Train Epoch: 148 
task: sign, mean loss: 0.00104, accuracy: 1.00000, avg. loss over tasks: 0.00104, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.13603, Variance: 0.01252
Semantic Loss - Mean: 0.00500, Variance: 0.00313

Test Epoch: 148 
task: sign, mean loss: 3.89230, accuracy: 0.42012, avg. loss over tasks: 3.89230
Diversity Loss - Mean: -0.13651, Variance: 0.01350
Semantic Loss - Mean: 3.29284, Variance: 0.04387

Train Epoch: 149 
task: sign, mean loss: 0.00198, accuracy: 1.00000, avg. loss over tasks: 0.00198, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.13678, Variance: 0.01253
Semantic Loss - Mean: 0.00494, Variance: 0.00311

Test Epoch: 149 
task: sign, mean loss: 3.78081, accuracy: 0.40237, avg. loss over tasks: 3.78081
Diversity Loss - Mean: -0.13678, Variance: 0.01350
Semantic Loss - Mean: 3.22533, Variance: 0.04377

Train Epoch: 150 
task: sign, mean loss: 0.00518, accuracy: 0.99457, avg. loss over tasks: 0.00518, lr: 3e-07
Diversity Loss - Mean: -0.13648, Variance: 0.01253
Semantic Loss - Mean: 0.01135, Variance: 0.00310

Test Epoch: 150 
task: sign, mean loss: 3.84137, accuracy: 0.40237, avg. loss over tasks: 3.84137
Diversity Loss - Mean: -0.13672, Variance: 0.01350
Semantic Loss - Mean: 3.25817, Variance: 0.04367

