Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09451, accuracy: 0.63587, avg. loss over tasks: 1.09451, lr: 3e-05
Diversity Loss - Mean: -0.01067, Variance: 0.01049
Semantic Loss - Mean: 1.43121, Variance: 0.07255

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17653, accuracy: 0.66272, avg. loss over tasks: 1.17653
Diversity Loss - Mean: -0.03163, Variance: 0.01245
Semantic Loss - Mean: 1.16179, Variance: 0.05372

Train Epoch: 2 
task: sign, mean loss: 0.96854, accuracy: 0.66304, avg. loss over tasks: 0.96854, lr: 6e-05
Diversity Loss - Mean: -0.02268, Variance: 0.01047
Semantic Loss - Mean: 0.98430, Variance: 0.03926

Test Epoch: 2 
task: sign, mean loss: 1.11042, accuracy: 0.66864, avg. loss over tasks: 1.11042
Diversity Loss - Mean: -0.03638, Variance: 0.01211
Semantic Loss - Mean: 1.14869, Variance: 0.03211

Train Epoch: 3 
task: sign, mean loss: 0.80152, accuracy: 0.67935, avg. loss over tasks: 0.80152, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.04696, Variance: 0.01033
Semantic Loss - Mean: 0.99003, Variance: 0.02716

Test Epoch: 3 
task: sign, mean loss: 1.26924, accuracy: 0.59763, avg. loss over tasks: 1.26924
Diversity Loss - Mean: -0.06909, Variance: 0.01150
Semantic Loss - Mean: 1.10942, Variance: 0.02959

Train Epoch: 4 
task: sign, mean loss: 0.75369, accuracy: 0.72283, avg. loss over tasks: 0.75369, lr: 0.00012
Diversity Loss - Mean: -0.07370, Variance: 0.01008
Semantic Loss - Mean: 0.89047, Variance: 0.02111

Test Epoch: 4 
task: sign, mean loss: 1.44792, accuracy: 0.48521, avg. loss over tasks: 1.44792
Diversity Loss - Mean: -0.07777, Variance: 0.01108
Semantic Loss - Mean: 1.08532, Variance: 0.02587

Train Epoch: 5 
task: sign, mean loss: 0.79250, accuracy: 0.72283, avg. loss over tasks: 0.79250, lr: 0.00015
Diversity Loss - Mean: -0.07459, Variance: 0.00980
Semantic Loss - Mean: 0.82877, Variance: 0.01733

Test Epoch: 5 
task: sign, mean loss: 2.16205, accuracy: 0.28402, avg. loss over tasks: 2.16205
Diversity Loss - Mean: -0.06987, Variance: 0.01039
Semantic Loss - Mean: 1.36089, Variance: 0.02501

Train Epoch: 6 
task: sign, mean loss: 0.88419, accuracy: 0.72826, avg. loss over tasks: 0.88419, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.08581, Variance: 0.00983
Semantic Loss - Mean: 0.87358, Variance: 0.01495

Test Epoch: 6 
task: sign, mean loss: 1.62676, accuracy: 0.66864, avg. loss over tasks: 1.62676
Diversity Loss - Mean: -0.07593, Variance: 0.01100
Semantic Loss - Mean: 1.28532, Variance: 0.02220

Train Epoch: 7 
task: sign, mean loss: 0.68826, accuracy: 0.76087, avg. loss over tasks: 0.68826, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.07516, Variance: 0.00996
Semantic Loss - Mean: 0.68386, Variance: 0.01307

Test Epoch: 7 
task: sign, mean loss: 2.89500, accuracy: 0.20118, avg. loss over tasks: 2.89500
Diversity Loss - Mean: -0.02745, Variance: 0.01077
Semantic Loss - Mean: 2.04459, Variance: 0.02349

Train Epoch: 8 
task: sign, mean loss: 0.56058, accuracy: 0.77717, avg. loss over tasks: 0.56058, lr: 0.00024
Diversity Loss - Mean: -0.05190, Variance: 0.00997
Semantic Loss - Mean: 0.60928, Variance: 0.01188

Test Epoch: 8 
task: sign, mean loss: 2.82097, accuracy: 0.42012, avg. loss over tasks: 2.82097
Diversity Loss - Mean: -0.00283, Variance: 0.01084
Semantic Loss - Mean: 1.86119, Variance: 0.02386

Train Epoch: 9 
task: sign, mean loss: 0.83888, accuracy: 0.69565, avg. loss over tasks: 0.83888, lr: 0.00027
Diversity Loss - Mean: -0.05140, Variance: 0.00993
Semantic Loss - Mean: 0.75602, Variance: 0.01114

Test Epoch: 9 
task: sign, mean loss: 2.84991, accuracy: 0.32544, avg. loss over tasks: 2.84991
Diversity Loss - Mean: -0.00354, Variance: 0.01080
Semantic Loss - Mean: 2.13126, Variance: 0.02462

Train Epoch: 10 
task: sign, mean loss: 0.83147, accuracy: 0.71196, avg. loss over tasks: 0.83147, lr: 0.0003
Diversity Loss - Mean: -0.06781, Variance: 0.00997
Semantic Loss - Mean: 0.77207, Variance: 0.01053

Test Epoch: 10 
task: sign, mean loss: 3.29494, accuracy: 0.28402, avg. loss over tasks: 3.29494
Diversity Loss - Mean: 0.00950, Variance: 0.01082
Semantic Loss - Mean: 2.51412, Variance: 0.02686

Train Epoch: 11 
task: sign, mean loss: 0.75998, accuracy: 0.70109, avg. loss over tasks: 0.75998, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.05675, Variance: 0.01005
Semantic Loss - Mean: 0.70279, Variance: 0.00994

Test Epoch: 11 
task: sign, mean loss: 2.44916, accuracy: 0.66272, avg. loss over tasks: 2.44916
Diversity Loss - Mean: -0.03293, Variance: 0.01165
Semantic Loss - Mean: 1.90455, Variance: 0.02868

Train Epoch: 12 
task: sign, mean loss: 0.64954, accuracy: 0.76630, avg. loss over tasks: 0.64954, lr: 0.000299849111021216
Diversity Loss - Mean: -0.04734, Variance: 0.01015
Semantic Loss - Mean: 0.64626, Variance: 0.00930

Test Epoch: 12 
task: sign, mean loss: 3.07732, accuracy: 0.25444, avg. loss over tasks: 3.07732
Diversity Loss - Mean: -0.00338, Variance: 0.01177
Semantic Loss - Mean: 2.21099, Variance: 0.03383

Train Epoch: 13 
task: sign, mean loss: 0.66586, accuracy: 0.74457, avg. loss over tasks: 0.66586, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.06868, Variance: 0.01038
Semantic Loss - Mean: 0.66481, Variance: 0.00885

Test Epoch: 13 
task: sign, mean loss: 1.45007, accuracy: 0.53254, avg. loss over tasks: 1.45007
Diversity Loss - Mean: -0.05524, Variance: 0.01235
Semantic Loss - Mean: 1.38388, Variance: 0.03398

Train Epoch: 14 
task: sign, mean loss: 0.68596, accuracy: 0.72826, avg. loss over tasks: 0.68596, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.07605, Variance: 0.01058
Semantic Loss - Mean: 0.72083, Variance: 0.00859

Test Epoch: 14 
task: sign, mean loss: 1.28454, accuracy: 0.66272, avg. loss over tasks: 1.28454
Diversity Loss - Mean: -0.09553, Variance: 0.01268
Semantic Loss - Mean: 1.26734, Variance: 0.03239

Train Epoch: 15 
task: sign, mean loss: 0.51163, accuracy: 0.80435, avg. loss over tasks: 0.51163, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.07324, Variance: 0.01072
Semantic Loss - Mean: 0.52453, Variance: 0.00823

Test Epoch: 15 
task: sign, mean loss: 1.29652, accuracy: 0.65680, avg. loss over tasks: 1.29652
Diversity Loss - Mean: -0.10513, Variance: 0.01312
Semantic Loss - Mean: 1.21573, Variance: 0.03065

Train Epoch: 16 
task: sign, mean loss: 0.40780, accuracy: 0.88043, avg. loss over tasks: 0.40780, lr: 0.000298643821800925
Diversity Loss - Mean: -0.07248, Variance: 0.01084
Semantic Loss - Mean: 0.43906, Variance: 0.00782

Test Epoch: 16 
task: sign, mean loss: 1.78146, accuracy: 0.53254, avg. loss over tasks: 1.78146
Diversity Loss - Mean: -0.08545, Variance: 0.01332
Semantic Loss - Mean: 1.58886, Variance: 0.02932

Train Epoch: 17 
task: sign, mean loss: 0.45273, accuracy: 0.80978, avg. loss over tasks: 0.45273, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.06727, Variance: 0.01093
Semantic Loss - Mean: 0.48537, Variance: 0.00763

Test Epoch: 17 
task: sign, mean loss: 1.97538, accuracy: 0.55030, avg. loss over tasks: 1.97538
Diversity Loss - Mean: -0.06901, Variance: 0.01348
Semantic Loss - Mean: 1.69258, Variance: 0.02856

Train Epoch: 18 
task: sign, mean loss: 0.56840, accuracy: 0.81522, avg. loss over tasks: 0.56840, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.07605, Variance: 0.01107
Semantic Loss - Mean: 0.58681, Variance: 0.00737

Test Epoch: 18 
task: sign, mean loss: 1.88714, accuracy: 0.45562, avg. loss over tasks: 1.88714
Diversity Loss - Mean: -0.06977, Variance: 0.01371
Semantic Loss - Mean: 1.52730, Variance: 0.02868

Train Epoch: 19 
task: sign, mean loss: 0.46092, accuracy: 0.80978, avg. loss over tasks: 0.46092, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.08681, Variance: 0.01121
Semantic Loss - Mean: 0.49117, Variance: 0.00718

Test Epoch: 19 
task: sign, mean loss: 1.55723, accuracy: 0.43195, avg. loss over tasks: 1.55723
Diversity Loss - Mean: -0.06478, Variance: 0.01392
Semantic Loss - Mean: 1.43716, Variance: 0.02794

Train Epoch: 20 
task: sign, mean loss: 0.37244, accuracy: 0.83696, avg. loss over tasks: 0.37244, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.08782, Variance: 0.01134
Semantic Loss - Mean: 0.40054, Variance: 0.00691

Test Epoch: 20 
task: sign, mean loss: 1.98796, accuracy: 0.51479, avg. loss over tasks: 1.98796
Diversity Loss - Mean: -0.07511, Variance: 0.01392
Semantic Loss - Mean: 1.76824, Variance: 0.02771

Train Epoch: 21 
task: sign, mean loss: 0.40706, accuracy: 0.88587, avg. loss over tasks: 0.40706, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.07836, Variance: 0.01141
Semantic Loss - Mean: 0.43623, Variance: 0.00681

Test Epoch: 21 
task: sign, mean loss: 2.34418, accuracy: 0.50888, avg. loss over tasks: 2.34418
Diversity Loss - Mean: -0.07647, Variance: 0.01401
Semantic Loss - Mean: 2.00689, Variance: 0.02709

Train Epoch: 22 
task: sign, mean loss: 0.32647, accuracy: 0.86957, avg. loss over tasks: 0.32647, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.07985, Variance: 0.01148
Semantic Loss - Mean: 0.37246, Variance: 0.00663

Test Epoch: 22 
task: sign, mean loss: 2.13222, accuracy: 0.40828, avg. loss over tasks: 2.13222
Diversity Loss - Mean: -0.05220, Variance: 0.01405
Semantic Loss - Mean: 1.87497, Variance: 0.02714

Train Epoch: 23 
task: sign, mean loss: 0.23607, accuracy: 0.90761, avg. loss over tasks: 0.23607, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.08253, Variance: 0.01156
Semantic Loss - Mean: 0.25716, Variance: 0.00642

Test Epoch: 23 
task: sign, mean loss: 1.77541, accuracy: 0.36095, avg. loss over tasks: 1.77541
Diversity Loss - Mean: -0.05112, Variance: 0.01413
Semantic Loss - Mean: 1.48553, Variance: 0.02699

Train Epoch: 24 
task: sign, mean loss: 0.17244, accuracy: 0.92935, avg. loss over tasks: 0.17244, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.08326, Variance: 0.01164
Semantic Loss - Mean: 0.21483, Variance: 0.00626

Test Epoch: 24 
task: sign, mean loss: 1.93580, accuracy: 0.32544, avg. loss over tasks: 1.93580
Diversity Loss - Mean: -0.03981, Variance: 0.01410
Semantic Loss - Mean: 1.73868, Variance: 0.02693

Train Epoch: 25 
task: sign, mean loss: 0.10139, accuracy: 0.96739, avg. loss over tasks: 0.10139, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.07346, Variance: 0.01169
Semantic Loss - Mean: 0.13393, Variance: 0.00608

Test Epoch: 25 
task: sign, mean loss: 1.44588, accuracy: 0.65680, avg. loss over tasks: 1.44588
Diversity Loss - Mean: -0.06896, Variance: 0.01409
Semantic Loss - Mean: 1.19233, Variance: 0.02713

Train Epoch: 26 
task: sign, mean loss: 0.12026, accuracy: 0.95652, avg. loss over tasks: 0.12026, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.07340, Variance: 0.01178
Semantic Loss - Mean: 0.16569, Variance: 0.00602

Test Epoch: 26 
task: sign, mean loss: 2.04849, accuracy: 0.45562, avg. loss over tasks: 2.04849
Diversity Loss - Mean: -0.03524, Variance: 0.01401
Semantic Loss - Mean: 1.89757, Variance: 0.02815

Train Epoch: 27 
task: sign, mean loss: 0.26843, accuracy: 0.90217, avg. loss over tasks: 0.26843, lr: 0.000289228031029578
Diversity Loss - Mean: -0.07276, Variance: 0.01181
Semantic Loss - Mean: 0.34073, Variance: 0.00615

Test Epoch: 27 
task: sign, mean loss: 1.86966, accuracy: 0.44379, avg. loss over tasks: 1.86966
Diversity Loss - Mean: -0.07081, Variance: 0.01397
Semantic Loss - Mean: 1.60701, Variance: 0.02952

Train Epoch: 28 
task: sign, mean loss: 0.13653, accuracy: 0.94565, avg. loss over tasks: 0.13653, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.07799, Variance: 0.01184
Semantic Loss - Mean: 0.18806, Variance: 0.00605

Test Epoch: 28 
task: sign, mean loss: 1.97696, accuracy: 0.47337, avg. loss over tasks: 1.97696
Diversity Loss - Mean: -0.04928, Variance: 0.01392
Semantic Loss - Mean: 1.94118, Variance: 0.03173

Train Epoch: 29 
task: sign, mean loss: 0.21692, accuracy: 0.94022, avg. loss over tasks: 0.21692, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.08073, Variance: 0.01187
Semantic Loss - Mean: 0.30575, Variance: 0.00620

Test Epoch: 29 
task: sign, mean loss: 1.19009, accuracy: 0.66272, avg. loss over tasks: 1.19009
Diversity Loss - Mean: -0.06457, Variance: 0.01389
Semantic Loss - Mean: 1.12777, Variance: 0.03396

Train Epoch: 30 
task: sign, mean loss: 0.18363, accuracy: 0.94022, avg. loss over tasks: 0.18363, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.08653, Variance: 0.01194
Semantic Loss - Mean: 0.21858, Variance: 0.00618

Test Epoch: 30 
task: sign, mean loss: 2.09654, accuracy: 0.55621, avg. loss over tasks: 2.09654
Diversity Loss - Mean: -0.08991, Variance: 0.01390
Semantic Loss - Mean: 1.77926, Variance: 0.03388

Train Epoch: 31 
task: sign, mean loss: 0.28660, accuracy: 0.90217, avg. loss over tasks: 0.28660, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.09024, Variance: 0.01203
Semantic Loss - Mean: 0.32654, Variance: 0.00643

Test Epoch: 31 
task: sign, mean loss: 1.61278, accuracy: 0.60947, avg. loss over tasks: 1.61278
Diversity Loss - Mean: -0.05324, Variance: 0.01387
Semantic Loss - Mean: 1.19396, Variance: 0.03412

Train Epoch: 32 
task: sign, mean loss: 0.26627, accuracy: 0.89674, avg. loss over tasks: 0.26627, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.09227, Variance: 0.01212
Semantic Loss - Mean: 0.32745, Variance: 0.00643

Test Epoch: 32 
task: sign, mean loss: 1.61439, accuracy: 0.56213, avg. loss over tasks: 1.61439
Diversity Loss - Mean: -0.04935, Variance: 0.01390
Semantic Loss - Mean: 1.38835, Variance: 0.03430

Train Epoch: 33 
task: sign, mean loss: 0.11641, accuracy: 0.96196, avg. loss over tasks: 0.11641, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.08127, Variance: 0.01217
Semantic Loss - Mean: 0.14983, Variance: 0.00633

Test Epoch: 33 
task: sign, mean loss: 1.28530, accuracy: 0.62722, avg. loss over tasks: 1.28530
Diversity Loss - Mean: -0.06914, Variance: 0.01392
Semantic Loss - Mean: 1.03861, Variance: 0.03408

Train Epoch: 34 
task: sign, mean loss: 0.16357, accuracy: 0.92935, avg. loss over tasks: 0.16357, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.08289, Variance: 0.01221
Semantic Loss - Mean: 0.17810, Variance: 0.00634

Test Epoch: 34 
task: sign, mean loss: 3.23749, accuracy: 0.36095, avg. loss over tasks: 3.23749
Diversity Loss - Mean: -0.03391, Variance: 0.01387
Semantic Loss - Mean: 2.40277, Variance: 0.03436

Train Epoch: 35 
task: sign, mean loss: 0.16212, accuracy: 0.96196, avg. loss over tasks: 0.16212, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.08652, Variance: 0.01228
Semantic Loss - Mean: 0.19985, Variance: 0.00635

Test Epoch: 35 
task: sign, mean loss: 1.37140, accuracy: 0.60355, avg. loss over tasks: 1.37140
Diversity Loss - Mean: -0.07714, Variance: 0.01388
Semantic Loss - Mean: 1.24506, Variance: 0.03422

Train Epoch: 36 
task: sign, mean loss: 0.13118, accuracy: 0.95109, avg. loss over tasks: 0.13118, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.08073, Variance: 0.01229
Semantic Loss - Mean: 0.17568, Variance: 0.00625

Test Epoch: 36 
task: sign, mean loss: 1.03439, accuracy: 0.71598, avg. loss over tasks: 1.03439
Diversity Loss - Mean: -0.07204, Variance: 0.01389
Semantic Loss - Mean: 0.84377, Variance: 0.03411

Train Epoch: 37 
task: sign, mean loss: 0.17198, accuracy: 0.91848, avg. loss over tasks: 0.17198, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.09020, Variance: 0.01234
Semantic Loss - Mean: 0.21912, Variance: 0.00622

Test Epoch: 37 
task: sign, mean loss: 2.62470, accuracy: 0.35503, avg. loss over tasks: 2.62470
Diversity Loss - Mean: -0.03925, Variance: 0.01388
Semantic Loss - Mean: 2.10975, Variance: 0.03544

Train Epoch: 38 
task: sign, mean loss: 0.12795, accuracy: 0.94565, avg. loss over tasks: 0.12795, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.09008, Variance: 0.01238
Semantic Loss - Mean: 0.16893, Variance: 0.00617

Test Epoch: 38 
task: sign, mean loss: 1.09904, accuracy: 0.67456, avg. loss over tasks: 1.09904
Diversity Loss - Mean: -0.07245, Variance: 0.01388
Semantic Loss - Mean: 0.93622, Variance: 0.03534

Train Epoch: 39 
task: sign, mean loss: 0.06117, accuracy: 0.97826, avg. loss over tasks: 0.06117, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.09418, Variance: 0.01242
Semantic Loss - Mean: 0.11171, Variance: 0.00611

Test Epoch: 39 
task: sign, mean loss: 1.11742, accuracy: 0.75148, avg. loss over tasks: 1.11742
Diversity Loss - Mean: -0.08535, Variance: 0.01389
Semantic Loss - Mean: 0.96455, Variance: 0.03514

Train Epoch: 40 
task: sign, mean loss: 0.04821, accuracy: 0.98370, avg. loss over tasks: 0.04821, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.09616, Variance: 0.01246
Semantic Loss - Mean: 0.08628, Variance: 0.00609

Test Epoch: 40 
task: sign, mean loss: 2.06665, accuracy: 0.41420, avg. loss over tasks: 2.06665
Diversity Loss - Mean: -0.06431, Variance: 0.01388
Semantic Loss - Mean: 2.15956, Variance: 0.03652

Train Epoch: 41 
task: sign, mean loss: 0.04665, accuracy: 0.98913, avg. loss over tasks: 0.04665, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.09792, Variance: 0.01250
Semantic Loss - Mean: 0.08543, Variance: 0.00606

Test Epoch: 41 
task: sign, mean loss: 2.49470, accuracy: 0.50296, avg. loss over tasks: 2.49470
Diversity Loss - Mean: -0.07399, Variance: 0.01385
Semantic Loss - Mean: 2.05040, Variance: 0.03844

Train Epoch: 42 
task: sign, mean loss: 0.06257, accuracy: 0.98370, avg. loss over tasks: 0.06257, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.09807, Variance: 0.01255
Semantic Loss - Mean: 0.09313, Variance: 0.00598

Test Epoch: 42 
task: sign, mean loss: 2.27317, accuracy: 0.50296, avg. loss over tasks: 2.27317
Diversity Loss - Mean: -0.08511, Variance: 0.01385
Semantic Loss - Mean: 1.82527, Variance: 0.04056

Train Epoch: 43 
task: sign, mean loss: 0.04758, accuracy: 0.98370, avg. loss over tasks: 0.04758, lr: 0.000260757131773478
Diversity Loss - Mean: -0.09696, Variance: 0.01257
Semantic Loss - Mean: 0.08772, Variance: 0.00593

Test Epoch: 43 
task: sign, mean loss: 1.76161, accuracy: 0.53254, avg. loss over tasks: 1.76161
Diversity Loss - Mean: -0.08875, Variance: 0.01387
Semantic Loss - Mean: 1.47289, Variance: 0.04141

Train Epoch: 44 
task: sign, mean loss: 0.05294, accuracy: 0.98370, avg. loss over tasks: 0.05294, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.10135, Variance: 0.01261
Semantic Loss - Mean: 0.12324, Variance: 0.00609

Test Epoch: 44 
task: sign, mean loss: 1.87117, accuracy: 0.71598, avg. loss over tasks: 1.87117
Diversity Loss - Mean: -0.08522, Variance: 0.01390
Semantic Loss - Mean: 1.59414, Variance: 0.04178

Train Epoch: 45 
task: sign, mean loss: 0.04637, accuracy: 0.98370, avg. loss over tasks: 0.04637, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.10186, Variance: 0.01264
Semantic Loss - Mean: 0.08943, Variance: 0.00601

Test Epoch: 45 
task: sign, mean loss: 0.94538, accuracy: 0.76923, avg. loss over tasks: 0.94538
Diversity Loss - Mean: -0.08900, Variance: 0.01392
Semantic Loss - Mean: 0.98918, Variance: 0.04281

Train Epoch: 46 
task: sign, mean loss: 0.10410, accuracy: 0.97283, avg. loss over tasks: 0.10410, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.10326, Variance: 0.01267
Semantic Loss - Mean: 0.14377, Variance: 0.00596

Test Epoch: 46 
task: sign, mean loss: 1.64176, accuracy: 0.66864, avg. loss over tasks: 1.64176
Diversity Loss - Mean: -0.08975, Variance: 0.01392
Semantic Loss - Mean: 1.23300, Variance: 0.04259

Train Epoch: 47 
task: sign, mean loss: 0.12773, accuracy: 0.95109, avg. loss over tasks: 0.12773, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.10927, Variance: 0.01270
Semantic Loss - Mean: 0.16527, Variance: 0.00603

Test Epoch: 47 
task: sign, mean loss: 1.67351, accuracy: 0.60947, avg. loss over tasks: 1.67351
Diversity Loss - Mean: -0.09077, Variance: 0.01394
Semantic Loss - Mean: 1.36634, Variance: 0.04389

Train Epoch: 48 
task: sign, mean loss: 0.09812, accuracy: 0.96739, avg. loss over tasks: 0.09812, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.10813, Variance: 0.01274
Semantic Loss - Mean: 0.13866, Variance: 0.00602

Test Epoch: 48 
task: sign, mean loss: 2.66792, accuracy: 0.30178, avg. loss over tasks: 2.66792
Diversity Loss - Mean: -0.05916, Variance: 0.01399
Semantic Loss - Mean: 2.34819, Variance: 0.04666

Train Epoch: 49 
task: sign, mean loss: 0.09123, accuracy: 0.97826, avg. loss over tasks: 0.09123, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.10713, Variance: 0.01278
Semantic Loss - Mean: 0.14289, Variance: 0.00609

Test Epoch: 49 
task: sign, mean loss: 0.87554, accuracy: 0.80473, avg. loss over tasks: 0.87554
Diversity Loss - Mean: -0.07742, Variance: 0.01404
Semantic Loss - Mean: 0.76277, Variance: 0.04636

Train Epoch: 50 
task: sign, mean loss: 0.04966, accuracy: 0.97283, avg. loss over tasks: 0.04966, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.10575, Variance: 0.01282
Semantic Loss - Mean: 0.12030, Variance: 0.00614

Test Epoch: 50 
task: sign, mean loss: 0.86950, accuracy: 0.81657, avg. loss over tasks: 0.86950
Diversity Loss - Mean: -0.08601, Variance: 0.01410
Semantic Loss - Mean: 0.95576, Variance: 0.04719

Train Epoch: 51 
task: sign, mean loss: 0.03095, accuracy: 0.98913, avg. loss over tasks: 0.03095, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.10488, Variance: 0.01284
Semantic Loss - Mean: 0.08569, Variance: 0.00613

Test Epoch: 51 
task: sign, mean loss: 1.15835, accuracy: 0.78698, avg. loss over tasks: 1.15835
Diversity Loss - Mean: -0.09798, Variance: 0.01411
Semantic Loss - Mean: 1.14894, Variance: 0.04863

Train Epoch: 52 
task: sign, mean loss: 0.09702, accuracy: 0.97826, avg. loss over tasks: 0.09702, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.10898, Variance: 0.01287
Semantic Loss - Mean: 0.10823, Variance: 0.00619

Test Epoch: 52 
task: sign, mean loss: 1.41957, accuracy: 0.68639, avg. loss over tasks: 1.41957
Diversity Loss - Mean: -0.08472, Variance: 0.01414
Semantic Loss - Mean: 1.17586, Variance: 0.04899

Train Epoch: 53 
task: sign, mean loss: 0.18567, accuracy: 0.93478, avg. loss over tasks: 0.18567, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.10915, Variance: 0.01290
Semantic Loss - Mean: 0.24291, Variance: 0.00636

Test Epoch: 53 
task: sign, mean loss: 1.19399, accuracy: 0.63314, avg. loss over tasks: 1.19399
Diversity Loss - Mean: -0.09226, Variance: 0.01422
Semantic Loss - Mean: 1.02374, Variance: 0.04892

Train Epoch: 54 
task: sign, mean loss: 0.12320, accuracy: 0.96196, avg. loss over tasks: 0.12320, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.10811, Variance: 0.01292
Semantic Loss - Mean: 0.15302, Variance: 0.00633

Test Epoch: 54 
task: sign, mean loss: 2.55014, accuracy: 0.43787, avg. loss over tasks: 2.55014
Diversity Loss - Mean: -0.07703, Variance: 0.01430
Semantic Loss - Mean: 1.78576, Variance: 0.04986

Train Epoch: 55 
task: sign, mean loss: 0.16728, accuracy: 0.96739, avg. loss over tasks: 0.16728, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.11367, Variance: 0.01298
Semantic Loss - Mean: 0.22442, Variance: 0.00643

Test Epoch: 55 
task: sign, mean loss: 1.79806, accuracy: 0.73373, avg. loss over tasks: 1.79806
Diversity Loss - Mean: -0.09295, Variance: 0.01435
Semantic Loss - Mean: 1.78022, Variance: 0.05009

Train Epoch: 56 
task: sign, mean loss: 0.07748, accuracy: 0.97283, avg. loss over tasks: 0.07748, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.11022, Variance: 0.01300
Semantic Loss - Mean: 0.12816, Variance: 0.00638

Test Epoch: 56 
task: sign, mean loss: 1.98137, accuracy: 0.73964, avg. loss over tasks: 1.98137
Diversity Loss - Mean: -0.09502, Variance: 0.01437
Semantic Loss - Mean: 1.90681, Variance: 0.05134

Train Epoch: 57 
task: sign, mean loss: 0.17667, accuracy: 0.97826, avg. loss over tasks: 0.17667, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.11171, Variance: 0.01302
Semantic Loss - Mean: 0.16827, Variance: 0.00633

Test Epoch: 57 
task: sign, mean loss: 2.44534, accuracy: 0.63314, avg. loss over tasks: 2.44534
Diversity Loss - Mean: -0.09177, Variance: 0.01442
Semantic Loss - Mean: 2.09906, Variance: 0.05131

Train Epoch: 58 
task: sign, mean loss: 0.05770, accuracy: 0.97826, avg. loss over tasks: 0.05770, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.11443, Variance: 0.01304
Semantic Loss - Mean: 0.10596, Variance: 0.00634

Test Epoch: 58 
task: sign, mean loss: 2.23357, accuracy: 0.53846, avg. loss over tasks: 2.23357
Diversity Loss - Mean: -0.09979, Variance: 0.01446
Semantic Loss - Mean: 1.97289, Variance: 0.05290

Train Epoch: 59 
task: sign, mean loss: 0.08817, accuracy: 0.96739, avg. loss over tasks: 0.08817, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.11145, Variance: 0.01306
Semantic Loss - Mean: 0.13298, Variance: 0.00634

Test Epoch: 59 
task: sign, mean loss: 0.69707, accuracy: 0.83432, avg. loss over tasks: 0.69707
Diversity Loss - Mean: -0.11163, Variance: 0.01452
Semantic Loss - Mean: 0.70488, Variance: 0.05242

Train Epoch: 60 
task: sign, mean loss: 0.12801, accuracy: 0.95109, avg. loss over tasks: 0.12801, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.11091, Variance: 0.01309
Semantic Loss - Mean: 0.20753, Variance: 0.00641

Test Epoch: 60 
task: sign, mean loss: 0.63342, accuracy: 0.76923, avg. loss over tasks: 0.63342
Diversity Loss - Mean: -0.11001, Variance: 0.01459
Semantic Loss - Mean: 0.62231, Variance: 0.05168

Train Epoch: 61 
task: sign, mean loss: 0.16112, accuracy: 0.95109, avg. loss over tasks: 0.16112, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.11019, Variance: 0.01312
Semantic Loss - Mean: 0.24427, Variance: 0.00661

Test Epoch: 61 
task: sign, mean loss: 1.51832, accuracy: 0.65680, avg. loss over tasks: 1.51832
Diversity Loss - Mean: -0.09354, Variance: 0.01467
Semantic Loss - Mean: 1.25351, Variance: 0.05165

Train Epoch: 62 
task: sign, mean loss: 0.12212, accuracy: 0.95652, avg. loss over tasks: 0.12212, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.10854, Variance: 0.01316
Semantic Loss - Mean: 0.21254, Variance: 0.00687

Test Epoch: 62 
task: sign, mean loss: 2.20562, accuracy: 0.75740, avg. loss over tasks: 2.20562
Diversity Loss - Mean: -0.10314, Variance: 0.01466
Semantic Loss - Mean: 1.66568, Variance: 0.05171

Train Epoch: 63 
task: sign, mean loss: 0.12165, accuracy: 0.96196, avg. loss over tasks: 0.12165, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.11126, Variance: 0.01319
Semantic Loss - Mean: 0.15435, Variance: 0.00689

Test Epoch: 63 
task: sign, mean loss: 1.62690, accuracy: 0.40237, avg. loss over tasks: 1.62690
Diversity Loss - Mean: -0.09437, Variance: 0.01468
Semantic Loss - Mean: 1.35611, Variance: 0.05246

Train Epoch: 64 
task: sign, mean loss: 0.14892, accuracy: 0.93478, avg. loss over tasks: 0.14892, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.11485, Variance: 0.01324
Semantic Loss - Mean: 0.15973, Variance: 0.00692

Test Epoch: 64 
task: sign, mean loss: 1.68349, accuracy: 0.45562, avg. loss over tasks: 1.68349
Diversity Loss - Mean: -0.10173, Variance: 0.01472
Semantic Loss - Mean: 1.22093, Variance: 0.05391

Train Epoch: 65 
task: sign, mean loss: 0.05090, accuracy: 0.98913, avg. loss over tasks: 0.05090, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.11745, Variance: 0.01328
Semantic Loss - Mean: 0.11081, Variance: 0.00695

Test Epoch: 65 
task: sign, mean loss: 1.07756, accuracy: 0.63905, avg. loss over tasks: 1.07756
Diversity Loss - Mean: -0.09994, Variance: 0.01473
Semantic Loss - Mean: 1.07213, Variance: 0.05353

Train Epoch: 66 
task: sign, mean loss: 0.03509, accuracy: 0.98370, avg. loss over tasks: 0.03509, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.11526, Variance: 0.01331
Semantic Loss - Mean: 0.07637, Variance: 0.00692

Test Epoch: 66 
task: sign, mean loss: 0.59763, accuracy: 0.86982, avg. loss over tasks: 0.59763
Diversity Loss - Mean: -0.10982, Variance: 0.01474
Semantic Loss - Mean: 0.59209, Variance: 0.05293

Train Epoch: 67 
task: sign, mean loss: 0.03482, accuracy: 0.98913, avg. loss over tasks: 0.03482, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.11441, Variance: 0.01335
Semantic Loss - Mean: 0.06006, Variance: 0.00690

Test Epoch: 67 
task: sign, mean loss: 1.01093, accuracy: 0.68639, avg. loss over tasks: 1.01093
Diversity Loss - Mean: -0.10229, Variance: 0.01476
Semantic Loss - Mean: 1.05625, Variance: 0.05307

Train Epoch: 68 
task: sign, mean loss: 0.04912, accuracy: 0.98370, avg. loss over tasks: 0.04912, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.11331, Variance: 0.01337
Semantic Loss - Mean: 0.10410, Variance: 0.00695

Test Epoch: 68 
task: sign, mean loss: 0.93648, accuracy: 0.76923, avg. loss over tasks: 0.93648
Diversity Loss - Mean: -0.11151, Variance: 0.01477
Semantic Loss - Mean: 0.89510, Variance: 0.05311

Train Epoch: 69 
task: sign, mean loss: 0.01808, accuracy: 1.00000, avg. loss over tasks: 0.01808, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.11485, Variance: 0.01341
Semantic Loss - Mean: 0.07724, Variance: 0.00694

Test Epoch: 69 
task: sign, mean loss: 1.14759, accuracy: 0.76331, avg. loss over tasks: 1.14759
Diversity Loss - Mean: -0.11661, Variance: 0.01480
Semantic Loss - Mean: 1.05582, Variance: 0.05314

Train Epoch: 70 
task: sign, mean loss: 0.01415, accuracy: 0.99457, avg. loss over tasks: 0.01415, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.11442, Variance: 0.01344
Semantic Loss - Mean: 0.04033, Variance: 0.00687

Test Epoch: 70 
task: sign, mean loss: 1.15291, accuracy: 0.77515, avg. loss over tasks: 1.15291
Diversity Loss - Mean: -0.11288, Variance: 0.01481
Semantic Loss - Mean: 1.19333, Variance: 0.05298

Train Epoch: 71 
task: sign, mean loss: 0.01759, accuracy: 0.99457, avg. loss over tasks: 0.01759, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.11634, Variance: 0.01348
Semantic Loss - Mean: 0.04080, Variance: 0.00685

Test Epoch: 71 
task: sign, mean loss: 1.19200, accuracy: 0.75148, avg. loss over tasks: 1.19200
Diversity Loss - Mean: -0.11047, Variance: 0.01481
Semantic Loss - Mean: 1.28138, Variance: 0.05323

Train Epoch: 72 
task: sign, mean loss: 0.00577, accuracy: 1.00000, avg. loss over tasks: 0.00577, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.11543, Variance: 0.01351
Semantic Loss - Mean: 0.03266, Variance: 0.00678

Test Epoch: 72 
task: sign, mean loss: 1.46014, accuracy: 0.60947, avg. loss over tasks: 1.46014
Diversity Loss - Mean: -0.10670, Variance: 0.01482
Semantic Loss - Mean: 1.56458, Variance: 0.05394

Train Epoch: 73 
task: sign, mean loss: 0.01607, accuracy: 1.00000, avg. loss over tasks: 0.01607, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.11782, Variance: 0.01354
Semantic Loss - Mean: 0.04370, Variance: 0.00673

Test Epoch: 73 
task: sign, mean loss: 0.95269, accuracy: 0.80473, avg. loss over tasks: 0.95269
Diversity Loss - Mean: -0.11331, Variance: 0.01482
Semantic Loss - Mean: 0.87366, Variance: 0.05348

Train Epoch: 74 
task: sign, mean loss: 0.01478, accuracy: 0.99457, avg. loss over tasks: 0.01478, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.11703, Variance: 0.01357
Semantic Loss - Mean: 0.02973, Variance: 0.00666

Test Epoch: 74 
task: sign, mean loss: 1.40253, accuracy: 0.69231, avg. loss over tasks: 1.40253
Diversity Loss - Mean: -0.11306, Variance: 0.01483
Semantic Loss - Mean: 1.30258, Variance: 0.05365

Train Epoch: 75 
task: sign, mean loss: 0.00490, accuracy: 1.00000, avg. loss over tasks: 0.00490, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.11630, Variance: 0.01359
Semantic Loss - Mean: 0.02808, Variance: 0.00659

Test Epoch: 75 
task: sign, mean loss: 1.46342, accuracy: 0.67456, avg. loss over tasks: 1.46342
Diversity Loss - Mean: -0.11422, Variance: 0.01484
Semantic Loss - Mean: 1.30656, Variance: 0.05357

Train Epoch: 76 
task: sign, mean loss: 0.00475, accuracy: 1.00000, avg. loss over tasks: 0.00475, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.11807, Variance: 0.01361
Semantic Loss - Mean: 0.02824, Variance: 0.00656

Test Epoch: 76 
task: sign, mean loss: 1.32418, accuracy: 0.74556, avg. loss over tasks: 1.32418
Diversity Loss - Mean: -0.11533, Variance: 0.01485
Semantic Loss - Mean: 1.19844, Variance: 0.05351

Train Epoch: 77 
task: sign, mean loss: 0.00181, accuracy: 1.00000, avg. loss over tasks: 0.00181, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.11938, Variance: 0.01364
Semantic Loss - Mean: 0.02754, Variance: 0.00651

Test Epoch: 77 
task: sign, mean loss: 1.34817, accuracy: 0.73373, avg. loss over tasks: 1.34817
Diversity Loss - Mean: -0.11381, Variance: 0.01486
Semantic Loss - Mean: 1.48313, Variance: 0.05578

Train Epoch: 78 
task: sign, mean loss: 0.00229, accuracy: 1.00000, avg. loss over tasks: 0.00229, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.11901, Variance: 0.01366
Semantic Loss - Mean: 0.02591, Variance: 0.00644

Test Epoch: 78 
task: sign, mean loss: 1.33798, accuracy: 0.72189, avg. loss over tasks: 1.33798
Diversity Loss - Mean: -0.11412, Variance: 0.01486
Semantic Loss - Mean: 1.51636, Variance: 0.05596

Train Epoch: 79 
task: sign, mean loss: 0.00153, accuracy: 1.00000, avg. loss over tasks: 0.00153, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.12001, Variance: 0.01368
Semantic Loss - Mean: 0.01370, Variance: 0.00637

Test Epoch: 79 
task: sign, mean loss: 1.37153, accuracy: 0.71598, avg. loss over tasks: 1.37153
Diversity Loss - Mean: -0.11576, Variance: 0.01488
Semantic Loss - Mean: 1.40264, Variance: 0.05595

Train Epoch: 80 
task: sign, mean loss: 0.00182, accuracy: 1.00000, avg. loss over tasks: 0.00182, lr: 0.00015015
Diversity Loss - Mean: -0.12030, Variance: 0.01371
Semantic Loss - Mean: 0.00992, Variance: 0.00629

Test Epoch: 80 
task: sign, mean loss: 1.27109, accuracy: 0.74556, avg. loss over tasks: 1.27109
Diversity Loss - Mean: -0.11881, Variance: 0.01489
Semantic Loss - Mean: 1.26214, Variance: 0.05583

Train Epoch: 81 
task: sign, mean loss: 0.01841, accuracy: 0.99457, avg. loss over tasks: 0.01841, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.12145, Variance: 0.01373
Semantic Loss - Mean: 0.03145, Variance: 0.00624

Test Epoch: 81 
task: sign, mean loss: 1.19912, accuracy: 0.75148, avg. loss over tasks: 1.19912
Diversity Loss - Mean: -0.11928, Variance: 0.01490
Semantic Loss - Mean: 1.21102, Variance: 0.05569

Train Epoch: 82 
task: sign, mean loss: 0.04736, accuracy: 0.98913, avg. loss over tasks: 0.04736, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.12313, Variance: 0.01376
Semantic Loss - Mean: 0.08208, Variance: 0.00633

Test Epoch: 82 
task: sign, mean loss: 0.98329, accuracy: 0.83432, avg. loss over tasks: 0.98329
Diversity Loss - Mean: -0.12206, Variance: 0.01490
Semantic Loss - Mean: 0.94991, Variance: 0.05530

Train Epoch: 83 
task: sign, mean loss: 0.02950, accuracy: 0.98913, avg. loss over tasks: 0.02950, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.12280, Variance: 0.01379
Semantic Loss - Mean: 0.07931, Variance: 0.00632

Test Epoch: 83 
task: sign, mean loss: 0.74849, accuracy: 0.82249, avg. loss over tasks: 0.74849
Diversity Loss - Mean: -0.11701, Variance: 0.01495
Semantic Loss - Mean: 1.10840, Variance: 0.05519

Train Epoch: 84 
task: sign, mean loss: 0.01785, accuracy: 0.98913, avg. loss over tasks: 0.01785, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.11872, Variance: 0.01381
Semantic Loss - Mean: 0.06210, Variance: 0.00627

Test Epoch: 84 
task: sign, mean loss: 1.14641, accuracy: 0.78107, avg. loss over tasks: 1.14641
Diversity Loss - Mean: -0.11906, Variance: 0.01497
Semantic Loss - Mean: 1.20697, Variance: 0.05506

Train Epoch: 85 
task: sign, mean loss: 0.00529, accuracy: 1.00000, avg. loss over tasks: 0.00529, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.12143, Variance: 0.01383
Semantic Loss - Mean: 0.01467, Variance: 0.00621

Test Epoch: 85 
task: sign, mean loss: 1.35436, accuracy: 0.73964, avg. loss over tasks: 1.35436
Diversity Loss - Mean: -0.11690, Variance: 0.01498
Semantic Loss - Mean: 1.30303, Variance: 0.05515

Train Epoch: 86 
task: sign, mean loss: 0.01316, accuracy: 0.99457, avg. loss over tasks: 0.01316, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.12302, Variance: 0.01386
Semantic Loss - Mean: 0.04654, Variance: 0.00621

Test Epoch: 86 
task: sign, mean loss: 1.36829, accuracy: 0.76331, avg. loss over tasks: 1.36829
Diversity Loss - Mean: -0.11799, Variance: 0.01499
Semantic Loss - Mean: 1.31254, Variance: 0.05494

Train Epoch: 87 
task: sign, mean loss: 0.00901, accuracy: 1.00000, avg. loss over tasks: 0.00901, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.12210, Variance: 0.01389
Semantic Loss - Mean: 0.03357, Variance: 0.00618

Test Epoch: 87 
task: sign, mean loss: 1.17854, accuracy: 0.83432, avg. loss over tasks: 1.17854
Diversity Loss - Mean: -0.12085, Variance: 0.01499
Semantic Loss - Mean: 1.11151, Variance: 0.05471

Train Epoch: 88 
task: sign, mean loss: 0.01836, accuracy: 0.99457, avg. loss over tasks: 0.01836, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.12360, Variance: 0.01392
Semantic Loss - Mean: 0.02730, Variance: 0.00612

Test Epoch: 88 
task: sign, mean loss: 1.10453, accuracy: 0.84615, avg. loss over tasks: 1.10453
Diversity Loss - Mean: -0.11795, Variance: 0.01499
Semantic Loss - Mean: 1.14072, Variance: 0.05491

Train Epoch: 89 
task: sign, mean loss: 0.01085, accuracy: 0.99457, avg. loss over tasks: 0.01085, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.12251, Variance: 0.01395
Semantic Loss - Mean: 0.03192, Variance: 0.00608

Test Epoch: 89 
task: sign, mean loss: 1.37239, accuracy: 0.60947, avg. loss over tasks: 1.37239
Diversity Loss - Mean: -0.11378, Variance: 0.01499
Semantic Loss - Mean: 1.53399, Variance: 0.05544

Train Epoch: 90 
task: sign, mean loss: 0.00417, accuracy: 1.00000, avg. loss over tasks: 0.00417, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.12316, Variance: 0.01397
Semantic Loss - Mean: 0.02782, Variance: 0.00603

Test Epoch: 90 
task: sign, mean loss: 1.28905, accuracy: 0.66864, avg. loss over tasks: 1.28905
Diversity Loss - Mean: -0.11680, Variance: 0.01499
Semantic Loss - Mean: 1.34727, Variance: 0.05598

Train Epoch: 91 
task: sign, mean loss: 0.01021, accuracy: 0.99457, avg. loss over tasks: 0.01021, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.12504, Variance: 0.01399
Semantic Loss - Mean: 0.03997, Variance: 0.00599

Test Epoch: 91 
task: sign, mean loss: 1.15535, accuracy: 0.76331, avg. loss over tasks: 1.15535
Diversity Loss - Mean: -0.11962, Variance: 0.01499
Semantic Loss - Mean: 1.17174, Variance: 0.05611

Train Epoch: 92 
task: sign, mean loss: 0.00438, accuracy: 1.00000, avg. loss over tasks: 0.00438, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.12363, Variance: 0.01400
Semantic Loss - Mean: 0.02513, Variance: 0.00593

Test Epoch: 92 
task: sign, mean loss: 1.22610, accuracy: 0.76331, avg. loss over tasks: 1.22610
Diversity Loss - Mean: -0.12014, Variance: 0.01499
Semantic Loss - Mean: 1.24864, Variance: 0.05661

Train Epoch: 93 
task: sign, mean loss: 0.00486, accuracy: 1.00000, avg. loss over tasks: 0.00486, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.12411, Variance: 0.01402
Semantic Loss - Mean: 0.02914, Variance: 0.00597

Test Epoch: 93 
task: sign, mean loss: 1.16015, accuracy: 0.82840, avg. loss over tasks: 1.16015
Diversity Loss - Mean: -0.12180, Variance: 0.01499
Semantic Loss - Mean: 1.16012, Variance: 0.05656

Train Epoch: 94 
task: sign, mean loss: 0.00594, accuracy: 1.00000, avg. loss over tasks: 0.00594, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.12516, Variance: 0.01403
Semantic Loss - Mean: 0.02519, Variance: 0.00592

Test Epoch: 94 
task: sign, mean loss: 1.11702, accuracy: 0.82249, avg. loss over tasks: 1.11702
Diversity Loss - Mean: -0.12228, Variance: 0.01499
Semantic Loss - Mean: 1.19659, Variance: 0.05668

Train Epoch: 95 
task: sign, mean loss: 0.00497, accuracy: 1.00000, avg. loss over tasks: 0.00497, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.12526, Variance: 0.01405
Semantic Loss - Mean: 0.02769, Variance: 0.00588

Test Epoch: 95 
task: sign, mean loss: 0.94997, accuracy: 0.85799, avg. loss over tasks: 0.94997
Diversity Loss - Mean: -0.12148, Variance: 0.01499
Semantic Loss - Mean: 0.99011, Variance: 0.05658

Train Epoch: 96 
task: sign, mean loss: 0.00724, accuracy: 0.99457, avg. loss over tasks: 0.00724, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.12586, Variance: 0.01406
Semantic Loss - Mean: 0.03957, Variance: 0.00585

Test Epoch: 96 
task: sign, mean loss: 0.92452, accuracy: 0.83432, avg. loss over tasks: 0.92452
Diversity Loss - Mean: -0.12133, Variance: 0.01499
Semantic Loss - Mean: 0.95440, Variance: 0.05644

Train Epoch: 97 
task: sign, mean loss: 0.00305, accuracy: 1.00000, avg. loss over tasks: 0.00305, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.12595, Variance: 0.01408
Semantic Loss - Mean: 0.03909, Variance: 0.00585

Test Epoch: 97 
task: sign, mean loss: 1.15814, accuracy: 0.82249, avg. loss over tasks: 1.15814
Diversity Loss - Mean: -0.11992, Variance: 0.01499
Semantic Loss - Mean: 1.35367, Variance: 0.05678

Train Epoch: 98 
task: sign, mean loss: 0.00316, accuracy: 1.00000, avg. loss over tasks: 0.00316, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.12686, Variance: 0.01410
Semantic Loss - Mean: 0.01566, Variance: 0.00580

Test Epoch: 98 
task: sign, mean loss: 1.03360, accuracy: 0.84024, avg. loss over tasks: 1.03360
Diversity Loss - Mean: -0.12150, Variance: 0.01499
Semantic Loss - Mean: 1.20421, Variance: 0.05699

Train Epoch: 99 
task: sign, mean loss: 0.00098, accuracy: 1.00000, avg. loss over tasks: 0.00098, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.12612, Variance: 0.01411
Semantic Loss - Mean: 0.01233, Variance: 0.00575

Test Epoch: 99 
task: sign, mean loss: 0.92661, accuracy: 0.84024, avg. loss over tasks: 0.92661
Diversity Loss - Mean: -0.12211, Variance: 0.01500
Semantic Loss - Mean: 1.16370, Variance: 0.05701

Train Epoch: 100 
task: sign, mean loss: 0.01081, accuracy: 0.99457, avg. loss over tasks: 0.01081, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.12727, Variance: 0.01413
Semantic Loss - Mean: 0.02991, Variance: 0.00572

Test Epoch: 100 
task: sign, mean loss: 0.74086, accuracy: 0.86982, avg. loss over tasks: 0.74086
Diversity Loss - Mean: -0.12232, Variance: 0.01501
Semantic Loss - Mean: 0.98873, Variance: 0.05687

Train Epoch: 101 
task: sign, mean loss: 0.00606, accuracy: 1.00000, avg. loss over tasks: 0.00606, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.12734, Variance: 0.01415
Semantic Loss - Mean: 0.02357, Variance: 0.00568

Test Epoch: 101 
task: sign, mean loss: 0.96715, accuracy: 0.83432, avg. loss over tasks: 0.96715
Diversity Loss - Mean: -0.12229, Variance: 0.01503
Semantic Loss - Mean: 1.37595, Variance: 0.05757

Train Epoch: 102 
task: sign, mean loss: 0.00436, accuracy: 1.00000, avg. loss over tasks: 0.00436, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.12696, Variance: 0.01416
Semantic Loss - Mean: 0.02336, Variance: 0.00566

Test Epoch: 102 
task: sign, mean loss: 0.88443, accuracy: 0.86982, avg. loss over tasks: 0.88443
Diversity Loss - Mean: -0.12310, Variance: 0.01504
Semantic Loss - Mean: 1.20656, Variance: 0.05808

Train Epoch: 103 
task: sign, mean loss: 0.00463, accuracy: 1.00000, avg. loss over tasks: 0.00463, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.12674, Variance: 0.01417
Semantic Loss - Mean: 0.02359, Variance: 0.00561

Test Epoch: 103 
task: sign, mean loss: 1.03049, accuracy: 0.84615, avg. loss over tasks: 1.03049
Diversity Loss - Mean: -0.12367, Variance: 0.01505
Semantic Loss - Mean: 1.29449, Variance: 0.05854

Train Epoch: 104 
task: sign, mean loss: 0.00558, accuracy: 1.00000, avg. loss over tasks: 0.00558, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.12813, Variance: 0.01418
Semantic Loss - Mean: 0.03375, Variance: 0.00560

Test Epoch: 104 
task: sign, mean loss: 1.06070, accuracy: 0.82840, avg. loss over tasks: 1.06070
Diversity Loss - Mean: -0.12364, Variance: 0.01506
Semantic Loss - Mean: 1.25277, Variance: 0.05887

Train Epoch: 105 
task: sign, mean loss: 0.00116, accuracy: 1.00000, avg. loss over tasks: 0.00116, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.12804, Variance: 0.01420
Semantic Loss - Mean: 0.02892, Variance: 0.00559

Test Epoch: 105 
task: sign, mean loss: 1.28085, accuracy: 0.80473, avg. loss over tasks: 1.28085
Diversity Loss - Mean: -0.12394, Variance: 0.01507
Semantic Loss - Mean: 1.59655, Variance: 0.05910

Train Epoch: 106 
task: sign, mean loss: 0.00241, accuracy: 1.00000, avg. loss over tasks: 0.00241, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.12745, Variance: 0.01421
Semantic Loss - Mean: 0.01585, Variance: 0.00556

Test Epoch: 106 
task: sign, mean loss: 1.21009, accuracy: 0.78107, avg. loss over tasks: 1.21009
Diversity Loss - Mean: -0.12374, Variance: 0.01508
Semantic Loss - Mean: 1.57565, Variance: 0.05939

Train Epoch: 107 
task: sign, mean loss: 0.00134, accuracy: 1.00000, avg. loss over tasks: 0.00134, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.12746, Variance: 0.01422
Semantic Loss - Mean: 0.01172, Variance: 0.00551

Test Epoch: 107 
task: sign, mean loss: 1.22610, accuracy: 0.80473, avg. loss over tasks: 1.22610
Diversity Loss - Mean: -0.12510, Variance: 0.01508
Semantic Loss - Mean: 1.55617, Variance: 0.05973

Train Epoch: 108 
task: sign, mean loss: 0.00245, accuracy: 1.00000, avg. loss over tasks: 0.00245, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.12765, Variance: 0.01423
Semantic Loss - Mean: 0.01748, Variance: 0.00547

Test Epoch: 108 
task: sign, mean loss: 1.12892, accuracy: 0.82249, avg. loss over tasks: 1.12892
Diversity Loss - Mean: -0.12570, Variance: 0.01508
Semantic Loss - Mean: 1.40112, Variance: 0.05973

Train Epoch: 109 
task: sign, mean loss: 0.00298, accuracy: 1.00000, avg. loss over tasks: 0.00298, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.12780, Variance: 0.01424
Semantic Loss - Mean: 0.03108, Variance: 0.00543

Test Epoch: 109 
task: sign, mean loss: 1.21603, accuracy: 0.81657, avg. loss over tasks: 1.21603
Diversity Loss - Mean: -0.12587, Variance: 0.01509
Semantic Loss - Mean: 1.51812, Variance: 0.05962

Train Epoch: 110 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.12830, Variance: 0.01425
Semantic Loss - Mean: 0.01522, Variance: 0.00540

Test Epoch: 110 
task: sign, mean loss: 1.27109, accuracy: 0.80473, avg. loss over tasks: 1.27109
Diversity Loss - Mean: -0.12573, Variance: 0.01510
Semantic Loss - Mean: 1.59501, Variance: 0.05974

Train Epoch: 111 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.12906, Variance: 0.01426
Semantic Loss - Mean: 0.02882, Variance: 0.00540

Test Epoch: 111 
task: sign, mean loss: 1.22517, accuracy: 0.81657, avg. loss over tasks: 1.22517
Diversity Loss - Mean: -0.12675, Variance: 0.01510
Semantic Loss - Mean: 1.43491, Variance: 0.05969

Train Epoch: 112 
task: sign, mean loss: 0.00046, accuracy: 1.00000, avg. loss over tasks: 0.00046, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.12880, Variance: 0.01427
Semantic Loss - Mean: 0.01121, Variance: 0.00536

Test Epoch: 112 
task: sign, mean loss: 1.24446, accuracy: 0.82840, avg. loss over tasks: 1.24446
Diversity Loss - Mean: -0.12705, Variance: 0.01511
Semantic Loss - Mean: 1.49186, Variance: 0.05965

Train Epoch: 113 
task: sign, mean loss: 0.00290, accuracy: 1.00000, avg. loss over tasks: 0.00290, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.12914, Variance: 0.01428
Semantic Loss - Mean: 0.01359, Variance: 0.00532

Test Epoch: 113 
task: sign, mean loss: 1.17583, accuracy: 0.82840, avg. loss over tasks: 1.17583
Diversity Loss - Mean: -0.12734, Variance: 0.01511
Semantic Loss - Mean: 1.41164, Variance: 0.05941

Train Epoch: 114 
task: sign, mean loss: 0.00209, accuracy: 1.00000, avg. loss over tasks: 0.00209, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.12859, Variance: 0.01429
Semantic Loss - Mean: 0.02387, Variance: 0.00529

Test Epoch: 114 
task: sign, mean loss: 1.22131, accuracy: 0.81657, avg. loss over tasks: 1.22131
Diversity Loss - Mean: -0.12676, Variance: 0.01512
Semantic Loss - Mean: 1.49708, Variance: 0.05922

Train Epoch: 115 
task: sign, mean loss: 0.00071, accuracy: 1.00000, avg. loss over tasks: 0.00071, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.12850, Variance: 0.01430
Semantic Loss - Mean: 0.01744, Variance: 0.00526

Test Epoch: 115 
task: sign, mean loss: 1.30246, accuracy: 0.79290, avg. loss over tasks: 1.30246
Diversity Loss - Mean: -0.12700, Variance: 0.01513
Semantic Loss - Mean: 1.60146, Variance: 0.05912

Train Epoch: 116 
task: sign, mean loss: 0.00124, accuracy: 1.00000, avg. loss over tasks: 0.00124, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.12898, Variance: 0.01432
Semantic Loss - Mean: 0.01698, Variance: 0.00521

Test Epoch: 116 
task: sign, mean loss: 1.14444, accuracy: 0.81657, avg. loss over tasks: 1.14444
Diversity Loss - Mean: -0.12735, Variance: 0.01514
Semantic Loss - Mean: 1.38347, Variance: 0.05895

Train Epoch: 117 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.12971, Variance: 0.01433
Semantic Loss - Mean: 0.01372, Variance: 0.00518

Test Epoch: 117 
task: sign, mean loss: 1.17578, accuracy: 0.81657, avg. loss over tasks: 1.17578
Diversity Loss - Mean: -0.12728, Variance: 0.01514
Semantic Loss - Mean: 1.37892, Variance: 0.05877

Train Epoch: 118 
task: sign, mean loss: 0.00094, accuracy: 1.00000, avg. loss over tasks: 0.00094, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.12932, Variance: 0.01434
Semantic Loss - Mean: 0.01606, Variance: 0.00515

Test Epoch: 118 
task: sign, mean loss: 1.24128, accuracy: 0.81065, avg. loss over tasks: 1.24128
Diversity Loss - Mean: -0.12748, Variance: 0.01515
Semantic Loss - Mean: 1.45140, Variance: 0.05856

Train Epoch: 119 
task: sign, mean loss: 0.00032, accuracy: 1.00000, avg. loss over tasks: 0.00032, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.12963, Variance: 0.01435
Semantic Loss - Mean: 0.01015, Variance: 0.00511

Test Epoch: 119 
task: sign, mean loss: 1.21605, accuracy: 0.81065, avg. loss over tasks: 1.21605
Diversity Loss - Mean: -0.12716, Variance: 0.01516
Semantic Loss - Mean: 1.43991, Variance: 0.05834

Train Epoch: 120 
task: sign, mean loss: 0.00153, accuracy: 1.00000, avg. loss over tasks: 0.00153, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.12973, Variance: 0.01436
Semantic Loss - Mean: 0.02875, Variance: 0.00510

Test Epoch: 120 
task: sign, mean loss: 1.17041, accuracy: 0.81065, avg. loss over tasks: 1.17041
Diversity Loss - Mean: -0.12784, Variance: 0.01516
Semantic Loss - Mean: 1.41356, Variance: 0.05812

Train Epoch: 121 
task: sign, mean loss: 0.00079, accuracy: 1.00000, avg. loss over tasks: 0.00079, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.13083, Variance: 0.01437
Semantic Loss - Mean: 0.01448, Variance: 0.00507

Test Epoch: 121 
task: sign, mean loss: 1.25158, accuracy: 0.80473, avg. loss over tasks: 1.25158
Diversity Loss - Mean: -0.12761, Variance: 0.01517
Semantic Loss - Mean: 1.54536, Variance: 0.05799

Train Epoch: 122 
task: sign, mean loss: 0.00131, accuracy: 1.00000, avg. loss over tasks: 0.00131, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.12982, Variance: 0.01439
Semantic Loss - Mean: 0.02084, Variance: 0.00505

Test Epoch: 122 
task: sign, mean loss: 1.20644, accuracy: 0.81657, avg. loss over tasks: 1.20644
Diversity Loss - Mean: -0.12830, Variance: 0.01518
Semantic Loss - Mean: 1.46499, Variance: 0.05777

Train Epoch: 123 
task: sign, mean loss: 0.00049, accuracy: 1.00000, avg. loss over tasks: 0.00049, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.13021, Variance: 0.01440
Semantic Loss - Mean: 0.00755, Variance: 0.00501

Test Epoch: 123 
task: sign, mean loss: 1.23456, accuracy: 0.82840, avg. loss over tasks: 1.23456
Diversity Loss - Mean: -0.12819, Variance: 0.01519
Semantic Loss - Mean: 1.47275, Variance: 0.05754

Train Epoch: 124 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.12994, Variance: 0.01441
Semantic Loss - Mean: 0.01066, Variance: 0.00497

Test Epoch: 124 
task: sign, mean loss: 1.12664, accuracy: 0.84024, avg. loss over tasks: 1.12664
Diversity Loss - Mean: -0.12810, Variance: 0.01520
Semantic Loss - Mean: 1.37731, Variance: 0.05729

Train Epoch: 125 
task: sign, mean loss: 0.00028, accuracy: 1.00000, avg. loss over tasks: 0.00028, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.13039, Variance: 0.01443
Semantic Loss - Mean: 0.00879, Variance: 0.00494

Test Epoch: 125 
task: sign, mean loss: 1.19444, accuracy: 0.81657, avg. loss over tasks: 1.19444
Diversity Loss - Mean: -0.12831, Variance: 0.01521
Semantic Loss - Mean: 1.45703, Variance: 0.05704

Train Epoch: 126 
task: sign, mean loss: 0.00068, accuracy: 1.00000, avg. loss over tasks: 0.00068, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.13081, Variance: 0.01444
Semantic Loss - Mean: 0.00734, Variance: 0.00490

Test Epoch: 126 
task: sign, mean loss: 1.14627, accuracy: 0.82249, avg. loss over tasks: 1.14627
Diversity Loss - Mean: -0.12828, Variance: 0.01521
Semantic Loss - Mean: 1.37573, Variance: 0.05679

Train Epoch: 127 
task: sign, mean loss: 0.00095, accuracy: 1.00000, avg. loss over tasks: 0.00095, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.13076, Variance: 0.01445
Semantic Loss - Mean: 0.00934, Variance: 0.00487

Test Epoch: 127 
task: sign, mean loss: 1.20385, accuracy: 0.79882, avg. loss over tasks: 1.20385
Diversity Loss - Mean: -0.12874, Variance: 0.01522
Semantic Loss - Mean: 1.44726, Variance: 0.05657

Train Epoch: 128 
task: sign, mean loss: 0.00092, accuracy: 1.00000, avg. loss over tasks: 0.00092, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.13052, Variance: 0.01446
Semantic Loss - Mean: 0.01220, Variance: 0.00483

Test Epoch: 128 
task: sign, mean loss: 1.26286, accuracy: 0.76331, avg. loss over tasks: 1.26286
Diversity Loss - Mean: -0.12839, Variance: 0.01523
Semantic Loss - Mean: 1.53150, Variance: 0.05639

Train Epoch: 129 
task: sign, mean loss: 0.00619, accuracy: 1.00000, avg. loss over tasks: 0.00619, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.13090, Variance: 0.01447
Semantic Loss - Mean: 0.01295, Variance: 0.00481

Test Epoch: 129 
task: sign, mean loss: 1.20720, accuracy: 0.82249, avg. loss over tasks: 1.20720
Diversity Loss - Mean: -0.12900, Variance: 0.01524
Semantic Loss - Mean: 1.40910, Variance: 0.05622

Train Epoch: 130 
task: sign, mean loss: 0.00166, accuracy: 1.00000, avg. loss over tasks: 0.00166, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.13076, Variance: 0.01449
Semantic Loss - Mean: 0.00942, Variance: 0.00477

Test Epoch: 130 
task: sign, mean loss: 1.25413, accuracy: 0.79290, avg. loss over tasks: 1.25413
Diversity Loss - Mean: -0.12885, Variance: 0.01525
Semantic Loss - Mean: 1.46306, Variance: 0.05610

Train Epoch: 131 
task: sign, mean loss: 0.00049, accuracy: 1.00000, avg. loss over tasks: 0.00049, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.13151, Variance: 0.01450
Semantic Loss - Mean: 0.01058, Variance: 0.00474

Test Epoch: 131 
task: sign, mean loss: 1.29020, accuracy: 0.79290, avg. loss over tasks: 1.29020
Diversity Loss - Mean: -0.12884, Variance: 0.01526
Semantic Loss - Mean: 1.50221, Variance: 0.05600

Train Epoch: 132 
task: sign, mean loss: 0.00034, accuracy: 1.00000, avg. loss over tasks: 0.00034, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.13075, Variance: 0.01451
Semantic Loss - Mean: 0.00625, Variance: 0.00471

Test Epoch: 132 
task: sign, mean loss: 1.23664, accuracy: 0.81657, avg. loss over tasks: 1.23664
Diversity Loss - Mean: -0.12896, Variance: 0.01527
Semantic Loss - Mean: 1.43831, Variance: 0.05587

Train Epoch: 133 
task: sign, mean loss: 0.00140, accuracy: 1.00000, avg. loss over tasks: 0.00140, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.13109, Variance: 0.01452
Semantic Loss - Mean: 0.00872, Variance: 0.00467

Test Epoch: 133 
task: sign, mean loss: 1.30325, accuracy: 0.79290, avg. loss over tasks: 1.30325
Diversity Loss - Mean: -0.12851, Variance: 0.01528
Semantic Loss - Mean: 1.53185, Variance: 0.05578

Train Epoch: 134 
task: sign, mean loss: 0.00119, accuracy: 1.00000, avg. loss over tasks: 0.00119, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.13066, Variance: 0.01454
Semantic Loss - Mean: 0.01266, Variance: 0.00465

Test Epoch: 134 
task: sign, mean loss: 1.24557, accuracy: 0.81657, avg. loss over tasks: 1.24557
Diversity Loss - Mean: -0.12886, Variance: 0.01529
Semantic Loss - Mean: 1.43603, Variance: 0.05569

Train Epoch: 135 
task: sign, mean loss: 0.00474, accuracy: 0.99457, avg. loss over tasks: 0.00474, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.13102, Variance: 0.01455
Semantic Loss - Mean: 0.00986, Variance: 0.00461

Test Epoch: 135 
task: sign, mean loss: 1.22268, accuracy: 0.82840, avg. loss over tasks: 1.22268
Diversity Loss - Mean: -0.12933, Variance: 0.01529
Semantic Loss - Mean: 1.37333, Variance: 0.05558

Train Epoch: 136 
task: sign, mean loss: 0.00038, accuracy: 1.00000, avg. loss over tasks: 0.00038, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.13129, Variance: 0.01456
Semantic Loss - Mean: 0.00733, Variance: 0.00458

Test Epoch: 136 
task: sign, mean loss: 1.18795, accuracy: 0.82840, avg. loss over tasks: 1.18795
Diversity Loss - Mean: -0.12896, Variance: 0.01530
Semantic Loss - Mean: 1.36706, Variance: 0.05548

Train Epoch: 137 
task: sign, mean loss: 0.00085, accuracy: 1.00000, avg. loss over tasks: 0.00085, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.13078, Variance: 0.01458
Semantic Loss - Mean: 0.01268, Variance: 0.00455

Test Epoch: 137 
task: sign, mean loss: 1.21373, accuracy: 0.82840, avg. loss over tasks: 1.21373
Diversity Loss - Mean: -0.12883, Variance: 0.01530
Semantic Loss - Mean: 1.38671, Variance: 0.05536

Train Epoch: 138 
task: sign, mean loss: 0.00316, accuracy: 1.00000, avg. loss over tasks: 0.00316, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.13088, Variance: 0.01459
Semantic Loss - Mean: 0.01238, Variance: 0.00453

Test Epoch: 138 
task: sign, mean loss: 1.15405, accuracy: 0.82840, avg. loss over tasks: 1.15405
Diversity Loss - Mean: -0.12874, Variance: 0.01531
Semantic Loss - Mean: 1.32499, Variance: 0.05519

Train Epoch: 139 
task: sign, mean loss: 0.00150, accuracy: 1.00000, avg. loss over tasks: 0.00150, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.13105, Variance: 0.01460
Semantic Loss - Mean: 0.01618, Variance: 0.00451

Test Epoch: 139 
task: sign, mean loss: 1.22064, accuracy: 0.82249, avg. loss over tasks: 1.22064
Diversity Loss - Mean: -0.12893, Variance: 0.01532
Semantic Loss - Mean: 1.41564, Variance: 0.05505

Train Epoch: 140 
task: sign, mean loss: 0.00273, accuracy: 1.00000, avg. loss over tasks: 0.00273, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.13130, Variance: 0.01462
Semantic Loss - Mean: 0.01305, Variance: 0.00448

Test Epoch: 140 
task: sign, mean loss: 1.25975, accuracy: 0.82249, avg. loss over tasks: 1.25975
Diversity Loss - Mean: -0.12944, Variance: 0.01532
Semantic Loss - Mean: 1.45135, Variance: 0.05492

Train Epoch: 141 
task: sign, mean loss: 0.00097, accuracy: 1.00000, avg. loss over tasks: 0.00097, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.13108, Variance: 0.01463
Semantic Loss - Mean: 0.01295, Variance: 0.00446

Test Epoch: 141 
task: sign, mean loss: 1.16483, accuracy: 0.83432, avg. loss over tasks: 1.16483
Diversity Loss - Mean: -0.12918, Variance: 0.01533
Semantic Loss - Mean: 1.34677, Variance: 0.05474

Train Epoch: 142 
task: sign, mean loss: 0.00086, accuracy: 1.00000, avg. loss over tasks: 0.00086, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.13109, Variance: 0.01464
Semantic Loss - Mean: 0.03340, Variance: 0.00447

Test Epoch: 142 
task: sign, mean loss: 1.14431, accuracy: 0.83432, avg. loss over tasks: 1.14431
Diversity Loss - Mean: -0.12936, Variance: 0.01533
Semantic Loss - Mean: 1.31431, Variance: 0.05454

Train Epoch: 143 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.13173, Variance: 0.01465
Semantic Loss - Mean: 0.00353, Variance: 0.00444

Test Epoch: 143 
task: sign, mean loss: 1.19828, accuracy: 0.82840, avg. loss over tasks: 1.19828
Diversity Loss - Mean: -0.12916, Variance: 0.01534
Semantic Loss - Mean: 1.42809, Variance: 0.05438

Train Epoch: 144 
task: sign, mean loss: 0.00061, accuracy: 1.00000, avg. loss over tasks: 0.00061, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.13133, Variance: 0.01467
Semantic Loss - Mean: 0.00632, Variance: 0.00441

Test Epoch: 144 
task: sign, mean loss: 1.23822, accuracy: 0.82249, avg. loss over tasks: 1.23822
Diversity Loss - Mean: -0.12919, Variance: 0.01535
Semantic Loss - Mean: 1.48828, Variance: 0.05425

Train Epoch: 145 
task: sign, mean loss: 0.00374, accuracy: 1.00000, avg. loss over tasks: 0.00374, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.13105, Variance: 0.01468
Semantic Loss - Mean: 0.00880, Variance: 0.00438

Test Epoch: 145 
task: sign, mean loss: 1.18440, accuracy: 0.82249, avg. loss over tasks: 1.18440
Diversity Loss - Mean: -0.12925, Variance: 0.01536
Semantic Loss - Mean: 1.41317, Variance: 0.05410

Train Epoch: 146 
task: sign, mean loss: 0.00069, accuracy: 1.00000, avg. loss over tasks: 0.00069, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.13074, Variance: 0.01469
Semantic Loss - Mean: 0.01365, Variance: 0.00436

Test Epoch: 146 
task: sign, mean loss: 1.31148, accuracy: 0.79882, avg. loss over tasks: 1.31148
Diversity Loss - Mean: -0.12888, Variance: 0.01536
Semantic Loss - Mean: 1.55795, Variance: 0.05397

Train Epoch: 147 
task: sign, mean loss: 0.00054, accuracy: 1.00000, avg. loss over tasks: 0.00054, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.13100, Variance: 0.01470
Semantic Loss - Mean: 0.01399, Variance: 0.00434

Test Epoch: 147 
task: sign, mean loss: 1.27899, accuracy: 0.81657, avg. loss over tasks: 1.27899
Diversity Loss - Mean: -0.12897, Variance: 0.01537
Semantic Loss - Mean: 1.52549, Variance: 0.05383

Train Epoch: 148 
task: sign, mean loss: 0.00045, accuracy: 1.00000, avg. loss over tasks: 0.00045, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.13054, Variance: 0.01471
Semantic Loss - Mean: 0.00992, Variance: 0.00431

Test Epoch: 148 
task: sign, mean loss: 1.25524, accuracy: 0.82249, avg. loss over tasks: 1.25524
Diversity Loss - Mean: -0.12897, Variance: 0.01538
Semantic Loss - Mean: 1.48741, Variance: 0.05369

Train Epoch: 149 
task: sign, mean loss: 0.00044, accuracy: 1.00000, avg. loss over tasks: 0.00044, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.13062, Variance: 0.01472
Semantic Loss - Mean: 0.01285, Variance: 0.00429

Test Epoch: 149 
task: sign, mean loss: 1.21279, accuracy: 0.82840, avg. loss over tasks: 1.21279
Diversity Loss - Mean: -0.12911, Variance: 0.01538
Semantic Loss - Mean: 1.43729, Variance: 0.05354

Train Epoch: 150 
task: sign, mean loss: 0.00030, accuracy: 1.00000, avg. loss over tasks: 0.00030, lr: 3e-07
Diversity Loss - Mean: -0.13092, Variance: 0.01473
Semantic Loss - Mean: 0.00667, Variance: 0.00426

Test Epoch: 150 
task: sign, mean loss: 1.19715, accuracy: 0.82249, avg. loss over tasks: 1.19715
Diversity Loss - Mean: -0.12904, Variance: 0.01539
Semantic Loss - Mean: 1.41542, Variance: 0.05338

