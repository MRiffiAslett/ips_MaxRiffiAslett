Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09242, accuracy: 0.63587, avg. loss over tasks: 1.09242, lr: 3e-05
Diversity Loss - Mean: -0.00911, Variance: 0.01048
Semantic Loss - Mean: 1.43132, Variance: 0.07224

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17917, accuracy: 0.66272, avg. loss over tasks: 1.17917
Diversity Loss - Mean: -0.02763, Variance: 0.01245
Semantic Loss - Mean: 1.16064, Variance: 0.05334

Train Epoch: 2 
task: sign, mean loss: 0.96824, accuracy: 0.66848, avg. loss over tasks: 0.96824, lr: 6e-05
Diversity Loss - Mean: -0.01055, Variance: 0.01044
Semantic Loss - Mean: 0.98220, Variance: 0.03918

Test Epoch: 2 
task: sign, mean loss: 1.10354, accuracy: 0.66272, avg. loss over tasks: 1.10354
Diversity Loss - Mean: -0.01777, Variance: 0.01189
Semantic Loss - Mean: 1.15446, Variance: 0.03223

Train Epoch: 3 
task: sign, mean loss: 0.80605, accuracy: 0.69565, avg. loss over tasks: 0.80605, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.01437, Variance: 0.01017
Semantic Loss - Mean: 0.99567, Variance: 0.02717

Test Epoch: 3 
task: sign, mean loss: 1.28491, accuracy: 0.60947, avg. loss over tasks: 1.28491
Diversity Loss - Mean: -0.02742, Variance: 0.01103
Semantic Loss - Mean: 1.11680, Variance: 0.02891

Train Epoch: 4 
task: sign, mean loss: 0.76427, accuracy: 0.71196, avg. loss over tasks: 0.76427, lr: 0.00012
Diversity Loss - Mean: -0.02388, Variance: 0.00987
Semantic Loss - Mean: 0.89009, Variance: 0.02102

Test Epoch: 4 
task: sign, mean loss: 1.47859, accuracy: 0.48521, avg. loss over tasks: 1.47859
Diversity Loss - Mean: -0.01813, Variance: 0.01038
Semantic Loss - Mean: 1.08032, Variance: 0.02330

Train Epoch: 5 
task: sign, mean loss: 0.70658, accuracy: 0.71739, avg. loss over tasks: 0.70658, lr: 0.00015
Diversity Loss - Mean: -0.00876, Variance: 0.00952
Semantic Loss - Mean: 0.77212, Variance: 0.01717

Test Epoch: 5 
task: sign, mean loss: 1.72483, accuracy: 0.53254, avg. loss over tasks: 1.72483
Diversity Loss - Mean: -0.01190, Variance: 0.01019
Semantic Loss - Mean: 1.26776, Variance: 0.02258

Train Epoch: 6 
task: sign, mean loss: 0.64041, accuracy: 0.78261, avg. loss over tasks: 0.64041, lr: 0.00017999999999999998
Diversity Loss - Mean: 0.00505, Variance: 0.00932
Semantic Loss - Mean: 0.69879, Variance: 0.01475

Test Epoch: 6 
task: sign, mean loss: 2.11680, accuracy: 0.48521, avg. loss over tasks: 2.11680
Diversity Loss - Mean: 0.04367, Variance: 0.01012
Semantic Loss - Mean: 1.53782, Variance: 0.02307

Train Epoch: 7 
task: sign, mean loss: 0.55763, accuracy: 0.78804, avg. loss over tasks: 0.55763, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.00027, Variance: 0.00922
Semantic Loss - Mean: 0.57476, Variance: 0.01305

Test Epoch: 7 
task: sign, mean loss: 1.66646, accuracy: 0.52663, avg. loss over tasks: 1.66646
Diversity Loss - Mean: 0.00744, Variance: 0.01000
Semantic Loss - Mean: 1.45567, Variance: 0.02399

Train Epoch: 8 
task: sign, mean loss: 0.72574, accuracy: 0.76087, avg. loss over tasks: 0.72574, lr: 0.00024
Diversity Loss - Mean: 0.01047, Variance: 0.00908
Semantic Loss - Mean: 0.71958, Variance: 0.01196

Test Epoch: 8 
task: sign, mean loss: 2.50289, accuracy: 0.44970, avg. loss over tasks: 2.50289
Diversity Loss - Mean: 0.10093, Variance: 0.00998
Semantic Loss - Mean: 1.88803, Variance: 0.02521

Train Epoch: 9 
task: sign, mean loss: 0.81196, accuracy: 0.77717, avg. loss over tasks: 0.81196, lr: 0.00027
Diversity Loss - Mean: -0.00211, Variance: 0.00912
Semantic Loss - Mean: 0.76956, Variance: 0.01125

Test Epoch: 9 
task: sign, mean loss: 1.65675, accuracy: 0.62722, avg. loss over tasks: 1.65675
Diversity Loss - Mean: -0.00229, Variance: 0.01008
Semantic Loss - Mean: 1.42999, Variance: 0.02517

Train Epoch: 10 
task: sign, mean loss: 0.80683, accuracy: 0.70109, avg. loss over tasks: 0.80683, lr: 0.0003
Diversity Loss - Mean: -0.01296, Variance: 0.00914
Semantic Loss - Mean: 0.71830, Variance: 0.01057

Test Epoch: 10 
task: sign, mean loss: 2.00728, accuracy: 0.57988, avg. loss over tasks: 2.00728
Diversity Loss - Mean: 0.02084, Variance: 0.01000
Semantic Loss - Mean: 1.70180, Variance: 0.02455

Train Epoch: 11 
task: sign, mean loss: 0.65153, accuracy: 0.71739, avg. loss over tasks: 0.65153, lr: 0.0002999622730061346
Diversity Loss - Mean: 0.01689, Variance: 0.00898
Semantic Loss - Mean: 0.61329, Variance: 0.00986

Test Epoch: 11 
task: sign, mean loss: 2.02996, accuracy: 0.56213, avg. loss over tasks: 2.02996
Diversity Loss - Mean: 0.06322, Variance: 0.01020
Semantic Loss - Mean: 1.62734, Variance: 0.02522

Train Epoch: 12 
task: sign, mean loss: 0.46405, accuracy: 0.85326, avg. loss over tasks: 0.46405, lr: 0.000299849111021216
Diversity Loss - Mean: 0.03336, Variance: 0.00891
Semantic Loss - Mean: 0.46490, Variance: 0.00928

Test Epoch: 12 
task: sign, mean loss: 3.71201, accuracy: 0.27219, avg. loss over tasks: 3.71201
Diversity Loss - Mean: 0.11201, Variance: 0.01045
Semantic Loss - Mean: 2.58040, Variance: 0.02831

Train Epoch: 13 
task: sign, mean loss: 0.53985, accuracy: 0.83152, avg. loss over tasks: 0.53985, lr: 0.0002996605710257114
Diversity Loss - Mean: 0.02567, Variance: 0.00888
Semantic Loss - Mean: 0.57385, Variance: 0.00909

Test Epoch: 13 
task: sign, mean loss: 1.89513, accuracy: 0.66272, avg. loss over tasks: 1.89513
Diversity Loss - Mean: 0.03195, Variance: 0.01067
Semantic Loss - Mean: 1.75331, Variance: 0.02853

Train Epoch: 14 
task: sign, mean loss: 0.44584, accuracy: 0.84783, avg. loss over tasks: 0.44584, lr: 0.00029939674795518656
Diversity Loss - Mean: 0.01183, Variance: 0.00891
Semantic Loss - Mean: 0.47662, Variance: 0.00867

Test Epoch: 14 
task: sign, mean loss: 1.62612, accuracy: 0.67456, avg. loss over tasks: 1.62612
Diversity Loss - Mean: 0.00871, Variance: 0.01078
Semantic Loss - Mean: 1.50820, Variance: 0.02720

Train Epoch: 15 
task: sign, mean loss: 0.45415, accuracy: 0.82065, avg. loss over tasks: 0.45415, lr: 0.0002990577746525024
Diversity Loss - Mean: 0.03438, Variance: 0.00883
Semantic Loss - Mean: 0.47525, Variance: 0.00847

Test Epoch: 15 
task: sign, mean loss: 1.62612, accuracy: 0.65089, avg. loss over tasks: 1.62612
Diversity Loss - Mean: -0.01250, Variance: 0.01069
Semantic Loss - Mean: 1.49297, Variance: 0.02601

Train Epoch: 16 
task: sign, mean loss: 0.43147, accuracy: 0.80978, avg. loss over tasks: 0.43147, lr: 0.000298643821800925
Diversity Loss - Mean: 0.03949, Variance: 0.00871
Semantic Loss - Mean: 0.46221, Variance: 0.00814

Test Epoch: 16 
task: sign, mean loss: 2.23698, accuracy: 0.66272, avg. loss over tasks: 2.23698
Diversity Loss - Mean: 0.03459, Variance: 0.01057
Semantic Loss - Mean: 1.99196, Variance: 0.02599

Train Epoch: 17 
task: sign, mean loss: 0.33526, accuracy: 0.86957, avg. loss over tasks: 0.33526, lr: 0.0002981550978381814
Diversity Loss - Mean: 0.02218, Variance: 0.00869
Semantic Loss - Mean: 0.36231, Variance: 0.00776

Test Epoch: 17 
task: sign, mean loss: 2.06636, accuracy: 0.60355, avg. loss over tasks: 2.06636
Diversity Loss - Mean: 0.04434, Variance: 0.01060
Semantic Loss - Mean: 1.83194, Variance: 0.02512

Train Epoch: 18 
task: sign, mean loss: 0.37560, accuracy: 0.89130, avg. loss over tasks: 0.37560, lr: 0.00029759184885150465
Diversity Loss - Mean: 0.03351, Variance: 0.00867
Semantic Loss - Mean: 0.41752, Variance: 0.00772

Test Epoch: 18 
task: sign, mean loss: 1.95679, accuracy: 0.56805, avg. loss over tasks: 1.95679
Diversity Loss - Mean: 0.00257, Variance: 0.01069
Semantic Loss - Mean: 1.80713, Variance: 0.02475

Train Epoch: 19 
task: sign, mean loss: 0.41667, accuracy: 0.83696, avg. loss over tasks: 0.41667, lr: 0.0002969543584537218
Diversity Loss - Mean: 0.02367, Variance: 0.00867
Semantic Loss - Mean: 0.44825, Variance: 0.00751

Test Epoch: 19 
task: sign, mean loss: 1.88965, accuracy: 0.66272, avg. loss over tasks: 1.88965
Diversity Loss - Mean: 0.02334, Variance: 0.01078
Semantic Loss - Mean: 1.74686, Variance: 0.02394

Train Epoch: 20 
task: sign, mean loss: 0.53621, accuracy: 0.77717, avg. loss over tasks: 0.53621, lr: 0.0002962429476404462
Diversity Loss - Mean: 0.02162, Variance: 0.00869
Semantic Loss - Mean: 0.58525, Variance: 0.00735

Test Epoch: 20 
task: sign, mean loss: 2.78871, accuracy: 0.18935, avg. loss over tasks: 2.78871
Diversity Loss - Mean: 0.09035, Variance: 0.01089
Semantic Loss - Mean: 2.35712, Variance: 0.02442

Train Epoch: 21 
task: sign, mean loss: 0.29923, accuracy: 0.86957, avg. loss over tasks: 0.29923, lr: 0.00029545797462844647
Diversity Loss - Mean: 0.01967, Variance: 0.00867
Semantic Loss - Mean: 0.34562, Variance: 0.00717

Test Epoch: 21 
task: sign, mean loss: 2.14676, accuracy: 0.43195, avg. loss over tasks: 2.14676
Diversity Loss - Mean: 0.04836, Variance: 0.01088
Semantic Loss - Mean: 1.93822, Variance: 0.02408

Train Epoch: 22 
task: sign, mean loss: 0.25668, accuracy: 0.88043, avg. loss over tasks: 0.25668, lr: 0.0002945998346752736
Diversity Loss - Mean: 0.03408, Variance: 0.00865
Semantic Loss - Mean: 0.30473, Variance: 0.00696

Test Epoch: 22 
task: sign, mean loss: 1.85305, accuracy: 0.59172, avg. loss over tasks: 1.85305
Diversity Loss - Mean: 0.02056, Variance: 0.01082
Semantic Loss - Mean: 1.69240, Variance: 0.02367

Train Epoch: 23 
task: sign, mean loss: 0.13678, accuracy: 0.95109, avg. loss over tasks: 0.13678, lr: 0.0002936689598802368
Diversity Loss - Mean: 0.05189, Variance: 0.00859
Semantic Loss - Mean: 0.16403, Variance: 0.00674

Test Epoch: 23 
task: sign, mean loss: 2.71194, accuracy: 0.43787, avg. loss over tasks: 2.71194
Diversity Loss - Mean: 0.04228, Variance: 0.01077
Semantic Loss - Mean: 2.33907, Variance: 0.02335

Train Epoch: 24 
task: sign, mean loss: 0.14113, accuracy: 0.95109, avg. loss over tasks: 0.14113, lr: 0.00029266581896682876
Diversity Loss - Mean: 0.05751, Variance: 0.00853
Semantic Loss - Mean: 0.17039, Variance: 0.00654

Test Epoch: 24 
task: sign, mean loss: 2.55053, accuracy: 0.38462, avg. loss over tasks: 2.55053
Diversity Loss - Mean: 0.05336, Variance: 0.01074
Semantic Loss - Mean: 2.25016, Variance: 0.02426

Train Epoch: 25 
task: sign, mean loss: 0.10274, accuracy: 0.97283, avg. loss over tasks: 0.10274, lr: 0.00029159091704670885
Diversity Loss - Mean: 0.05746, Variance: 0.00846
Semantic Loss - Mean: 0.12419, Variance: 0.00636

Test Epoch: 25 
task: sign, mean loss: 2.49359, accuracy: 0.39053, avg. loss over tasks: 2.49359
Diversity Loss - Mean: 0.09777, Variance: 0.01071
Semantic Loss - Mean: 2.02887, Variance: 0.02442

Train Epoch: 26 
task: sign, mean loss: 0.29061, accuracy: 0.90217, avg. loss over tasks: 0.29061, lr: 0.00029044479536536455
Diversity Loss - Mean: 0.05849, Variance: 0.00842
Semantic Loss - Mean: 0.30641, Variance: 0.00665

Test Epoch: 26 
task: sign, mean loss: 2.79776, accuracy: 0.39645, avg. loss over tasks: 2.79776
Diversity Loss - Mean: 0.10162, Variance: 0.01068
Semantic Loss - Mean: 2.66019, Variance: 0.02824

Train Epoch: 27 
task: sign, mean loss: 0.26311, accuracy: 0.91304, avg. loss over tasks: 0.26311, lr: 0.000289228031029578
Diversity Loss - Mean: 0.04819, Variance: 0.00842
Semantic Loss - Mean: 0.37570, Variance: 0.00687

Test Epoch: 27 
task: sign, mean loss: 2.23794, accuracy: 0.40828, avg. loss over tasks: 2.23794
Diversity Loss - Mean: 0.01620, Variance: 0.01078
Semantic Loss - Mean: 2.11203, Variance: 0.03001

Train Epoch: 28 
task: sign, mean loss: 1.19033, accuracy: 0.63043, avg. loss over tasks: 1.19033, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.02372, Variance: 0.00861
Semantic Loss - Mean: 1.16016, Variance: 0.00693

Test Epoch: 28 
task: sign, mean loss: 1.63413, accuracy: 0.66272, avg. loss over tasks: 1.63413
Diversity Loss - Mean: -0.02495, Variance: 0.01114
Semantic Loss - Mean: 1.49622, Variance: 0.03025

Train Epoch: 29 
task: sign, mean loss: 1.11845, accuracy: 0.62500, avg. loss over tasks: 1.11845, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.06994, Variance: 0.00904
Semantic Loss - Mean: 1.11486, Variance: 0.00675

Test Epoch: 29 
task: sign, mean loss: 1.15591, accuracy: 0.66272, avg. loss over tasks: 1.15591
Diversity Loss - Mean: -0.05533, Variance: 0.01135
Semantic Loss - Mean: 1.21744, Variance: 0.02941

Train Epoch: 30 
task: sign, mean loss: 1.02146, accuracy: 0.65761, avg. loss over tasks: 1.02146, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.08619, Variance: 0.00958
Semantic Loss - Mean: 1.02793, Variance: 0.00655

Test Epoch: 30 
task: sign, mean loss: 1.21872, accuracy: 0.66272, avg. loss over tasks: 1.21872
Diversity Loss - Mean: -0.09133, Variance: 0.01170
Semantic Loss - Mean: 1.17151, Variance: 0.02858

Train Epoch: 31 
task: sign, mean loss: 1.05723, accuracy: 0.66304, avg. loss over tasks: 1.05723, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.09558, Variance: 0.01008
Semantic Loss - Mean: 1.03691, Variance: 0.00635

Test Epoch: 31 
task: sign, mean loss: 1.09073, accuracy: 0.66272, avg. loss over tasks: 1.09073
Diversity Loss - Mean: -0.08408, Variance: 0.01201
Semantic Loss - Mean: 1.11279, Variance: 0.02777

Train Epoch: 32 
task: sign, mean loss: 1.05005, accuracy: 0.65761, avg. loss over tasks: 1.05005, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.09508, Variance: 0.01055
Semantic Loss - Mean: 1.01135, Variance: 0.00616

Test Epoch: 32 
task: sign, mean loss: 1.05936, accuracy: 0.66272, avg. loss over tasks: 1.05936
Diversity Loss - Mean: -0.08290, Variance: 0.01230
Semantic Loss - Mean: 1.06468, Variance: 0.02695

Train Epoch: 33 
task: sign, mean loss: 1.01621, accuracy: 0.65761, avg. loss over tasks: 1.01621, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.09966, Variance: 0.01100
Semantic Loss - Mean: 0.99607, Variance: 0.00598

Test Epoch: 33 
task: sign, mean loss: 1.15315, accuracy: 0.66272, avg. loss over tasks: 1.15315
Diversity Loss - Mean: -0.08165, Variance: 0.01254
Semantic Loss - Mean: 1.10753, Variance: 0.02618

Train Epoch: 34 
task: sign, mean loss: 0.98131, accuracy: 0.65761, avg. loss over tasks: 0.98131, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.10235, Variance: 0.01140
Semantic Loss - Mean: 0.97704, Variance: 0.00582

Test Epoch: 34 
task: sign, mean loss: 1.12894, accuracy: 0.66272, avg. loss over tasks: 1.12894
Diversity Loss - Mean: -0.06623, Variance: 0.01272
Semantic Loss - Mean: 1.12489, Variance: 0.02546

Train Epoch: 35 
task: sign, mean loss: 1.03138, accuracy: 0.65761, avg. loss over tasks: 1.03138, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.10284, Variance: 0.01177
Semantic Loss - Mean: 1.02774, Variance: 0.00566

Test Epoch: 35 
task: sign, mean loss: 1.16339, accuracy: 0.62130, avg. loss over tasks: 1.16339
Diversity Loss - Mean: -0.06715, Variance: 0.01288
Semantic Loss - Mean: 1.13304, Variance: 0.02479

Train Epoch: 36 
task: sign, mean loss: 0.98445, accuracy: 0.65217, avg. loss over tasks: 0.98445, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.10612, Variance: 0.01214
Semantic Loss - Mean: 0.97874, Variance: 0.00551

Test Epoch: 36 
task: sign, mean loss: 1.24681, accuracy: 0.61538, avg. loss over tasks: 1.24681
Diversity Loss - Mean: -0.06511, Variance: 0.01302
Semantic Loss - Mean: 1.17522, Variance: 0.02420

Train Epoch: 37 
task: sign, mean loss: 0.99518, accuracy: 0.65761, avg. loss over tasks: 0.99518, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.11342, Variance: 0.01248
Semantic Loss - Mean: 0.96937, Variance: 0.00537

Test Epoch: 37 
task: sign, mean loss: 1.20863, accuracy: 0.43787, avg. loss over tasks: 1.20863
Diversity Loss - Mean: -0.07370, Variance: 0.01318
Semantic Loss - Mean: 1.15101, Variance: 0.02363

Train Epoch: 38 
task: sign, mean loss: 0.96515, accuracy: 0.63043, avg. loss over tasks: 0.96515, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.10069, Variance: 0.01276
Semantic Loss - Mean: 0.95412, Variance: 0.00524

Test Epoch: 38 
task: sign, mean loss: 1.09983, accuracy: 0.65680, avg. loss over tasks: 1.09983
Diversity Loss - Mean: -0.08651, Variance: 0.01336
Semantic Loss - Mean: 1.10819, Variance: 0.02303

Train Epoch: 39 
task: sign, mean loss: 0.91754, accuracy: 0.67391, avg. loss over tasks: 0.91754, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.09466, Variance: 0.01301
Semantic Loss - Mean: 0.90981, Variance: 0.00511

Test Epoch: 39 
task: sign, mean loss: 1.12478, accuracy: 0.61538, avg. loss over tasks: 1.12478
Diversity Loss - Mean: -0.09426, Variance: 0.01361
Semantic Loss - Mean: 1.11370, Variance: 0.02246

Train Epoch: 40 
task: sign, mean loss: 0.91049, accuracy: 0.70109, avg. loss over tasks: 0.91049, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.08145, Variance: 0.01320
Semantic Loss - Mean: 0.91466, Variance: 0.00500

Test Epoch: 40 
task: sign, mean loss: 1.30599, accuracy: 0.56213, avg. loss over tasks: 1.30599
Diversity Loss - Mean: -0.07536, Variance: 0.01377
Semantic Loss - Mean: 1.26138, Variance: 0.02195

Train Epoch: 41 
task: sign, mean loss: 0.94034, accuracy: 0.64130, avg. loss over tasks: 0.94034, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.08676, Variance: 0.01338
Semantic Loss - Mean: 0.91389, Variance: 0.00489

Test Epoch: 41 
task: sign, mean loss: 1.30375, accuracy: 0.44379, avg. loss over tasks: 1.30375
Diversity Loss - Mean: -0.01609, Variance: 0.01377
Semantic Loss - Mean: 1.27404, Variance: 0.02154

Train Epoch: 42 
task: sign, mean loss: 0.90834, accuracy: 0.65761, avg. loss over tasks: 0.90834, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.09050, Variance: 0.01357
Semantic Loss - Mean: 0.89777, Variance: 0.00480

Test Epoch: 42 
task: sign, mean loss: 1.17046, accuracy: 0.66272, avg. loss over tasks: 1.17046
Diversity Loss - Mean: -0.10067, Variance: 0.01406
Semantic Loss - Mean: 1.17667, Variance: 0.02105

Train Epoch: 43 
task: sign, mean loss: 0.90780, accuracy: 0.66304, avg. loss over tasks: 0.90780, lr: 0.000260757131773478
Diversity Loss - Mean: -0.07695, Variance: 0.01371
Semantic Loss - Mean: 0.90866, Variance: 0.00471

Test Epoch: 43 
task: sign, mean loss: 1.20888, accuracy: 0.66272, avg. loss over tasks: 1.20888
Diversity Loss - Mean: -0.10374, Variance: 0.01431
Semantic Loss - Mean: 1.16979, Variance: 0.02058

Train Epoch: 44 
task: sign, mean loss: 0.88384, accuracy: 0.71196, avg. loss over tasks: 0.88384, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.09014, Variance: 0.01388
Semantic Loss - Mean: 0.86505, Variance: 0.00462

Test Epoch: 44 
task: sign, mean loss: 1.17741, accuracy: 0.60947, avg. loss over tasks: 1.17741
Diversity Loss - Mean: -0.10853, Variance: 0.01454
Semantic Loss - Mean: 1.15768, Variance: 0.02013

Train Epoch: 45 
task: sign, mean loss: 0.83443, accuracy: 0.72826, avg. loss over tasks: 0.83443, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.09295, Variance: 0.01406
Semantic Loss - Mean: 0.82689, Variance: 0.00452

Test Epoch: 45 
task: sign, mean loss: 1.30430, accuracy: 0.57396, avg. loss over tasks: 1.30430
Diversity Loss - Mean: -0.09551, Variance: 0.01473
Semantic Loss - Mean: 1.26042, Variance: 0.01971

Train Epoch: 46 
task: sign, mean loss: 0.77766, accuracy: 0.70652, avg. loss over tasks: 0.77766, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.08306, Variance: 0.01420
Semantic Loss - Mean: 0.78829, Variance: 0.00445

Test Epoch: 46 
task: sign, mean loss: 1.28760, accuracy: 0.65680, avg. loss over tasks: 1.28760
Diversity Loss - Mean: -0.10669, Variance: 0.01495
Semantic Loss - Mean: 1.24380, Variance: 0.01933

Train Epoch: 47 
task: sign, mean loss: 0.80699, accuracy: 0.72283, avg. loss over tasks: 0.80699, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.07994, Variance: 0.01430
Semantic Loss - Mean: 0.81563, Variance: 0.00439

Test Epoch: 47 
task: sign, mean loss: 1.25879, accuracy: 0.62130, avg. loss over tasks: 1.25879
Diversity Loss - Mean: -0.09654, Variance: 0.01518
Semantic Loss - Mean: 1.21828, Variance: 0.01896

Train Epoch: 48 
task: sign, mean loss: 0.69634, accuracy: 0.73913, avg. loss over tasks: 0.69634, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.07098, Variance: 0.01438
Semantic Loss - Mean: 0.73287, Variance: 0.00432

Test Epoch: 48 
task: sign, mean loss: 1.21810, accuracy: 0.62722, avg. loss over tasks: 1.21810
Diversity Loss - Mean: -0.09962, Variance: 0.01542
Semantic Loss - Mean: 1.17583, Variance: 0.01863

Train Epoch: 49 
task: sign, mean loss: 0.78095, accuracy: 0.72826, avg. loss over tasks: 0.78095, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.06510, Variance: 0.01443
Semantic Loss - Mean: 0.79394, Variance: 0.00431

Test Epoch: 49 
task: sign, mean loss: 1.23196, accuracy: 0.65680, avg. loss over tasks: 1.23196
Diversity Loss - Mean: -0.10448, Variance: 0.01564
Semantic Loss - Mean: 1.18729, Variance: 0.01831

Train Epoch: 50 
task: sign, mean loss: 0.80360, accuracy: 0.70109, avg. loss over tasks: 0.80360, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.07425, Variance: 0.01451
Semantic Loss - Mean: 0.80547, Variance: 0.00426

Test Epoch: 50 
task: sign, mean loss: 1.25387, accuracy: 0.65089, avg. loss over tasks: 1.25387
Diversity Loss - Mean: -0.10639, Variance: 0.01585
Semantic Loss - Mean: 1.18671, Variance: 0.01807

Train Epoch: 51 
task: sign, mean loss: 0.74147, accuracy: 0.69565, avg. loss over tasks: 0.74147, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.07993, Variance: 0.01459
Semantic Loss - Mean: 0.75799, Variance: 0.00420

Test Epoch: 51 
task: sign, mean loss: 1.29462, accuracy: 0.60355, avg. loss over tasks: 1.29462
Diversity Loss - Mean: -0.08873, Variance: 0.01599
Semantic Loss - Mean: 1.24582, Variance: 0.01779

Train Epoch: 52 
task: sign, mean loss: 0.71527, accuracy: 0.72826, avg. loss over tasks: 0.71527, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.06185, Variance: 0.01463
Semantic Loss - Mean: 0.72793, Variance: 0.00414

Test Epoch: 52 
task: sign, mean loss: 1.56867, accuracy: 0.47929, avg. loss over tasks: 1.56867
Diversity Loss - Mean: -0.06302, Variance: 0.01603
Semantic Loss - Mean: 1.48648, Variance: 0.01754

Train Epoch: 53 
task: sign, mean loss: 0.81298, accuracy: 0.70109, avg. loss over tasks: 0.81298, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.07978, Variance: 0.01469
Semantic Loss - Mean: 0.81415, Variance: 0.00410

Test Epoch: 53 
task: sign, mean loss: 1.83891, accuracy: 0.39645, avg. loss over tasks: 1.83891
Diversity Loss - Mean: -0.03264, Variance: 0.01597
Semantic Loss - Mean: 1.65074, Variance: 0.01740

Train Epoch: 54 
task: sign, mean loss: 0.65410, accuracy: 0.73370, avg. loss over tasks: 0.65410, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.06079, Variance: 0.01473
Semantic Loss - Mean: 0.65495, Variance: 0.00406

Test Epoch: 54 
task: sign, mean loss: 1.37246, accuracy: 0.65680, avg. loss over tasks: 1.37246
Diversity Loss - Mean: -0.07945, Variance: 0.01610
Semantic Loss - Mean: 1.32758, Variance: 0.01717

Train Epoch: 55 
task: sign, mean loss: 0.60206, accuracy: 0.76630, avg. loss over tasks: 0.60206, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.05959, Variance: 0.01477
Semantic Loss - Mean: 0.63970, Variance: 0.00405

Test Epoch: 55 
task: sign, mean loss: 1.46927, accuracy: 0.64497, avg. loss over tasks: 1.46927
Diversity Loss - Mean: -0.08706, Variance: 0.01627
Semantic Loss - Mean: 1.40065, Variance: 0.01695

Train Epoch: 56 
task: sign, mean loss: 0.70969, accuracy: 0.71196, avg. loss over tasks: 0.70969, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.04156, Variance: 0.01478
Semantic Loss - Mean: 0.69082, Variance: 0.00405

Test Epoch: 56 
task: sign, mean loss: 1.61556, accuracy: 0.63905, avg. loss over tasks: 1.61556
Diversity Loss - Mean: -0.06872, Variance: 0.01633
Semantic Loss - Mean: 1.52360, Variance: 0.01677

Train Epoch: 57 
task: sign, mean loss: 0.61761, accuracy: 0.74457, avg. loss over tasks: 0.61761, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.05796, Variance: 0.01482
Semantic Loss - Mean: 0.60687, Variance: 0.00402

Test Epoch: 57 
task: sign, mean loss: 1.49299, accuracy: 0.63905, avg. loss over tasks: 1.49299
Diversity Loss - Mean: -0.08595, Variance: 0.01646
Semantic Loss - Mean: 1.45625, Variance: 0.01657

Train Epoch: 58 
task: sign, mean loss: 0.51478, accuracy: 0.83152, avg. loss over tasks: 0.51478, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.04988, Variance: 0.01485
Semantic Loss - Mean: 0.53808, Variance: 0.00397

Test Epoch: 58 
task: sign, mean loss: 1.62906, accuracy: 0.61538, avg. loss over tasks: 1.62906
Diversity Loss - Mean: -0.06655, Variance: 0.01653
Semantic Loss - Mean: 1.55426, Variance: 0.01644

Train Epoch: 59 
task: sign, mean loss: 0.51308, accuracy: 0.78804, avg. loss over tasks: 0.51308, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.03872, Variance: 0.01486
Semantic Loss - Mean: 0.53604, Variance: 0.00395

Test Epoch: 59 
task: sign, mean loss: 1.69336, accuracy: 0.59763, avg. loss over tasks: 1.69336
Diversity Loss - Mean: -0.06186, Variance: 0.01659
Semantic Loss - Mean: 1.60647, Variance: 0.01637

Train Epoch: 60 
task: sign, mean loss: 0.43418, accuracy: 0.86957, avg. loss over tasks: 0.43418, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.02282, Variance: 0.01484
Semantic Loss - Mean: 0.46015, Variance: 0.00393

Test Epoch: 60 
task: sign, mean loss: 1.95492, accuracy: 0.63905, avg. loss over tasks: 1.95492
Diversity Loss - Mean: -0.04813, Variance: 0.01662
Semantic Loss - Mean: 1.85987, Variance: 0.01620

Train Epoch: 61 
task: sign, mean loss: 0.43174, accuracy: 0.86413, avg. loss over tasks: 0.43174, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.03388, Variance: 0.01484
Semantic Loss - Mean: 0.48238, Variance: 0.00394

Test Epoch: 61 
task: sign, mean loss: 1.72167, accuracy: 0.63905, avg. loss over tasks: 1.72167
Diversity Loss - Mean: -0.05348, Variance: 0.01667
Semantic Loss - Mean: 1.56057, Variance: 0.01610

Train Epoch: 62 
task: sign, mean loss: 0.53365, accuracy: 0.81522, avg. loss over tasks: 0.53365, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.02820, Variance: 0.01483
Semantic Loss - Mean: 0.58841, Variance: 0.00395

Test Epoch: 62 
task: sign, mean loss: 1.55556, accuracy: 0.62722, avg. loss over tasks: 1.55556
Diversity Loss - Mean: -0.07058, Variance: 0.01674
Semantic Loss - Mean: 1.45025, Variance: 0.01593

Train Epoch: 63 
task: sign, mean loss: 0.39095, accuracy: 0.84239, avg. loss over tasks: 0.39095, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.03640, Variance: 0.01484
Semantic Loss - Mean: 0.40994, Variance: 0.00393

Test Epoch: 63 
task: sign, mean loss: 1.60128, accuracy: 0.63905, avg. loss over tasks: 1.60128
Diversity Loss - Mean: -0.07413, Variance: 0.01682
Semantic Loss - Mean: 1.52401, Variance: 0.01576

Train Epoch: 64 
task: sign, mean loss: 0.41061, accuracy: 0.85870, avg. loss over tasks: 0.41061, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.03774, Variance: 0.01484
Semantic Loss - Mean: 0.43866, Variance: 0.00393

Test Epoch: 64 
task: sign, mean loss: 1.74044, accuracy: 0.63314, avg. loss over tasks: 1.74044
Diversity Loss - Mean: -0.06550, Variance: 0.01688
Semantic Loss - Mean: 1.64783, Variance: 0.01561

Train Epoch: 65 
task: sign, mean loss: 0.40400, accuracy: 0.84239, avg. loss over tasks: 0.40400, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.02776, Variance: 0.01482
Semantic Loss - Mean: 0.45032, Variance: 0.00398

Test Epoch: 65 
task: sign, mean loss: 1.98655, accuracy: 0.65680, avg. loss over tasks: 1.98655
Diversity Loss - Mean: -0.04563, Variance: 0.01691
Semantic Loss - Mean: 1.89705, Variance: 0.01553

Train Epoch: 66 
task: sign, mean loss: 0.41098, accuracy: 0.85326, avg. loss over tasks: 0.41098, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.02996, Variance: 0.01481
Semantic Loss - Mean: 0.44326, Variance: 0.00398

Test Epoch: 66 
task: sign, mean loss: 2.49921, accuracy: 0.38462, avg. loss over tasks: 2.49921
Diversity Loss - Mean: -0.01385, Variance: 0.01695
Semantic Loss - Mean: 2.15872, Variance: 0.01547

Train Epoch: 67 
task: sign, mean loss: 0.37788, accuracy: 0.83696, avg. loss over tasks: 0.37788, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.02901, Variance: 0.01478
Semantic Loss - Mean: 0.41252, Variance: 0.00401

Test Epoch: 67 
task: sign, mean loss: 2.06944, accuracy: 0.62130, avg. loss over tasks: 2.06944
Diversity Loss - Mean: -0.03793, Variance: 0.01695
Semantic Loss - Mean: 1.90901, Variance: 0.01539

Train Epoch: 68 
task: sign, mean loss: 0.37014, accuracy: 0.86957, avg. loss over tasks: 0.37014, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.02337, Variance: 0.01474
Semantic Loss - Mean: 0.38550, Variance: 0.00401

Test Epoch: 68 
task: sign, mean loss: 2.13841, accuracy: 0.56213, avg. loss over tasks: 2.13841
Diversity Loss - Mean: -0.01005, Variance: 0.01689
Semantic Loss - Mean: 1.94452, Variance: 0.01545

Train Epoch: 69 
task: sign, mean loss: 0.35094, accuracy: 0.84783, avg. loss over tasks: 0.35094, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.02926, Variance: 0.01472
Semantic Loss - Mean: 0.39214, Variance: 0.00402

Test Epoch: 69 
task: sign, mean loss: 1.98143, accuracy: 0.59172, avg. loss over tasks: 1.98143
Diversity Loss - Mean: -0.04011, Variance: 0.01687
Semantic Loss - Mean: 1.82724, Variance: 0.01535

Train Epoch: 70 
task: sign, mean loss: 0.30694, accuracy: 0.89674, avg. loss over tasks: 0.30694, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.02940, Variance: 0.01469
Semantic Loss - Mean: 0.35729, Variance: 0.00403

Test Epoch: 70 
task: sign, mean loss: 2.02547, accuracy: 0.57988, avg. loss over tasks: 2.02547
Diversity Loss - Mean: -0.04326, Variance: 0.01686
Semantic Loss - Mean: 1.84592, Variance: 0.01533

Train Epoch: 71 
task: sign, mean loss: 0.32142, accuracy: 0.89130, avg. loss over tasks: 0.32142, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.02175, Variance: 0.01465
Semantic Loss - Mean: 0.34077, Variance: 0.00404

Test Epoch: 71 
task: sign, mean loss: 2.28221, accuracy: 0.55621, avg. loss over tasks: 2.28221
Diversity Loss - Mean: -0.02912, Variance: 0.01681
Semantic Loss - Mean: 1.99550, Variance: 0.01533

Train Epoch: 72 
task: sign, mean loss: 0.31053, accuracy: 0.89130, avg. loss over tasks: 0.31053, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.01205, Variance: 0.01461
Semantic Loss - Mean: 0.36905, Variance: 0.00404

Test Epoch: 72 
task: sign, mean loss: 2.06200, accuracy: 0.61538, avg. loss over tasks: 2.06200
Diversity Loss - Mean: -0.04760, Variance: 0.01682
Semantic Loss - Mean: 1.88867, Variance: 0.01546

Train Epoch: 73 
task: sign, mean loss: 0.32484, accuracy: 0.89130, avg. loss over tasks: 0.32484, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.01684, Variance: 0.01458
Semantic Loss - Mean: 0.36577, Variance: 0.00406

Test Epoch: 73 
task: sign, mean loss: 2.29195, accuracy: 0.50888, avg. loss over tasks: 2.29195
Diversity Loss - Mean: -0.02592, Variance: 0.01679
Semantic Loss - Mean: 2.00516, Variance: 0.01549

Train Epoch: 74 
task: sign, mean loss: 0.25292, accuracy: 0.89130, avg. loss over tasks: 0.25292, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.00691, Variance: 0.01452
Semantic Loss - Mean: 0.29694, Variance: 0.00407

Test Epoch: 74 
task: sign, mean loss: 2.54990, accuracy: 0.52663, avg. loss over tasks: 2.54990
Diversity Loss - Mean: -0.02498, Variance: 0.01675
Semantic Loss - Mean: 2.21808, Variance: 0.01544

Train Epoch: 75 
task: sign, mean loss: 0.24691, accuracy: 0.90761, avg. loss over tasks: 0.24691, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.00811, Variance: 0.01448
Semantic Loss - Mean: 0.29621, Variance: 0.00407

Test Epoch: 75 
task: sign, mean loss: 2.04872, accuracy: 0.66272, avg. loss over tasks: 2.04872
Diversity Loss - Mean: -0.04869, Variance: 0.01675
Semantic Loss - Mean: 1.88400, Variance: 0.01546

Train Epoch: 76 
task: sign, mean loss: 0.32162, accuracy: 0.88043, avg. loss over tasks: 0.32162, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.01487, Variance: 0.01444
Semantic Loss - Mean: 0.35894, Variance: 0.00411

Test Epoch: 76 
task: sign, mean loss: 2.06332, accuracy: 0.31361, avg. loss over tasks: 2.06332
Diversity Loss - Mean: 0.03901, Variance: 0.01669
Semantic Loss - Mean: 1.74401, Variance: 0.01553

Train Epoch: 77 
task: sign, mean loss: 0.31664, accuracy: 0.89130, avg. loss over tasks: 0.31664, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.01387, Variance: 0.01440
Semantic Loss - Mean: 0.36316, Variance: 0.00412

Test Epoch: 77 
task: sign, mean loss: 1.96499, accuracy: 0.55030, avg. loss over tasks: 1.96499
Diversity Loss - Mean: -0.03367, Variance: 0.01667
Semantic Loss - Mean: 1.75748, Variance: 0.01556

Train Epoch: 78 
task: sign, mean loss: 0.32950, accuracy: 0.85870, avg. loss over tasks: 0.32950, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.01430, Variance: 0.01436
Semantic Loss - Mean: 0.38109, Variance: 0.00415

Test Epoch: 78 
task: sign, mean loss: 2.23836, accuracy: 0.60947, avg. loss over tasks: 2.23836
Diversity Loss - Mean: -0.02796, Variance: 0.01664
Semantic Loss - Mean: 1.94035, Variance: 0.01549

Train Epoch: 79 
task: sign, mean loss: 0.23695, accuracy: 0.90217, avg. loss over tasks: 0.23695, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.01768, Variance: 0.01432
Semantic Loss - Mean: 0.26802, Variance: 0.00413

Test Epoch: 79 
task: sign, mean loss: 2.08013, accuracy: 0.65089, avg. loss over tasks: 2.08013
Diversity Loss - Mean: -0.03504, Variance: 0.01663
Semantic Loss - Mean: 1.87946, Variance: 0.01549

Train Epoch: 80 
task: sign, mean loss: 0.15312, accuracy: 0.94022, avg. loss over tasks: 0.15312, lr: 0.00015015
Diversity Loss - Mean: -0.00526, Variance: 0.01427
Semantic Loss - Mean: 0.17765, Variance: 0.00412

Test Epoch: 80 
task: sign, mean loss: 2.32015, accuracy: 0.60355, avg. loss over tasks: 2.32015
Diversity Loss - Mean: -0.03889, Variance: 0.01660
Semantic Loss - Mean: 2.03771, Variance: 0.01545

Train Epoch: 81 
task: sign, mean loss: 0.12648, accuracy: 0.94565, avg. loss over tasks: 0.12648, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.00631, Variance: 0.01423
Semantic Loss - Mean: 0.16168, Variance: 0.00414

Test Epoch: 81 
task: sign, mean loss: 2.48837, accuracy: 0.63905, avg. loss over tasks: 2.48837
Diversity Loss - Mean: -0.03733, Variance: 0.01658
Semantic Loss - Mean: 2.17659, Variance: 0.01543

Train Epoch: 82 
task: sign, mean loss: 0.19644, accuracy: 0.94022, avg. loss over tasks: 0.19644, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.00808, Variance: 0.01418
Semantic Loss - Mean: 0.20893, Variance: 0.00422

Test Epoch: 82 
task: sign, mean loss: 2.37650, accuracy: 0.65089, avg. loss over tasks: 2.37650
Diversity Loss - Mean: -0.04189, Variance: 0.01656
Semantic Loss - Mean: 2.12046, Variance: 0.01541

Train Epoch: 83 
task: sign, mean loss: 0.23904, accuracy: 0.92391, avg. loss over tasks: 0.23904, lr: 0.0001400697395090358
Diversity Loss - Mean: 0.00839, Variance: 0.01412
Semantic Loss - Mean: 0.25621, Variance: 0.00427

Test Epoch: 83 
task: sign, mean loss: 2.14017, accuracy: 0.60355, avg. loss over tasks: 2.14017
Diversity Loss - Mean: -0.05794, Variance: 0.01654
Semantic Loss - Mean: 1.87680, Variance: 0.01544

Train Epoch: 84 
task: sign, mean loss: 0.26735, accuracy: 0.92935, avg. loss over tasks: 0.26735, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.00291, Variance: 0.01406
Semantic Loss - Mean: 0.30941, Variance: 0.00432

Test Epoch: 84 
task: sign, mean loss: 1.86783, accuracy: 0.61538, avg. loss over tasks: 1.86783
Diversity Loss - Mean: -0.06377, Variance: 0.01652
Semantic Loss - Mean: 1.66069, Variance: 0.01553

Train Epoch: 85 
task: sign, mean loss: 0.19267, accuracy: 0.90217, avg. loss over tasks: 0.19267, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.01721, Variance: 0.01403
Semantic Loss - Mean: 0.23225, Variance: 0.00431

Test Epoch: 85 
task: sign, mean loss: 2.39582, accuracy: 0.59763, avg. loss over tasks: 2.39582
Diversity Loss - Mean: -0.03089, Variance: 0.01649
Semantic Loss - Mean: 2.11497, Variance: 0.01553

Train Epoch: 86 
task: sign, mean loss: 0.24178, accuracy: 0.91848, avg. loss over tasks: 0.24178, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.01121, Variance: 0.01398
Semantic Loss - Mean: 0.27834, Variance: 0.00433

Test Epoch: 86 
task: sign, mean loss: 2.43647, accuracy: 0.60355, avg. loss over tasks: 2.43647
Diversity Loss - Mean: -0.03782, Variance: 0.01647
Semantic Loss - Mean: 2.17916, Variance: 0.01558

Train Epoch: 87 
task: sign, mean loss: 0.22730, accuracy: 0.92391, avg. loss over tasks: 0.22730, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.02260, Variance: 0.01395
Semantic Loss - Mean: 0.26101, Variance: 0.00438

Test Epoch: 87 
task: sign, mean loss: 2.27848, accuracy: 0.61538, avg. loss over tasks: 2.27848
Diversity Loss - Mean: -0.05857, Variance: 0.01647
Semantic Loss - Mean: 2.06785, Variance: 0.01554

Train Epoch: 88 
task: sign, mean loss: 0.18036, accuracy: 0.94022, avg. loss over tasks: 0.18036, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.02202, Variance: 0.01392
Semantic Loss - Mean: 0.20316, Variance: 0.00437

Test Epoch: 88 
task: sign, mean loss: 1.95942, accuracy: 0.61538, avg. loss over tasks: 1.95942
Diversity Loss - Mean: -0.05669, Variance: 0.01647
Semantic Loss - Mean: 1.75760, Variance: 0.01549

Train Epoch: 89 
task: sign, mean loss: 0.18709, accuracy: 0.92391, avg. loss over tasks: 0.18709, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.02204, Variance: 0.01388
Semantic Loss - Mean: 0.20721, Variance: 0.00437

Test Epoch: 89 
task: sign, mean loss: 2.06448, accuracy: 0.63314, avg. loss over tasks: 2.06448
Diversity Loss - Mean: -0.06117, Variance: 0.01648
Semantic Loss - Mean: 1.88474, Variance: 0.01545

Train Epoch: 90 
task: sign, mean loss: 0.13890, accuracy: 0.95652, avg. loss over tasks: 0.13890, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.01710, Variance: 0.01384
Semantic Loss - Mean: 0.18302, Variance: 0.00438

Test Epoch: 90 
task: sign, mean loss: 2.17081, accuracy: 0.63905, avg. loss over tasks: 2.17081
Diversity Loss - Mean: -0.06476, Variance: 0.01648
Semantic Loss - Mean: 1.95284, Variance: 0.01557

Train Epoch: 91 
task: sign, mean loss: 0.13210, accuracy: 0.94565, avg. loss over tasks: 0.13210, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.01867, Variance: 0.01381
Semantic Loss - Mean: 0.17548, Variance: 0.00437

Test Epoch: 91 
task: sign, mean loss: 2.53432, accuracy: 0.64497, avg. loss over tasks: 2.53432
Diversity Loss - Mean: -0.06504, Variance: 0.01648
Semantic Loss - Mean: 2.24292, Variance: 0.01557

Train Epoch: 92 
task: sign, mean loss: 0.13703, accuracy: 0.94022, avg. loss over tasks: 0.13703, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.01373, Variance: 0.01377
Semantic Loss - Mean: 0.16057, Variance: 0.00436

Test Epoch: 92 
task: sign, mean loss: 2.61890, accuracy: 0.63905, avg. loss over tasks: 2.61890
Diversity Loss - Mean: -0.06392, Variance: 0.01647
Semantic Loss - Mean: 2.31356, Variance: 0.01554

Train Epoch: 93 
task: sign, mean loss: 0.15786, accuracy: 0.96739, avg. loss over tasks: 0.15786, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.01204, Variance: 0.01373
Semantic Loss - Mean: 0.18763, Variance: 0.00437

Test Epoch: 93 
task: sign, mean loss: 2.56210, accuracy: 0.65089, avg. loss over tasks: 2.56210
Diversity Loss - Mean: -0.06328, Variance: 0.01646
Semantic Loss - Mean: 2.30921, Variance: 0.01548

Train Epoch: 94 
task: sign, mean loss: 0.07802, accuracy: 0.97826, avg. loss over tasks: 0.07802, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.01656, Variance: 0.01369
Semantic Loss - Mean: 0.10772, Variance: 0.00435

Test Epoch: 94 
task: sign, mean loss: 2.60618, accuracy: 0.60947, avg. loss over tasks: 2.60618
Diversity Loss - Mean: -0.06199, Variance: 0.01645
Semantic Loss - Mean: 2.24777, Variance: 0.01540

Train Epoch: 95 
task: sign, mean loss: 0.11262, accuracy: 0.96196, avg. loss over tasks: 0.11262, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.01469, Variance: 0.01365
Semantic Loss - Mean: 0.15305, Variance: 0.00435

Test Epoch: 95 
task: sign, mean loss: 2.56751, accuracy: 0.65680, avg. loss over tasks: 2.56751
Diversity Loss - Mean: -0.07076, Variance: 0.01644
Semantic Loss - Mean: 2.20782, Variance: 0.01539

Train Epoch: 96 
task: sign, mean loss: 0.10662, accuracy: 0.95652, avg. loss over tasks: 0.10662, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.01600, Variance: 0.01362
Semantic Loss - Mean: 0.13748, Variance: 0.00433

Test Epoch: 96 
task: sign, mean loss: 2.45367, accuracy: 0.63905, avg. loss over tasks: 2.45367
Diversity Loss - Mean: -0.07444, Variance: 0.01643
Semantic Loss - Mean: 2.13090, Variance: 0.01536

Train Epoch: 97 
task: sign, mean loss: 0.08981, accuracy: 0.97826, avg. loss over tasks: 0.08981, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.01738, Variance: 0.01359
Semantic Loss - Mean: 0.12336, Variance: 0.00432

Test Epoch: 97 
task: sign, mean loss: 2.52419, accuracy: 0.65089, avg. loss over tasks: 2.52419
Diversity Loss - Mean: -0.07030, Variance: 0.01642
Semantic Loss - Mean: 2.21225, Variance: 0.01531

Train Epoch: 98 
task: sign, mean loss: 0.07649, accuracy: 0.98370, avg. loss over tasks: 0.07649, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.01659, Variance: 0.01355
Semantic Loss - Mean: 0.11031, Variance: 0.00432

Test Epoch: 98 
task: sign, mean loss: 2.44835, accuracy: 0.62130, avg. loss over tasks: 2.44835
Diversity Loss - Mean: -0.07056, Variance: 0.01641
Semantic Loss - Mean: 2.14517, Variance: 0.01528

Train Epoch: 99 
task: sign, mean loss: 0.10154, accuracy: 0.96196, avg. loss over tasks: 0.10154, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.01735, Variance: 0.01352
Semantic Loss - Mean: 0.12743, Variance: 0.00432

Test Epoch: 99 
task: sign, mean loss: 2.50024, accuracy: 0.62722, avg. loss over tasks: 2.50024
Diversity Loss - Mean: -0.06649, Variance: 0.01640
Semantic Loss - Mean: 2.15137, Variance: 0.01522

Train Epoch: 100 
task: sign, mean loss: 0.10771, accuracy: 0.96196, avg. loss over tasks: 0.10771, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.01578, Variance: 0.01348
Semantic Loss - Mean: 0.14977, Variance: 0.00435

Test Epoch: 100 
task: sign, mean loss: 2.81341, accuracy: 0.61538, avg. loss over tasks: 2.81341
Diversity Loss - Mean: -0.06192, Variance: 0.01638
Semantic Loss - Mean: 2.37377, Variance: 0.01518

Train Epoch: 101 
task: sign, mean loss: 0.09209, accuracy: 0.96739, avg. loss over tasks: 0.09209, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.01665, Variance: 0.01345
Semantic Loss - Mean: 0.10745, Variance: 0.00432

Test Epoch: 101 
task: sign, mean loss: 2.55704, accuracy: 0.59763, avg. loss over tasks: 2.55704
Diversity Loss - Mean: -0.06175, Variance: 0.01636
Semantic Loss - Mean: 2.16972, Variance: 0.01513

Train Epoch: 102 
task: sign, mean loss: 0.03726, accuracy: 0.99457, avg. loss over tasks: 0.03726, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.00702, Variance: 0.01341
Semantic Loss - Mean: 0.06579, Variance: 0.00431

Test Epoch: 102 
task: sign, mean loss: 2.62465, accuracy: 0.59763, avg. loss over tasks: 2.62465
Diversity Loss - Mean: -0.06120, Variance: 0.01634
Semantic Loss - Mean: 2.23816, Variance: 0.01510

Train Epoch: 103 
task: sign, mean loss: 0.09859, accuracy: 0.95109, avg. loss over tasks: 0.09859, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.00993, Variance: 0.01337
Semantic Loss - Mean: 0.13561, Variance: 0.00430

Test Epoch: 103 
task: sign, mean loss: 2.55191, accuracy: 0.64497, avg. loss over tasks: 2.55191
Diversity Loss - Mean: -0.05921, Variance: 0.01634
Semantic Loss - Mean: 2.25857, Variance: 0.01504

Train Epoch: 104 
task: sign, mean loss: 0.04938, accuracy: 0.98913, avg. loss over tasks: 0.04938, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.00572, Variance: 0.01333
Semantic Loss - Mean: 0.07601, Variance: 0.00430

Test Epoch: 104 
task: sign, mean loss: 2.64392, accuracy: 0.64497, avg. loss over tasks: 2.64392
Diversity Loss - Mean: -0.05098, Variance: 0.01633
Semantic Loss - Mean: 2.32400, Variance: 0.01503

Train Epoch: 105 
task: sign, mean loss: 0.09907, accuracy: 0.96196, avg. loss over tasks: 0.09907, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.00634, Variance: 0.01329
Semantic Loss - Mean: 0.12598, Variance: 0.00429

Test Epoch: 105 
task: sign, mean loss: 2.75859, accuracy: 0.63314, avg. loss over tasks: 2.75859
Diversity Loss - Mean: -0.06390, Variance: 0.01632
Semantic Loss - Mean: 2.42257, Variance: 0.01505

Train Epoch: 106 
task: sign, mean loss: 0.09586, accuracy: 0.97283, avg. loss over tasks: 0.09586, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.01085, Variance: 0.01326
Semantic Loss - Mean: 0.11228, Variance: 0.00427

Test Epoch: 106 
task: sign, mean loss: 2.87272, accuracy: 0.60947, avg. loss over tasks: 2.87272
Diversity Loss - Mean: -0.06085, Variance: 0.01631
Semantic Loss - Mean: 2.48420, Variance: 0.01503

Train Epoch: 107 
task: sign, mean loss: 0.03115, accuracy: 0.99457, avg. loss over tasks: 0.03115, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.00648, Variance: 0.01322
Semantic Loss - Mean: 0.06249, Variance: 0.00425

Test Epoch: 107 
task: sign, mean loss: 2.80470, accuracy: 0.63314, avg. loss over tasks: 2.80470
Diversity Loss - Mean: -0.06227, Variance: 0.01629
Semantic Loss - Mean: 2.42650, Variance: 0.01498

Train Epoch: 108 
task: sign, mean loss: 0.08767, accuracy: 0.97826, avg. loss over tasks: 0.08767, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.00872, Variance: 0.01318
Semantic Loss - Mean: 0.11223, Variance: 0.00425

Test Epoch: 108 
task: sign, mean loss: 2.75134, accuracy: 0.62130, avg. loss over tasks: 2.75134
Diversity Loss - Mean: -0.06677, Variance: 0.01627
Semantic Loss - Mean: 2.32926, Variance: 0.01498

Train Epoch: 109 
task: sign, mean loss: 0.05760, accuracy: 0.98370, avg. loss over tasks: 0.05760, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.01233, Variance: 0.01314
Semantic Loss - Mean: 0.08394, Variance: 0.00424

Test Epoch: 109 
task: sign, mean loss: 2.72464, accuracy: 0.59172, avg. loss over tasks: 2.72464
Diversity Loss - Mean: -0.06375, Variance: 0.01625
Semantic Loss - Mean: 2.32092, Variance: 0.01497

Train Epoch: 110 
task: sign, mean loss: 0.04637, accuracy: 0.98370, avg. loss over tasks: 0.04637, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.00830, Variance: 0.01310
Semantic Loss - Mean: 0.07026, Variance: 0.00421

Test Epoch: 110 
task: sign, mean loss: 2.71691, accuracy: 0.59763, avg. loss over tasks: 2.71691
Diversity Loss - Mean: -0.06254, Variance: 0.01624
Semantic Loss - Mean: 2.32984, Variance: 0.01494

Train Epoch: 111 
task: sign, mean loss: 0.06042, accuracy: 0.97826, avg. loss over tasks: 0.06042, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.01532, Variance: 0.01307
Semantic Loss - Mean: 0.09232, Variance: 0.00422

Test Epoch: 111 
task: sign, mean loss: 2.84264, accuracy: 0.62130, avg. loss over tasks: 2.84264
Diversity Loss - Mean: -0.06686, Variance: 0.01622
Semantic Loss - Mean: 2.43393, Variance: 0.01491

Train Epoch: 112 
task: sign, mean loss: 0.01828, accuracy: 1.00000, avg. loss over tasks: 0.01828, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.01237, Variance: 0.01304
Semantic Loss - Mean: 0.04358, Variance: 0.00422

Test Epoch: 112 
task: sign, mean loss: 2.95542, accuracy: 0.60355, avg. loss over tasks: 2.95542
Diversity Loss - Mean: -0.06383, Variance: 0.01620
Semantic Loss - Mean: 2.48390, Variance: 0.01492

Train Epoch: 113 
task: sign, mean loss: 0.05880, accuracy: 0.97826, avg. loss over tasks: 0.05880, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.01262, Variance: 0.01300
Semantic Loss - Mean: 0.07861, Variance: 0.00419

Test Epoch: 113 
task: sign, mean loss: 2.78671, accuracy: 0.59763, avg. loss over tasks: 2.78671
Diversity Loss - Mean: -0.06704, Variance: 0.01618
Semantic Loss - Mean: 2.33926, Variance: 0.01496

Train Epoch: 114 
task: sign, mean loss: 0.09917, accuracy: 0.96739, avg. loss over tasks: 0.09917, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.01826, Variance: 0.01297
Semantic Loss - Mean: 0.13003, Variance: 0.00422

Test Epoch: 114 
task: sign, mean loss: 2.84695, accuracy: 0.59763, avg. loss over tasks: 2.84695
Diversity Loss - Mean: -0.06674, Variance: 0.01617
Semantic Loss - Mean: 2.36906, Variance: 0.01501

Train Epoch: 115 
task: sign, mean loss: 0.10390, accuracy: 0.96739, avg. loss over tasks: 0.10390, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.01157, Variance: 0.01293
Semantic Loss - Mean: 0.12257, Variance: 0.00421

Test Epoch: 115 
task: sign, mean loss: 2.64000, accuracy: 0.60355, avg. loss over tasks: 2.64000
Diversity Loss - Mean: -0.06867, Variance: 0.01615
Semantic Loss - Mean: 2.24772, Variance: 0.01505

Train Epoch: 116 
task: sign, mean loss: 0.06152, accuracy: 0.97826, avg. loss over tasks: 0.06152, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.01129, Variance: 0.01290
Semantic Loss - Mean: 0.08939, Variance: 0.00420

Test Epoch: 116 
task: sign, mean loss: 2.47049, accuracy: 0.63314, avg. loss over tasks: 2.47049
Diversity Loss - Mean: -0.06776, Variance: 0.01614
Semantic Loss - Mean: 2.15723, Variance: 0.01503

Train Epoch: 117 
task: sign, mean loss: 0.06165, accuracy: 0.98370, avg. loss over tasks: 0.06165, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.01489, Variance: 0.01287
Semantic Loss - Mean: 0.07974, Variance: 0.00421

Test Epoch: 117 
task: sign, mean loss: 2.58933, accuracy: 0.63314, avg. loss over tasks: 2.58933
Diversity Loss - Mean: -0.06990, Variance: 0.01613
Semantic Loss - Mean: 2.25184, Variance: 0.01495

Train Epoch: 118 
task: sign, mean loss: 0.06141, accuracy: 0.97826, avg. loss over tasks: 0.06141, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.01716, Variance: 0.01284
Semantic Loss - Mean: 0.09199, Variance: 0.00420

Test Epoch: 118 
task: sign, mean loss: 2.55309, accuracy: 0.62722, avg. loss over tasks: 2.55309
Diversity Loss - Mean: -0.07078, Variance: 0.01611
Semantic Loss - Mean: 2.19034, Variance: 0.01490

Train Epoch: 119 
task: sign, mean loss: 0.02432, accuracy: 0.99457, avg. loss over tasks: 0.02432, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.01207, Variance: 0.01280
Semantic Loss - Mean: 0.04377, Variance: 0.00417

Test Epoch: 119 
task: sign, mean loss: 2.64670, accuracy: 0.62722, avg. loss over tasks: 2.64670
Diversity Loss - Mean: -0.06796, Variance: 0.01610
Semantic Loss - Mean: 2.27531, Variance: 0.01487

Train Epoch: 120 
task: sign, mean loss: 0.06139, accuracy: 0.97826, avg. loss over tasks: 0.06139, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.02110, Variance: 0.01278
Semantic Loss - Mean: 0.09646, Variance: 0.00418

Test Epoch: 120 
task: sign, mean loss: 2.65138, accuracy: 0.61538, avg. loss over tasks: 2.65138
Diversity Loss - Mean: -0.06640, Variance: 0.01608
Semantic Loss - Mean: 2.28415, Variance: 0.01482

Train Epoch: 121 
task: sign, mean loss: 0.04203, accuracy: 0.98370, avg. loss over tasks: 0.04203, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.01691, Variance: 0.01274
Semantic Loss - Mean: 0.07369, Variance: 0.00417

Test Epoch: 121 
task: sign, mean loss: 2.77198, accuracy: 0.61538, avg. loss over tasks: 2.77198
Diversity Loss - Mean: -0.06180, Variance: 0.01606
Semantic Loss - Mean: 2.36939, Variance: 0.01478

Train Epoch: 122 
task: sign, mean loss: 0.07258, accuracy: 0.96739, avg. loss over tasks: 0.07258, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.02308, Variance: 0.01272
Semantic Loss - Mean: 0.09574, Variance: 0.00415

Test Epoch: 122 
task: sign, mean loss: 2.63477, accuracy: 0.61538, avg. loss over tasks: 2.63477
Diversity Loss - Mean: -0.06441, Variance: 0.01604
Semantic Loss - Mean: 2.25097, Variance: 0.01478

Train Epoch: 123 
task: sign, mean loss: 0.03815, accuracy: 0.99457, avg. loss over tasks: 0.03815, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.01914, Variance: 0.01269
Semantic Loss - Mean: 0.07422, Variance: 0.00415

Test Epoch: 123 
task: sign, mean loss: 2.60324, accuracy: 0.62130, avg. loss over tasks: 2.60324
Diversity Loss - Mean: -0.06281, Variance: 0.01603
Semantic Loss - Mean: 2.22084, Variance: 0.01478

Train Epoch: 124 
task: sign, mean loss: 0.09177, accuracy: 0.97283, avg. loss over tasks: 0.09177, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.01553, Variance: 0.01266
Semantic Loss - Mean: 0.12242, Variance: 0.00415

Test Epoch: 124 
task: sign, mean loss: 2.66798, accuracy: 0.63314, avg. loss over tasks: 2.66798
Diversity Loss - Mean: -0.07030, Variance: 0.01601
Semantic Loss - Mean: 2.26313, Variance: 0.01481

Train Epoch: 125 
task: sign, mean loss: 0.03391, accuracy: 0.98913, avg. loss over tasks: 0.03391, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.01555, Variance: 0.01263
Semantic Loss - Mean: 0.05219, Variance: 0.00413

Test Epoch: 125 
task: sign, mean loss: 2.71775, accuracy: 0.62130, avg. loss over tasks: 2.71775
Diversity Loss - Mean: -0.06687, Variance: 0.01600
Semantic Loss - Mean: 2.32684, Variance: 0.01484

Train Epoch: 126 
task: sign, mean loss: 0.05205, accuracy: 0.98913, avg. loss over tasks: 0.05205, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.01369, Variance: 0.01260
Semantic Loss - Mean: 0.06589, Variance: 0.00410

Test Epoch: 126 
task: sign, mean loss: 2.73227, accuracy: 0.62722, avg. loss over tasks: 2.73227
Diversity Loss - Mean: -0.06757, Variance: 0.01598
Semantic Loss - Mean: 2.33160, Variance: 0.01485

Train Epoch: 127 
task: sign, mean loss: 0.06772, accuracy: 0.97283, avg. loss over tasks: 0.06772, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.01561, Variance: 0.01257
Semantic Loss - Mean: 0.08393, Variance: 0.00409

Test Epoch: 127 
task: sign, mean loss: 2.68874, accuracy: 0.62130, avg. loss over tasks: 2.68874
Diversity Loss - Mean: -0.06890, Variance: 0.01597
Semantic Loss - Mean: 2.29494, Variance: 0.01486

Train Epoch: 128 
task: sign, mean loss: 0.06434, accuracy: 0.97826, avg. loss over tasks: 0.06434, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.01440, Variance: 0.01255
Semantic Loss - Mean: 0.08437, Variance: 0.00409

Test Epoch: 128 
task: sign, mean loss: 2.78112, accuracy: 0.62130, avg. loss over tasks: 2.78112
Diversity Loss - Mean: -0.06672, Variance: 0.01596
Semantic Loss - Mean: 2.36288, Variance: 0.01485

Train Epoch: 129 
task: sign, mean loss: 0.06101, accuracy: 0.96196, avg. loss over tasks: 0.06101, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.02087, Variance: 0.01252
Semantic Loss - Mean: 0.07193, Variance: 0.00407

Test Epoch: 129 
task: sign, mean loss: 2.83214, accuracy: 0.63314, avg. loss over tasks: 2.83214
Diversity Loss - Mean: -0.06737, Variance: 0.01594
Semantic Loss - Mean: 2.44079, Variance: 0.01482

Train Epoch: 130 
task: sign, mean loss: 0.07518, accuracy: 0.97283, avg. loss over tasks: 0.07518, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.01331, Variance: 0.01249
Semantic Loss - Mean: 0.09627, Variance: 0.00406

Test Epoch: 130 
task: sign, mean loss: 2.74526, accuracy: 0.63905, avg. loss over tasks: 2.74526
Diversity Loss - Mean: -0.06888, Variance: 0.01593
Semantic Loss - Mean: 2.35892, Variance: 0.01478

Train Epoch: 131 
task: sign, mean loss: 0.04986, accuracy: 0.98913, avg. loss over tasks: 0.04986, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.01438, Variance: 0.01247
Semantic Loss - Mean: 0.06262, Variance: 0.00406

Test Epoch: 131 
task: sign, mean loss: 2.75624, accuracy: 0.62722, avg. loss over tasks: 2.75624
Diversity Loss - Mean: -0.06677, Variance: 0.01591
Semantic Loss - Mean: 2.36080, Variance: 0.01474

Train Epoch: 132 
task: sign, mean loss: 0.02136, accuracy: 0.99457, avg. loss over tasks: 0.02136, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.01352, Variance: 0.01244
Semantic Loss - Mean: 0.03563, Variance: 0.00403

Test Epoch: 132 
task: sign, mean loss: 2.74941, accuracy: 0.63314, avg. loss over tasks: 2.74941
Diversity Loss - Mean: -0.06871, Variance: 0.01590
Semantic Loss - Mean: 2.34555, Variance: 0.01470

Train Epoch: 133 
task: sign, mean loss: 0.03035, accuracy: 0.98370, avg. loss over tasks: 0.03035, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.01406, Variance: 0.01241
Semantic Loss - Mean: 0.05089, Variance: 0.00402

Test Epoch: 133 
task: sign, mean loss: 2.74311, accuracy: 0.63314, avg. loss over tasks: 2.74311
Diversity Loss - Mean: -0.07015, Variance: 0.01588
Semantic Loss - Mean: 2.34630, Variance: 0.01466

Train Epoch: 134 
task: sign, mean loss: 0.02565, accuracy: 0.99457, avg. loss over tasks: 0.02565, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.01463, Variance: 0.01239
Semantic Loss - Mean: 0.04391, Variance: 0.00400

Test Epoch: 134 
task: sign, mean loss: 2.73343, accuracy: 0.62722, avg. loss over tasks: 2.73343
Diversity Loss - Mean: -0.06862, Variance: 0.01587
Semantic Loss - Mean: 2.34400, Variance: 0.01462

Train Epoch: 135 
task: sign, mean loss: 0.07563, accuracy: 0.98913, avg. loss over tasks: 0.07563, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.01861, Variance: 0.01236
Semantic Loss - Mean: 0.08264, Variance: 0.00401

Test Epoch: 135 
task: sign, mean loss: 2.76200, accuracy: 0.62722, avg. loss over tasks: 2.76200
Diversity Loss - Mean: -0.06710, Variance: 0.01586
Semantic Loss - Mean: 2.36478, Variance: 0.01458

Train Epoch: 136 
task: sign, mean loss: 0.01318, accuracy: 1.00000, avg. loss over tasks: 0.01318, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.01453, Variance: 0.01234
Semantic Loss - Mean: 0.04147, Variance: 0.00402

Test Epoch: 136 
task: sign, mean loss: 2.65321, accuracy: 0.62722, avg. loss over tasks: 2.65321
Diversity Loss - Mean: -0.07043, Variance: 0.01585
Semantic Loss - Mean: 2.26873, Variance: 0.01455

Train Epoch: 137 
task: sign, mean loss: 0.03034, accuracy: 0.98370, avg. loss over tasks: 0.03034, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.01693, Variance: 0.01231
Semantic Loss - Mean: 0.04528, Variance: 0.00400

Test Epoch: 137 
task: sign, mean loss: 2.67396, accuracy: 0.62130, avg. loss over tasks: 2.67396
Diversity Loss - Mean: -0.06786, Variance: 0.01584
Semantic Loss - Mean: 2.27227, Variance: 0.01451

Train Epoch: 138 
task: sign, mean loss: 0.04413, accuracy: 0.98370, avg. loss over tasks: 0.04413, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.01842, Variance: 0.01229
Semantic Loss - Mean: 0.06672, Variance: 0.00399

Test Epoch: 138 
task: sign, mean loss: 2.67522, accuracy: 0.62722, avg. loss over tasks: 2.67522
Diversity Loss - Mean: -0.07007, Variance: 0.01582
Semantic Loss - Mean: 2.27782, Variance: 0.01448

Train Epoch: 139 
task: sign, mean loss: 0.02699, accuracy: 0.99457, avg. loss over tasks: 0.02699, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.01959, Variance: 0.01227
Semantic Loss - Mean: 0.05304, Variance: 0.00398

Test Epoch: 139 
task: sign, mean loss: 2.71519, accuracy: 0.62722, avg. loss over tasks: 2.71519
Diversity Loss - Mean: -0.06844, Variance: 0.01581
Semantic Loss - Mean: 2.30481, Variance: 0.01444

Train Epoch: 140 
task: sign, mean loss: 0.08072, accuracy: 0.96196, avg. loss over tasks: 0.08072, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.01819, Variance: 0.01225
Semantic Loss - Mean: 0.09476, Variance: 0.00399

Test Epoch: 140 
task: sign, mean loss: 2.68777, accuracy: 0.63314, avg. loss over tasks: 2.68777
Diversity Loss - Mean: -0.06977, Variance: 0.01580
Semantic Loss - Mean: 2.29072, Variance: 0.01441

Train Epoch: 141 
task: sign, mean loss: 0.07718, accuracy: 0.97826, avg. loss over tasks: 0.07718, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.01608, Variance: 0.01222
Semantic Loss - Mean: 0.10139, Variance: 0.00403

Test Epoch: 141 
task: sign, mean loss: 2.71521, accuracy: 0.62722, avg. loss over tasks: 2.71521
Diversity Loss - Mean: -0.07115, Variance: 0.01579
Semantic Loss - Mean: 2.31602, Variance: 0.01438

Train Epoch: 142 
task: sign, mean loss: 0.02734, accuracy: 1.00000, avg. loss over tasks: 0.02734, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.01659, Variance: 0.01220
Semantic Loss - Mean: 0.07367, Variance: 0.00407

Test Epoch: 142 
task: sign, mean loss: 2.75474, accuracy: 0.63905, avg. loss over tasks: 2.75474
Diversity Loss - Mean: -0.07077, Variance: 0.01578
Semantic Loss - Mean: 2.35035, Variance: 0.01434

Train Epoch: 143 
task: sign, mean loss: 0.02123, accuracy: 1.00000, avg. loss over tasks: 0.02123, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.01489, Variance: 0.01218
Semantic Loss - Mean: 0.04418, Variance: 0.00406

Test Epoch: 143 
task: sign, mean loss: 2.71277, accuracy: 0.62722, avg. loss over tasks: 2.71277
Diversity Loss - Mean: -0.06946, Variance: 0.01577
Semantic Loss - Mean: 2.31451, Variance: 0.01430

Train Epoch: 144 
task: sign, mean loss: 0.04552, accuracy: 0.97826, avg. loss over tasks: 0.04552, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.01385, Variance: 0.01216
Semantic Loss - Mean: 0.07323, Variance: 0.00404

Test Epoch: 144 
task: sign, mean loss: 2.70425, accuracy: 0.62130, avg. loss over tasks: 2.70425
Diversity Loss - Mean: -0.06934, Variance: 0.01576
Semantic Loss - Mean: 2.30802, Variance: 0.01427

Train Epoch: 145 
task: sign, mean loss: 0.03457, accuracy: 0.99457, avg. loss over tasks: 0.03457, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.01753, Variance: 0.01214
Semantic Loss - Mean: 0.04969, Variance: 0.00402

Test Epoch: 145 
task: sign, mean loss: 2.68939, accuracy: 0.62130, avg. loss over tasks: 2.68939
Diversity Loss - Mean: -0.06771, Variance: 0.01575
Semantic Loss - Mean: 2.29271, Variance: 0.01425

Train Epoch: 146 
task: sign, mean loss: 0.02993, accuracy: 0.98913, avg. loss over tasks: 0.02993, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.02556, Variance: 0.01212
Semantic Loss - Mean: 0.06194, Variance: 0.00404

Test Epoch: 146 
task: sign, mean loss: 2.66547, accuracy: 0.62722, avg. loss over tasks: 2.66547
Diversity Loss - Mean: -0.06580, Variance: 0.01573
Semantic Loss - Mean: 2.28156, Variance: 0.01422

Train Epoch: 147 
task: sign, mean loss: 0.04179, accuracy: 0.98913, avg. loss over tasks: 0.04179, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.01699, Variance: 0.01210
Semantic Loss - Mean: 0.06118, Variance: 0.00406

Test Epoch: 147 
task: sign, mean loss: 2.67279, accuracy: 0.61538, avg. loss over tasks: 2.67279
Diversity Loss - Mean: -0.06558, Variance: 0.01572
Semantic Loss - Mean: 2.28525, Variance: 0.01419

Train Epoch: 148 
task: sign, mean loss: 0.06658, accuracy: 0.96739, avg. loss over tasks: 0.06658, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.01786, Variance: 0.01208
Semantic Loss - Mean: 0.07691, Variance: 0.00405

Test Epoch: 148 
task: sign, mean loss: 2.77851, accuracy: 0.61538, avg. loss over tasks: 2.77851
Diversity Loss - Mean: -0.06609, Variance: 0.01571
Semantic Loss - Mean: 2.37729, Variance: 0.01416

Train Epoch: 149 
task: sign, mean loss: 0.02717, accuracy: 0.99457, avg. loss over tasks: 0.02717, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.01555, Variance: 0.01206
Semantic Loss - Mean: 0.04332, Variance: 0.00404

Test Epoch: 149 
task: sign, mean loss: 2.73225, accuracy: 0.62722, avg. loss over tasks: 2.73225
Diversity Loss - Mean: -0.06877, Variance: 0.01570
Semantic Loss - Mean: 2.33118, Variance: 0.01413

Train Epoch: 150 
task: sign, mean loss: 0.02984, accuracy: 0.98370, avg. loss over tasks: 0.02984, lr: 3e-07
Diversity Loss - Mean: -0.01114, Variance: 0.01203
Semantic Loss - Mean: 0.05372, Variance: 0.00403

Test Epoch: 150 
task: sign, mean loss: 2.70017, accuracy: 0.62722, avg. loss over tasks: 2.70017
Diversity Loss - Mean: -0.06915, Variance: 0.01569
Semantic Loss - Mean: 2.30666, Variance: 0.01409

