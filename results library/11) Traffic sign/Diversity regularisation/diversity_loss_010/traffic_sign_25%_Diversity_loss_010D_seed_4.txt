Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09258, accuracy: 0.63587, avg. loss over tasks: 1.09258, lr: 3e-05
Diversity Loss - Mean: -0.00918, Variance: 0.01049
Semantic Loss - Mean: 1.43115, Variance: 0.07289

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.18342, accuracy: 0.66272, avg. loss over tasks: 1.18342
Diversity Loss - Mean: -0.02747, Variance: 0.01238
Semantic Loss - Mean: 1.16140, Variance: 0.05429

Train Epoch: 2 
task: sign, mean loss: 0.96655, accuracy: 0.67391, avg. loss over tasks: 0.96655, lr: 6e-05
Diversity Loss - Mean: -0.01051, Variance: 0.01040
Semantic Loss - Mean: 0.98237, Variance: 0.03943

Test Epoch: 2 
task: sign, mean loss: 1.09561, accuracy: 0.66272, avg. loss over tasks: 1.09561
Diversity Loss - Mean: -0.01705, Variance: 0.01188
Semantic Loss - Mean: 1.15158, Variance: 0.03261

Train Epoch: 3 
task: sign, mean loss: 0.78791, accuracy: 0.69022, avg. loss over tasks: 0.78791, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.01439, Variance: 0.01009
Semantic Loss - Mean: 0.99222, Variance: 0.02727

Test Epoch: 3 
task: sign, mean loss: 1.31822, accuracy: 0.59172, avg. loss over tasks: 1.31822
Diversity Loss - Mean: -0.02953, Variance: 0.01106
Semantic Loss - Mean: 1.12224, Variance: 0.02955

Train Epoch: 4 
task: sign, mean loss: 0.75813, accuracy: 0.70652, avg. loss over tasks: 0.75813, lr: 0.00012
Diversity Loss - Mean: -0.02394, Variance: 0.00972
Semantic Loss - Mean: 0.88394, Variance: 0.02108

Test Epoch: 4 
task: sign, mean loss: 1.73494, accuracy: 0.48521, avg. loss over tasks: 1.73494
Diversity Loss - Mean: -0.00370, Variance: 0.01039
Semantic Loss - Mean: 1.17607, Variance: 0.02595

Train Epoch: 5 
task: sign, mean loss: 0.71628, accuracy: 0.74457, avg. loss over tasks: 0.71628, lr: 0.00015
Diversity Loss - Mean: -0.00968, Variance: 0.00930
Semantic Loss - Mean: 0.78455, Variance: 0.01729

Test Epoch: 5 
task: sign, mean loss: 1.73491, accuracy: 0.53254, avg. loss over tasks: 1.73491
Diversity Loss - Mean: -0.00049, Variance: 0.01022
Semantic Loss - Mean: 1.22708, Variance: 0.02318

Train Epoch: 6 
task: sign, mean loss: 0.65311, accuracy: 0.75543, avg. loss over tasks: 0.65311, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.00445, Variance: 0.00898
Semantic Loss - Mean: 0.72497, Variance: 0.01485

Test Epoch: 6 
task: sign, mean loss: 2.46323, accuracy: 0.66272, avg. loss over tasks: 2.46323
Diversity Loss - Mean: 0.06683, Variance: 0.01057
Semantic Loss - Mean: 1.60343, Variance: 0.02197

Train Epoch: 7 
task: sign, mean loss: 0.59413, accuracy: 0.77717, avg. loss over tasks: 0.59413, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.01121, Variance: 0.00881
Semantic Loss - Mean: 0.60263, Variance: 0.01321

Test Epoch: 7 
task: sign, mean loss: 3.50956, accuracy: 0.30178, avg. loss over tasks: 3.50956
Diversity Loss - Mean: 0.06896, Variance: 0.01031
Semantic Loss - Mean: 2.32850, Variance: 0.02949

Train Epoch: 8 
task: sign, mean loss: 0.59184, accuracy: 0.79891, avg. loss over tasks: 0.59184, lr: 0.00024
Diversity Loss - Mean: 0.02769, Variance: 0.00848
Semantic Loss - Mean: 0.61094, Variance: 0.01209

Test Epoch: 8 
task: sign, mean loss: 1.75346, accuracy: 0.64497, avg. loss over tasks: 1.75346
Diversity Loss - Mean: -0.00058, Variance: 0.01014
Semantic Loss - Mean: 1.44376, Variance: 0.03175

Train Epoch: 9 
task: sign, mean loss: 0.53072, accuracy: 0.80978, avg. loss over tasks: 0.53072, lr: 0.00027
Diversity Loss - Mean: 0.03044, Variance: 0.00820
Semantic Loss - Mean: 0.55054, Variance: 0.01115

Test Epoch: 9 
task: sign, mean loss: 2.49618, accuracy: 0.47929, avg. loss over tasks: 2.49618
Diversity Loss - Mean: 0.10064, Variance: 0.00987
Semantic Loss - Mean: 1.91983, Variance: 0.03109

Train Epoch: 10 
task: sign, mean loss: 0.89264, accuracy: 0.73370, avg. loss over tasks: 0.89264, lr: 0.0003
Diversity Loss - Mean: 0.01390, Variance: 0.00808
Semantic Loss - Mean: 0.77213, Variance: 0.01054

Test Epoch: 10 
task: sign, mean loss: 2.72770, accuracy: 0.46746, avg. loss over tasks: 2.72770
Diversity Loss - Mean: 0.07633, Variance: 0.00984
Semantic Loss - Mean: 2.05585, Variance: 0.03210

Train Epoch: 11 
task: sign, mean loss: 0.67780, accuracy: 0.68478, avg. loss over tasks: 0.67780, lr: 0.0002999622730061346
Diversity Loss - Mean: 0.02188, Variance: 0.00799
Semantic Loss - Mean: 0.61578, Variance: 0.00997

Test Epoch: 11 
task: sign, mean loss: 2.05889, accuracy: 0.43787, avg. loss over tasks: 2.05889
Diversity Loss - Mean: 0.04123, Variance: 0.01027
Semantic Loss - Mean: 1.65750, Variance: 0.03124

Train Epoch: 12 
task: sign, mean loss: 0.54282, accuracy: 0.77717, avg. loss over tasks: 0.54282, lr: 0.000299849111021216
Diversity Loss - Mean: 0.03740, Variance: 0.00791
Semantic Loss - Mean: 0.52298, Variance: 0.00939

Test Epoch: 12 
task: sign, mean loss: 2.15111, accuracy: 0.60355, avg. loss over tasks: 2.15111
Diversity Loss - Mean: 0.05769, Variance: 0.01029
Semantic Loss - Mean: 1.74763, Variance: 0.03174

Train Epoch: 13 
task: sign, mean loss: 0.49110, accuracy: 0.86413, avg. loss over tasks: 0.49110, lr: 0.0002996605710257114
Diversity Loss - Mean: 0.03781, Variance: 0.00791
Semantic Loss - Mean: 0.54175, Variance: 0.00921

Test Epoch: 13 
task: sign, mean loss: 1.96975, accuracy: 0.67456, avg. loss over tasks: 1.96975
Diversity Loss - Mean: 0.04729, Variance: 0.01034
Semantic Loss - Mean: 1.70493, Variance: 0.03186

Train Epoch: 14 
task: sign, mean loss: 0.31445, accuracy: 0.87500, avg. loss over tasks: 0.31445, lr: 0.00029939674795518656
Diversity Loss - Mean: 0.03545, Variance: 0.00796
Semantic Loss - Mean: 0.35357, Variance: 0.00867

Test Epoch: 14 
task: sign, mean loss: 2.69575, accuracy: 0.35503, avg. loss over tasks: 2.69575
Diversity Loss - Mean: 0.05395, Variance: 0.01034
Semantic Loss - Mean: 2.39234, Variance: 0.03326

Train Epoch: 15 
task: sign, mean loss: 0.25414, accuracy: 0.91304, avg. loss over tasks: 0.25414, lr: 0.0002990577746525024
Diversity Loss - Mean: 0.04627, Variance: 0.00797
Semantic Loss - Mean: 0.30298, Variance: 0.00836

Test Epoch: 15 
task: sign, mean loss: 2.18434, accuracy: 0.50296, avg. loss over tasks: 2.18434
Diversity Loss - Mean: 0.03676, Variance: 0.01033
Semantic Loss - Mean: 1.83590, Variance: 0.03373

Train Epoch: 16 
task: sign, mean loss: 0.38593, accuracy: 0.86413, avg. loss over tasks: 0.38593, lr: 0.000298643821800925
Diversity Loss - Mean: 0.05052, Variance: 0.00796
Semantic Loss - Mean: 0.40114, Variance: 0.00815

Test Epoch: 16 
task: sign, mean loss: 2.49419, accuracy: 0.66272, avg. loss over tasks: 2.49419
Diversity Loss - Mean: 0.08605, Variance: 0.01039
Semantic Loss - Mean: 2.31606, Variance: 0.03344

Train Epoch: 17 
task: sign, mean loss: 0.59172, accuracy: 0.76087, avg. loss over tasks: 0.59172, lr: 0.0002981550978381814
Diversity Loss - Mean: 0.03066, Variance: 0.00802
Semantic Loss - Mean: 0.60316, Variance: 0.00813

Test Epoch: 17 
task: sign, mean loss: 2.56331, accuracy: 0.44970, avg. loss over tasks: 2.56331
Diversity Loss - Mean: 0.10879, Variance: 0.01041
Semantic Loss - Mean: 2.21474, Variance: 0.03548

Train Epoch: 18 
task: sign, mean loss: 0.56464, accuracy: 0.79348, avg. loss over tasks: 0.56464, lr: 0.00029759184885150465
Diversity Loss - Mean: 0.00482, Variance: 0.00818
Semantic Loss - Mean: 0.58831, Variance: 0.00792

Test Epoch: 18 
task: sign, mean loss: 1.91768, accuracy: 0.48521, avg. loss over tasks: 1.91768
Diversity Loss - Mean: 0.05918, Variance: 0.01033
Semantic Loss - Mean: 1.77273, Variance: 0.03484

Train Epoch: 19 
task: sign, mean loss: 0.37433, accuracy: 0.86413, avg. loss over tasks: 0.37433, lr: 0.0002969543584537218
Diversity Loss - Mean: 0.00453, Variance: 0.00830
Semantic Loss - Mean: 0.40892, Variance: 0.00766

Test Epoch: 19 
task: sign, mean loss: 1.67268, accuracy: 0.43787, avg. loss over tasks: 1.67268
Diversity Loss - Mean: 0.02858, Variance: 0.01033
Semantic Loss - Mean: 1.53926, Variance: 0.03496

Train Epoch: 20 
task: sign, mean loss: 0.28058, accuracy: 0.86957, avg. loss over tasks: 0.28058, lr: 0.0002962429476404462
Diversity Loss - Mean: 0.02510, Variance: 0.00835
Semantic Loss - Mean: 0.31974, Variance: 0.00738

Test Epoch: 20 
task: sign, mean loss: 2.88400, accuracy: 0.36095, avg. loss over tasks: 2.88400
Diversity Loss - Mean: 0.10026, Variance: 0.01033
Semantic Loss - Mean: 2.34396, Variance: 0.03498

Train Epoch: 21 
task: sign, mean loss: 0.35700, accuracy: 0.86957, avg. loss over tasks: 0.35700, lr: 0.00029545797462844647
Diversity Loss - Mean: 0.05460, Variance: 0.00828
Semantic Loss - Mean: 0.38770, Variance: 0.00738

Test Epoch: 21 
task: sign, mean loss: 2.46580, accuracy: 0.36095, avg. loss over tasks: 2.46580
Diversity Loss - Mean: 0.07796, Variance: 0.01029
Semantic Loss - Mean: 1.99356, Variance: 0.03670

Train Epoch: 22 
task: sign, mean loss: 0.32728, accuracy: 0.85326, avg. loss over tasks: 0.32728, lr: 0.0002945998346752736
Diversity Loss - Mean: 0.04896, Variance: 0.00823
Semantic Loss - Mean: 0.40454, Variance: 0.00742

Test Epoch: 22 
task: sign, mean loss: 0.97304, accuracy: 0.57396, avg. loss over tasks: 0.97304
Diversity Loss - Mean: 0.05412, Variance: 0.01026
Semantic Loss - Mean: 1.04317, Variance: 0.03650

Train Epoch: 23 
task: sign, mean loss: 0.16253, accuracy: 0.95109, avg. loss over tasks: 0.16253, lr: 0.0002936689598802368
Diversity Loss - Mean: 0.05414, Variance: 0.00819
Semantic Loss - Mean: 0.18889, Variance: 0.00722

Test Epoch: 23 
task: sign, mean loss: 1.80365, accuracy: 0.59763, avg. loss over tasks: 1.80365
Diversity Loss - Mean: 0.04663, Variance: 0.01021
Semantic Loss - Mean: 1.61481, Variance: 0.03594

Train Epoch: 24 
task: sign, mean loss: 0.12053, accuracy: 0.95652, avg. loss over tasks: 0.12053, lr: 0.00029266581896682876
Diversity Loss - Mean: 0.05845, Variance: 0.00812
Semantic Loss - Mean: 0.14077, Variance: 0.00700

Test Epoch: 24 
task: sign, mean loss: 2.35747, accuracy: 0.37278, avg. loss over tasks: 2.35747
Diversity Loss - Mean: 0.07868, Variance: 0.01007
Semantic Loss - Mean: 2.02438, Variance: 0.03609

Train Epoch: 25 
task: sign, mean loss: 0.21126, accuracy: 0.91304, avg. loss over tasks: 0.21126, lr: 0.00029159091704670885
Diversity Loss - Mean: 0.07777, Variance: 0.00806
Semantic Loss - Mean: 0.24300, Variance: 0.00689

Test Epoch: 25 
task: sign, mean loss: 2.77911, accuracy: 0.37278, avg. loss over tasks: 2.77911
Diversity Loss - Mean: 0.15085, Variance: 0.00992
Semantic Loss - Mean: 2.27517, Variance: 0.03624

Train Epoch: 26 
task: sign, mean loss: 0.20173, accuracy: 0.90217, avg. loss over tasks: 0.20173, lr: 0.00029044479536536455
Diversity Loss - Mean: 0.06367, Variance: 0.00803
Semantic Loss - Mean: 0.23399, Variance: 0.00687

Test Epoch: 26 
task: sign, mean loss: 2.14372, accuracy: 0.39645, avg. loss over tasks: 2.14372
Diversity Loss - Mean: 0.10979, Variance: 0.00982
Semantic Loss - Mean: 1.93725, Variance: 0.03635

Train Epoch: 27 
task: sign, mean loss: 0.34904, accuracy: 0.88043, avg. loss over tasks: 0.34904, lr: 0.000289228031029578
Diversity Loss - Mean: 0.04727, Variance: 0.00799
Semantic Loss - Mean: 0.39543, Variance: 0.00728

Test Epoch: 27 
task: sign, mean loss: 2.00282, accuracy: 0.38462, avg. loss over tasks: 2.00282
Diversity Loss - Mean: 0.10843, Variance: 0.00977
Semantic Loss - Mean: 1.54507, Variance: 0.03641

Train Epoch: 28 
task: sign, mean loss: 0.42602, accuracy: 0.88587, avg. loss over tasks: 0.42602, lr: 0.0002879412367168349
Diversity Loss - Mean: 0.03722, Variance: 0.00796
Semantic Loss - Mean: 0.51142, Variance: 0.00772

Test Epoch: 28 
task: sign, mean loss: 1.29098, accuracy: 0.51479, avg. loss over tasks: 1.29098
Diversity Loss - Mean: 0.11960, Variance: 0.00978
Semantic Loss - Mean: 1.14910, Variance: 0.03686

Train Epoch: 29 
task: sign, mean loss: 0.34395, accuracy: 0.84783, avg. loss over tasks: 0.34395, lr: 0.00028658506036682353
Diversity Loss - Mean: 0.02573, Variance: 0.00794
Semantic Loss - Mean: 0.41230, Variance: 0.00781

Test Epoch: 29 
task: sign, mean loss: 0.92261, accuracy: 0.76923, avg. loss over tasks: 0.92261
Diversity Loss - Mean: 0.07392, Variance: 0.00972
Semantic Loss - Mean: 0.80677, Variance: 0.03614

Train Epoch: 30 
task: sign, mean loss: 0.32936, accuracy: 0.86413, avg. loss over tasks: 0.32936, lr: 0.00028516018485517746
Diversity Loss - Mean: 0.01493, Variance: 0.00794
Semantic Loss - Mean: 0.38609, Variance: 0.00792

Test Epoch: 30 
task: sign, mean loss: 1.49074, accuracy: 0.71006, avg. loss over tasks: 1.49074
Diversity Loss - Mean: 0.08366, Variance: 0.00966
Semantic Loss - Mean: 1.37367, Variance: 0.03615

Train Epoch: 31 
task: sign, mean loss: 0.34050, accuracy: 0.84239, avg. loss over tasks: 0.34050, lr: 0.00028366732764962686
Diversity Loss - Mean: 0.02383, Variance: 0.00791
Semantic Loss - Mean: 0.39506, Variance: 0.00795

Test Epoch: 31 
task: sign, mean loss: 0.73198, accuracy: 0.78698, avg. loss over tasks: 0.73198
Diversity Loss - Mean: 0.04066, Variance: 0.00962
Semantic Loss - Mean: 0.69916, Variance: 0.03529

Train Epoch: 32 
task: sign, mean loss: 0.27467, accuracy: 0.89674, avg. loss over tasks: 0.27467, lr: 0.00028210724044873213
Diversity Loss - Mean: 0.00338, Variance: 0.00791
Semantic Loss - Mean: 0.33769, Variance: 0.00825

Test Epoch: 32 
task: sign, mean loss: 2.47833, accuracy: 0.37870, avg. loss over tasks: 2.47833
Diversity Loss - Mean: 0.09838, Variance: 0.00951
Semantic Loss - Mean: 2.03631, Variance: 0.03619

Train Epoch: 33 
task: sign, mean loss: 0.16340, accuracy: 0.92391, avg. loss over tasks: 0.16340, lr: 0.00028048070880338095
Diversity Loss - Mean: 0.01398, Variance: 0.00788
Semantic Loss - Mean: 0.23594, Variance: 0.00824

Test Epoch: 33 
task: sign, mean loss: 0.92194, accuracy: 0.75740, avg. loss over tasks: 0.92194
Diversity Loss - Mean: 0.06204, Variance: 0.00953
Semantic Loss - Mean: 0.87667, Variance: 0.03542

Train Epoch: 34 
task: sign, mean loss: 0.37073, accuracy: 0.90217, avg. loss over tasks: 0.37073, lr: 0.00027878855172123963
Diversity Loss - Mean: 0.01993, Variance: 0.00786
Semantic Loss - Mean: 0.41567, Variance: 0.00838

Test Epoch: 34 
task: sign, mean loss: 0.83735, accuracy: 0.72189, avg. loss over tasks: 0.83735
Diversity Loss - Mean: 0.03826, Variance: 0.00961
Semantic Loss - Mean: 0.77788, Variance: 0.03473

Train Epoch: 35 
task: sign, mean loss: 0.24901, accuracy: 0.91848, avg. loss over tasks: 0.24901, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.00900, Variance: 0.00788
Semantic Loss - Mean: 0.30698, Variance: 0.00858

Test Epoch: 35 
task: sign, mean loss: 1.18506, accuracy: 0.63905, avg. loss over tasks: 1.18506
Diversity Loss - Mean: 0.08060, Variance: 0.00959
Semantic Loss - Mean: 1.15124, Variance: 0.03504

Train Epoch: 36 
task: sign, mean loss: 0.11696, accuracy: 0.98370, avg. loss over tasks: 0.11696, lr: 0.00027521080207013716
Diversity Loss - Mean: 0.02695, Variance: 0.00785
Semantic Loss - Mean: 0.18863, Variance: 0.00854

Test Epoch: 36 
task: sign, mean loss: 1.35567, accuracy: 0.60947, avg. loss over tasks: 1.35567
Diversity Loss - Mean: 0.12106, Variance: 0.00954
Semantic Loss - Mean: 1.26209, Variance: 0.03438

Train Epoch: 37 
task: sign, mean loss: 0.09840, accuracy: 0.96196, avg. loss over tasks: 0.09840, lr: 0.0002733270110058693
Diversity Loss - Mean: 0.03980, Variance: 0.00781
Semantic Loss - Mean: 0.16932, Variance: 0.00855

Test Epoch: 37 
task: sign, mean loss: 0.59073, accuracy: 0.82249, avg. loss over tasks: 0.59073
Diversity Loss - Mean: 0.05970, Variance: 0.00949
Semantic Loss - Mean: 0.60801, Variance: 0.03385

Train Epoch: 38 
task: sign, mean loss: 0.17096, accuracy: 0.94022, avg. loss over tasks: 0.17096, lr: 0.00027138119660708587
Diversity Loss - Mean: 0.02912, Variance: 0.00777
Semantic Loss - Mean: 0.22813, Variance: 0.00856

Test Epoch: 38 
task: sign, mean loss: 0.74584, accuracy: 0.83432, avg. loss over tasks: 0.74584
Diversity Loss - Mean: 0.03827, Variance: 0.00945
Semantic Loss - Mean: 0.69300, Variance: 0.03320

Train Epoch: 39 
task: sign, mean loss: 0.12952, accuracy: 0.95652, avg. loss over tasks: 0.12952, lr: 0.0002693743386499349
Diversity Loss - Mean: 0.01804, Variance: 0.00772
Semantic Loss - Mean: 0.21994, Variance: 0.00895

Test Epoch: 39 
task: sign, mean loss: 1.02260, accuracy: 0.71598, avg. loss over tasks: 1.02260
Diversity Loss - Mean: 0.04607, Variance: 0.00943
Semantic Loss - Mean: 0.88722, Variance: 0.03271

Train Epoch: 40 
task: sign, mean loss: 0.28085, accuracy: 0.90761, avg. loss over tasks: 0.28085, lr: 0.00026730744764783427
Diversity Loss - Mean: 0.02614, Variance: 0.00770
Semantic Loss - Mean: 0.33445, Variance: 0.00901

Test Epoch: 40 
task: sign, mean loss: 2.20229, accuracy: 0.45562, avg. loss over tasks: 2.20229
Diversity Loss - Mean: 0.14850, Variance: 0.00940
Semantic Loss - Mean: 1.90284, Variance: 0.03460

Train Epoch: 41 
task: sign, mean loss: 0.12012, accuracy: 0.95652, avg. loss over tasks: 0.12012, lr: 0.00026518156434264794
Diversity Loss - Mean: 0.01604, Variance: 0.00769
Semantic Loss - Mean: 0.20313, Variance: 0.00910

Test Epoch: 41 
task: sign, mean loss: 1.76231, accuracy: 0.57988, avg. loss over tasks: 1.76231
Diversity Loss - Mean: 0.08554, Variance: 0.00932
Semantic Loss - Mean: 1.47579, Variance: 0.03516

Train Epoch: 42 
task: sign, mean loss: 0.17545, accuracy: 0.94022, avg. loss over tasks: 0.17545, lr: 0.0002629977591806411
Diversity Loss - Mean: 0.01129, Variance: 0.00768
Semantic Loss - Mean: 0.20970, Variance: 0.00916

Test Epoch: 42 
task: sign, mean loss: 0.95885, accuracy: 0.82840, avg. loss over tasks: 0.95885
Diversity Loss - Mean: 0.04967, Variance: 0.00931
Semantic Loss - Mean: 0.84196, Variance: 0.03479

Train Epoch: 43 
task: sign, mean loss: 0.14648, accuracy: 0.93478, avg. loss over tasks: 0.14648, lr: 0.000260757131773478
Diversity Loss - Mean: 0.01127, Variance: 0.00766
Semantic Loss - Mean: 0.23472, Variance: 0.00917

Test Epoch: 43 
task: sign, mean loss: 0.51466, accuracy: 0.84615, avg. loss over tasks: 0.51466
Diversity Loss - Mean: 0.07656, Variance: 0.00927
Semantic Loss - Mean: 0.54428, Variance: 0.03428

Train Epoch: 44 
task: sign, mean loss: 0.14107, accuracy: 0.94565, avg. loss over tasks: 0.14107, lr: 0.0002584608103445346
Diversity Loss - Mean: 0.01678, Variance: 0.00763
Semantic Loss - Mean: 0.22773, Variance: 0.00938

Test Epoch: 44 
task: sign, mean loss: 0.70143, accuracy: 0.82840, avg. loss over tasks: 0.70143
Diversity Loss - Mean: 0.07978, Variance: 0.00922
Semantic Loss - Mean: 0.68369, Variance: 0.03370

Train Epoch: 45 
task: sign, mean loss: 0.11073, accuracy: 0.97283, avg. loss over tasks: 0.11073, lr: 0.0002561099511608041
Diversity Loss - Mean: 0.00838, Variance: 0.00760
Semantic Loss - Mean: 0.19486, Variance: 0.00942

Test Epoch: 45 
task: sign, mean loss: 0.68129, accuracy: 0.81657, avg. loss over tasks: 0.68129
Diversity Loss - Mean: 0.04596, Variance: 0.00919
Semantic Loss - Mean: 0.67246, Variance: 0.03322

Train Epoch: 46 
task: sign, mean loss: 0.11073, accuracy: 0.95109, avg. loss over tasks: 0.11073, lr: 0.00025370573795068164
Diversity Loss - Mean: 0.00196, Variance: 0.00759
Semantic Loss - Mean: 0.17269, Variance: 0.00939

Test Epoch: 46 
task: sign, mean loss: 0.73934, accuracy: 0.84024, avg. loss over tasks: 0.73934
Diversity Loss - Mean: 0.06180, Variance: 0.00917
Semantic Loss - Mean: 0.67670, Variance: 0.03281

Train Epoch: 47 
task: sign, mean loss: 0.07031, accuracy: 0.98913, avg. loss over tasks: 0.07031, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.00152, Variance: 0.00758
Semantic Loss - Mean: 0.11955, Variance: 0.00937

Test Epoch: 47 
task: sign, mean loss: 0.78273, accuracy: 0.85207, avg. loss over tasks: 0.78273
Diversity Loss - Mean: 0.06111, Variance: 0.00914
Semantic Loss - Mean: 0.70694, Variance: 0.03262

Train Epoch: 48 
task: sign, mean loss: 0.03431, accuracy: 0.99457, avg. loss over tasks: 0.03431, lr: 0.0002487421180820659
Diversity Loss - Mean: 0.00902, Variance: 0.00756
Semantic Loss - Mean: 0.09145, Variance: 0.00934

Test Epoch: 48 
task: sign, mean loss: 0.55109, accuracy: 0.89349, avg. loss over tasks: 0.55109
Diversity Loss - Mean: 0.06575, Variance: 0.00910
Semantic Loss - Mean: 0.52361, Variance: 0.03204

Train Epoch: 49 
task: sign, mean loss: 0.04126, accuracy: 0.98370, avg. loss over tasks: 0.04126, lr: 0.0002461852107556558
Diversity Loss - Mean: 0.02326, Variance: 0.00753
Semantic Loss - Mean: 0.11418, Variance: 0.00957

Test Epoch: 49 
task: sign, mean loss: 0.68678, accuracy: 0.88166, avg. loss over tasks: 0.68678
Diversity Loss - Mean: 0.06803, Variance: 0.00906
Semantic Loss - Mean: 0.66847, Variance: 0.03147

Train Epoch: 50 
task: sign, mean loss: 0.02409, accuracy: 0.99457, avg. loss over tasks: 0.02409, lr: 0.00024357994680853121
Diversity Loss - Mean: 0.01928, Variance: 0.00750
Semantic Loss - Mean: 0.11546, Variance: 0.00969

Test Epoch: 50 
task: sign, mean loss: 0.90768, accuracy: 0.82840, avg. loss over tasks: 0.90768
Diversity Loss - Mean: 0.06756, Variance: 0.00902
Semantic Loss - Mean: 0.86622, Variance: 0.03117

Train Epoch: 51 
task: sign, mean loss: 0.03910, accuracy: 0.99457, avg. loss over tasks: 0.03910, lr: 0.00024092763806954684
Diversity Loss - Mean: 0.02273, Variance: 0.00746
Semantic Loss - Mean: 0.13938, Variance: 0.00976

Test Epoch: 51 
task: sign, mean loss: 0.80526, accuracy: 0.86391, avg. loss over tasks: 0.80526
Diversity Loss - Mean: 0.07136, Variance: 0.00900
Semantic Loss - Mean: 0.67045, Variance: 0.03085

Train Epoch: 52 
task: sign, mean loss: 0.04880, accuracy: 0.98913, avg. loss over tasks: 0.04880, lr: 0.00023822962005602707
Diversity Loss - Mean: 0.02112, Variance: 0.00743
Semantic Loss - Mean: 0.10754, Variance: 0.00984

Test Epoch: 52 
task: sign, mean loss: 0.73857, accuracy: 0.82249, avg. loss over tasks: 0.73857
Diversity Loss - Mean: 0.03979, Variance: 0.00897
Semantic Loss - Mean: 0.69631, Variance: 0.03064

Train Epoch: 53 
task: sign, mean loss: 0.06776, accuracy: 0.97826, avg. loss over tasks: 0.06776, lr: 0.00023548725130129248
Diversity Loss - Mean: 0.01070, Variance: 0.00741
Semantic Loss - Mean: 0.13984, Variance: 0.00999

Test Epoch: 53 
task: sign, mean loss: 0.39480, accuracy: 0.85207, avg. loss over tasks: 0.39480
Diversity Loss - Mean: 0.00369, Variance: 0.00901
Semantic Loss - Mean: 0.37548, Variance: 0.03020

Train Epoch: 54 
task: sign, mean loss: 0.02504, accuracy: 0.99457, avg. loss over tasks: 0.02504, lr: 0.00023270191267059755
Diversity Loss - Mean: 0.00871, Variance: 0.00741
Semantic Loss - Mean: 0.09291, Variance: 0.01014

Test Epoch: 54 
task: sign, mean loss: 0.78052, accuracy: 0.79290, avg. loss over tasks: 0.78052
Diversity Loss - Mean: 0.02718, Variance: 0.00902
Semantic Loss - Mean: 0.75273, Variance: 0.02996

Train Epoch: 55 
task: sign, mean loss: 0.05494, accuracy: 0.98370, avg. loss over tasks: 0.05494, lr: 0.00022987500666582316
Diversity Loss - Mean: 0.00827, Variance: 0.00741
Semantic Loss - Mean: 0.11566, Variance: 0.01013

Test Epoch: 55 
task: sign, mean loss: 0.57063, accuracy: 0.89941, avg. loss over tasks: 0.57063
Diversity Loss - Mean: -0.00798, Variance: 0.00905
Semantic Loss - Mean: 0.47077, Variance: 0.02955

Train Epoch: 56 
task: sign, mean loss: 0.11290, accuracy: 0.96196, avg. loss over tasks: 0.11290, lr: 0.00022700795671927503
Diversity Loss - Mean: 0.00495, Variance: 0.00740
Semantic Loss - Mean: 0.18129, Variance: 0.01038

Test Epoch: 56 
task: sign, mean loss: 0.79134, accuracy: 0.85207, avg. loss over tasks: 0.79134
Diversity Loss - Mean: -0.02436, Variance: 0.00908
Semantic Loss - Mean: 0.61027, Variance: 0.02944

Train Epoch: 57 
task: sign, mean loss: 0.32533, accuracy: 0.90217, avg. loss over tasks: 0.32533, lr: 0.00022410220647694235
Diversity Loss - Mean: 0.00291, Variance: 0.00739
Semantic Loss - Mean: 0.36324, Variance: 0.01049

Test Epoch: 57 
task: sign, mean loss: 3.20376, accuracy: 0.38462, avg. loss over tasks: 3.20376
Diversity Loss - Mean: 0.09562, Variance: 0.00908
Semantic Loss - Mean: 2.03827, Variance: 0.03018

Train Epoch: 58 
task: sign, mean loss: 0.16133, accuracy: 0.94565, avg. loss over tasks: 0.16133, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.01134, Variance: 0.00740
Semantic Loss - Mean: 0.20990, Variance: 0.01058

Test Epoch: 58 
task: sign, mean loss: 0.73047, accuracy: 0.84615, avg. loss over tasks: 0.73047
Diversity Loss - Mean: -0.02690, Variance: 0.00913
Semantic Loss - Mean: 0.60475, Variance: 0.02977

Train Epoch: 59 
task: sign, mean loss: 0.05164, accuracy: 0.97283, avg. loss over tasks: 0.05164, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.01563, Variance: 0.00741
Semantic Loss - Mean: 0.14680, Variance: 0.01069

Test Epoch: 59 
task: sign, mean loss: 0.53977, accuracy: 0.86391, avg. loss over tasks: 0.53977
Diversity Loss - Mean: -0.00161, Variance: 0.00916
Semantic Loss - Mean: 0.49562, Variance: 0.02935

Train Epoch: 60 
task: sign, mean loss: 0.06295, accuracy: 0.97283, avg. loss over tasks: 0.06295, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.01781, Variance: 0.00741
Semantic Loss - Mean: 0.13932, Variance: 0.01070

Test Epoch: 60 
task: sign, mean loss: 0.54349, accuracy: 0.88757, avg. loss over tasks: 0.54349
Diversity Loss - Mean: 0.00372, Variance: 0.00918
Semantic Loss - Mean: 0.49693, Variance: 0.02897

Train Epoch: 61 
task: sign, mean loss: 0.06096, accuracy: 0.98370, avg. loss over tasks: 0.06096, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.02037, Variance: 0.00741
Semantic Loss - Mean: 0.13421, Variance: 0.01081

Test Epoch: 61 
task: sign, mean loss: 0.69040, accuracy: 0.85799, avg. loss over tasks: 0.69040
Diversity Loss - Mean: 0.00964, Variance: 0.00919
Semantic Loss - Mean: 0.62004, Variance: 0.02872

Train Epoch: 62 
task: sign, mean loss: 0.03791, accuracy: 0.99457, avg. loss over tasks: 0.03791, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.01394, Variance: 0.00739
Semantic Loss - Mean: 0.12124, Variance: 0.01099

Test Epoch: 62 
task: sign, mean loss: 0.40328, accuracy: 0.90533, avg. loss over tasks: 0.40328
Diversity Loss - Mean: -0.01012, Variance: 0.00920
Semantic Loss - Mean: 0.36052, Variance: 0.02830

Train Epoch: 63 
task: sign, mean loss: 0.05048, accuracy: 0.97826, avg. loss over tasks: 0.05048, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.02361, Variance: 0.00739
Semantic Loss - Mean: 0.11222, Variance: 0.01099

Test Epoch: 63 
task: sign, mean loss: 0.39293, accuracy: 0.90533, avg. loss over tasks: 0.39293
Diversity Loss - Mean: 0.00389, Variance: 0.00919
Semantic Loss - Mean: 0.34231, Variance: 0.02791

Train Epoch: 64 
task: sign, mean loss: 0.10264, accuracy: 0.97283, avg. loss over tasks: 0.10264, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.01853, Variance: 0.00739
Semantic Loss - Mean: 0.15931, Variance: 0.01102

Test Epoch: 64 
task: sign, mean loss: 0.73534, accuracy: 0.83432, avg. loss over tasks: 0.73534
Diversity Loss - Mean: 0.01717, Variance: 0.00917
Semantic Loss - Mean: 0.71653, Variance: 0.02763

Train Epoch: 65 
task: sign, mean loss: 0.03595, accuracy: 0.98370, avg. loss over tasks: 0.03595, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.02284, Variance: 0.00738
Semantic Loss - Mean: 0.11559, Variance: 0.01119

Test Epoch: 65 
task: sign, mean loss: 0.44831, accuracy: 0.90533, avg. loss over tasks: 0.44831
Diversity Loss - Mean: 0.00973, Variance: 0.00916
Semantic Loss - Mean: 0.42791, Variance: 0.02732

Train Epoch: 66 
task: sign, mean loss: 0.05650, accuracy: 0.99457, avg. loss over tasks: 0.05650, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.01914, Variance: 0.00738
Semantic Loss - Mean: 0.11267, Variance: 0.01126

Test Epoch: 66 
task: sign, mean loss: 0.33134, accuracy: 0.92308, avg. loss over tasks: 0.33134
Diversity Loss - Mean: 0.00169, Variance: 0.00916
Semantic Loss - Mean: 0.29548, Variance: 0.02695

Train Epoch: 67 
task: sign, mean loss: 0.01876, accuracy: 1.00000, avg. loss over tasks: 0.01876, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.01558, Variance: 0.00737
Semantic Loss - Mean: 0.08476, Variance: 0.01122

Test Epoch: 67 
task: sign, mean loss: 0.36024, accuracy: 0.91716, avg. loss over tasks: 0.36024
Diversity Loss - Mean: 0.02434, Variance: 0.00913
Semantic Loss - Mean: 0.30887, Variance: 0.02659

Train Epoch: 68 
task: sign, mean loss: 0.18328, accuracy: 0.96196, avg. loss over tasks: 0.18328, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.01678, Variance: 0.00737
Semantic Loss - Mean: 0.25427, Variance: 0.01139

Test Epoch: 68 
task: sign, mean loss: 0.42862, accuracy: 0.91124, avg. loss over tasks: 0.42862
Diversity Loss - Mean: -0.00356, Variance: 0.00913
Semantic Loss - Mean: 0.38804, Variance: 0.02624

Train Epoch: 69 
task: sign, mean loss: 0.06042, accuracy: 0.97283, avg. loss over tasks: 0.06042, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.03144, Variance: 0.00737
Semantic Loss - Mean: 0.15951, Variance: 0.01151

Test Epoch: 69 
task: sign, mean loss: 0.47783, accuracy: 0.86982, avg. loss over tasks: 0.47783
Diversity Loss - Mean: -0.00187, Variance: 0.00913
Semantic Loss - Mean: 0.50419, Variance: 0.02599

Train Epoch: 70 
task: sign, mean loss: 0.03371, accuracy: 0.98370, avg. loss over tasks: 0.03371, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.02465, Variance: 0.00737
Semantic Loss - Mean: 0.09668, Variance: 0.01143

Test Epoch: 70 
task: sign, mean loss: 0.29190, accuracy: 0.91124, avg. loss over tasks: 0.29190
Diversity Loss - Mean: 0.01032, Variance: 0.00912
Semantic Loss - Mean: 0.32750, Variance: 0.02569

Train Epoch: 71 
task: sign, mean loss: 0.03617, accuracy: 0.98913, avg. loss over tasks: 0.03617, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.02848, Variance: 0.00737
Semantic Loss - Mean: 0.09705, Variance: 0.01140

Test Epoch: 71 
task: sign, mean loss: 0.36281, accuracy: 0.92899, avg. loss over tasks: 0.36281
Diversity Loss - Mean: 0.01786, Variance: 0.00912
Semantic Loss - Mean: 0.34649, Variance: 0.02538

Train Epoch: 72 
task: sign, mean loss: 0.02543, accuracy: 0.99457, avg. loss over tasks: 0.02543, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.02202, Variance: 0.00738
Semantic Loss - Mean: 0.07209, Variance: 0.01134

Test Epoch: 72 
task: sign, mean loss: 0.79659, accuracy: 0.85799, avg. loss over tasks: 0.79659
Diversity Loss - Mean: 0.04983, Variance: 0.00910
Semantic Loss - Mean: 0.79397, Variance: 0.02522

Train Epoch: 73 
task: sign, mean loss: 0.04159, accuracy: 0.98913, avg. loss over tasks: 0.04159, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.02026, Variance: 0.00738
Semantic Loss - Mean: 0.09648, Variance: 0.01140

Test Epoch: 73 
task: sign, mean loss: 0.34627, accuracy: 0.92308, avg. loss over tasks: 0.34627
Diversity Loss - Mean: 0.02179, Variance: 0.00910
Semantic Loss - Mean: 0.32184, Variance: 0.02490

Train Epoch: 74 
task: sign, mean loss: 0.01639, accuracy: 0.99457, avg. loss over tasks: 0.01639, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.02111, Variance: 0.00738
Semantic Loss - Mean: 0.07215, Variance: 0.01132

Test Epoch: 74 
task: sign, mean loss: 0.50932, accuracy: 0.91124, avg. loss over tasks: 0.50932
Diversity Loss - Mean: 0.02349, Variance: 0.00909
Semantic Loss - Mean: 0.47508, Variance: 0.02468

Train Epoch: 75 
task: sign, mean loss: 0.01867, accuracy: 0.99457, avg. loss over tasks: 0.01867, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.02344, Variance: 0.00738
Semantic Loss - Mean: 0.09314, Variance: 0.01137

Test Epoch: 75 
task: sign, mean loss: 0.36860, accuracy: 0.95266, avg. loss over tasks: 0.36860
Diversity Loss - Mean: 0.00700, Variance: 0.00909
Semantic Loss - Mean: 0.31597, Variance: 0.02438

Train Epoch: 76 
task: sign, mean loss: 0.01310, accuracy: 0.99457, avg. loss over tasks: 0.01310, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.02280, Variance: 0.00739
Semantic Loss - Mean: 0.07510, Variance: 0.01142

Test Epoch: 76 
task: sign, mean loss: 0.36689, accuracy: 0.93491, avg. loss over tasks: 0.36689
Diversity Loss - Mean: 0.00786, Variance: 0.00908
Semantic Loss - Mean: 0.33347, Variance: 0.02412

Train Epoch: 77 
task: sign, mean loss: 0.01949, accuracy: 0.98913, avg. loss over tasks: 0.01949, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.02133, Variance: 0.00739
Semantic Loss - Mean: 0.07358, Variance: 0.01141

Test Epoch: 77 
task: sign, mean loss: 0.32448, accuracy: 0.94083, avg. loss over tasks: 0.32448
Diversity Loss - Mean: 0.01670, Variance: 0.00906
Semantic Loss - Mean: 0.32984, Variance: 0.02387

Train Epoch: 78 
task: sign, mean loss: 0.00603, accuracy: 1.00000, avg. loss over tasks: 0.00603, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.02369, Variance: 0.00739
Semantic Loss - Mean: 0.05292, Variance: 0.01133

Test Epoch: 78 
task: sign, mean loss: 0.37082, accuracy: 0.94675, avg. loss over tasks: 0.37082
Diversity Loss - Mean: 0.00670, Variance: 0.00905
Semantic Loss - Mean: 0.39199, Variance: 0.02369

Train Epoch: 79 
task: sign, mean loss: 0.01578, accuracy: 0.99457, avg. loss over tasks: 0.01578, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.02525, Variance: 0.00739
Semantic Loss - Mean: 0.06149, Variance: 0.01129

Test Epoch: 79 
task: sign, mean loss: 0.37460, accuracy: 0.92899, avg. loss over tasks: 0.37460
Diversity Loss - Mean: 0.01194, Variance: 0.00904
Semantic Loss - Mean: 0.38263, Variance: 0.02353

Train Epoch: 80 
task: sign, mean loss: 0.01709, accuracy: 0.98913, avg. loss over tasks: 0.01709, lr: 0.00015015
Diversity Loss - Mean: -0.02217, Variance: 0.00739
Semantic Loss - Mean: 0.04165, Variance: 0.01118

Test Epoch: 80 
task: sign, mean loss: 0.55446, accuracy: 0.91124, avg. loss over tasks: 0.55446
Diversity Loss - Mean: 0.02182, Variance: 0.00903
Semantic Loss - Mean: 0.48862, Variance: 0.02335

Train Epoch: 81 
task: sign, mean loss: 0.24966, accuracy: 0.97283, avg. loss over tasks: 0.24966, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.01892, Variance: 0.00740
Semantic Loss - Mean: 0.23786, Variance: 0.01124

Test Epoch: 81 
task: sign, mean loss: 0.98172, accuracy: 0.85799, avg. loss over tasks: 0.98172
Diversity Loss - Mean: 0.04020, Variance: 0.00900
Semantic Loss - Mean: 0.85811, Variance: 0.02326

Train Epoch: 82 
task: sign, mean loss: 0.11486, accuracy: 0.96739, avg. loss over tasks: 0.11486, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.02614, Variance: 0.00740
Semantic Loss - Mean: 0.14735, Variance: 0.01121

Test Epoch: 82 
task: sign, mean loss: 1.65047, accuracy: 0.63314, avg. loss over tasks: 1.65047
Diversity Loss - Mean: 0.02698, Variance: 0.00899
Semantic Loss - Mean: 1.51214, Variance: 0.02400

Train Epoch: 83 
task: sign, mean loss: 0.13495, accuracy: 0.94565, avg. loss over tasks: 0.13495, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.03594, Variance: 0.00741
Semantic Loss - Mean: 0.18554, Variance: 0.01125

Test Epoch: 83 
task: sign, mean loss: 0.71428, accuracy: 0.76923, avg. loss over tasks: 0.71428
Diversity Loss - Mean: -0.02401, Variance: 0.00902
Semantic Loss - Mean: 0.65160, Variance: 0.02406

Train Epoch: 84 
task: sign, mean loss: 0.08781, accuracy: 0.97826, avg. loss over tasks: 0.08781, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.03275, Variance: 0.00741
Semantic Loss - Mean: 0.13884, Variance: 0.01122

Test Epoch: 84 
task: sign, mean loss: 0.33922, accuracy: 0.87574, avg. loss over tasks: 0.33922
Diversity Loss - Mean: -0.02492, Variance: 0.00904
Semantic Loss - Mean: 0.32004, Variance: 0.02387

Train Epoch: 85 
task: sign, mean loss: 0.01005, accuracy: 1.00000, avg. loss over tasks: 0.01005, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.03398, Variance: 0.00743
Semantic Loss - Mean: 0.05126, Variance: 0.01113

Test Epoch: 85 
task: sign, mean loss: 0.33545, accuracy: 0.90533, avg. loss over tasks: 0.33545
Diversity Loss - Mean: -0.03224, Variance: 0.00906
Semantic Loss - Mean: 0.33276, Variance: 0.02371

Train Epoch: 86 
task: sign, mean loss: 0.01464, accuracy: 1.00000, avg. loss over tasks: 0.01464, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.03481, Variance: 0.00744
Semantic Loss - Mean: 0.04946, Variance: 0.01105

Test Epoch: 86 
task: sign, mean loss: 0.35502, accuracy: 0.91124, avg. loss over tasks: 0.35502
Diversity Loss - Mean: -0.02296, Variance: 0.00908
Semantic Loss - Mean: 0.34673, Variance: 0.02353

Train Epoch: 87 
task: sign, mean loss: 0.01026, accuracy: 1.00000, avg. loss over tasks: 0.01026, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.03028, Variance: 0.00745
Semantic Loss - Mean: 0.04609, Variance: 0.01097

Test Epoch: 87 
task: sign, mean loss: 0.35606, accuracy: 0.91716, avg. loss over tasks: 0.35606
Diversity Loss - Mean: -0.02461, Variance: 0.00909
Semantic Loss - Mean: 0.34789, Variance: 0.02336

Train Epoch: 88 
task: sign, mean loss: 0.00353, accuracy: 1.00000, avg. loss over tasks: 0.00353, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.03334, Variance: 0.00746
Semantic Loss - Mean: 0.02639, Variance: 0.01087

Test Epoch: 88 
task: sign, mean loss: 0.28507, accuracy: 0.92308, avg. loss over tasks: 0.28507
Diversity Loss - Mean: -0.02423, Variance: 0.00911
Semantic Loss - Mean: 0.27765, Variance: 0.02319

Train Epoch: 89 
task: sign, mean loss: 0.01914, accuracy: 0.99457, avg. loss over tasks: 0.01914, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.03240, Variance: 0.00747
Semantic Loss - Mean: 0.04306, Variance: 0.01078

Test Epoch: 89 
task: sign, mean loss: 0.29510, accuracy: 0.92308, avg. loss over tasks: 0.29510
Diversity Loss - Mean: -0.01898, Variance: 0.00911
Semantic Loss - Mean: 0.31245, Variance: 0.02303

Train Epoch: 90 
task: sign, mean loss: 0.01205, accuracy: 0.99457, avg. loss over tasks: 0.01205, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.03600, Variance: 0.00749
Semantic Loss - Mean: 0.04651, Variance: 0.01071

Test Epoch: 90 
task: sign, mean loss: 0.71275, accuracy: 0.86982, avg. loss over tasks: 0.71275
Diversity Loss - Mean: -0.01304, Variance: 0.00911
Semantic Loss - Mean: 0.64742, Variance: 0.02287

Train Epoch: 91 
task: sign, mean loss: 0.01840, accuracy: 0.99457, avg. loss over tasks: 0.01840, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.03172, Variance: 0.00750
Semantic Loss - Mean: 0.04779, Variance: 0.01065

Test Epoch: 91 
task: sign, mean loss: 0.54081, accuracy: 0.88166, avg. loss over tasks: 0.54081
Diversity Loss - Mean: -0.01782, Variance: 0.00911
Semantic Loss - Mean: 0.50682, Variance: 0.02274

Train Epoch: 92 
task: sign, mean loss: 0.00738, accuracy: 0.99457, avg. loss over tasks: 0.00738, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.03318, Variance: 0.00751
Semantic Loss - Mean: 0.05348, Variance: 0.01062

Test Epoch: 92 
task: sign, mean loss: 0.52160, accuracy: 0.88166, avg. loss over tasks: 0.52160
Diversity Loss - Mean: -0.01839, Variance: 0.00911
Semantic Loss - Mean: 0.47265, Variance: 0.02260

Train Epoch: 93 
task: sign, mean loss: 0.00418, accuracy: 1.00000, avg. loss over tasks: 0.00418, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.03359, Variance: 0.00751
Semantic Loss - Mean: 0.03513, Variance: 0.01058

Test Epoch: 93 
task: sign, mean loss: 0.54264, accuracy: 0.89349, avg. loss over tasks: 0.54264
Diversity Loss - Mean: -0.03287, Variance: 0.00912
Semantic Loss - Mean: 0.43701, Variance: 0.02249

Train Epoch: 94 
task: sign, mean loss: 0.00187, accuracy: 1.00000, avg. loss over tasks: 0.00187, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.03467, Variance: 0.00752
Semantic Loss - Mean: 0.02150, Variance: 0.01048

Test Epoch: 94 
task: sign, mean loss: 0.54112, accuracy: 0.88166, avg. loss over tasks: 0.54112
Diversity Loss - Mean: -0.02846, Variance: 0.00913
Semantic Loss - Mean: 0.44812, Variance: 0.02243

Train Epoch: 95 
task: sign, mean loss: 0.00592, accuracy: 1.00000, avg. loss over tasks: 0.00592, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.03923, Variance: 0.00753
Semantic Loss - Mean: 0.03550, Variance: 0.01041

Test Epoch: 95 
task: sign, mean loss: 0.55190, accuracy: 0.88757, avg. loss over tasks: 0.55190
Diversity Loss - Mean: -0.01984, Variance: 0.00912
Semantic Loss - Mean: 0.48174, Variance: 0.02240

Train Epoch: 96 
task: sign, mean loss: 0.01300, accuracy: 0.99457, avg. loss over tasks: 0.01300, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.03308, Variance: 0.00753
Semantic Loss - Mean: 0.03871, Variance: 0.01033

Test Epoch: 96 
task: sign, mean loss: 0.38135, accuracy: 0.90533, avg. loss over tasks: 0.38135
Diversity Loss - Mean: -0.01550, Variance: 0.00912
Semantic Loss - Mean: 0.34181, Variance: 0.02229

Train Epoch: 97 
task: sign, mean loss: 0.05632, accuracy: 0.99457, avg. loss over tasks: 0.05632, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.03905, Variance: 0.00754
Semantic Loss - Mean: 0.07086, Variance: 0.01031

Test Epoch: 97 
task: sign, mean loss: 0.63539, accuracy: 0.88166, avg. loss over tasks: 0.63539
Diversity Loss - Mean: -0.02387, Variance: 0.00913
Semantic Loss - Mean: 0.56673, Variance: 0.02233

Train Epoch: 98 
task: sign, mean loss: 0.00229, accuracy: 1.00000, avg. loss over tasks: 0.00229, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.04028, Variance: 0.00755
Semantic Loss - Mean: 0.02311, Variance: 0.01022

Test Epoch: 98 
task: sign, mean loss: 0.44188, accuracy: 0.89941, avg. loss over tasks: 0.44188
Diversity Loss - Mean: -0.02292, Variance: 0.00913
Semantic Loss - Mean: 0.40116, Variance: 0.02226

Train Epoch: 99 
task: sign, mean loss: 0.05748, accuracy: 0.98913, avg. loss over tasks: 0.05748, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.04006, Variance: 0.00756
Semantic Loss - Mean: 0.07210, Variance: 0.01018

Test Epoch: 99 
task: sign, mean loss: 0.30506, accuracy: 0.91124, avg. loss over tasks: 0.30506
Diversity Loss - Mean: -0.01659, Variance: 0.00913
Semantic Loss - Mean: 0.32007, Variance: 0.02218

Train Epoch: 100 
task: sign, mean loss: 0.02546, accuracy: 0.98913, avg. loss over tasks: 0.02546, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.03970, Variance: 0.00758
Semantic Loss - Mean: 0.06917, Variance: 0.01019

Test Epoch: 100 
task: sign, mean loss: 0.37335, accuracy: 0.91124, avg. loss over tasks: 0.37335
Diversity Loss - Mean: -0.00617, Variance: 0.00913
Semantic Loss - Mean: 0.45329, Variance: 0.02216

Train Epoch: 101 
task: sign, mean loss: 0.01667, accuracy: 0.98913, avg. loss over tasks: 0.01667, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.03920, Variance: 0.00758
Semantic Loss - Mean: 0.03764, Variance: 0.01013

Test Epoch: 101 
task: sign, mean loss: 0.41015, accuracy: 0.91124, avg. loss over tasks: 0.41015
Diversity Loss - Mean: -0.01240, Variance: 0.00913
Semantic Loss - Mean: 0.46762, Variance: 0.02211

Train Epoch: 102 
task: sign, mean loss: 0.00460, accuracy: 1.00000, avg. loss over tasks: 0.00460, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.03900, Variance: 0.00759
Semantic Loss - Mean: 0.03026, Variance: 0.01008

Test Epoch: 102 
task: sign, mean loss: 0.41386, accuracy: 0.89941, avg. loss over tasks: 0.41386
Diversity Loss - Mean: -0.01749, Variance: 0.00914
Semantic Loss - Mean: 0.45471, Variance: 0.02202

Train Epoch: 103 
task: sign, mean loss: 0.01226, accuracy: 1.00000, avg. loss over tasks: 0.01226, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.03866, Variance: 0.00761
Semantic Loss - Mean: 0.04917, Variance: 0.01003

Test Epoch: 103 
task: sign, mean loss: 0.43501, accuracy: 0.91124, avg. loss over tasks: 0.43501
Diversity Loss - Mean: -0.01683, Variance: 0.00914
Semantic Loss - Mean: 0.47042, Variance: 0.02191

Train Epoch: 104 
task: sign, mean loss: 0.00229, accuracy: 1.00000, avg. loss over tasks: 0.00229, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.03902, Variance: 0.00762
Semantic Loss - Mean: 0.01891, Variance: 0.00996

Test Epoch: 104 
task: sign, mean loss: 0.39812, accuracy: 0.91716, avg. loss over tasks: 0.39812
Diversity Loss - Mean: -0.01623, Variance: 0.00914
Semantic Loss - Mean: 0.42623, Variance: 0.02179

Train Epoch: 105 
task: sign, mean loss: 0.00536, accuracy: 1.00000, avg. loss over tasks: 0.00536, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.04138, Variance: 0.00762
Semantic Loss - Mean: 0.02717, Variance: 0.00988

Test Epoch: 105 
task: sign, mean loss: 0.41627, accuracy: 0.91124, avg. loss over tasks: 0.41627
Diversity Loss - Mean: -0.01646, Variance: 0.00915
Semantic Loss - Mean: 0.46553, Variance: 0.02171

Train Epoch: 106 
task: sign, mean loss: 0.00451, accuracy: 1.00000, avg. loss over tasks: 0.00451, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.04120, Variance: 0.00763
Semantic Loss - Mean: 0.02617, Variance: 0.00981

Test Epoch: 106 
task: sign, mean loss: 0.47405, accuracy: 0.89349, avg. loss over tasks: 0.47405
Diversity Loss - Mean: -0.01641, Variance: 0.00915
Semantic Loss - Mean: 0.51258, Variance: 0.02162

Train Epoch: 107 
task: sign, mean loss: 0.00480, accuracy: 1.00000, avg. loss over tasks: 0.00480, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.03838, Variance: 0.00764
Semantic Loss - Mean: 0.02769, Variance: 0.00976

Test Epoch: 107 
task: sign, mean loss: 0.49929, accuracy: 0.89349, avg. loss over tasks: 0.49929
Diversity Loss - Mean: -0.02235, Variance: 0.00915
Semantic Loss - Mean: 0.51120, Variance: 0.02149

Train Epoch: 108 
task: sign, mean loss: 0.00186, accuracy: 1.00000, avg. loss over tasks: 0.00186, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.04050, Variance: 0.00765
Semantic Loss - Mean: 0.01993, Variance: 0.00969

Test Epoch: 108 
task: sign, mean loss: 0.42406, accuracy: 0.91124, avg. loss over tasks: 0.42406
Diversity Loss - Mean: -0.02677, Variance: 0.00916
Semantic Loss - Mean: 0.42688, Variance: 0.02135

Train Epoch: 109 
task: sign, mean loss: 0.00352, accuracy: 1.00000, avg. loss over tasks: 0.00352, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.04271, Variance: 0.00766
Semantic Loss - Mean: 0.02731, Variance: 0.00963

Test Epoch: 109 
task: sign, mean loss: 0.44155, accuracy: 0.89941, avg. loss over tasks: 0.44155
Diversity Loss - Mean: -0.02386, Variance: 0.00916
Semantic Loss - Mean: 0.45454, Variance: 0.02121

Train Epoch: 110 
task: sign, mean loss: 0.00161, accuracy: 1.00000, avg. loss over tasks: 0.00161, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.04116, Variance: 0.00766
Semantic Loss - Mean: 0.01139, Variance: 0.00954

Test Epoch: 110 
task: sign, mean loss: 0.47085, accuracy: 0.89941, avg. loss over tasks: 0.47085
Diversity Loss - Mean: -0.02070, Variance: 0.00916
Semantic Loss - Mean: 0.50088, Variance: 0.02110

Train Epoch: 111 
task: sign, mean loss: 0.00165, accuracy: 1.00000, avg. loss over tasks: 0.00165, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.04372, Variance: 0.00767
Semantic Loss - Mean: 0.02935, Variance: 0.00949

Test Epoch: 111 
task: sign, mean loss: 0.45069, accuracy: 0.89349, avg. loss over tasks: 0.45069
Diversity Loss - Mean: -0.02366, Variance: 0.00917
Semantic Loss - Mean: 0.46253, Variance: 0.02095

Train Epoch: 112 
task: sign, mean loss: 0.00101, accuracy: 1.00000, avg. loss over tasks: 0.00101, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.04214, Variance: 0.00768
Semantic Loss - Mean: 0.01734, Variance: 0.00941

Test Epoch: 112 
task: sign, mean loss: 0.45891, accuracy: 0.90533, avg. loss over tasks: 0.45891
Diversity Loss - Mean: -0.02236, Variance: 0.00917
Semantic Loss - Mean: 0.46724, Variance: 0.02082

Train Epoch: 113 
task: sign, mean loss: 0.00158, accuracy: 1.00000, avg. loss over tasks: 0.00158, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.04425, Variance: 0.00769
Semantic Loss - Mean: 0.01518, Variance: 0.00934

Test Epoch: 113 
task: sign, mean loss: 0.47258, accuracy: 0.91124, avg. loss over tasks: 0.47258
Diversity Loss - Mean: -0.02376, Variance: 0.00917
Semantic Loss - Mean: 0.46794, Variance: 0.02069

Train Epoch: 114 
task: sign, mean loss: 0.00141, accuracy: 1.00000, avg. loss over tasks: 0.00141, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.04270, Variance: 0.00770
Semantic Loss - Mean: 0.02501, Variance: 0.00929

Test Epoch: 114 
task: sign, mean loss: 0.48680, accuracy: 0.91124, avg. loss over tasks: 0.48680
Diversity Loss - Mean: -0.02440, Variance: 0.00918
Semantic Loss - Mean: 0.48448, Variance: 0.02056

Train Epoch: 115 
task: sign, mean loss: 0.00263, accuracy: 1.00000, avg. loss over tasks: 0.00263, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.04218, Variance: 0.00770
Semantic Loss - Mean: 0.01529, Variance: 0.00921

Test Epoch: 115 
task: sign, mean loss: 0.50435, accuracy: 0.89941, avg. loss over tasks: 0.50435
Diversity Loss - Mean: -0.02109, Variance: 0.00918
Semantic Loss - Mean: 0.51923, Variance: 0.02045

Train Epoch: 116 
task: sign, mean loss: 0.01857, accuracy: 0.98913, avg. loss over tasks: 0.01857, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.04431, Variance: 0.00771
Semantic Loss - Mean: 0.03101, Variance: 0.00914

Test Epoch: 116 
task: sign, mean loss: 0.41555, accuracy: 0.91716, avg. loss over tasks: 0.41555
Diversity Loss - Mean: -0.02791, Variance: 0.00918
Semantic Loss - Mean: 0.41015, Variance: 0.02032

Train Epoch: 117 
task: sign, mean loss: 0.00116, accuracy: 1.00000, avg. loss over tasks: 0.00116, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.04607, Variance: 0.00772
Semantic Loss - Mean: 0.01490, Variance: 0.00907

Test Epoch: 117 
task: sign, mean loss: 0.51056, accuracy: 0.91716, avg. loss over tasks: 0.51056
Diversity Loss - Mean: -0.02948, Variance: 0.00918
Semantic Loss - Mean: 0.47755, Variance: 0.02020

Train Epoch: 118 
task: sign, mean loss: 0.00391, accuracy: 1.00000, avg. loss over tasks: 0.00391, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.04415, Variance: 0.00773
Semantic Loss - Mean: 0.01656, Variance: 0.00901

Test Epoch: 118 
task: sign, mean loss: 0.53196, accuracy: 0.91124, avg. loss over tasks: 0.53196
Diversity Loss - Mean: -0.02752, Variance: 0.00919
Semantic Loss - Mean: 0.50168, Variance: 0.02009

Train Epoch: 119 
task: sign, mean loss: 0.00246, accuracy: 1.00000, avg. loss over tasks: 0.00246, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.04743, Variance: 0.00774
Semantic Loss - Mean: 0.01736, Variance: 0.00894

Test Epoch: 119 
task: sign, mean loss: 0.48996, accuracy: 0.92308, avg. loss over tasks: 0.48996
Diversity Loss - Mean: -0.02753, Variance: 0.00919
Semantic Loss - Mean: 0.46337, Variance: 0.01997

Train Epoch: 120 
task: sign, mean loss: 0.00081, accuracy: 1.00000, avg. loss over tasks: 0.00081, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.04832, Variance: 0.00774
Semantic Loss - Mean: 0.01999, Variance: 0.00888

Test Epoch: 120 
task: sign, mean loss: 0.44699, accuracy: 0.91716, avg. loss over tasks: 0.44699
Diversity Loss - Mean: -0.03208, Variance: 0.00919
Semantic Loss - Mean: 0.41975, Variance: 0.01983

Train Epoch: 121 
task: sign, mean loss: 0.00154, accuracy: 1.00000, avg. loss over tasks: 0.00154, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.04931, Variance: 0.00775
Semantic Loss - Mean: 0.02629, Variance: 0.00882

Test Epoch: 121 
task: sign, mean loss: 0.46449, accuracy: 0.91716, avg. loss over tasks: 0.46449
Diversity Loss - Mean: -0.02643, Variance: 0.00920
Semantic Loss - Mean: 0.46021, Variance: 0.01970

Train Epoch: 122 
task: sign, mean loss: 0.00311, accuracy: 1.00000, avg. loss over tasks: 0.00311, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.04781, Variance: 0.00776
Semantic Loss - Mean: 0.02259, Variance: 0.00877

Test Epoch: 122 
task: sign, mean loss: 0.44739, accuracy: 0.92308, avg. loss over tasks: 0.44739
Diversity Loss - Mean: -0.02873, Variance: 0.00920
Semantic Loss - Mean: 0.43217, Variance: 0.01956

Train Epoch: 123 
task: sign, mean loss: 0.00080, accuracy: 1.00000, avg. loss over tasks: 0.00080, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.05036, Variance: 0.00777
Semantic Loss - Mean: 0.01297, Variance: 0.00871

Test Epoch: 123 
task: sign, mean loss: 0.46563, accuracy: 0.92308, avg. loss over tasks: 0.46563
Diversity Loss - Mean: -0.02571, Variance: 0.00920
Semantic Loss - Mean: 0.45202, Variance: 0.01943

Train Epoch: 124 
task: sign, mean loss: 0.00131, accuracy: 1.00000, avg. loss over tasks: 0.00131, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.04670, Variance: 0.00778
Semantic Loss - Mean: 0.01380, Variance: 0.00865

Test Epoch: 124 
task: sign, mean loss: 0.41715, accuracy: 0.92899, avg. loss over tasks: 0.41715
Diversity Loss - Mean: -0.02813, Variance: 0.00920
Semantic Loss - Mean: 0.40695, Variance: 0.01929

Train Epoch: 125 
task: sign, mean loss: 0.00112, accuracy: 1.00000, avg. loss over tasks: 0.00112, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.04915, Variance: 0.00779
Semantic Loss - Mean: 0.01056, Variance: 0.00859

Test Epoch: 125 
task: sign, mean loss: 0.45234, accuracy: 0.92308, avg. loss over tasks: 0.45234
Diversity Loss - Mean: -0.02633, Variance: 0.00921
Semantic Loss - Mean: 0.44040, Variance: 0.01916

Train Epoch: 126 
task: sign, mean loss: 0.00721, accuracy: 0.99457, avg. loss over tasks: 0.00721, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.04600, Variance: 0.00779
Semantic Loss - Mean: 0.01421, Variance: 0.00852

Test Epoch: 126 
task: sign, mean loss: 0.41672, accuracy: 0.92308, avg. loss over tasks: 0.41672
Diversity Loss - Mean: -0.03168, Variance: 0.00921
Semantic Loss - Mean: 0.40008, Variance: 0.01903

Train Epoch: 127 
task: sign, mean loss: 0.02024, accuracy: 0.99457, avg. loss over tasks: 0.02024, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.04591, Variance: 0.00780
Semantic Loss - Mean: 0.02592, Variance: 0.00846

Test Epoch: 127 
task: sign, mean loss: 0.40254, accuracy: 0.92308, avg. loss over tasks: 0.40254
Diversity Loss - Mean: -0.03146, Variance: 0.00922
Semantic Loss - Mean: 0.38779, Variance: 0.01890

Train Epoch: 128 
task: sign, mean loss: 0.00093, accuracy: 1.00000, avg. loss over tasks: 0.00093, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.04658, Variance: 0.00781
Semantic Loss - Mean: 0.00906, Variance: 0.00840

Test Epoch: 128 
task: sign, mean loss: 0.42477, accuracy: 0.91716, avg. loss over tasks: 0.42477
Diversity Loss - Mean: -0.02542, Variance: 0.00922
Semantic Loss - Mean: 0.42973, Variance: 0.01878

Train Epoch: 129 
task: sign, mean loss: 0.00291, accuracy: 1.00000, avg. loss over tasks: 0.00291, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.04813, Variance: 0.00781
Semantic Loss - Mean: 0.01705, Variance: 0.00834

Test Epoch: 129 
task: sign, mean loss: 0.38329, accuracy: 0.91716, avg. loss over tasks: 0.38329
Diversity Loss - Mean: -0.03260, Variance: 0.00922
Semantic Loss - Mean: 0.36820, Variance: 0.01865

Train Epoch: 130 
task: sign, mean loss: 0.00261, accuracy: 1.00000, avg. loss over tasks: 0.00261, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.04694, Variance: 0.00782
Semantic Loss - Mean: 0.01592, Variance: 0.00828

Test Epoch: 130 
task: sign, mean loss: 0.38921, accuracy: 0.92308, avg. loss over tasks: 0.38921
Diversity Loss - Mean: -0.02923, Variance: 0.00923
Semantic Loss - Mean: 0.38869, Variance: 0.01852

Train Epoch: 131 
task: sign, mean loss: 0.00159, accuracy: 1.00000, avg. loss over tasks: 0.00159, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.04819, Variance: 0.00783
Semantic Loss - Mean: 0.01536, Variance: 0.00823

Test Epoch: 131 
task: sign, mean loss: 0.46294, accuracy: 0.91716, avg. loss over tasks: 0.46294
Diversity Loss - Mean: -0.02314, Variance: 0.00922
Semantic Loss - Mean: 0.46031, Variance: 0.01840

Train Epoch: 132 
task: sign, mean loss: 0.00104, accuracy: 1.00000, avg. loss over tasks: 0.00104, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.05144, Variance: 0.00783
Semantic Loss - Mean: 0.00876, Variance: 0.00817

Test Epoch: 132 
task: sign, mean loss: 0.43109, accuracy: 0.92308, avg. loss over tasks: 0.43109
Diversity Loss - Mean: -0.02940, Variance: 0.00923
Semantic Loss - Mean: 0.41958, Variance: 0.01829

Train Epoch: 133 
task: sign, mean loss: 0.00179, accuracy: 1.00000, avg. loss over tasks: 0.00179, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.05178, Variance: 0.00784
Semantic Loss - Mean: 0.01615, Variance: 0.00812

Test Epoch: 133 
task: sign, mean loss: 0.46272, accuracy: 0.92308, avg. loss over tasks: 0.46272
Diversity Loss - Mean: -0.02817, Variance: 0.00923
Semantic Loss - Mean: 0.44297, Variance: 0.01817

Train Epoch: 134 
task: sign, mean loss: 0.00117, accuracy: 1.00000, avg. loss over tasks: 0.00117, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.05089, Variance: 0.00785
Semantic Loss - Mean: 0.00805, Variance: 0.00806

Test Epoch: 134 
task: sign, mean loss: 0.41301, accuracy: 0.92308, avg. loss over tasks: 0.41301
Diversity Loss - Mean: -0.03071, Variance: 0.00923
Semantic Loss - Mean: 0.40656, Variance: 0.01806

Train Epoch: 135 
task: sign, mean loss: 0.00392, accuracy: 1.00000, avg. loss over tasks: 0.00392, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.05027, Variance: 0.00786
Semantic Loss - Mean: 0.01068, Variance: 0.00800

Test Epoch: 135 
task: sign, mean loss: 0.44776, accuracy: 0.91124, avg. loss over tasks: 0.44776
Diversity Loss - Mean: -0.03464, Variance: 0.00923
Semantic Loss - Mean: 0.41281, Variance: 0.01794

Train Epoch: 136 
task: sign, mean loss: 0.00078, accuracy: 1.00000, avg. loss over tasks: 0.00078, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.05125, Variance: 0.00786
Semantic Loss - Mean: 0.00993, Variance: 0.00795

Test Epoch: 136 
task: sign, mean loss: 0.41320, accuracy: 0.92308, avg. loss over tasks: 0.41320
Diversity Loss - Mean: -0.03150, Variance: 0.00924
Semantic Loss - Mean: 0.40411, Variance: 0.01783

Train Epoch: 137 
task: sign, mean loss: 0.00088, accuracy: 1.00000, avg. loss over tasks: 0.00088, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.05116, Variance: 0.00787
Semantic Loss - Mean: 0.01579, Variance: 0.00790

Test Epoch: 137 
task: sign, mean loss: 0.39756, accuracy: 0.92308, avg. loss over tasks: 0.39756
Diversity Loss - Mean: -0.03051, Variance: 0.00924
Semantic Loss - Mean: 0.39347, Variance: 0.01772

Train Epoch: 138 
task: sign, mean loss: 0.00103, accuracy: 1.00000, avg. loss over tasks: 0.00103, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.05035, Variance: 0.00788
Semantic Loss - Mean: 0.01598, Variance: 0.00785

Test Epoch: 138 
task: sign, mean loss: 0.40894, accuracy: 0.92308, avg. loss over tasks: 0.40894
Diversity Loss - Mean: -0.02836, Variance: 0.00924
Semantic Loss - Mean: 0.41252, Variance: 0.01761

Train Epoch: 139 
task: sign, mean loss: 0.00096, accuracy: 1.00000, avg. loss over tasks: 0.00096, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.05060, Variance: 0.00788
Semantic Loss - Mean: 0.01291, Variance: 0.00780

Test Epoch: 139 
task: sign, mean loss: 0.40774, accuracy: 0.92308, avg. loss over tasks: 0.40774
Diversity Loss - Mean: -0.02842, Variance: 0.00925
Semantic Loss - Mean: 0.41259, Variance: 0.01751

Train Epoch: 140 
task: sign, mean loss: 0.00310, accuracy: 1.00000, avg. loss over tasks: 0.00310, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.05161, Variance: 0.00789
Semantic Loss - Mean: 0.02649, Variance: 0.00776

Test Epoch: 140 
task: sign, mean loss: 0.40938, accuracy: 0.92308, avg. loss over tasks: 0.40938
Diversity Loss - Mean: -0.03293, Variance: 0.00925
Semantic Loss - Mean: 0.39332, Variance: 0.01740

Train Epoch: 141 
task: sign, mean loss: 0.00095, accuracy: 1.00000, avg. loss over tasks: 0.00095, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.05047, Variance: 0.00790
Semantic Loss - Mean: 0.01565, Variance: 0.00771

Test Epoch: 141 
task: sign, mean loss: 0.39572, accuracy: 0.92308, avg. loss over tasks: 0.39572
Diversity Loss - Mean: -0.03282, Variance: 0.00925
Semantic Loss - Mean: 0.38771, Variance: 0.01730

Train Epoch: 142 
task: sign, mean loss: 0.00159, accuracy: 1.00000, avg. loss over tasks: 0.00159, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.04939, Variance: 0.00790
Semantic Loss - Mean: 0.02079, Variance: 0.00766

Test Epoch: 142 
task: sign, mean loss: 0.37288, accuracy: 0.92308, avg. loss over tasks: 0.37288
Diversity Loss - Mean: -0.03470, Variance: 0.00926
Semantic Loss - Mean: 0.36267, Variance: 0.01719

Train Epoch: 143 
task: sign, mean loss: 0.00223, accuracy: 1.00000, avg. loss over tasks: 0.00223, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.04958, Variance: 0.00791
Semantic Loss - Mean: 0.01052, Variance: 0.00761

Test Epoch: 143 
task: sign, mean loss: 0.40786, accuracy: 0.92308, avg. loss over tasks: 0.40786
Diversity Loss - Mean: -0.03053, Variance: 0.00926
Semantic Loss - Mean: 0.40245, Variance: 0.01709

Train Epoch: 144 
task: sign, mean loss: 0.00117, accuracy: 1.00000, avg. loss over tasks: 0.00117, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.04989, Variance: 0.00791
Semantic Loss - Mean: 0.01081, Variance: 0.00756

Test Epoch: 144 
task: sign, mean loss: 0.40887, accuracy: 0.92308, avg. loss over tasks: 0.40887
Diversity Loss - Mean: -0.03057, Variance: 0.00926
Semantic Loss - Mean: 0.40570, Variance: 0.01700

Train Epoch: 145 
task: sign, mean loss: 0.00227, accuracy: 1.00000, avg. loss over tasks: 0.00227, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.04919, Variance: 0.00792
Semantic Loss - Mean: 0.01898, Variance: 0.00752

Test Epoch: 145 
task: sign, mean loss: 0.40821, accuracy: 0.92308, avg. loss over tasks: 0.40821
Diversity Loss - Mean: -0.03208, Variance: 0.00926
Semantic Loss - Mean: 0.39774, Variance: 0.01690

Train Epoch: 146 
task: sign, mean loss: 0.00116, accuracy: 1.00000, avg. loss over tasks: 0.00116, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.04901, Variance: 0.00793
Semantic Loss - Mean: 0.02436, Variance: 0.00750

Test Epoch: 146 
task: sign, mean loss: 0.47712, accuracy: 0.91716, avg. loss over tasks: 0.47712
Diversity Loss - Mean: -0.02165, Variance: 0.00926
Semantic Loss - Mean: 0.48127, Variance: 0.01681

Train Epoch: 147 
task: sign, mean loss: 0.00071, accuracy: 1.00000, avg. loss over tasks: 0.00071, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.05173, Variance: 0.00793
Semantic Loss - Mean: 0.01684, Variance: 0.00747

Test Epoch: 147 
task: sign, mean loss: 0.44049, accuracy: 0.92308, avg. loss over tasks: 0.44049
Diversity Loss - Mean: -0.02654, Variance: 0.00926
Semantic Loss - Mean: 0.44804, Variance: 0.01672

Train Epoch: 148 
task: sign, mean loss: 0.00151, accuracy: 1.00000, avg. loss over tasks: 0.00151, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.04837, Variance: 0.00794
Semantic Loss - Mean: 0.00989, Variance: 0.00742

Test Epoch: 148 
task: sign, mean loss: 0.43224, accuracy: 0.91716, avg. loss over tasks: 0.43224
Diversity Loss - Mean: -0.02978, Variance: 0.00926
Semantic Loss - Mean: 0.42597, Variance: 0.01663

Train Epoch: 149 
task: sign, mean loss: 0.00132, accuracy: 1.00000, avg. loss over tasks: 0.00132, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.05031, Variance: 0.00794
Semantic Loss - Mean: 0.02575, Variance: 0.00739

Test Epoch: 149 
task: sign, mean loss: 0.41085, accuracy: 0.92308, avg. loss over tasks: 0.41085
Diversity Loss - Mean: -0.03254, Variance: 0.00927
Semantic Loss - Mean: 0.39676, Variance: 0.01653

Train Epoch: 150 
task: sign, mean loss: 0.00121, accuracy: 1.00000, avg. loss over tasks: 0.00121, lr: 3e-07
Diversity Loss - Mean: -0.05016, Variance: 0.00795
Semantic Loss - Mean: 0.00848, Variance: 0.00734

Test Epoch: 150 
task: sign, mean loss: 0.43274, accuracy: 0.92308, avg. loss over tasks: 0.43274
Diversity Loss - Mean: -0.02972, Variance: 0.00927
Semantic Loss - Mean: 0.41943, Variance: 0.01644

