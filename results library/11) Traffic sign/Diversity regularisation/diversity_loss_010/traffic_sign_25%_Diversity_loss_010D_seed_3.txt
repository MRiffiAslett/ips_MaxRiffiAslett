Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09339, accuracy: 0.63587, avg. loss over tasks: 1.09339, lr: 3e-05
Diversity Loss - Mean: -0.00918, Variance: 0.01049
Semantic Loss - Mean: 1.43070, Variance: 0.07258

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17667, accuracy: 0.66272, avg. loss over tasks: 1.17667
Diversity Loss - Mean: -0.02751, Variance: 0.01245
Semantic Loss - Mean: 1.16022, Variance: 0.05344

Train Epoch: 2 
task: sign, mean loss: 0.96407, accuracy: 0.67935, avg. loss over tasks: 0.96407, lr: 6e-05
Diversity Loss - Mean: -0.01094, Variance: 0.01042
Semantic Loss - Mean: 0.98325, Variance: 0.03918

Test Epoch: 2 
task: sign, mean loss: 1.10002, accuracy: 0.66272, avg. loss over tasks: 1.10002
Diversity Loss - Mean: -0.01661, Variance: 0.01186
Semantic Loss - Mean: 1.15462, Variance: 0.03250

Train Epoch: 3 
task: sign, mean loss: 0.80262, accuracy: 0.69565, avg. loss over tasks: 0.80262, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.01375, Variance: 0.01017
Semantic Loss - Mean: 0.99697, Variance: 0.02711

Test Epoch: 3 
task: sign, mean loss: 1.29085, accuracy: 0.61538, avg. loss over tasks: 1.29085
Diversity Loss - Mean: -0.03078, Variance: 0.01111
Semantic Loss - Mean: 1.12548, Variance: 0.02943

Train Epoch: 4 
task: sign, mean loss: 0.73924, accuracy: 0.70652, avg. loss over tasks: 0.73924, lr: 0.00012
Diversity Loss - Mean: -0.02479, Variance: 0.00981
Semantic Loss - Mean: 0.88419, Variance: 0.02103

Test Epoch: 4 
task: sign, mean loss: 1.39774, accuracy: 0.58580, avg. loss over tasks: 1.39774
Diversity Loss - Mean: -0.02162, Variance: 0.01041
Semantic Loss - Mean: 1.07397, Variance: 0.02390

Train Epoch: 5 
task: sign, mean loss: 0.73175, accuracy: 0.72826, avg. loss over tasks: 0.73175, lr: 0.00015
Diversity Loss - Mean: -0.00592, Variance: 0.00940
Semantic Loss - Mean: 0.77602, Variance: 0.01725

Test Epoch: 5 
task: sign, mean loss: 1.78988, accuracy: 0.53846, avg. loss over tasks: 1.78988
Diversity Loss - Mean: -0.01459, Variance: 0.01001
Semantic Loss - Mean: 1.27737, Variance: 0.02219

Train Epoch: 6 
task: sign, mean loss: 0.69851, accuracy: 0.78804, avg. loss over tasks: 0.69851, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.00203, Variance: 0.00920
Semantic Loss - Mean: 0.72424, Variance: 0.01511

Test Epoch: 6 
task: sign, mean loss: 2.24267, accuracy: 0.66272, avg. loss over tasks: 2.24267
Diversity Loss - Mean: 0.05640, Variance: 0.01083
Semantic Loss - Mean: 1.55225, Variance: 0.02566

Train Epoch: 7 
task: sign, mean loss: 0.57502, accuracy: 0.80435, avg. loss over tasks: 0.57502, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.01450, Variance: 0.00923
Semantic Loss - Mean: 0.60374, Variance: 0.01327

Test Epoch: 7 
task: sign, mean loss: 1.72575, accuracy: 0.63905, avg. loss over tasks: 1.72575
Diversity Loss - Mean: 0.02000, Variance: 0.01091
Semantic Loss - Mean: 1.44700, Variance: 0.02544

Train Epoch: 8 
task: sign, mean loss: 0.51150, accuracy: 0.80978, avg. loss over tasks: 0.51150, lr: 0.00024
Diversity Loss - Mean: 0.02024, Variance: 0.00905
Semantic Loss - Mean: 0.55090, Variance: 0.01199

Test Epoch: 8 
task: sign, mean loss: 2.43424, accuracy: 0.40237, avg. loss over tasks: 2.43424
Diversity Loss - Mean: 0.01708, Variance: 0.01083
Semantic Loss - Mean: 1.71927, Variance: 0.02734

Train Epoch: 9 
task: sign, mean loss: 0.68699, accuracy: 0.79348, avg. loss over tasks: 0.68699, lr: 0.00027
Diversity Loss - Mean: 0.02466, Variance: 0.00881
Semantic Loss - Mean: 0.61539, Variance: 0.01103

Test Epoch: 9 
task: sign, mean loss: 3.44465, accuracy: 0.34320, avg. loss over tasks: 3.44465
Diversity Loss - Mean: 0.07262, Variance: 0.01056
Semantic Loss - Mean: 2.44641, Variance: 0.02897

Train Epoch: 10 
task: sign, mean loss: 0.93365, accuracy: 0.67391, avg. loss over tasks: 0.93365, lr: 0.0003
Diversity Loss - Mean: 0.00834, Variance: 0.00873
Semantic Loss - Mean: 0.90855, Variance: 0.01057

Test Epoch: 10 
task: sign, mean loss: 2.79954, accuracy: 0.40237, avg. loss over tasks: 2.79954
Diversity Loss - Mean: 0.10173, Variance: 0.01052
Semantic Loss - Mean: 2.13861, Variance: 0.03031

Train Epoch: 11 
task: sign, mean loss: 0.83143, accuracy: 0.69022, avg. loss over tasks: 0.83143, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.00336, Variance: 0.00883
Semantic Loss - Mean: 0.75989, Variance: 0.01013

Test Epoch: 11 
task: sign, mean loss: 1.73984, accuracy: 0.53254, avg. loss over tasks: 1.73984
Diversity Loss - Mean: 0.01360, Variance: 0.01069
Semantic Loss - Mean: 1.40518, Variance: 0.02960

Train Epoch: 12 
task: sign, mean loss: 0.60839, accuracy: 0.79891, avg. loss over tasks: 0.60839, lr: 0.000299849111021216
Diversity Loss - Mean: 0.02234, Variance: 0.00878
Semantic Loss - Mean: 0.59527, Variance: 0.00949

Test Epoch: 12 
task: sign, mean loss: 3.24058, accuracy: 0.13018, avg. loss over tasks: 3.24058
Diversity Loss - Mean: 0.07588, Variance: 0.01096
Semantic Loss - Mean: 2.23274, Variance: 0.03102

Train Epoch: 13 
task: sign, mean loss: 0.46288, accuracy: 0.80978, avg. loss over tasks: 0.46288, lr: 0.0002996605710257114
Diversity Loss - Mean: 0.02670, Variance: 0.00872
Semantic Loss - Mean: 0.50363, Variance: 0.00893

Test Epoch: 13 
task: sign, mean loss: 2.38845, accuracy: 0.36686, avg. loss over tasks: 2.38845
Diversity Loss - Mean: 0.02929, Variance: 0.01102
Semantic Loss - Mean: 2.05299, Variance: 0.03097

Train Epoch: 14 
task: sign, mean loss: 0.33729, accuracy: 0.88587, avg. loss over tasks: 0.33729, lr: 0.00029939674795518656
Diversity Loss - Mean: 0.03015, Variance: 0.00867
Semantic Loss - Mean: 0.39071, Variance: 0.00857

Test Epoch: 14 
task: sign, mean loss: 2.52013, accuracy: 0.66272, avg. loss over tasks: 2.52013
Diversity Loss - Mean: 0.09546, Variance: 0.01083
Semantic Loss - Mean: 2.18153, Variance: 0.03151

Train Epoch: 15 
task: sign, mean loss: 0.43080, accuracy: 0.83152, avg. loss over tasks: 0.43080, lr: 0.0002990577746525024
Diversity Loss - Mean: 0.03025, Variance: 0.00861
Semantic Loss - Mean: 0.45147, Variance: 0.00820

Test Epoch: 15 
task: sign, mean loss: 1.68297, accuracy: 0.50296, avg. loss over tasks: 1.68297
Diversity Loss - Mean: 0.00786, Variance: 0.01086
Semantic Loss - Mean: 1.58712, Variance: 0.03126

Train Epoch: 16 
task: sign, mean loss: 0.36875, accuracy: 0.82609, avg. loss over tasks: 0.36875, lr: 0.000298643821800925
Diversity Loss - Mean: 0.02991, Variance: 0.00857
Semantic Loss - Mean: 0.37864, Variance: 0.00784

Test Epoch: 16 
task: sign, mean loss: 1.90250, accuracy: 0.51479, avg. loss over tasks: 1.90250
Diversity Loss - Mean: 0.01881, Variance: 0.01084
Semantic Loss - Mean: 1.76838, Variance: 0.03082

Train Epoch: 17 
task: sign, mean loss: 0.26659, accuracy: 0.88587, avg. loss over tasks: 0.26659, lr: 0.0002981550978381814
Diversity Loss - Mean: 0.03797, Variance: 0.00856
Semantic Loss - Mean: 0.29301, Variance: 0.00757

Test Epoch: 17 
task: sign, mean loss: 3.04904, accuracy: 0.26627, avg. loss over tasks: 3.04904
Diversity Loss - Mean: 0.03920, Variance: 0.01083
Semantic Loss - Mean: 2.77215, Variance: 0.03144

Train Epoch: 18 
task: sign, mean loss: 0.40734, accuracy: 0.89674, avg. loss over tasks: 0.40734, lr: 0.00029759184885150465
Diversity Loss - Mean: 0.03239, Variance: 0.00861
Semantic Loss - Mean: 0.42400, Variance: 0.00756

Test Epoch: 18 
task: sign, mean loss: 2.33512, accuracy: 0.66272, avg. loss over tasks: 2.33512
Diversity Loss - Mean: 0.01612, Variance: 0.01102
Semantic Loss - Mean: 2.16704, Variance: 0.03124

Train Epoch: 19 
task: sign, mean loss: 0.80162, accuracy: 0.70652, avg. loss over tasks: 0.80162, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.03085, Variance: 0.00888
Semantic Loss - Mean: 0.82702, Variance: 0.00746

Test Epoch: 19 
task: sign, mean loss: 1.44953, accuracy: 0.50296, avg. loss over tasks: 1.44953
Diversity Loss - Mean: -0.05491, Variance: 0.01115
Semantic Loss - Mean: 1.42902, Variance: 0.03060

Train Epoch: 20 
task: sign, mean loss: 0.58836, accuracy: 0.72283, avg. loss over tasks: 0.58836, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.02012, Variance: 0.00907
Semantic Loss - Mean: 0.56867, Variance: 0.00724

Test Epoch: 20 
task: sign, mean loss: 1.70913, accuracy: 0.46746, avg. loss over tasks: 1.70913
Diversity Loss - Mean: -0.01858, Variance: 0.01123
Semantic Loss - Mean: 1.54665, Variance: 0.03001

Train Epoch: 21 
task: sign, mean loss: 0.33169, accuracy: 0.86413, avg. loss over tasks: 0.33169, lr: 0.00029545797462844647
Diversity Loss - Mean: 0.01272, Variance: 0.00912
Semantic Loss - Mean: 0.34971, Variance: 0.00705

Test Epoch: 21 
task: sign, mean loss: 2.19698, accuracy: 0.47929, avg. loss over tasks: 2.19698
Diversity Loss - Mean: -0.00792, Variance: 0.01132
Semantic Loss - Mean: 1.93148, Variance: 0.03129

Train Epoch: 22 
task: sign, mean loss: 0.37705, accuracy: 0.89130, avg. loss over tasks: 0.37705, lr: 0.0002945998346752736
Diversity Loss - Mean: 0.04594, Variance: 0.00908
Semantic Loss - Mean: 0.41550, Variance: 0.00701

Test Epoch: 22 
task: sign, mean loss: 1.93707, accuracy: 0.48521, avg. loss over tasks: 1.93707
Diversity Loss - Mean: 0.03321, Variance: 0.01137
Semantic Loss - Mean: 1.75982, Variance: 0.03096

Train Epoch: 23 
task: sign, mean loss: 0.25413, accuracy: 0.90761, avg. loss over tasks: 0.25413, lr: 0.0002936689598802368
Diversity Loss - Mean: 0.03923, Variance: 0.00904
Semantic Loss - Mean: 0.29494, Variance: 0.00678

Test Epoch: 23 
task: sign, mean loss: 2.39639, accuracy: 0.33728, avg. loss over tasks: 2.39639
Diversity Loss - Mean: 0.01495, Variance: 0.01134
Semantic Loss - Mean: 2.07421, Variance: 0.03074

Train Epoch: 24 
task: sign, mean loss: 0.26954, accuracy: 0.89130, avg. loss over tasks: 0.26954, lr: 0.00029266581896682876
Diversity Loss - Mean: 0.03260, Variance: 0.00904
Semantic Loss - Mean: 0.31110, Variance: 0.00666

Test Epoch: 24 
task: sign, mean loss: 2.56347, accuracy: 0.28994, avg. loss over tasks: 2.56347
Diversity Loss - Mean: 0.04705, Variance: 0.01139
Semantic Loss - Mean: 2.35218, Variance: 0.03112

Train Epoch: 25 
task: sign, mean loss: 0.31818, accuracy: 0.88587, avg. loss over tasks: 0.31818, lr: 0.00029159091704670885
Diversity Loss - Mean: 0.02818, Variance: 0.00905
Semantic Loss - Mean: 0.36490, Variance: 0.00661

Test Epoch: 25 
task: sign, mean loss: 1.94313, accuracy: 0.63905, avg. loss over tasks: 1.94313
Diversity Loss - Mean: -0.01347, Variance: 0.01152
Semantic Loss - Mean: 1.76592, Variance: 0.03278

Train Epoch: 26 
task: sign, mean loss: 0.39693, accuracy: 0.84783, avg. loss over tasks: 0.39693, lr: 0.00029044479536536455
Diversity Loss - Mean: 0.01435, Variance: 0.00911
Semantic Loss - Mean: 0.44100, Variance: 0.00661

Test Epoch: 26 
task: sign, mean loss: 2.18450, accuracy: 0.44379, avg. loss over tasks: 2.18450
Diversity Loss - Mean: 0.01575, Variance: 0.01153
Semantic Loss - Mean: 1.76790, Variance: 0.03499

Train Epoch: 27 
task: sign, mean loss: 0.33631, accuracy: 0.89130, avg. loss over tasks: 0.33631, lr: 0.000289228031029578
Diversity Loss - Mean: 0.02054, Variance: 0.00914
Semantic Loss - Mean: 0.36641, Variance: 0.00647

Test Epoch: 27 
task: sign, mean loss: 1.33905, accuracy: 0.54438, avg. loss over tasks: 1.33905
Diversity Loss - Mean: 0.00068, Variance: 0.01167
Semantic Loss - Mean: 1.29793, Variance: 0.03483

Train Epoch: 28 
task: sign, mean loss: 0.18234, accuracy: 0.94022, avg. loss over tasks: 0.18234, lr: 0.0002879412367168349
Diversity Loss - Mean: 0.03255, Variance: 0.00915
Semantic Loss - Mean: 0.22829, Variance: 0.00631

Test Epoch: 28 
task: sign, mean loss: 1.54235, accuracy: 0.37870, avg. loss over tasks: 1.54235
Diversity Loss - Mean: 0.02280, Variance: 0.01177
Semantic Loss - Mean: 1.48470, Variance: 0.03432

Train Epoch: 29 
task: sign, mean loss: 0.23069, accuracy: 0.94565, avg. loss over tasks: 0.23069, lr: 0.00028658506036682353
Diversity Loss - Mean: 0.03703, Variance: 0.00916
Semantic Loss - Mean: 0.25986, Variance: 0.00634

Test Epoch: 29 
task: sign, mean loss: 1.28985, accuracy: 0.59763, avg. loss over tasks: 1.28985
Diversity Loss - Mean: 0.03380, Variance: 0.01180
Semantic Loss - Mean: 1.21358, Variance: 0.03381

Train Epoch: 30 
task: sign, mean loss: 0.15412, accuracy: 0.94022, avg. loss over tasks: 0.15412, lr: 0.00028516018485517746
Diversity Loss - Mean: 0.04080, Variance: 0.00917
Semantic Loss - Mean: 0.19946, Variance: 0.00623

Test Epoch: 30 
task: sign, mean loss: 1.65578, accuracy: 0.49704, avg. loss over tasks: 1.65578
Diversity Loss - Mean: 0.03056, Variance: 0.01186
Semantic Loss - Mean: 1.54485, Variance: 0.03401

Train Epoch: 31 
task: sign, mean loss: 0.19737, accuracy: 0.91304, avg. loss over tasks: 0.19737, lr: 0.00028366732764962686
Diversity Loss - Mean: 0.03590, Variance: 0.00919
Semantic Loss - Mean: 0.22018, Variance: 0.00611

Test Epoch: 31 
task: sign, mean loss: 1.72420, accuracy: 0.38462, avg. loss over tasks: 1.72420
Diversity Loss - Mean: 0.02437, Variance: 0.01190
Semantic Loss - Mean: 1.43529, Variance: 0.03349

Train Epoch: 32 
task: sign, mean loss: 0.13911, accuracy: 0.96739, avg. loss over tasks: 0.13911, lr: 0.00028210724044873213
Diversity Loss - Mean: 0.04820, Variance: 0.00918
Semantic Loss - Mean: 0.17297, Variance: 0.00602

Test Epoch: 32 
task: sign, mean loss: 1.06150, accuracy: 0.63905, avg. loss over tasks: 1.06150
Diversity Loss - Mean: 0.04036, Variance: 0.01194
Semantic Loss - Mean: 0.99332, Variance: 0.03306

Train Epoch: 33 
task: sign, mean loss: 0.13069, accuracy: 0.97826, avg. loss over tasks: 0.13069, lr: 0.00028048070880338095
Diversity Loss - Mean: 0.06099, Variance: 0.00914
Semantic Loss - Mean: 0.13608, Variance: 0.00592

Test Epoch: 33 
task: sign, mean loss: 1.07433, accuracy: 0.63314, avg. loss over tasks: 1.07433
Diversity Loss - Mean: 0.02781, Variance: 0.01199
Semantic Loss - Mean: 1.12661, Variance: 0.03286

Train Epoch: 34 
task: sign, mean loss: 0.21578, accuracy: 0.93478, avg. loss over tasks: 0.21578, lr: 0.00027878855172123963
Diversity Loss - Mean: 0.04061, Variance: 0.00912
Semantic Loss - Mean: 0.23884, Variance: 0.00588

Test Epoch: 34 
task: sign, mean loss: 1.87327, accuracy: 0.68639, avg. loss over tasks: 1.87327
Diversity Loss - Mean: -0.00834, Variance: 0.01205
Semantic Loss - Mean: 1.71359, Variance: 0.03268

Train Epoch: 35 
task: sign, mean loss: 0.24338, accuracy: 0.91848, avg. loss over tasks: 0.24338, lr: 0.00027703162125435835
Diversity Loss - Mean: 0.01115, Variance: 0.00914
Semantic Loss - Mean: 0.27396, Variance: 0.00583

Test Epoch: 35 
task: sign, mean loss: 2.19088, accuracy: 0.42604, avg. loss over tasks: 2.19088
Diversity Loss - Mean: 0.04637, Variance: 0.01198
Semantic Loss - Mean: 1.83917, Variance: 0.03267

Train Epoch: 36 
task: sign, mean loss: 0.15114, accuracy: 0.94565, avg. loss over tasks: 0.15114, lr: 0.00027521080207013716
Diversity Loss - Mean: 0.02904, Variance: 0.00913
Semantic Loss - Mean: 0.16134, Variance: 0.00576

Test Epoch: 36 
task: sign, mean loss: 1.83810, accuracy: 0.43195, avg. loss over tasks: 1.83810
Diversity Loss - Mean: 0.02520, Variance: 0.01204
Semantic Loss - Mean: 1.73272, Variance: 0.03261

Train Epoch: 37 
task: sign, mean loss: 0.18416, accuracy: 0.94565, avg. loss over tasks: 0.18416, lr: 0.0002733270110058693
Diversity Loss - Mean: 0.01866, Variance: 0.00914
Semantic Loss - Mean: 0.20621, Variance: 0.00587

Test Epoch: 37 
task: sign, mean loss: 4.02646, accuracy: 0.14793, avg. loss over tasks: 4.02646
Diversity Loss - Mean: 0.15695, Variance: 0.01194
Semantic Loss - Mean: 3.28355, Variance: 0.03452

Train Epoch: 38 
task: sign, mean loss: 0.20381, accuracy: 0.92391, avg. loss over tasks: 0.20381, lr: 0.00027138119660708587
Diversity Loss - Mean: 0.04411, Variance: 0.00912
Semantic Loss - Mean: 0.20978, Variance: 0.00586

Test Epoch: 38 
task: sign, mean loss: 1.21751, accuracy: 0.69822, avg. loss over tasks: 1.21751
Diversity Loss - Mean: 0.07982, Variance: 0.01192
Semantic Loss - Mean: 0.93988, Variance: 0.03395

Train Epoch: 39 
task: sign, mean loss: 0.22911, accuracy: 0.92935, avg. loss over tasks: 0.22911, lr: 0.0002693743386499349
Diversity Loss - Mean: 0.03728, Variance: 0.00909
Semantic Loss - Mean: 0.30969, Variance: 0.00604

Test Epoch: 39 
task: sign, mean loss: 1.68241, accuracy: 0.59172, avg. loss over tasks: 1.68241
Diversity Loss - Mean: 0.08842, Variance: 0.01186
Semantic Loss - Mean: 1.47273, Variance: 0.03400

Train Epoch: 40 
task: sign, mean loss: 0.23387, accuracy: 0.92935, avg. loss over tasks: 0.23387, lr: 0.00026730744764783427
Diversity Loss - Mean: 0.03386, Variance: 0.00907
Semantic Loss - Mean: 0.26779, Variance: 0.00611

Test Epoch: 40 
task: sign, mean loss: 1.13437, accuracy: 0.75148, avg. loss over tasks: 1.13437
Diversity Loss - Mean: 0.00673, Variance: 0.01185
Semantic Loss - Mean: 1.13243, Variance: 0.03367

Train Epoch: 41 
task: sign, mean loss: 0.14433, accuracy: 0.95109, avg. loss over tasks: 0.14433, lr: 0.00026518156434264794
Diversity Loss - Mean: 0.03261, Variance: 0.00905
Semantic Loss - Mean: 0.18200, Variance: 0.00606

Test Epoch: 41 
task: sign, mean loss: 1.32837, accuracy: 0.74556, avg. loss over tasks: 1.32837
Diversity Loss - Mean: 0.03239, Variance: 0.01179
Semantic Loss - Mean: 1.16540, Variance: 0.03386

Train Epoch: 42 
task: sign, mean loss: 0.26119, accuracy: 0.91848, avg. loss over tasks: 0.26119, lr: 0.0002629977591806411
Diversity Loss - Mean: 0.01776, Variance: 0.00905
Semantic Loss - Mean: 0.27955, Variance: 0.00620

Test Epoch: 42 
task: sign, mean loss: 2.40319, accuracy: 0.44379, avg. loss over tasks: 2.40319
Diversity Loss - Mean: 0.04746, Variance: 0.01171
Semantic Loss - Mean: 2.32270, Variance: 0.03451

Train Epoch: 43 
task: sign, mean loss: 0.24991, accuracy: 0.89674, avg. loss over tasks: 0.24991, lr: 0.000260757131773478
Diversity Loss - Mean: 0.02982, Variance: 0.00905
Semantic Loss - Mean: 0.29611, Variance: 0.00629

Test Epoch: 43 
task: sign, mean loss: 2.66732, accuracy: 0.23669, avg. loss over tasks: 2.66732
Diversity Loss - Mean: 0.13145, Variance: 0.01160
Semantic Loss - Mean: 2.19487, Variance: 0.03531

Train Epoch: 44 
task: sign, mean loss: 0.19221, accuracy: 0.92391, avg. loss over tasks: 0.19221, lr: 0.0002584608103445346
Diversity Loss - Mean: 0.01445, Variance: 0.00907
Semantic Loss - Mean: 0.27441, Variance: 0.00663

Test Epoch: 44 
task: sign, mean loss: 1.82280, accuracy: 0.40828, avg. loss over tasks: 1.82280
Diversity Loss - Mean: 0.10848, Variance: 0.01149
Semantic Loss - Mean: 1.58367, Variance: 0.03548

Train Epoch: 45 
task: sign, mean loss: 0.21260, accuracy: 0.92391, avg. loss over tasks: 0.21260, lr: 0.0002561099511608041
Diversity Loss - Mean: 0.00677, Variance: 0.00909
Semantic Loss - Mean: 0.27391, Variance: 0.00662

Test Epoch: 45 
task: sign, mean loss: 1.08694, accuracy: 0.69822, avg. loss over tasks: 1.08694
Diversity Loss - Mean: 0.04477, Variance: 0.01149
Semantic Loss - Mean: 0.98112, Variance: 0.03533

Train Epoch: 46 
task: sign, mean loss: 0.15953, accuracy: 0.94565, avg. loss over tasks: 0.15953, lr: 0.00025370573795068164
Diversity Loss - Mean: 0.01928, Variance: 0.00909
Semantic Loss - Mean: 0.20269, Variance: 0.00660

Test Epoch: 46 
task: sign, mean loss: 0.69743, accuracy: 0.79290, avg. loss over tasks: 0.69743
Diversity Loss - Mean: 0.00415, Variance: 0.01153
Semantic Loss - Mean: 0.65331, Variance: 0.03474

Train Epoch: 47 
task: sign, mean loss: 0.11342, accuracy: 0.95109, avg. loss over tasks: 0.11342, lr: 0.0002512493813079214
Diversity Loss - Mean: 0.01267, Variance: 0.00910
Semantic Loss - Mean: 0.18959, Variance: 0.00675

Test Epoch: 47 
task: sign, mean loss: 0.93234, accuracy: 0.75148, avg. loss over tasks: 0.93234
Diversity Loss - Mean: 0.02541, Variance: 0.01152
Semantic Loss - Mean: 0.86471, Variance: 0.03425

Train Epoch: 48 
task: sign, mean loss: 0.07594, accuracy: 0.97283, avg. loss over tasks: 0.07594, lr: 0.0002487421180820659
Diversity Loss - Mean: 0.01379, Variance: 0.00911
Semantic Loss - Mean: 0.11382, Variance: 0.00675

Test Epoch: 48 
task: sign, mean loss: 0.69385, accuracy: 0.80473, avg. loss over tasks: 0.69385
Diversity Loss - Mean: 0.03729, Variance: 0.01151
Semantic Loss - Mean: 0.83900, Variance: 0.03396

Train Epoch: 49 
task: sign, mean loss: 0.07081, accuracy: 0.97283, avg. loss over tasks: 0.07081, lr: 0.0002461852107556558
Diversity Loss - Mean: 0.02458, Variance: 0.00911
Semantic Loss - Mean: 0.09359, Variance: 0.00682

Test Epoch: 49 
task: sign, mean loss: 1.12116, accuracy: 0.62130, avg. loss over tasks: 1.12116
Diversity Loss - Mean: 0.06347, Variance: 0.01148
Semantic Loss - Mean: 1.08209, Variance: 0.03381

Train Epoch: 50 
task: sign, mean loss: 0.05820, accuracy: 0.99457, avg. loss over tasks: 0.05820, lr: 0.00024357994680853121
Diversity Loss - Mean: 0.03521, Variance: 0.00909
Semantic Loss - Mean: 0.10990, Variance: 0.00684

Test Epoch: 50 
task: sign, mean loss: 1.31634, accuracy: 0.49704, avg. loss over tasks: 1.31634
Diversity Loss - Mean: 0.08166, Variance: 0.01144
Semantic Loss - Mean: 1.26352, Variance: 0.03449

Train Epoch: 51 
task: sign, mean loss: 0.05548, accuracy: 0.98913, avg. loss over tasks: 0.05548, lr: 0.00024092763806954684
Diversity Loss - Mean: 0.03970, Variance: 0.00907
Semantic Loss - Mean: 0.12625, Variance: 0.00690

Test Epoch: 51 
task: sign, mean loss: 0.66849, accuracy: 0.81657, avg. loss over tasks: 0.66849
Diversity Loss - Mean: 0.04819, Variance: 0.01144
Semantic Loss - Mean: 0.63305, Variance: 0.03398

Train Epoch: 52 
task: sign, mean loss: 0.06494, accuracy: 0.97826, avg. loss over tasks: 0.06494, lr: 0.00023822962005602707
Diversity Loss - Mean: 0.04815, Variance: 0.00905
Semantic Loss - Mean: 0.11013, Variance: 0.00695

Test Epoch: 52 
task: sign, mean loss: 1.33924, accuracy: 0.73964, avg. loss over tasks: 1.33924
Diversity Loss - Mean: 0.11914, Variance: 0.01137
Semantic Loss - Mean: 1.18387, Variance: 0.03397

Train Epoch: 53 
task: sign, mean loss: 0.08617, accuracy: 0.96739, avg. loss over tasks: 0.08617, lr: 0.00023548725130129248
Diversity Loss - Mean: 0.03400, Variance: 0.00904
Semantic Loss - Mean: 0.15230, Variance: 0.00704

Test Epoch: 53 
task: sign, mean loss: 1.44009, accuracy: 0.74556, avg. loss over tasks: 1.44009
Diversity Loss - Mean: 0.05254, Variance: 0.01135
Semantic Loss - Mean: 1.31572, Variance: 0.03418

Train Epoch: 54 
task: sign, mean loss: 0.08833, accuracy: 0.96739, avg. loss over tasks: 0.08833, lr: 0.00023270191267059755
Diversity Loss - Mean: 0.03243, Variance: 0.00904
Semantic Loss - Mean: 0.16579, Variance: 0.00718

Test Epoch: 54 
task: sign, mean loss: 0.93980, accuracy: 0.78698, avg. loss over tasks: 0.93980
Diversity Loss - Mean: 0.02636, Variance: 0.01136
Semantic Loss - Mean: 0.87677, Variance: 0.03429

Train Epoch: 55 
task: sign, mean loss: 0.07230, accuracy: 0.97826, avg. loss over tasks: 0.07230, lr: 0.00022987500666582316
Diversity Loss - Mean: 0.02310, Variance: 0.00904
Semantic Loss - Mean: 0.09737, Variance: 0.00719

Test Epoch: 55 
task: sign, mean loss: 1.08494, accuracy: 0.73373, avg. loss over tasks: 1.08494
Diversity Loss - Mean: 0.05357, Variance: 0.01135
Semantic Loss - Mean: 0.97032, Variance: 0.03437

Train Epoch: 56 
task: sign, mean loss: 0.09486, accuracy: 0.96739, avg. loss over tasks: 0.09486, lr: 0.00022700795671927503
Diversity Loss - Mean: 0.02393, Variance: 0.00903
Semantic Loss - Mean: 0.12796, Variance: 0.00717

Test Epoch: 56 
task: sign, mean loss: 2.37317, accuracy: 0.45562, avg. loss over tasks: 2.37317
Diversity Loss - Mean: 0.08883, Variance: 0.01130
Semantic Loss - Mean: 2.07825, Variance: 0.03542

Train Epoch: 57 
task: sign, mean loss: 0.02811, accuracy: 0.99457, avg. loss over tasks: 0.02811, lr: 0.00022410220647694235
Diversity Loss - Mean: 0.02298, Variance: 0.00903
Semantic Loss - Mean: 0.07312, Variance: 0.00720

Test Epoch: 57 
task: sign, mean loss: 1.18057, accuracy: 0.69822, avg. loss over tasks: 1.18057
Diversity Loss - Mean: 0.06995, Variance: 0.01127
Semantic Loss - Mean: 1.11053, Variance: 0.03558

Train Epoch: 58 
task: sign, mean loss: 0.01764, accuracy: 0.99457, avg. loss over tasks: 0.01764, lr: 0.00022115921907157884
Diversity Loss - Mean: 0.01763, Variance: 0.00903
Semantic Loss - Mean: 0.06626, Variance: 0.00726

Test Epoch: 58 
task: sign, mean loss: 0.92576, accuracy: 0.81065, avg. loss over tasks: 0.92576
Diversity Loss - Mean: 0.04972, Variance: 0.01125
Semantic Loss - Mean: 0.87787, Variance: 0.03531

Train Epoch: 59 
task: sign, mean loss: 0.05881, accuracy: 0.98370, avg. loss over tasks: 0.05881, lr: 0.00021818047638597106
Diversity Loss - Mean: 0.01325, Variance: 0.00903
Semantic Loss - Mean: 0.12692, Variance: 0.00739

Test Epoch: 59 
task: sign, mean loss: 0.86253, accuracy: 0.78107, avg. loss over tasks: 0.86253
Diversity Loss - Mean: 0.04398, Variance: 0.01125
Semantic Loss - Mean: 0.88228, Variance: 0.03520

Train Epoch: 60 
task: sign, mean loss: 0.04528, accuracy: 0.98913, avg. loss over tasks: 0.04528, lr: 0.00021516747830676604
Diversity Loss - Mean: 0.01371, Variance: 0.00903
Semantic Loss - Mean: 0.10264, Variance: 0.00739

Test Epoch: 60 
task: sign, mean loss: 0.88136, accuracy: 0.78698, avg. loss over tasks: 0.88136
Diversity Loss - Mean: 0.05326, Variance: 0.01124
Semantic Loss - Mean: 0.88800, Variance: 0.03547

Train Epoch: 61 
task: sign, mean loss: 0.09039, accuracy: 0.97283, avg. loss over tasks: 0.09039, lr: 0.0002121217419692331
Diversity Loss - Mean: 0.00994, Variance: 0.00903
Semantic Loss - Mean: 0.17118, Variance: 0.00766

Test Epoch: 61 
task: sign, mean loss: 0.86056, accuracy: 0.81657, avg. loss over tasks: 0.86056
Diversity Loss - Mean: 0.05870, Variance: 0.01123
Semantic Loss - Mean: 0.76215, Variance: 0.03504

Train Epoch: 62 
task: sign, mean loss: 0.17577, accuracy: 0.94565, avg. loss over tasks: 0.17577, lr: 0.00020904480099334042
Diversity Loss - Mean: 0.00632, Variance: 0.00904
Semantic Loss - Mean: 0.23573, Variance: 0.00778

Test Epoch: 62 
task: sign, mean loss: 1.32174, accuracy: 0.75740, avg. loss over tasks: 1.32174
Diversity Loss - Mean: 0.06313, Variance: 0.01123
Semantic Loss - Mean: 1.12451, Variance: 0.03487

Train Epoch: 63 
task: sign, mean loss: 0.12042, accuracy: 0.94565, avg. loss over tasks: 0.12042, lr: 0.00020593820471153146
Diversity Loss - Mean: 0.01031, Variance: 0.00905
Semantic Loss - Mean: 0.16461, Variance: 0.00778

Test Epoch: 63 
task: sign, mean loss: 1.18135, accuracy: 0.72781, avg. loss over tasks: 1.18135
Diversity Loss - Mean: 0.06391, Variance: 0.01121
Semantic Loss - Mean: 1.12577, Variance: 0.03485

Train Epoch: 64 
task: sign, mean loss: 0.06100, accuracy: 0.98370, avg. loss over tasks: 0.06100, lr: 0.0002028035173885892
Diversity Loss - Mean: 0.00580, Variance: 0.00906
Semantic Loss - Mean: 0.12696, Variance: 0.00796

Test Epoch: 64 
task: sign, mean loss: 0.96607, accuracy: 0.71598, avg. loss over tasks: 0.96607
Diversity Loss - Mean: 0.06626, Variance: 0.01118
Semantic Loss - Mean: 0.98086, Variance: 0.03500

Train Epoch: 65 
task: sign, mean loss: 0.12293, accuracy: 0.96196, avg. loss over tasks: 0.12293, lr: 0.00019964231743398178
Diversity Loss - Mean: 0.00553, Variance: 0.00907
Semantic Loss - Mean: 0.21281, Variance: 0.00815

Test Epoch: 65 
task: sign, mean loss: 0.77466, accuracy: 0.80473, avg. loss over tasks: 0.77466
Diversity Loss - Mean: 0.05148, Variance: 0.01118
Semantic Loss - Mean: 0.71998, Variance: 0.03473

Train Epoch: 66 
task: sign, mean loss: 0.13061, accuracy: 0.95109, avg. loss over tasks: 0.13061, lr: 0.00019645619660708585
Diversity Loss - Mean: 0.00647, Variance: 0.00908
Semantic Loss - Mean: 0.19898, Variance: 0.00839

Test Epoch: 66 
task: sign, mean loss: 0.74463, accuracy: 0.83432, avg. loss over tasks: 0.74463
Diversity Loss - Mean: 0.02112, Variance: 0.01117
Semantic Loss - Mean: 0.68187, Variance: 0.03446

Train Epoch: 67 
task: sign, mean loss: 0.04911, accuracy: 0.98913, avg. loss over tasks: 0.04911, lr: 0.00019324675921568777
Diversity Loss - Mean: 0.00372, Variance: 0.00908
Semantic Loss - Mean: 0.12073, Variance: 0.00842

Test Epoch: 67 
task: sign, mean loss: 0.82070, accuracy: 0.83432, avg. loss over tasks: 0.82070
Diversity Loss - Mean: 0.02554, Variance: 0.01116
Semantic Loss - Mean: 0.78139, Variance: 0.03411

Train Epoch: 68 
task: sign, mean loss: 0.21509, accuracy: 0.96196, avg. loss over tasks: 0.21509, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.00280, Variance: 0.00908
Semantic Loss - Mean: 0.23576, Variance: 0.00859

Test Epoch: 68 
task: sign, mean loss: 0.97751, accuracy: 0.82249, avg. loss over tasks: 0.97751
Diversity Loss - Mean: -0.00762, Variance: 0.01117
Semantic Loss - Mean: 0.96947, Variance: 0.03391

Train Epoch: 69 
task: sign, mean loss: 0.05818, accuracy: 0.98913, avg. loss over tasks: 0.05818, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.01701, Variance: 0.00911
Semantic Loss - Mean: 0.12558, Variance: 0.00854

Test Epoch: 69 
task: sign, mean loss: 1.29867, accuracy: 0.73373, avg. loss over tasks: 1.29867
Diversity Loss - Mean: -0.00416, Variance: 0.01119
Semantic Loss - Mean: 1.24940, Variance: 0.03383

Train Epoch: 70 
task: sign, mean loss: 0.02961, accuracy: 0.99457, avg. loss over tasks: 0.02961, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.02322, Variance: 0.00914
Semantic Loss - Mean: 0.06327, Variance: 0.00847

Test Epoch: 70 
task: sign, mean loss: 1.02975, accuracy: 0.81065, avg. loss over tasks: 1.02975
Diversity Loss - Mean: -0.01892, Variance: 0.01122
Semantic Loss - Mean: 0.90851, Variance: 0.03364

Train Epoch: 71 
task: sign, mean loss: 0.02095, accuracy: 0.99457, avg. loss over tasks: 0.02095, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.02515, Variance: 0.00916
Semantic Loss - Mean: 0.06562, Variance: 0.00845

Test Epoch: 71 
task: sign, mean loss: 0.76110, accuracy: 0.81657, avg. loss over tasks: 0.76110
Diversity Loss - Mean: -0.03638, Variance: 0.01126
Semantic Loss - Mean: 0.66315, Variance: 0.03332

Train Epoch: 72 
task: sign, mean loss: 0.02369, accuracy: 0.98913, avg. loss over tasks: 0.02369, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.02001, Variance: 0.00918
Semantic Loss - Mean: 0.06433, Variance: 0.00841

Test Epoch: 72 
task: sign, mean loss: 0.77523, accuracy: 0.84024, avg. loss over tasks: 0.77523
Diversity Loss - Mean: -0.01756, Variance: 0.01128
Semantic Loss - Mean: 0.72054, Variance: 0.03304

Train Epoch: 73 
task: sign, mean loss: 0.03921, accuracy: 0.99457, avg. loss over tasks: 0.03921, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.01279, Variance: 0.00919
Semantic Loss - Mean: 0.09628, Variance: 0.00839

Test Epoch: 73 
task: sign, mean loss: 0.83953, accuracy: 0.82840, avg. loss over tasks: 0.83953
Diversity Loss - Mean: -0.01867, Variance: 0.01131
Semantic Loss - Mean: 0.75953, Variance: 0.03284

Train Epoch: 74 
task: sign, mean loss: 0.07167, accuracy: 0.97826, avg. loss over tasks: 0.07167, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.00916, Variance: 0.00920
Semantic Loss - Mean: 0.10490, Variance: 0.00836

Test Epoch: 74 
task: sign, mean loss: 1.51603, accuracy: 0.78698, avg. loss over tasks: 1.51603
Diversity Loss - Mean: 0.03503, Variance: 0.01130
Semantic Loss - Mean: 1.42331, Variance: 0.03301

Train Epoch: 75 
task: sign, mean loss: 0.14394, accuracy: 0.96739, avg. loss over tasks: 0.14394, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.02378, Variance: 0.00922
Semantic Loss - Mean: 0.18709, Variance: 0.00842

Test Epoch: 75 
task: sign, mean loss: 0.93399, accuracy: 0.83432, avg. loss over tasks: 0.93399
Diversity Loss - Mean: -0.02730, Variance: 0.01134
Semantic Loss - Mean: 0.70933, Variance: 0.03270

Train Epoch: 76 
task: sign, mean loss: 0.13867, accuracy: 0.95652, avg. loss over tasks: 0.13867, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.01933, Variance: 0.00923
Semantic Loss - Mean: 0.21305, Variance: 0.00843

Test Epoch: 76 
task: sign, mean loss: 0.50701, accuracy: 0.86391, avg. loss over tasks: 0.50701
Diversity Loss - Mean: -0.03739, Variance: 0.01139
Semantic Loss - Mean: 0.48962, Variance: 0.03251

Train Epoch: 77 
task: sign, mean loss: 0.05757, accuracy: 0.98370, avg. loss over tasks: 0.05757, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.01755, Variance: 0.00924
Semantic Loss - Mean: 0.10016, Variance: 0.00843

Test Epoch: 77 
task: sign, mean loss: 1.03439, accuracy: 0.76331, avg. loss over tasks: 1.03439
Diversity Loss - Mean: 0.00963, Variance: 0.01140
Semantic Loss - Mean: 1.18220, Variance: 0.03278

Train Epoch: 78 
task: sign, mean loss: 0.02656, accuracy: 0.99457, avg. loss over tasks: 0.02656, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.00996, Variance: 0.00925
Semantic Loss - Mean: 0.07813, Variance: 0.00841

Test Epoch: 78 
task: sign, mean loss: 0.63702, accuracy: 0.82840, avg. loss over tasks: 0.63702
Diversity Loss - Mean: 0.01232, Variance: 0.01142
Semantic Loss - Mean: 0.68139, Variance: 0.03248

Train Epoch: 79 
task: sign, mean loss: 0.01288, accuracy: 0.99457, avg. loss over tasks: 0.01288, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.01432, Variance: 0.00926
Semantic Loss - Mean: 0.05910, Variance: 0.00841

Test Epoch: 79 
task: sign, mean loss: 0.61791, accuracy: 0.82249, avg. loss over tasks: 0.61791
Diversity Loss - Mean: 0.00385, Variance: 0.01143
Semantic Loss - Mean: 0.60926, Variance: 0.03220

Train Epoch: 80 
task: sign, mean loss: 0.01140, accuracy: 0.99457, avg. loss over tasks: 0.01140, lr: 0.00015015
Diversity Loss - Mean: -0.01047, Variance: 0.00927
Semantic Loss - Mean: 0.04328, Variance: 0.00838

Test Epoch: 80 
task: sign, mean loss: 0.66681, accuracy: 0.84024, avg. loss over tasks: 0.66681
Diversity Loss - Mean: -0.00424, Variance: 0.01145
Semantic Loss - Mean: 0.65446, Variance: 0.03197

Train Epoch: 81 
task: sign, mean loss: 0.14878, accuracy: 0.98370, avg. loss over tasks: 0.14878, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.00705, Variance: 0.00927
Semantic Loss - Mean: 0.17189, Variance: 0.00839

Test Epoch: 81 
task: sign, mean loss: 0.69540, accuracy: 0.85207, avg. loss over tasks: 0.69540
Diversity Loss - Mean: 0.00012, Variance: 0.01146
Semantic Loss - Mean: 0.73757, Variance: 0.03185

Train Epoch: 82 
task: sign, mean loss: 0.27744, accuracy: 0.92391, avg. loss over tasks: 0.27744, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.01808, Variance: 0.00928
Semantic Loss - Mean: 0.31584, Variance: 0.00855

Test Epoch: 82 
task: sign, mean loss: 3.94056, accuracy: 0.23669, avg. loss over tasks: 3.94056
Diversity Loss - Mean: 0.04404, Variance: 0.01146
Semantic Loss - Mean: 3.11943, Variance: 0.03246

Train Epoch: 83 
task: sign, mean loss: 0.08755, accuracy: 0.95652, avg. loss over tasks: 0.08755, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.02584, Variance: 0.00931
Semantic Loss - Mean: 0.15994, Variance: 0.00864

Test Epoch: 83 
task: sign, mean loss: 1.30331, accuracy: 0.65680, avg. loss over tasks: 1.30331
Diversity Loss - Mean: 0.00173, Variance: 0.01147
Semantic Loss - Mean: 1.38741, Variance: 0.03324

Train Epoch: 84 
task: sign, mean loss: 0.08266, accuracy: 0.96196, avg. loss over tasks: 0.08266, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.02440, Variance: 0.00933
Semantic Loss - Mean: 0.12727, Variance: 0.00861

Test Epoch: 84 
task: sign, mean loss: 0.70727, accuracy: 0.77515, avg. loss over tasks: 0.70727
Diversity Loss - Mean: -0.02010, Variance: 0.01150
Semantic Loss - Mean: 0.71264, Variance: 0.03318

Train Epoch: 85 
task: sign, mean loss: 0.04112, accuracy: 0.97826, avg. loss over tasks: 0.04112, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.02039, Variance: 0.00935
Semantic Loss - Mean: 0.06460, Variance: 0.00855

Test Epoch: 85 
task: sign, mean loss: 0.68754, accuracy: 0.84024, avg. loss over tasks: 0.68754
Diversity Loss - Mean: -0.00058, Variance: 0.01151
Semantic Loss - Mean: 0.65128, Variance: 0.03292

Train Epoch: 86 
task: sign, mean loss: 0.03553, accuracy: 0.98370, avg. loss over tasks: 0.03553, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.01677, Variance: 0.00937
Semantic Loss - Mean: 0.07793, Variance: 0.00855

Test Epoch: 86 
task: sign, mean loss: 0.85505, accuracy: 0.79290, avg. loss over tasks: 0.85505
Diversity Loss - Mean: 0.00685, Variance: 0.01151
Semantic Loss - Mean: 0.93619, Variance: 0.03281

Train Epoch: 87 
task: sign, mean loss: 0.01913, accuracy: 0.99457, avg. loss over tasks: 0.01913, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.01650, Variance: 0.00938
Semantic Loss - Mean: 0.06787, Variance: 0.00854

Test Epoch: 87 
task: sign, mean loss: 0.90464, accuracy: 0.80473, avg. loss over tasks: 0.90464
Diversity Loss - Mean: 0.00937, Variance: 0.01151
Semantic Loss - Mean: 1.11036, Variance: 0.03275

Train Epoch: 88 
task: sign, mean loss: 0.01409, accuracy: 0.99457, avg. loss over tasks: 0.01409, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.01743, Variance: 0.00940
Semantic Loss - Mean: 0.04668, Variance: 0.00850

Test Epoch: 88 
task: sign, mean loss: 0.72670, accuracy: 0.82249, avg. loss over tasks: 0.72670
Diversity Loss - Mean: 0.00101, Variance: 0.01151
Semantic Loss - Mean: 0.91525, Variance: 0.03264

Train Epoch: 89 
task: sign, mean loss: 0.02701, accuracy: 0.98913, avg. loss over tasks: 0.02701, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.01822, Variance: 0.00941
Semantic Loss - Mean: 0.05551, Variance: 0.00843

Test Epoch: 89 
task: sign, mean loss: 0.82287, accuracy: 0.74556, avg. loss over tasks: 0.82287
Diversity Loss - Mean: 0.00630, Variance: 0.01151
Semantic Loss - Mean: 0.91647, Variance: 0.03247

Train Epoch: 90 
task: sign, mean loss: 0.01874, accuracy: 0.99457, avg. loss over tasks: 0.01874, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.01320, Variance: 0.00942
Semantic Loss - Mean: 0.04715, Variance: 0.00837

Test Epoch: 90 
task: sign, mean loss: 0.80439, accuracy: 0.74556, avg. loss over tasks: 0.80439
Diversity Loss - Mean: 0.00037, Variance: 0.01151
Semantic Loss - Mean: 0.86674, Variance: 0.03240

Train Epoch: 91 
task: sign, mean loss: 0.05297, accuracy: 0.98370, avg. loss over tasks: 0.05297, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.01316, Variance: 0.00943
Semantic Loss - Mean: 0.09188, Variance: 0.00839

Test Epoch: 91 
task: sign, mean loss: 0.66153, accuracy: 0.81065, avg. loss over tasks: 0.66153
Diversity Loss - Mean: -0.00644, Variance: 0.01152
Semantic Loss - Mean: 0.76853, Variance: 0.03220

Train Epoch: 92 
task: sign, mean loss: 0.00970, accuracy: 0.99457, avg. loss over tasks: 0.00970, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.01668, Variance: 0.00944
Semantic Loss - Mean: 0.04075, Variance: 0.00834

Test Epoch: 92 
task: sign, mean loss: 0.81551, accuracy: 0.81657, avg. loss over tasks: 0.81551
Diversity Loss - Mean: 0.00017, Variance: 0.01153
Semantic Loss - Mean: 0.92544, Variance: 0.03217

Train Epoch: 93 
task: sign, mean loss: 0.01340, accuracy: 1.00000, avg. loss over tasks: 0.01340, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.02014, Variance: 0.00945
Semantic Loss - Mean: 0.04424, Variance: 0.00832

Test Epoch: 93 
task: sign, mean loss: 0.48916, accuracy: 0.86982, avg. loss over tasks: 0.48916
Diversity Loss - Mean: -0.01575, Variance: 0.01154
Semantic Loss - Mean: 0.55566, Variance: 0.03199

Train Epoch: 94 
task: sign, mean loss: 0.00483, accuracy: 1.00000, avg. loss over tasks: 0.00483, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.01712, Variance: 0.00946
Semantic Loss - Mean: 0.02800, Variance: 0.00825

Test Epoch: 94 
task: sign, mean loss: 0.49952, accuracy: 0.86391, avg. loss over tasks: 0.49952
Diversity Loss - Mean: -0.00519, Variance: 0.01155
Semantic Loss - Mean: 0.61300, Variance: 0.03183

Train Epoch: 95 
task: sign, mean loss: 0.00933, accuracy: 0.99457, avg. loss over tasks: 0.00933, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.01663, Variance: 0.00947
Semantic Loss - Mean: 0.03884, Variance: 0.00820

Test Epoch: 95 
task: sign, mean loss: 0.58106, accuracy: 0.84024, avg. loss over tasks: 0.58106
Diversity Loss - Mean: -0.00019, Variance: 0.01156
Semantic Loss - Mean: 0.76318, Variance: 0.03169

Train Epoch: 96 
task: sign, mean loss: 0.00305, accuracy: 1.00000, avg. loss over tasks: 0.00305, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.01706, Variance: 0.00948
Semantic Loss - Mean: 0.03179, Variance: 0.00815

Test Epoch: 96 
task: sign, mean loss: 0.51121, accuracy: 0.85207, avg. loss over tasks: 0.51121
Diversity Loss - Mean: -0.00035, Variance: 0.01157
Semantic Loss - Mean: 0.70886, Variance: 0.03158

Train Epoch: 97 
task: sign, mean loss: 0.00355, accuracy: 1.00000, avg. loss over tasks: 0.00355, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.02087, Variance: 0.00949
Semantic Loss - Mean: 0.03954, Variance: 0.00810

Test Epoch: 97 
task: sign, mean loss: 0.63254, accuracy: 0.82840, avg. loss over tasks: 0.63254
Diversity Loss - Mean: -0.00129, Variance: 0.01157
Semantic Loss - Mean: 0.77900, Variance: 0.03156

Train Epoch: 98 
task: sign, mean loss: 0.00557, accuracy: 1.00000, avg. loss over tasks: 0.00557, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.02193, Variance: 0.00950
Semantic Loss - Mean: 0.03056, Variance: 0.00806

Test Epoch: 98 
task: sign, mean loss: 0.68500, accuracy: 0.82249, avg. loss over tasks: 0.68500
Diversity Loss - Mean: -0.00599, Variance: 0.01158
Semantic Loss - Mean: 0.77235, Variance: 0.03158

Train Epoch: 99 
task: sign, mean loss: 0.00306, accuracy: 1.00000, avg. loss over tasks: 0.00306, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.02384, Variance: 0.00951
Semantic Loss - Mean: 0.02026, Variance: 0.00798

Test Epoch: 99 
task: sign, mean loss: 0.63946, accuracy: 0.83432, avg. loss over tasks: 0.63946
Diversity Loss - Mean: -0.00073, Variance: 0.01158
Semantic Loss - Mean: 0.78139, Variance: 0.03153

Train Epoch: 100 
task: sign, mean loss: 0.00327, accuracy: 1.00000, avg. loss over tasks: 0.00327, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.02283, Variance: 0.00953
Semantic Loss - Mean: 0.04039, Variance: 0.00795

Test Epoch: 100 
task: sign, mean loss: 0.54630, accuracy: 0.86391, avg. loss over tasks: 0.54630
Diversity Loss - Mean: -0.00604, Variance: 0.01159
Semantic Loss - Mean: 0.67625, Variance: 0.03135

Train Epoch: 101 
task: sign, mean loss: 0.00320, accuracy: 1.00000, avg. loss over tasks: 0.00320, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.02123, Variance: 0.00954
Semantic Loss - Mean: 0.03155, Variance: 0.00791

Test Epoch: 101 
task: sign, mean loss: 0.53338, accuracy: 0.85207, avg. loss over tasks: 0.53338
Diversity Loss - Mean: -0.00612, Variance: 0.01161
Semantic Loss - Mean: 0.70045, Variance: 0.03126

Train Epoch: 102 
task: sign, mean loss: 0.03204, accuracy: 0.99457, avg. loss over tasks: 0.03204, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.02451, Variance: 0.00955
Semantic Loss - Mean: 0.05445, Variance: 0.00788

Test Epoch: 102 
task: sign, mean loss: 0.61351, accuracy: 0.86391, avg. loss over tasks: 0.61351
Diversity Loss - Mean: -0.00466, Variance: 0.01162
Semantic Loss - Mean: 0.79372, Variance: 0.03134

Train Epoch: 103 
task: sign, mean loss: 0.01609, accuracy: 0.99457, avg. loss over tasks: 0.01609, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.02490, Variance: 0.00955
Semantic Loss - Mean: 0.04624, Variance: 0.00783

Test Epoch: 103 
task: sign, mean loss: 0.66154, accuracy: 0.84615, avg. loss over tasks: 0.66154
Diversity Loss - Mean: -0.01634, Variance: 0.01163
Semantic Loss - Mean: 0.79555, Variance: 0.03145

Train Epoch: 104 
task: sign, mean loss: 0.01038, accuracy: 0.99457, avg. loss over tasks: 0.01038, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.02517, Variance: 0.00956
Semantic Loss - Mean: 0.03104, Variance: 0.00778

Test Epoch: 104 
task: sign, mean loss: 0.45716, accuracy: 0.88757, avg. loss over tasks: 0.45716
Diversity Loss - Mean: -0.01954, Variance: 0.01164
Semantic Loss - Mean: 0.51754, Variance: 0.03130

Train Epoch: 105 
task: sign, mean loss: 0.00318, accuracy: 1.00000, avg. loss over tasks: 0.00318, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.02753, Variance: 0.00958
Semantic Loss - Mean: 0.02160, Variance: 0.00773

Test Epoch: 105 
task: sign, mean loss: 0.53993, accuracy: 0.88757, avg. loss over tasks: 0.53993
Diversity Loss - Mean: -0.01241, Variance: 0.01166
Semantic Loss - Mean: 0.64681, Variance: 0.03114

Train Epoch: 106 
task: sign, mean loss: 0.00525, accuracy: 1.00000, avg. loss over tasks: 0.00525, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.02559, Variance: 0.00959
Semantic Loss - Mean: 0.04406, Variance: 0.00770

Test Epoch: 106 
task: sign, mean loss: 0.45254, accuracy: 0.87574, avg. loss over tasks: 0.45254
Diversity Loss - Mean: -0.01289, Variance: 0.01167
Semantic Loss - Mean: 0.59389, Variance: 0.03096

Train Epoch: 107 
task: sign, mean loss: 0.00281, accuracy: 1.00000, avg. loss over tasks: 0.00281, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.02470, Variance: 0.00960
Semantic Loss - Mean: 0.01458, Variance: 0.00764

Test Epoch: 107 
task: sign, mean loss: 0.59210, accuracy: 0.85207, avg. loss over tasks: 0.59210
Diversity Loss - Mean: -0.01923, Variance: 0.01168
Semantic Loss - Mean: 0.71424, Variance: 0.03083

Train Epoch: 108 
task: sign, mean loss: 0.00562, accuracy: 1.00000, avg. loss over tasks: 0.00562, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.02572, Variance: 0.00961
Semantic Loss - Mean: 0.03290, Variance: 0.00760

Test Epoch: 108 
task: sign, mean loss: 0.53752, accuracy: 0.87574, avg. loss over tasks: 0.53752
Diversity Loss - Mean: -0.02084, Variance: 0.01170
Semantic Loss - Mean: 0.66577, Variance: 0.03070

Train Epoch: 109 
task: sign, mean loss: 0.00291, accuracy: 1.00000, avg. loss over tasks: 0.00291, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.02648, Variance: 0.00962
Semantic Loss - Mean: 0.02776, Variance: 0.00755

Test Epoch: 109 
task: sign, mean loss: 0.53263, accuracy: 0.88166, avg. loss over tasks: 0.53263
Diversity Loss - Mean: -0.02103, Variance: 0.01171
Semantic Loss - Mean: 0.66981, Variance: 0.03053

Train Epoch: 110 
task: sign, mean loss: 0.00857, accuracy: 1.00000, avg. loss over tasks: 0.00857, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.02833, Variance: 0.00963
Semantic Loss - Mean: 0.02753, Variance: 0.00750

Test Epoch: 110 
task: sign, mean loss: 0.55230, accuracy: 0.88166, avg. loss over tasks: 0.55230
Diversity Loss - Mean: -0.01316, Variance: 0.01172
Semantic Loss - Mean: 0.73780, Variance: 0.03038

Train Epoch: 111 
task: sign, mean loss: 0.00801, accuracy: 0.99457, avg. loss over tasks: 0.00801, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.02799, Variance: 0.00964
Semantic Loss - Mean: 0.04451, Variance: 0.00747

Test Epoch: 111 
task: sign, mean loss: 0.58490, accuracy: 0.85799, avg. loss over tasks: 0.58490
Diversity Loss - Mean: -0.01758, Variance: 0.01174
Semantic Loss - Mean: 0.74395, Variance: 0.03023

Train Epoch: 112 
task: sign, mean loss: 0.00116, accuracy: 1.00000, avg. loss over tasks: 0.00116, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.02657, Variance: 0.00964
Semantic Loss - Mean: 0.02223, Variance: 0.00741

Test Epoch: 112 
task: sign, mean loss: 0.59181, accuracy: 0.86982, avg. loss over tasks: 0.59181
Diversity Loss - Mean: -0.01650, Variance: 0.01175
Semantic Loss - Mean: 0.82587, Variance: 0.03011

Train Epoch: 113 
task: sign, mean loss: 0.00202, accuracy: 1.00000, avg. loss over tasks: 0.00202, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.02760, Variance: 0.00965
Semantic Loss - Mean: 0.02045, Variance: 0.00736

Test Epoch: 113 
task: sign, mean loss: 0.51725, accuracy: 0.87574, avg. loss over tasks: 0.51725
Diversity Loss - Mean: -0.01958, Variance: 0.01176
Semantic Loss - Mean: 0.72282, Variance: 0.02996

Train Epoch: 114 
task: sign, mean loss: 0.00219, accuracy: 1.00000, avg. loss over tasks: 0.00219, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.03230, Variance: 0.00966
Semantic Loss - Mean: 0.03670, Variance: 0.00736

Test Epoch: 114 
task: sign, mean loss: 0.57123, accuracy: 0.87574, avg. loss over tasks: 0.57123
Diversity Loss - Mean: -0.02129, Variance: 0.01177
Semantic Loss - Mean: 0.75924, Variance: 0.02980

Train Epoch: 115 
task: sign, mean loss: 0.01150, accuracy: 0.99457, avg. loss over tasks: 0.01150, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.02859, Variance: 0.00967
Semantic Loss - Mean: 0.02983, Variance: 0.00731

Test Epoch: 115 
task: sign, mean loss: 0.58909, accuracy: 0.86982, avg. loss over tasks: 0.58909
Diversity Loss - Mean: -0.01678, Variance: 0.01178
Semantic Loss - Mean: 0.81886, Variance: 0.02966

Train Epoch: 116 
task: sign, mean loss: 0.00538, accuracy: 1.00000, avg. loss over tasks: 0.00538, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.02757, Variance: 0.00968
Semantic Loss - Mean: 0.03621, Variance: 0.00726

Test Epoch: 116 
task: sign, mean loss: 0.46434, accuracy: 0.90533, avg. loss over tasks: 0.46434
Diversity Loss - Mean: -0.02360, Variance: 0.01180
Semantic Loss - Mean: 0.58740, Variance: 0.02945

Train Epoch: 117 
task: sign, mean loss: 0.00469, accuracy: 1.00000, avg. loss over tasks: 0.00469, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.02912, Variance: 0.00969
Semantic Loss - Mean: 0.01901, Variance: 0.00721

Test Epoch: 117 
task: sign, mean loss: 0.50163, accuracy: 0.88757, avg. loss over tasks: 0.50163
Diversity Loss - Mean: -0.02397, Variance: 0.01181
Semantic Loss - Mean: 0.62828, Variance: 0.02927

Train Epoch: 118 
task: sign, mean loss: 0.00283, accuracy: 1.00000, avg. loss over tasks: 0.00283, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.02860, Variance: 0.00970
Semantic Loss - Mean: 0.02037, Variance: 0.00716

Test Epoch: 118 
task: sign, mean loss: 0.53016, accuracy: 0.87574, avg. loss over tasks: 0.53016
Diversity Loss - Mean: -0.02356, Variance: 0.01182
Semantic Loss - Mean: 0.66756, Variance: 0.02909

Train Epoch: 119 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.02841, Variance: 0.00970
Semantic Loss - Mean: 0.01472, Variance: 0.00711

Test Epoch: 119 
task: sign, mean loss: 0.51008, accuracy: 0.87574, avg. loss over tasks: 0.51008
Diversity Loss - Mean: -0.02301, Variance: 0.01183
Semantic Loss - Mean: 0.67900, Variance: 0.02896

Train Epoch: 120 
task: sign, mean loss: 0.00427, accuracy: 1.00000, avg. loss over tasks: 0.00427, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.03088, Variance: 0.00971
Semantic Loss - Mean: 0.03952, Variance: 0.00710

Test Epoch: 120 
task: sign, mean loss: 0.51349, accuracy: 0.88166, avg. loss over tasks: 0.51349
Diversity Loss - Mean: -0.02329, Variance: 0.01185
Semantic Loss - Mean: 0.68716, Variance: 0.02884

Train Epoch: 121 
task: sign, mean loss: 0.01249, accuracy: 0.99457, avg. loss over tasks: 0.01249, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.03433, Variance: 0.00972
Semantic Loss - Mean: 0.03186, Variance: 0.00706

Test Epoch: 121 
task: sign, mean loss: 0.56384, accuracy: 0.88757, avg. loss over tasks: 0.56384
Diversity Loss - Mean: -0.01627, Variance: 0.01185
Semantic Loss - Mean: 0.78406, Variance: 0.02875

Train Epoch: 122 
task: sign, mean loss: 0.02076, accuracy: 0.99457, avg. loss over tasks: 0.02076, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.03343, Variance: 0.00973
Semantic Loss - Mean: 0.05117, Variance: 0.00702

Test Epoch: 122 
task: sign, mean loss: 0.60341, accuracy: 0.86982, avg. loss over tasks: 0.60341
Diversity Loss - Mean: -0.01364, Variance: 0.01186
Semantic Loss - Mean: 0.83628, Variance: 0.02865

Train Epoch: 123 
task: sign, mean loss: 0.00174, accuracy: 1.00000, avg. loss over tasks: 0.00174, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.03342, Variance: 0.00974
Semantic Loss - Mean: 0.02515, Variance: 0.00698

Test Epoch: 123 
task: sign, mean loss: 0.61475, accuracy: 0.85207, avg. loss over tasks: 0.61475
Diversity Loss - Mean: -0.00912, Variance: 0.01186
Semantic Loss - Mean: 0.91228, Variance: 0.02855

Train Epoch: 124 
task: sign, mean loss: 0.00276, accuracy: 1.00000, avg. loss over tasks: 0.00276, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.03055, Variance: 0.00975
Semantic Loss - Mean: 0.01579, Variance: 0.00693

Test Epoch: 124 
task: sign, mean loss: 0.51186, accuracy: 0.87574, avg. loss over tasks: 0.51186
Diversity Loss - Mean: -0.01680, Variance: 0.01187
Semantic Loss - Mean: 0.75312, Variance: 0.02841

Train Epoch: 125 
task: sign, mean loss: 0.00243, accuracy: 1.00000, avg. loss over tasks: 0.00243, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.03398, Variance: 0.00976
Semantic Loss - Mean: 0.01402, Variance: 0.00688

Test Epoch: 125 
task: sign, mean loss: 0.53631, accuracy: 0.86982, avg. loss over tasks: 0.53631
Diversity Loss - Mean: -0.01824, Variance: 0.01188
Semantic Loss - Mean: 0.78331, Variance: 0.02828

Train Epoch: 126 
task: sign, mean loss: 0.00108, accuracy: 1.00000, avg. loss over tasks: 0.00108, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.03202, Variance: 0.00977
Semantic Loss - Mean: 0.02095, Variance: 0.00685

Test Epoch: 126 
task: sign, mean loss: 0.45295, accuracy: 0.87574, avg. loss over tasks: 0.45295
Diversity Loss - Mean: -0.02474, Variance: 0.01189
Semantic Loss - Mean: 0.64190, Variance: 0.02813

Train Epoch: 127 
task: sign, mean loss: 0.00595, accuracy: 1.00000, avg. loss over tasks: 0.00595, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.03324, Variance: 0.00978
Semantic Loss - Mean: 0.01758, Variance: 0.00680

Test Epoch: 127 
task: sign, mean loss: 0.51981, accuracy: 0.88166, avg. loss over tasks: 0.51981
Diversity Loss - Mean: -0.02383, Variance: 0.01190
Semantic Loss - Mean: 0.72023, Variance: 0.02799

Train Epoch: 128 
task: sign, mean loss: 0.00412, accuracy: 1.00000, avg. loss over tasks: 0.00412, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.03155, Variance: 0.00978
Semantic Loss - Mean: 0.02061, Variance: 0.00676

Test Epoch: 128 
task: sign, mean loss: 0.54863, accuracy: 0.87574, avg. loss over tasks: 0.54863
Diversity Loss - Mean: -0.01610, Variance: 0.01191
Semantic Loss - Mean: 0.81709, Variance: 0.02788

Train Epoch: 129 
task: sign, mean loss: 0.00399, accuracy: 1.00000, avg. loss over tasks: 0.00399, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.03456, Variance: 0.00979
Semantic Loss - Mean: 0.01589, Variance: 0.00672

Test Epoch: 129 
task: sign, mean loss: 0.56730, accuracy: 0.88757, avg. loss over tasks: 0.56730
Diversity Loss - Mean: -0.01851, Variance: 0.01192
Semantic Loss - Mean: 0.79416, Variance: 0.02776

Train Epoch: 130 
task: sign, mean loss: 0.00173, accuracy: 1.00000, avg. loss over tasks: 0.00173, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.03247, Variance: 0.00980
Semantic Loss - Mean: 0.01381, Variance: 0.00667

Test Epoch: 130 
task: sign, mean loss: 0.56501, accuracy: 0.86982, avg. loss over tasks: 0.56501
Diversity Loss - Mean: -0.01669, Variance: 0.01192
Semantic Loss - Mean: 0.78688, Variance: 0.02764

Train Epoch: 131 
task: sign, mean loss: 0.00839, accuracy: 1.00000, avg. loss over tasks: 0.00839, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.03357, Variance: 0.00981
Semantic Loss - Mean: 0.02437, Variance: 0.00663

Test Epoch: 131 
task: sign, mean loss: 0.63545, accuracy: 0.84024, avg. loss over tasks: 0.63545
Diversity Loss - Mean: -0.01393, Variance: 0.01193
Semantic Loss - Mean: 0.88995, Variance: 0.02753

Train Epoch: 132 
task: sign, mean loss: 0.01163, accuracy: 0.99457, avg. loss over tasks: 0.01163, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.03112, Variance: 0.00981
Semantic Loss - Mean: 0.01548, Variance: 0.00658

Test Epoch: 132 
task: sign, mean loss: 0.61985, accuracy: 0.86391, avg. loss over tasks: 0.61985
Diversity Loss - Mean: -0.01880, Variance: 0.01193
Semantic Loss - Mean: 0.82232, Variance: 0.02740

Train Epoch: 133 
task: sign, mean loss: 0.00473, accuracy: 1.00000, avg. loss over tasks: 0.00473, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.03279, Variance: 0.00982
Semantic Loss - Mean: 0.02104, Variance: 0.00654

Test Epoch: 133 
task: sign, mean loss: 0.66303, accuracy: 0.84024, avg. loss over tasks: 0.66303
Diversity Loss - Mean: -0.01346, Variance: 0.01193
Semantic Loss - Mean: 0.90184, Variance: 0.02727

Train Epoch: 134 
task: sign, mean loss: 0.00110, accuracy: 1.00000, avg. loss over tasks: 0.00110, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.03242, Variance: 0.00983
Semantic Loss - Mean: 0.01573, Variance: 0.00650

Test Epoch: 134 
task: sign, mean loss: 0.55400, accuracy: 0.87574, avg. loss over tasks: 0.55400
Diversity Loss - Mean: -0.01757, Variance: 0.01194
Semantic Loss - Mean: 0.76366, Variance: 0.02713

Train Epoch: 135 
task: sign, mean loss: 0.00386, accuracy: 1.00000, avg. loss over tasks: 0.00386, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.03413, Variance: 0.00983
Semantic Loss - Mean: 0.02620, Variance: 0.00651

Test Epoch: 135 
task: sign, mean loss: 0.65867, accuracy: 0.84615, avg. loss over tasks: 0.65867
Diversity Loss - Mean: -0.02221, Variance: 0.01195
Semantic Loss - Mean: 0.81771, Variance: 0.02700

Train Epoch: 136 
task: sign, mean loss: 0.00062, accuracy: 1.00000, avg. loss over tasks: 0.00062, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.03515, Variance: 0.00984
Semantic Loss - Mean: 0.01436, Variance: 0.00647

Test Epoch: 136 
task: sign, mean loss: 0.58082, accuracy: 0.88166, avg. loss over tasks: 0.58082
Diversity Loss - Mean: -0.01904, Variance: 0.01195
Semantic Loss - Mean: 0.78219, Variance: 0.02687

Train Epoch: 137 
task: sign, mean loss: 0.00177, accuracy: 1.00000, avg. loss over tasks: 0.00177, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.03534, Variance: 0.00985
Semantic Loss - Mean: 0.02572, Variance: 0.00644

Test Epoch: 137 
task: sign, mean loss: 0.50818, accuracy: 0.89349, avg. loss over tasks: 0.50818
Diversity Loss - Mean: -0.02119, Variance: 0.01196
Semantic Loss - Mean: 0.69989, Variance: 0.02673

Train Epoch: 138 
task: sign, mean loss: 0.00093, accuracy: 1.00000, avg. loss over tasks: 0.00093, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.03847, Variance: 0.00986
Semantic Loss - Mean: 0.01563, Variance: 0.00640

Test Epoch: 138 
task: sign, mean loss: 0.56341, accuracy: 0.87574, avg. loss over tasks: 0.56341
Diversity Loss - Mean: -0.01691, Variance: 0.01196
Semantic Loss - Mean: 0.79345, Variance: 0.02660

Train Epoch: 139 
task: sign, mean loss: 0.00169, accuracy: 1.00000, avg. loss over tasks: 0.00169, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.03638, Variance: 0.00986
Semantic Loss - Mean: 0.02393, Variance: 0.00636

Test Epoch: 139 
task: sign, mean loss: 0.56590, accuracy: 0.86982, avg. loss over tasks: 0.56590
Diversity Loss - Mean: -0.01732, Variance: 0.01197
Semantic Loss - Mean: 0.79742, Variance: 0.02648

Train Epoch: 140 
task: sign, mean loss: 0.04122, accuracy: 0.98913, avg. loss over tasks: 0.04122, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.03535, Variance: 0.00987
Semantic Loss - Mean: 0.03519, Variance: 0.00632

Test Epoch: 140 
task: sign, mean loss: 0.56178, accuracy: 0.87574, avg. loss over tasks: 0.56178
Diversity Loss - Mean: -0.02371, Variance: 0.01198
Semantic Loss - Mean: 0.75404, Variance: 0.02635

Train Epoch: 141 
task: sign, mean loss: 0.00196, accuracy: 1.00000, avg. loss over tasks: 0.00196, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.03596, Variance: 0.00988
Semantic Loss - Mean: 0.01634, Variance: 0.00630

Test Epoch: 141 
task: sign, mean loss: 0.51463, accuracy: 0.87574, avg. loss over tasks: 0.51463
Diversity Loss - Mean: -0.02534, Variance: 0.01198
Semantic Loss - Mean: 0.69661, Variance: 0.02622

Train Epoch: 142 
task: sign, mean loss: 0.00589, accuracy: 1.00000, avg. loss over tasks: 0.00589, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.03505, Variance: 0.00989
Semantic Loss - Mean: 0.04343, Variance: 0.00631

Test Epoch: 142 
task: sign, mean loss: 0.51385, accuracy: 0.88166, avg. loss over tasks: 0.51385
Diversity Loss - Mean: -0.02979, Variance: 0.01199
Semantic Loss - Mean: 0.66220, Variance: 0.02608

Train Epoch: 143 
task: sign, mean loss: 0.00246, accuracy: 1.00000, avg. loss over tasks: 0.00246, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.03647, Variance: 0.00989
Semantic Loss - Mean: 0.01205, Variance: 0.00627

Test Epoch: 143 
task: sign, mean loss: 0.51496, accuracy: 0.87574, avg. loss over tasks: 0.51496
Diversity Loss - Mean: -0.02337, Variance: 0.01200
Semantic Loss - Mean: 0.72010, Variance: 0.02595

Train Epoch: 144 
task: sign, mean loss: 0.00355, accuracy: 1.00000, avg. loss over tasks: 0.00355, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.03398, Variance: 0.00990
Semantic Loss - Mean: 0.01799, Variance: 0.00624

Test Epoch: 144 
task: sign, mean loss: 0.53054, accuracy: 0.89941, avg. loss over tasks: 0.53054
Diversity Loss - Mean: -0.02093, Variance: 0.01201
Semantic Loss - Mean: 0.77284, Variance: 0.02584

Train Epoch: 145 
task: sign, mean loss: 0.00135, accuracy: 1.00000, avg. loss over tasks: 0.00135, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.03435, Variance: 0.00991
Semantic Loss - Mean: 0.01742, Variance: 0.00621

Test Epoch: 145 
task: sign, mean loss: 0.54448, accuracy: 0.87574, avg. loss over tasks: 0.54448
Diversity Loss - Mean: -0.02104, Variance: 0.01202
Semantic Loss - Mean: 0.77021, Variance: 0.02573

Train Epoch: 146 
task: sign, mean loss: 0.00406, accuracy: 1.00000, avg. loss over tasks: 0.00406, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.03623, Variance: 0.00991
Semantic Loss - Mean: 0.02019, Variance: 0.00617

Test Epoch: 146 
task: sign, mean loss: 0.67514, accuracy: 0.82840, avg. loss over tasks: 0.67514
Diversity Loss - Mean: -0.01375, Variance: 0.01202
Semantic Loss - Mean: 0.95837, Variance: 0.02564

Train Epoch: 147 
task: sign, mean loss: 0.00183, accuracy: 1.00000, avg. loss over tasks: 0.00183, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.03370, Variance: 0.00992
Semantic Loss - Mean: 0.01883, Variance: 0.00614

Test Epoch: 147 
task: sign, mean loss: 0.62240, accuracy: 0.85799, avg. loss over tasks: 0.62240
Diversity Loss - Mean: -0.01320, Variance: 0.01202
Semantic Loss - Mean: 0.90739, Variance: 0.02554

Train Epoch: 148 
task: sign, mean loss: 0.00526, accuracy: 1.00000, avg. loss over tasks: 0.00526, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.03197, Variance: 0.00993
Semantic Loss - Mean: 0.02051, Variance: 0.00611

Test Epoch: 148 
task: sign, mean loss: 0.55830, accuracy: 0.89349, avg. loss over tasks: 0.55830
Diversity Loss - Mean: -0.01754, Variance: 0.01203
Semantic Loss - Mean: 0.81437, Variance: 0.02545

Train Epoch: 149 
task: sign, mean loss: 0.00328, accuracy: 1.00000, avg. loss over tasks: 0.00328, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.03385, Variance: 0.00993
Semantic Loss - Mean: 0.02490, Variance: 0.00608

Test Epoch: 149 
task: sign, mean loss: 0.57166, accuracy: 0.86982, avg. loss over tasks: 0.57166
Diversity Loss - Mean: -0.02270, Variance: 0.01203
Semantic Loss - Mean: 0.77269, Variance: 0.02533

Train Epoch: 150 
task: sign, mean loss: 0.00144, accuracy: 1.00000, avg. loss over tasks: 0.00144, lr: 3e-07
Diversity Loss - Mean: -0.03344, Variance: 0.00993
Semantic Loss - Mean: 0.01510, Variance: 0.00605

Test Epoch: 150 
task: sign, mean loss: 0.55722, accuracy: 0.88166, avg. loss over tasks: 0.55722
Diversity Loss - Mean: -0.02325, Variance: 0.01204
Semantic Loss - Mean: 0.75988, Variance: 0.02521

