Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09225, accuracy: 0.63587, avg. loss over tasks: 1.09225, lr: 3e-05
Diversity Loss - Mean: -0.00901, Variance: 0.01050
Semantic Loss - Mean: 1.43046, Variance: 0.07272

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17882, accuracy: 0.66272, avg. loss over tasks: 1.17882
Diversity Loss - Mean: -0.02745, Variance: 0.01240
Semantic Loss - Mean: 1.16152, Variance: 0.05335

Train Epoch: 2 
task: sign, mean loss: 0.97073, accuracy: 0.67391, avg. loss over tasks: 0.97073, lr: 6e-05
Diversity Loss - Mean: -0.01082, Variance: 0.01042
Semantic Loss - Mean: 0.98314, Variance: 0.03941

Test Epoch: 2 
task: sign, mean loss: 1.10779, accuracy: 0.66272, avg. loss over tasks: 1.10779
Diversity Loss - Mean: -0.01663, Variance: 0.01180
Semantic Loss - Mean: 1.14843, Variance: 0.03211

Train Epoch: 3 
task: sign, mean loss: 0.79811, accuracy: 0.69565, avg. loss over tasks: 0.79811, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.01425, Variance: 0.01010
Semantic Loss - Mean: 0.99395, Variance: 0.02722

Test Epoch: 3 
task: sign, mean loss: 1.26575, accuracy: 0.63905, avg. loss over tasks: 1.26575
Diversity Loss - Mean: -0.02836, Variance: 0.01099
Semantic Loss - Mean: 1.11075, Variance: 0.02944

Train Epoch: 4 
task: sign, mean loss: 0.76119, accuracy: 0.69022, avg. loss over tasks: 0.76119, lr: 0.00012
Diversity Loss - Mean: -0.02390, Variance: 0.00975
Semantic Loss - Mean: 0.87663, Variance: 0.02106

Test Epoch: 4 
task: sign, mean loss: 1.54492, accuracy: 0.62130, avg. loss over tasks: 1.54492
Diversity Loss - Mean: -0.01051, Variance: 0.01019
Semantic Loss - Mean: 1.09803, Variance: 0.02425

Train Epoch: 5 
task: sign, mean loss: 0.71143, accuracy: 0.70109, avg. loss over tasks: 0.71143, lr: 0.00015
Diversity Loss - Mean: -0.00549, Variance: 0.00938
Semantic Loss - Mean: 0.75802, Variance: 0.01736

Test Epoch: 5 
task: sign, mean loss: 1.97215, accuracy: 0.44379, avg. loss over tasks: 1.97215
Diversity Loss - Mean: 0.00713, Variance: 0.00984
Semantic Loss - Mean: 1.30178, Variance: 0.02145

Train Epoch: 6 
task: sign, mean loss: 0.65358, accuracy: 0.77174, avg. loss over tasks: 0.65358, lr: 0.00017999999999999998
Diversity Loss - Mean: 0.01095, Variance: 0.00907
Semantic Loss - Mean: 0.70212, Variance: 0.01498

Test Epoch: 6 
task: sign, mean loss: 2.15678, accuracy: 0.63314, avg. loss over tasks: 2.15678
Diversity Loss - Mean: 0.06470, Variance: 0.00993
Semantic Loss - Mean: 1.62416, Variance: 0.02285

Train Epoch: 7 
task: sign, mean loss: 0.51804, accuracy: 0.78261, avg. loss over tasks: 0.51804, lr: 0.00020999999999999998
Diversity Loss - Mean: 0.00519, Variance: 0.00887
Semantic Loss - Mean: 0.56074, Variance: 0.01314

Test Epoch: 7 
task: sign, mean loss: 1.91980, accuracy: 0.62130, avg. loss over tasks: 1.91980
Diversity Loss - Mean: 0.03455, Variance: 0.00995
Semantic Loss - Mean: 1.48122, Variance: 0.02122

Train Epoch: 8 
task: sign, mean loss: 0.55803, accuracy: 0.82065, avg. loss over tasks: 0.55803, lr: 0.00024
Diversity Loss - Mean: 0.01624, Variance: 0.00859
Semantic Loss - Mean: 0.60105, Variance: 0.01213

Test Epoch: 8 
task: sign, mean loss: 3.17352, accuracy: 0.24852, avg. loss over tasks: 3.17352
Diversity Loss - Mean: 0.06363, Variance: 0.01007
Semantic Loss - Mean: 2.21344, Variance: 0.02526

Train Epoch: 9 
task: sign, mean loss: 0.78553, accuracy: 0.69565, avg. loss over tasks: 0.78553, lr: 0.00027
Diversity Loss - Mean: 0.00466, Variance: 0.00851
Semantic Loss - Mean: 0.70976, Variance: 0.01163

Test Epoch: 9 
task: sign, mean loss: 2.46349, accuracy: 0.58580, avg. loss over tasks: 2.46349
Diversity Loss - Mean: 0.07639, Variance: 0.01018
Semantic Loss - Mean: 1.92502, Variance: 0.02658

Train Epoch: 10 
task: sign, mean loss: 0.84262, accuracy: 0.72283, avg. loss over tasks: 0.84262, lr: 0.0003
Diversity Loss - Mean: 0.00260, Variance: 0.00848
Semantic Loss - Mean: 0.71624, Variance: 0.01133

Test Epoch: 10 
task: sign, mean loss: 2.27153, accuracy: 0.53254, avg. loss over tasks: 2.27153
Diversity Loss - Mean: 0.02783, Variance: 0.01033
Semantic Loss - Mean: 1.79144, Variance: 0.02683

Train Epoch: 11 
task: sign, mean loss: 0.57196, accuracy: 0.78261, avg. loss over tasks: 0.57196, lr: 0.0002999622730061346
Diversity Loss - Mean: 0.03660, Variance: 0.00834
Semantic Loss - Mean: 0.55895, Variance: 0.01085

Test Epoch: 11 
task: sign, mean loss: 2.22508, accuracy: 0.56805, avg. loss over tasks: 2.22508
Diversity Loss - Mean: 0.03179, Variance: 0.01051
Semantic Loss - Mean: 1.80687, Variance: 0.02744

Train Epoch: 12 
task: sign, mean loss: 0.46562, accuracy: 0.84239, avg. loss over tasks: 0.46562, lr: 0.000299849111021216
Diversity Loss - Mean: 0.07660, Variance: 0.00820
Semantic Loss - Mean: 0.47700, Variance: 0.01067

Test Epoch: 12 
task: sign, mean loss: 3.50279, accuracy: 0.39053, avg. loss over tasks: 3.50279
Diversity Loss - Mean: 0.12867, Variance: 0.01042
Semantic Loss - Mean: 2.66413, Variance: 0.02695

Train Epoch: 13 
task: sign, mean loss: 0.75474, accuracy: 0.77174, avg. loss over tasks: 0.75474, lr: 0.0002996605710257114
Diversity Loss - Mean: 0.05555, Variance: 0.00819
Semantic Loss - Mean: 0.71401, Variance: 0.01021

Test Epoch: 13 
task: sign, mean loss: 1.72028, accuracy: 0.36686, avg. loss over tasks: 1.72028
Diversity Loss - Mean: 0.03251, Variance: 0.01050
Semantic Loss - Mean: 1.53282, Variance: 0.02711

Train Epoch: 14 
task: sign, mean loss: 0.44034, accuracy: 0.83696, avg. loss over tasks: 0.44034, lr: 0.00029939674795518656
Diversity Loss - Mean: 0.02392, Variance: 0.00825
Semantic Loss - Mean: 0.46565, Variance: 0.00964

Test Epoch: 14 
task: sign, mean loss: 1.85235, accuracy: 0.38462, avg. loss over tasks: 1.85235
Diversity Loss - Mean: 0.03418, Variance: 0.01056
Semantic Loss - Mean: 1.64727, Variance: 0.02653

Train Epoch: 15 
task: sign, mean loss: 0.23188, accuracy: 0.91848, avg. loss over tasks: 0.23188, lr: 0.0002990577746525024
Diversity Loss - Mean: 0.04235, Variance: 0.00825
Semantic Loss - Mean: 0.27294, Variance: 0.00921

Test Epoch: 15 
task: sign, mean loss: 2.03050, accuracy: 0.52071, avg. loss over tasks: 2.03050
Diversity Loss - Mean: 0.06556, Variance: 0.01053
Semantic Loss - Mean: 1.81909, Variance: 0.02599

Train Epoch: 16 
task: sign, mean loss: 0.20645, accuracy: 0.93478, avg. loss over tasks: 0.20645, lr: 0.000298643821800925
Diversity Loss - Mean: 0.06460, Variance: 0.00822
Semantic Loss - Mean: 0.25323, Variance: 0.00900

Test Epoch: 16 
task: sign, mean loss: 2.86593, accuracy: 0.33728, avg. loss over tasks: 2.86593
Diversity Loss - Mean: 0.10145, Variance: 0.01050
Semantic Loss - Mean: 2.25501, Variance: 0.02813

Train Epoch: 17 
task: sign, mean loss: 0.17357, accuracy: 0.93478, avg. loss over tasks: 0.17357, lr: 0.0002981550978381814
Diversity Loss - Mean: 0.07031, Variance: 0.00816
Semantic Loss - Mean: 0.19999, Variance: 0.00865

Test Epoch: 17 
task: sign, mean loss: 1.62186, accuracy: 0.62722, avg. loss over tasks: 1.62186
Diversity Loss - Mean: 0.06761, Variance: 0.01048
Semantic Loss - Mean: 1.38672, Variance: 0.02854

Train Epoch: 18 
task: sign, mean loss: 0.26245, accuracy: 0.91848, avg. loss over tasks: 0.26245, lr: 0.00029759184885150465
Diversity Loss - Mean: 0.05552, Variance: 0.00813
Semantic Loss - Mean: 0.31811, Variance: 0.00868

Test Epoch: 18 
task: sign, mean loss: 2.39843, accuracy: 0.44970, avg. loss over tasks: 2.39843
Diversity Loss - Mean: 0.10006, Variance: 0.01039
Semantic Loss - Mean: 2.07752, Variance: 0.02853

Train Epoch: 19 
task: sign, mean loss: 0.46311, accuracy: 0.85870, avg. loss over tasks: 0.46311, lr: 0.0002969543584537218
Diversity Loss - Mean: 0.05280, Variance: 0.00808
Semantic Loss - Mean: 0.46076, Variance: 0.00902

Test Epoch: 19 
task: sign, mean loss: 1.80715, accuracy: 0.26036, avg. loss over tasks: 1.80715
Diversity Loss - Mean: 0.11869, Variance: 0.01054
Semantic Loss - Mean: 1.63377, Variance: 0.02996

Train Epoch: 20 
task: sign, mean loss: 0.37096, accuracy: 0.88043, avg. loss over tasks: 0.37096, lr: 0.0002962429476404462
Diversity Loss - Mean: 0.01210, Variance: 0.00818
Semantic Loss - Mean: 0.41914, Variance: 0.00937

Test Epoch: 20 
task: sign, mean loss: 1.56141, accuracy: 0.58580, avg. loss over tasks: 1.56141
Diversity Loss - Mean: 0.01585, Variance: 0.01061
Semantic Loss - Mean: 1.34309, Variance: 0.02953

Train Epoch: 21 
task: sign, mean loss: 0.32625, accuracy: 0.88043, avg. loss over tasks: 0.32625, lr: 0.00029545797462844647
Diversity Loss - Mean: 0.02498, Variance: 0.00822
Semantic Loss - Mean: 0.35646, Variance: 0.00917

Test Epoch: 21 
task: sign, mean loss: 1.64670, accuracy: 0.52071, avg. loss over tasks: 1.64670
Diversity Loss - Mean: 0.07343, Variance: 0.01060
Semantic Loss - Mean: 1.41662, Variance: 0.02957

Train Epoch: 22 
task: sign, mean loss: 0.30487, accuracy: 0.89674, avg. loss over tasks: 0.30487, lr: 0.0002945998346752736
Diversity Loss - Mean: 0.03153, Variance: 0.00826
Semantic Loss - Mean: 0.35061, Variance: 0.00911

Test Epoch: 22 
task: sign, mean loss: 0.70900, accuracy: 0.73964, avg. loss over tasks: 0.70900
Diversity Loss - Mean: 0.02725, Variance: 0.01066
Semantic Loss - Mean: 0.70176, Variance: 0.02913

Train Epoch: 23 
task: sign, mean loss: 0.32538, accuracy: 0.87500, avg. loss over tasks: 0.32538, lr: 0.0002936689598802368
Diversity Loss - Mean: 0.03154, Variance: 0.00828
Semantic Loss - Mean: 0.36685, Variance: 0.00900

Test Epoch: 23 
task: sign, mean loss: 1.92239, accuracy: 0.61538, avg. loss over tasks: 1.92239
Diversity Loss - Mean: 0.05257, Variance: 0.01064
Semantic Loss - Mean: 1.65435, Variance: 0.02910

Train Epoch: 24 
task: sign, mean loss: 0.19960, accuracy: 0.92935, avg. loss over tasks: 0.19960, lr: 0.00029266581896682876
Diversity Loss - Mean: 0.03130, Variance: 0.00829
Semantic Loss - Mean: 0.24278, Variance: 0.00884

Test Epoch: 24 
task: sign, mean loss: 1.69604, accuracy: 0.67456, avg. loss over tasks: 1.69604
Diversity Loss - Mean: 0.05016, Variance: 0.01058
Semantic Loss - Mean: 1.46925, Variance: 0.02903

Train Epoch: 25 
task: sign, mean loss: 0.10586, accuracy: 0.95109, avg. loss over tasks: 0.10586, lr: 0.00029159091704670885
Diversity Loss - Mean: 0.04349, Variance: 0.00827
Semantic Loss - Mean: 0.15794, Variance: 0.00868

Test Epoch: 25 
task: sign, mean loss: 1.14138, accuracy: 0.73964, avg. loss over tasks: 1.14138
Diversity Loss - Mean: 0.02840, Variance: 0.01056
Semantic Loss - Mean: 1.07015, Variance: 0.02891

Train Epoch: 26 
task: sign, mean loss: 0.16858, accuracy: 0.92935, avg. loss over tasks: 0.16858, lr: 0.00029044479536536455
Diversity Loss - Mean: 0.05259, Variance: 0.00823
Semantic Loss - Mean: 0.22052, Variance: 0.00866

Test Epoch: 26 
task: sign, mean loss: 1.21016, accuracy: 0.77515, avg. loss over tasks: 1.21016
Diversity Loss - Mean: 0.02040, Variance: 0.01053
Semantic Loss - Mean: 1.08929, Variance: 0.02854

Train Epoch: 27 
task: sign, mean loss: 0.21346, accuracy: 0.93478, avg. loss over tasks: 0.21346, lr: 0.000289228031029578
Diversity Loss - Mean: 0.04550, Variance: 0.00820
Semantic Loss - Mean: 0.27596, Variance: 0.00885

Test Epoch: 27 
task: sign, mean loss: 0.56430, accuracy: 0.79882, avg. loss over tasks: 0.56430
Diversity Loss - Mean: 0.01957, Variance: 0.01054
Semantic Loss - Mean: 0.60375, Variance: 0.02823

Train Epoch: 28 
task: sign, mean loss: 0.18691, accuracy: 0.90761, avg. loss over tasks: 0.18691, lr: 0.0002879412367168349
Diversity Loss - Mean: 0.03068, Variance: 0.00822
Semantic Loss - Mean: 0.22219, Variance: 0.00884

Test Epoch: 28 
task: sign, mean loss: 1.03470, accuracy: 0.65089, avg. loss over tasks: 1.03470
Diversity Loss - Mean: 0.05469, Variance: 0.01055
Semantic Loss - Mean: 0.97794, Variance: 0.02810

Train Epoch: 29 
task: sign, mean loss: 0.13043, accuracy: 0.94565, avg. loss over tasks: 0.13043, lr: 0.00028658506036682353
Diversity Loss - Mean: 0.02591, Variance: 0.00823
Semantic Loss - Mean: 0.20358, Variance: 0.00885

Test Epoch: 29 
task: sign, mean loss: 0.57538, accuracy: 0.83432, avg. loss over tasks: 0.57538
Diversity Loss - Mean: 0.02649, Variance: 0.01060
Semantic Loss - Mean: 0.52939, Variance: 0.02731

Train Epoch: 30 
task: sign, mean loss: 0.16443, accuracy: 0.92935, avg. loss over tasks: 0.16443, lr: 0.00028516018485517746
Diversity Loss - Mean: 0.02909, Variance: 0.00824
Semantic Loss - Mean: 0.20344, Variance: 0.00877

Test Epoch: 30 
task: sign, mean loss: 1.00335, accuracy: 0.79290, avg. loss over tasks: 1.00335
Diversity Loss - Mean: -0.01517, Variance: 0.01069
Semantic Loss - Mean: 0.95717, Variance: 0.02763

Train Epoch: 31 
task: sign, mean loss: 0.19419, accuracy: 0.92391, avg. loss over tasks: 0.19419, lr: 0.00028366732764962686
Diversity Loss - Mean: 0.03321, Variance: 0.00825
Semantic Loss - Mean: 0.23432, Variance: 0.00873

Test Epoch: 31 
task: sign, mean loss: 0.86814, accuracy: 0.79290, avg. loss over tasks: 0.86814
Diversity Loss - Mean: 0.02149, Variance: 0.01076
Semantic Loss - Mean: 0.85375, Variance: 0.02745

Train Epoch: 32 
task: sign, mean loss: 0.17461, accuracy: 0.93478, avg. loss over tasks: 0.17461, lr: 0.00028210724044873213
Diversity Loss - Mean: 0.02791, Variance: 0.00826
Semantic Loss - Mean: 0.22327, Variance: 0.00871

Test Epoch: 32 
task: sign, mean loss: 1.97333, accuracy: 0.63314, avg. loss over tasks: 1.97333
Diversity Loss - Mean: 0.06270, Variance: 0.01074
Semantic Loss - Mean: 1.69921, Variance: 0.02726

Train Epoch: 33 
task: sign, mean loss: 0.17980, accuracy: 0.94022, avg. loss over tasks: 0.17980, lr: 0.00028048070880338095
Diversity Loss - Mean: 0.02183, Variance: 0.00827
Semantic Loss - Mean: 0.20645, Variance: 0.00865

Test Epoch: 33 
task: sign, mean loss: 1.24890, accuracy: 0.68047, avg. loss over tasks: 1.24890
Diversity Loss - Mean: 0.02817, Variance: 0.01076
Semantic Loss - Mean: 1.11685, Variance: 0.02681

Train Epoch: 34 
task: sign, mean loss: 0.24786, accuracy: 0.90761, avg. loss over tasks: 0.24786, lr: 0.00027878855172123963
Diversity Loss - Mean: 0.02232, Variance: 0.00828
Semantic Loss - Mean: 0.28434, Variance: 0.00857

Test Epoch: 34 
task: sign, mean loss: 1.04871, accuracy: 0.71598, avg. loss over tasks: 1.04871
Diversity Loss - Mean: 0.01279, Variance: 0.01080
Semantic Loss - Mean: 0.98038, Variance: 0.02661

Train Epoch: 35 
task: sign, mean loss: 0.24574, accuracy: 0.91304, avg. loss over tasks: 0.24574, lr: 0.00027703162125435835
Diversity Loss - Mean: 0.03050, Variance: 0.00831
Semantic Loss - Mean: 0.29745, Variance: 0.00866

Test Epoch: 35 
task: sign, mean loss: 1.24896, accuracy: 0.71006, avg. loss over tasks: 1.24896
Diversity Loss - Mean: 0.05125, Variance: 0.01077
Semantic Loss - Mean: 1.28028, Variance: 0.02688

Train Epoch: 36 
task: sign, mean loss: 0.15968, accuracy: 0.92935, avg. loss over tasks: 0.15968, lr: 0.00027521080207013716
Diversity Loss - Mean: 0.03317, Variance: 0.00832
Semantic Loss - Mean: 0.20737, Variance: 0.00852

Test Epoch: 36 
task: sign, mean loss: 0.93704, accuracy: 0.66864, avg. loss over tasks: 0.93704
Diversity Loss - Mean: 0.05930, Variance: 0.01072
Semantic Loss - Mean: 0.93794, Variance: 0.02657

Train Epoch: 37 
task: sign, mean loss: 0.11355, accuracy: 0.94022, avg. loss over tasks: 0.11355, lr: 0.0002733270110058693
Diversity Loss - Mean: 0.02861, Variance: 0.00833
Semantic Loss - Mean: 0.15500, Variance: 0.00837

Test Epoch: 37 
task: sign, mean loss: 0.89524, accuracy: 0.71598, avg. loss over tasks: 0.89524
Diversity Loss - Mean: 0.03473, Variance: 0.01070
Semantic Loss - Mean: 0.86433, Variance: 0.02614

Train Epoch: 38 
task: sign, mean loss: 0.14426, accuracy: 0.91848, avg. loss over tasks: 0.14426, lr: 0.00027138119660708587
Diversity Loss - Mean: 0.03205, Variance: 0.00834
Semantic Loss - Mean: 0.17020, Variance: 0.00825

Test Epoch: 38 
task: sign, mean loss: 0.72502, accuracy: 0.76923, avg. loss over tasks: 0.72502
Diversity Loss - Mean: 0.03490, Variance: 0.01070
Semantic Loss - Mean: 0.67983, Variance: 0.02565

Train Epoch: 39 
task: sign, mean loss: 0.14252, accuracy: 0.94022, avg. loss over tasks: 0.14252, lr: 0.0002693743386499349
Diversity Loss - Mean: 0.03457, Variance: 0.00834
Semantic Loss - Mean: 0.19224, Variance: 0.00846

Test Epoch: 39 
task: sign, mean loss: 0.46395, accuracy: 0.83432, avg. loss over tasks: 0.46395
Diversity Loss - Mean: 0.00786, Variance: 0.01077
Semantic Loss - Mean: 0.47110, Variance: 0.02526

Train Epoch: 40 
task: sign, mean loss: 0.10649, accuracy: 0.95109, avg. loss over tasks: 0.10649, lr: 0.00026730744764783427
Diversity Loss - Mean: 0.03192, Variance: 0.00835
Semantic Loss - Mean: 0.15755, Variance: 0.00855

Test Epoch: 40 
task: sign, mean loss: 0.71416, accuracy: 0.82840, avg. loss over tasks: 0.71416
Diversity Loss - Mean: 0.02892, Variance: 0.01079
Semantic Loss - Mean: 0.74746, Variance: 0.02497

Train Epoch: 41 
task: sign, mean loss: 0.05989, accuracy: 0.97826, avg. loss over tasks: 0.05989, lr: 0.00026518156434264794
Diversity Loss - Mean: 0.03495, Variance: 0.00835
Semantic Loss - Mean: 0.09645, Variance: 0.00844

Test Epoch: 41 
task: sign, mean loss: 2.28789, accuracy: 0.54438, avg. loss over tasks: 2.28789
Diversity Loss - Mean: 0.09331, Variance: 0.01075
Semantic Loss - Mean: 2.04588, Variance: 0.02628

Train Epoch: 42 
task: sign, mean loss: 0.07701, accuracy: 0.97283, avg. loss over tasks: 0.07701, lr: 0.0002629977591806411
Diversity Loss - Mean: 0.03413, Variance: 0.00835
Semantic Loss - Mean: 0.09285, Variance: 0.00831

Test Epoch: 42 
task: sign, mean loss: 1.62395, accuracy: 0.73964, avg. loss over tasks: 1.62395
Diversity Loss - Mean: 0.03222, Variance: 0.01072
Semantic Loss - Mean: 1.30158, Variance: 0.02602

Train Epoch: 43 
task: sign, mean loss: 0.07980, accuracy: 0.97826, avg. loss over tasks: 0.07980, lr: 0.000260757131773478
Diversity Loss - Mean: 0.03497, Variance: 0.00834
Semantic Loss - Mean: 0.11762, Variance: 0.00822

Test Epoch: 43 
task: sign, mean loss: 1.14231, accuracy: 0.77515, avg. loss over tasks: 1.14231
Diversity Loss - Mean: -0.00101, Variance: 0.01075
Semantic Loss - Mean: 0.99206, Variance: 0.02585

Train Epoch: 44 
task: sign, mean loss: 0.04649, accuracy: 0.98370, avg. loss over tasks: 0.04649, lr: 0.0002584608103445346
Diversity Loss - Mean: 0.02147, Variance: 0.00834
Semantic Loss - Mean: 0.08941, Variance: 0.00818

Test Epoch: 44 
task: sign, mean loss: 1.27165, accuracy: 0.72189, avg. loss over tasks: 1.27165
Diversity Loss - Mean: 0.03309, Variance: 0.01073
Semantic Loss - Mean: 1.21096, Variance: 0.02572

Train Epoch: 45 
task: sign, mean loss: 0.09153, accuracy: 0.96739, avg. loss over tasks: 0.09153, lr: 0.0002561099511608041
Diversity Loss - Mean: 0.01490, Variance: 0.00836
Semantic Loss - Mean: 0.12830, Variance: 0.00811

Test Epoch: 45 
task: sign, mean loss: 1.27601, accuracy: 0.63905, avg. loss over tasks: 1.27601
Diversity Loss - Mean: 0.03346, Variance: 0.01071
Semantic Loss - Mean: 1.16724, Variance: 0.02575

Train Epoch: 46 
task: sign, mean loss: 0.09653, accuracy: 0.96739, avg. loss over tasks: 0.09653, lr: 0.00025370573795068164
Diversity Loss - Mean: 0.01365, Variance: 0.00837
Semantic Loss - Mean: 0.14658, Variance: 0.00809

Test Epoch: 46 
task: sign, mean loss: 2.25853, accuracy: 0.69231, avg. loss over tasks: 2.25853
Diversity Loss - Mean: 0.04384, Variance: 0.01071
Semantic Loss - Mean: 1.93681, Variance: 0.02603

Train Epoch: 47 
task: sign, mean loss: 0.07308, accuracy: 0.97826, avg. loss over tasks: 0.07308, lr: 0.0002512493813079214
Diversity Loss - Mean: 0.01901, Variance: 0.00838
Semantic Loss - Mean: 0.13089, Variance: 0.00805

Test Epoch: 47 
task: sign, mean loss: 2.40343, accuracy: 0.48521, avg. loss over tasks: 2.40343
Diversity Loss - Mean: 0.08560, Variance: 0.01067
Semantic Loss - Mean: 2.16101, Variance: 0.02907

Train Epoch: 48 
task: sign, mean loss: 0.09831, accuracy: 0.96739, avg. loss over tasks: 0.09831, lr: 0.0002487421180820659
Diversity Loss - Mean: 0.02870, Variance: 0.00836
Semantic Loss - Mean: 0.13502, Variance: 0.00800

Test Epoch: 48 
task: sign, mean loss: 0.79518, accuracy: 0.79290, avg. loss over tasks: 0.79518
Diversity Loss - Mean: 0.01824, Variance: 0.01066
Semantic Loss - Mean: 0.80714, Variance: 0.02897

Train Epoch: 49 
task: sign, mean loss: 0.09890, accuracy: 0.98370, avg. loss over tasks: 0.09890, lr: 0.0002461852107556558
Diversity Loss - Mean: 0.03803, Variance: 0.00834
Semantic Loss - Mean: 0.13642, Variance: 0.00802

Test Epoch: 49 
task: sign, mean loss: 1.36888, accuracy: 0.76923, avg. loss over tasks: 1.36888
Diversity Loss - Mean: 0.04508, Variance: 0.01063
Semantic Loss - Mean: 1.30652, Variance: 0.02934

Train Epoch: 50 
task: sign, mean loss: 0.06887, accuracy: 0.97826, avg. loss over tasks: 0.06887, lr: 0.00024357994680853121
Diversity Loss - Mean: 0.03334, Variance: 0.00834
Semantic Loss - Mean: 0.13832, Variance: 0.00804

Test Epoch: 50 
task: sign, mean loss: 1.41834, accuracy: 0.79290, avg. loss over tasks: 1.41834
Diversity Loss - Mean: 0.05366, Variance: 0.01063
Semantic Loss - Mean: 1.41607, Variance: 0.02961

Train Epoch: 51 
task: sign, mean loss: 0.05947, accuracy: 0.97826, avg. loss over tasks: 0.05947, lr: 0.00024092763806954684
Diversity Loss - Mean: 0.01906, Variance: 0.00835
Semantic Loss - Mean: 0.11036, Variance: 0.00807

Test Epoch: 51 
task: sign, mean loss: 1.27243, accuracy: 0.79290, avg. loss over tasks: 1.27243
Diversity Loss - Mean: 0.05334, Variance: 0.01062
Semantic Loss - Mean: 1.12821, Variance: 0.02971

Train Epoch: 52 
task: sign, mean loss: 0.01987, accuracy: 0.98913, avg. loss over tasks: 0.01987, lr: 0.00023822962005602707
Diversity Loss - Mean: 0.01197, Variance: 0.00837
Semantic Loss - Mean: 0.06926, Variance: 0.00803

Test Epoch: 52 
task: sign, mean loss: 0.89901, accuracy: 0.84024, avg. loss over tasks: 0.89901
Diversity Loss - Mean: 0.02824, Variance: 0.01062
Semantic Loss - Mean: 0.81986, Variance: 0.02952

Train Epoch: 53 
task: sign, mean loss: 0.01897, accuracy: 1.00000, avg. loss over tasks: 0.01897, lr: 0.00023548725130129248
Diversity Loss - Mean: 0.01064, Variance: 0.00838
Semantic Loss - Mean: 0.07029, Variance: 0.00800

Test Epoch: 53 
task: sign, mean loss: 1.01069, accuracy: 0.81657, avg. loss over tasks: 1.01069
Diversity Loss - Mean: 0.00711, Variance: 0.01063
Semantic Loss - Mean: 0.84375, Variance: 0.02944

Train Epoch: 54 
task: sign, mean loss: 0.00919, accuracy: 1.00000, avg. loss over tasks: 0.00919, lr: 0.00023270191267059755
Diversity Loss - Mean: 0.02084, Variance: 0.00839
Semantic Loss - Mean: 0.05269, Variance: 0.00796

Test Epoch: 54 
task: sign, mean loss: 1.34503, accuracy: 0.79290, avg. loss over tasks: 1.34503
Diversity Loss - Mean: 0.02049, Variance: 0.01062
Semantic Loss - Mean: 1.19466, Variance: 0.03005

Train Epoch: 55 
task: sign, mean loss: 0.08061, accuracy: 0.98370, avg. loss over tasks: 0.08061, lr: 0.00022987500666582316
Diversity Loss - Mean: 0.02024, Variance: 0.00839
Semantic Loss - Mean: 0.07115, Variance: 0.00788

Test Epoch: 55 
task: sign, mean loss: 1.19939, accuracy: 0.78698, avg. loss over tasks: 1.19939
Diversity Loss - Mean: 0.01289, Variance: 0.01061
Semantic Loss - Mean: 1.10794, Variance: 0.03006

Train Epoch: 56 
task: sign, mean loss: 0.02859, accuracy: 0.98913, avg. loss over tasks: 0.02859, lr: 0.00022700795671927503
Diversity Loss - Mean: 0.01550, Variance: 0.00839
Semantic Loss - Mean: 0.05331, Variance: 0.00780

Test Epoch: 56 
task: sign, mean loss: 1.31113, accuracy: 0.76923, avg. loss over tasks: 1.31113
Diversity Loss - Mean: 0.02128, Variance: 0.01060
Semantic Loss - Mean: 1.30865, Variance: 0.03013

Train Epoch: 57 
task: sign, mean loss: 0.06464, accuracy: 0.97826, avg. loss over tasks: 0.06464, lr: 0.00022410220647694235
Diversity Loss - Mean: 0.00522, Variance: 0.00841
Semantic Loss - Mean: 0.08673, Variance: 0.00779

Test Epoch: 57 
task: sign, mean loss: 1.41487, accuracy: 0.76331, avg. loss over tasks: 1.41487
Diversity Loss - Mean: 0.01312, Variance: 0.01058
Semantic Loss - Mean: 1.40410, Variance: 0.03016

Train Epoch: 58 
task: sign, mean loss: 0.00757, accuracy: 1.00000, avg. loss over tasks: 0.00757, lr: 0.00022115921907157884
Diversity Loss - Mean: 0.00214, Variance: 0.00842
Semantic Loss - Mean: 0.03225, Variance: 0.00768

Test Epoch: 58 
task: sign, mean loss: 1.32984, accuracy: 0.78107, avg. loss over tasks: 1.32984
Diversity Loss - Mean: 0.00706, Variance: 0.01058
Semantic Loss - Mean: 1.22201, Variance: 0.03010

Train Epoch: 59 
task: sign, mean loss: 0.03223, accuracy: 0.98913, avg. loss over tasks: 0.03223, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.00143, Variance: 0.00842
Semantic Loss - Mean: 0.06038, Variance: 0.00762

Test Epoch: 59 
task: sign, mean loss: 1.11324, accuracy: 0.77515, avg. loss over tasks: 1.11324
Diversity Loss - Mean: 0.02016, Variance: 0.01056
Semantic Loss - Mean: 1.09703, Variance: 0.03054

Train Epoch: 60 
task: sign, mean loss: 0.04139, accuracy: 0.98370, avg. loss over tasks: 0.04139, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.00530, Variance: 0.00843
Semantic Loss - Mean: 0.07899, Variance: 0.00756

Test Epoch: 60 
task: sign, mean loss: 1.05528, accuracy: 0.72781, avg. loss over tasks: 1.05528
Diversity Loss - Mean: 0.04945, Variance: 0.01053
Semantic Loss - Mean: 1.18672, Variance: 0.03169

Train Epoch: 61 
task: sign, mean loss: 0.03807, accuracy: 0.98370, avg. loss over tasks: 0.03807, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.00178, Variance: 0.00843
Semantic Loss - Mean: 0.10135, Variance: 0.00766

Test Epoch: 61 
task: sign, mean loss: 1.05800, accuracy: 0.79290, avg. loss over tasks: 1.05800
Diversity Loss - Mean: 0.04165, Variance: 0.01052
Semantic Loss - Mean: 1.15399, Variance: 0.03189

Train Epoch: 62 
task: sign, mean loss: 0.01648, accuracy: 0.99457, avg. loss over tasks: 0.01648, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.00172, Variance: 0.00845
Semantic Loss - Mean: 0.05965, Variance: 0.00765

Test Epoch: 62 
task: sign, mean loss: 1.06002, accuracy: 0.75148, avg. loss over tasks: 1.06002
Diversity Loss - Mean: 0.03939, Variance: 0.01051
Semantic Loss - Mean: 1.27799, Variance: 0.03294

Train Epoch: 63 
task: sign, mean loss: 0.03551, accuracy: 0.98913, avg. loss over tasks: 0.03551, lr: 0.00020593820471153146
Diversity Loss - Mean: 0.00061, Variance: 0.00846
Semantic Loss - Mean: 0.07927, Variance: 0.00760

Test Epoch: 63 
task: sign, mean loss: 1.69309, accuracy: 0.72781, avg. loss over tasks: 1.69309
Diversity Loss - Mean: 0.03954, Variance: 0.01050
Semantic Loss - Mean: 1.61799, Variance: 0.03339

Train Epoch: 64 
task: sign, mean loss: 0.01079, accuracy: 1.00000, avg. loss over tasks: 0.01079, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.00656, Variance: 0.00847
Semantic Loss - Mean: 0.04953, Variance: 0.00766

Test Epoch: 64 
task: sign, mean loss: 1.59085, accuracy: 0.73964, avg. loss over tasks: 1.59085
Diversity Loss - Mean: 0.02927, Variance: 0.01048
Semantic Loss - Mean: 1.56804, Variance: 0.03409

Train Epoch: 65 
task: sign, mean loss: 0.06733, accuracy: 0.98370, avg. loss over tasks: 0.06733, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.00666, Variance: 0.00848
Semantic Loss - Mean: 0.09641, Variance: 0.00771

Test Epoch: 65 
task: sign, mean loss: 1.19311, accuracy: 0.76331, avg. loss over tasks: 1.19311
Diversity Loss - Mean: 0.01649, Variance: 0.01047
Semantic Loss - Mean: 1.18389, Variance: 0.03464

Train Epoch: 66 
task: sign, mean loss: 0.04733, accuracy: 0.98370, avg. loss over tasks: 0.04733, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.01047, Variance: 0.00849
Semantic Loss - Mean: 0.09328, Variance: 0.00773

Test Epoch: 66 
task: sign, mean loss: 0.75286, accuracy: 0.84615, avg. loss over tasks: 0.75286
Diversity Loss - Mean: -0.00663, Variance: 0.01049
Semantic Loss - Mean: 0.64898, Variance: 0.03433

Train Epoch: 67 
task: sign, mean loss: 0.03852, accuracy: 0.98913, avg. loss over tasks: 0.03852, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.01669, Variance: 0.00851
Semantic Loss - Mean: 0.07208, Variance: 0.00771

Test Epoch: 67 
task: sign, mean loss: 0.67293, accuracy: 0.82249, avg. loss over tasks: 0.67293
Diversity Loss - Mean: -0.00138, Variance: 0.01051
Semantic Loss - Mean: 0.85239, Variance: 0.03538

Train Epoch: 68 
task: sign, mean loss: 0.11614, accuracy: 0.97283, avg. loss over tasks: 0.11614, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.01239, Variance: 0.00852
Semantic Loss - Mean: 0.16936, Variance: 0.00780

Test Epoch: 68 
task: sign, mean loss: 0.80160, accuracy: 0.82840, avg. loss over tasks: 0.80160
Diversity Loss - Mean: -0.02588, Variance: 0.01054
Semantic Loss - Mean: 0.83596, Variance: 0.03545

Train Epoch: 69 
task: sign, mean loss: 0.01042, accuracy: 1.00000, avg. loss over tasks: 0.01042, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.02588, Variance: 0.00855
Semantic Loss - Mean: 0.07549, Variance: 0.00782

Test Epoch: 69 
task: sign, mean loss: 1.01190, accuracy: 0.80473, avg. loss over tasks: 1.01190
Diversity Loss - Mean: -0.01909, Variance: 0.01056
Semantic Loss - Mean: 0.93164, Variance: 0.03523

Train Epoch: 70 
task: sign, mean loss: 0.02044, accuracy: 0.99457, avg. loss over tasks: 0.02044, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.02294, Variance: 0.00858
Semantic Loss - Mean: 0.04801, Variance: 0.00775

Test Epoch: 70 
task: sign, mean loss: 0.91741, accuracy: 0.81657, avg. loss over tasks: 0.91741
Diversity Loss - Mean: -0.00437, Variance: 0.01058
Semantic Loss - Mean: 0.94266, Variance: 0.03507

Train Epoch: 71 
task: sign, mean loss: 0.02295, accuracy: 0.99457, avg. loss over tasks: 0.02295, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.01990, Variance: 0.00860
Semantic Loss - Mean: 0.06765, Variance: 0.00771

Test Epoch: 71 
task: sign, mean loss: 1.00029, accuracy: 0.81065, avg. loss over tasks: 1.00029
Diversity Loss - Mean: 0.00486, Variance: 0.01059
Semantic Loss - Mean: 1.15060, Variance: 0.03525

Train Epoch: 72 
task: sign, mean loss: 0.01121, accuracy: 1.00000, avg. loss over tasks: 0.01121, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.01971, Variance: 0.00862
Semantic Loss - Mean: 0.05442, Variance: 0.00764

Test Epoch: 72 
task: sign, mean loss: 1.41648, accuracy: 0.74556, avg. loss over tasks: 1.41648
Diversity Loss - Mean: 0.02733, Variance: 0.01058
Semantic Loss - Mean: 1.47359, Variance: 0.03552

Train Epoch: 73 
task: sign, mean loss: 0.01349, accuracy: 0.99457, avg. loss over tasks: 0.01349, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.02431, Variance: 0.00864
Semantic Loss - Mean: 0.05593, Variance: 0.00764

Test Epoch: 73 
task: sign, mean loss: 1.11807, accuracy: 0.79290, avg. loss over tasks: 1.11807
Diversity Loss - Mean: -0.00827, Variance: 0.01059
Semantic Loss - Mean: 1.06297, Variance: 0.03555

Train Epoch: 74 
task: sign, mean loss: 0.01736, accuracy: 0.99457, avg. loss over tasks: 0.01736, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.02273, Variance: 0.00867
Semantic Loss - Mean: 0.05319, Variance: 0.00768

Test Epoch: 74 
task: sign, mean loss: 1.44138, accuracy: 0.77515, avg. loss over tasks: 1.44138
Diversity Loss - Mean: 0.01609, Variance: 0.01059
Semantic Loss - Mean: 1.43408, Variance: 0.03575

Train Epoch: 75 
task: sign, mean loss: 0.02011, accuracy: 0.99457, avg. loss over tasks: 0.02011, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.02223, Variance: 0.00869
Semantic Loss - Mean: 0.06893, Variance: 0.00768

Test Epoch: 75 
task: sign, mean loss: 0.88792, accuracy: 0.86391, avg. loss over tasks: 0.88792
Diversity Loss - Mean: 0.00076, Variance: 0.01061
Semantic Loss - Mean: 0.87281, Variance: 0.03583

Train Epoch: 76 
task: sign, mean loss: 0.01096, accuracy: 0.99457, avg. loss over tasks: 0.01096, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.02072, Variance: 0.00871
Semantic Loss - Mean: 0.05489, Variance: 0.00767

Test Epoch: 76 
task: sign, mean loss: 0.80568, accuracy: 0.85799, avg. loss over tasks: 0.80568
Diversity Loss - Mean: -0.00689, Variance: 0.01063
Semantic Loss - Mean: 0.82960, Variance: 0.03593

Train Epoch: 77 
task: sign, mean loss: 0.00872, accuracy: 1.00000, avg. loss over tasks: 0.00872, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.02407, Variance: 0.00873
Semantic Loss - Mean: 0.06240, Variance: 0.00766

Test Epoch: 77 
task: sign, mean loss: 0.84907, accuracy: 0.86391, avg. loss over tasks: 0.84907
Diversity Loss - Mean: 0.00219, Variance: 0.01065
Semantic Loss - Mean: 0.95507, Variance: 0.03625

Train Epoch: 78 
task: sign, mean loss: 0.01216, accuracy: 0.99457, avg. loss over tasks: 0.01216, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.02188, Variance: 0.00875
Semantic Loss - Mean: 0.04036, Variance: 0.00759

Test Epoch: 78 
task: sign, mean loss: 0.86136, accuracy: 0.85207, avg. loss over tasks: 0.86136
Diversity Loss - Mean: 0.01036, Variance: 0.01066
Semantic Loss - Mean: 1.01692, Variance: 0.03652

Train Epoch: 79 
task: sign, mean loss: 0.00400, accuracy: 1.00000, avg. loss over tasks: 0.00400, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.02375, Variance: 0.00877
Semantic Loss - Mean: 0.03070, Variance: 0.00752

Test Epoch: 79 
task: sign, mean loss: 1.02889, accuracy: 0.84615, avg. loss over tasks: 1.02889
Diversity Loss - Mean: 0.01974, Variance: 0.01067
Semantic Loss - Mean: 1.12725, Variance: 0.03658

Train Epoch: 80 
task: sign, mean loss: 0.00414, accuracy: 1.00000, avg. loss over tasks: 0.00414, lr: 0.00015015
Diversity Loss - Mean: -0.01906, Variance: 0.00878
Semantic Loss - Mean: 0.02764, Variance: 0.00745

Test Epoch: 80 
task: sign, mean loss: 1.11708, accuracy: 0.84024, avg. loss over tasks: 1.11708
Diversity Loss - Mean: 0.01655, Variance: 0.01068
Semantic Loss - Mean: 1.18155, Variance: 0.03644

Train Epoch: 81 
task: sign, mean loss: 0.06687, accuracy: 0.98913, avg. loss over tasks: 0.06687, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.02533, Variance: 0.00880
Semantic Loss - Mean: 0.05435, Variance: 0.00740

Test Epoch: 81 
task: sign, mean loss: 1.39411, accuracy: 0.79290, avg. loss over tasks: 1.39411
Diversity Loss - Mean: 0.00749, Variance: 0.01070
Semantic Loss - Mean: 1.41113, Variance: 0.03654

Train Epoch: 82 
task: sign, mean loss: 0.08145, accuracy: 0.97826, avg. loss over tasks: 0.08145, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.02832, Variance: 0.00882
Semantic Loss - Mean: 0.10326, Variance: 0.00738

Test Epoch: 82 
task: sign, mean loss: 1.55700, accuracy: 0.78698, avg. loss over tasks: 1.55700
Diversity Loss - Mean: -0.03022, Variance: 0.01073
Semantic Loss - Mean: 1.41846, Variance: 0.03662

Train Epoch: 83 
task: sign, mean loss: 0.08048, accuracy: 0.98370, avg. loss over tasks: 0.08048, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.03001, Variance: 0.00884
Semantic Loss - Mean: 0.12315, Variance: 0.00735

Test Epoch: 83 
task: sign, mean loss: 1.75142, accuracy: 0.73373, avg. loss over tasks: 1.75142
Diversity Loss - Mean: -0.01463, Variance: 0.01074
Semantic Loss - Mean: 1.83224, Variance: 0.03710

Train Epoch: 84 
task: sign, mean loss: 0.05569, accuracy: 0.98370, avg. loss over tasks: 0.05569, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.03341, Variance: 0.00886
Semantic Loss - Mean: 0.12455, Variance: 0.00736

Test Epoch: 84 
task: sign, mean loss: 0.94126, accuracy: 0.80473, avg. loss over tasks: 0.94126
Diversity Loss - Mean: -0.03755, Variance: 0.01078
Semantic Loss - Mean: 0.90752, Variance: 0.03693

Train Epoch: 85 
task: sign, mean loss: 0.08464, accuracy: 0.97283, avg. loss over tasks: 0.08464, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.03539, Variance: 0.00888
Semantic Loss - Mean: 0.12674, Variance: 0.00734

Test Epoch: 85 
task: sign, mean loss: 0.89068, accuracy: 0.82840, avg. loss over tasks: 0.89068
Diversity Loss - Mean: -0.02656, Variance: 0.01080
Semantic Loss - Mean: 0.94678, Variance: 0.03682

Train Epoch: 86 
task: sign, mean loss: 0.01411, accuracy: 1.00000, avg. loss over tasks: 0.01411, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.03373, Variance: 0.00891
Semantic Loss - Mean: 0.07246, Variance: 0.00736

Test Epoch: 86 
task: sign, mean loss: 0.80645, accuracy: 0.84024, avg. loss over tasks: 0.80645
Diversity Loss - Mean: -0.00070, Variance: 0.01081
Semantic Loss - Mean: 0.95825, Variance: 0.03655

Train Epoch: 87 
task: sign, mean loss: 0.01107, accuracy: 1.00000, avg. loss over tasks: 0.01107, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.03210, Variance: 0.00893
Semantic Loss - Mean: 0.06245, Variance: 0.00732

Test Epoch: 87 
task: sign, mean loss: 0.98673, accuracy: 0.79882, avg. loss over tasks: 0.98673
Diversity Loss - Mean: 0.00786, Variance: 0.01081
Semantic Loss - Mean: 1.16227, Variance: 0.03647

Train Epoch: 88 
task: sign, mean loss: 0.01046, accuracy: 0.99457, avg. loss over tasks: 0.01046, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.03873, Variance: 0.00896
Semantic Loss - Mean: 0.04694, Variance: 0.00731

Test Epoch: 88 
task: sign, mean loss: 1.07546, accuracy: 0.78698, avg. loss over tasks: 1.07546
Diversity Loss - Mean: 0.00507, Variance: 0.01082
Semantic Loss - Mean: 1.20465, Variance: 0.03653

Train Epoch: 89 
task: sign, mean loss: 0.01020, accuracy: 1.00000, avg. loss over tasks: 0.01020, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.03303, Variance: 0.00899
Semantic Loss - Mean: 0.04238, Variance: 0.00731

Test Epoch: 89 
task: sign, mean loss: 1.20521, accuracy: 0.76331, avg. loss over tasks: 1.20521
Diversity Loss - Mean: 0.00425, Variance: 0.01083
Semantic Loss - Mean: 1.26685, Variance: 0.03649

Train Epoch: 90 
task: sign, mean loss: 0.00671, accuracy: 1.00000, avg. loss over tasks: 0.00671, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.03294, Variance: 0.00901
Semantic Loss - Mean: 0.04586, Variance: 0.00730

Test Epoch: 90 
task: sign, mean loss: 1.16225, accuracy: 0.79290, avg. loss over tasks: 1.16225
Diversity Loss - Mean: 0.00464, Variance: 0.01083
Semantic Loss - Mean: 1.25883, Variance: 0.03667

Train Epoch: 91 
task: sign, mean loss: 0.01515, accuracy: 0.99457, avg. loss over tasks: 0.01515, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.02917, Variance: 0.00902
Semantic Loss - Mean: 0.05105, Variance: 0.00731

Test Epoch: 91 
task: sign, mean loss: 1.16136, accuracy: 0.78698, avg. loss over tasks: 1.16136
Diversity Loss - Mean: -0.00244, Variance: 0.01084
Semantic Loss - Mean: 1.29990, Variance: 0.03674

Train Epoch: 92 
task: sign, mean loss: 0.00325, accuracy: 1.00000, avg. loss over tasks: 0.00325, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.03005, Variance: 0.00904
Semantic Loss - Mean: 0.04441, Variance: 0.00736

Test Epoch: 92 
task: sign, mean loss: 1.19121, accuracy: 0.79290, avg. loss over tasks: 1.19121
Diversity Loss - Mean: -0.00523, Variance: 0.01085
Semantic Loss - Mean: 1.29851, Variance: 0.03664

Train Epoch: 93 
task: sign, mean loss: 0.00413, accuracy: 1.00000, avg. loss over tasks: 0.00413, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.03134, Variance: 0.00906
Semantic Loss - Mean: 0.03077, Variance: 0.00736

Test Epoch: 93 
task: sign, mean loss: 1.04388, accuracy: 0.79290, avg. loss over tasks: 1.04388
Diversity Loss - Mean: -0.01828, Variance: 0.01086
Semantic Loss - Mean: 1.09505, Variance: 0.03658

Train Epoch: 94 
task: sign, mean loss: 0.00117, accuracy: 1.00000, avg. loss over tasks: 0.00117, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.03250, Variance: 0.00907
Semantic Loss - Mean: 0.01877, Variance: 0.00729

Test Epoch: 94 
task: sign, mean loss: 1.01607, accuracy: 0.79882, avg. loss over tasks: 1.01607
Diversity Loss - Mean: -0.01394, Variance: 0.01087
Semantic Loss - Mean: 1.05804, Variance: 0.03659

Train Epoch: 95 
task: sign, mean loss: 0.00260, accuracy: 1.00000, avg. loss over tasks: 0.00260, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.03321, Variance: 0.00909
Semantic Loss - Mean: 0.02653, Variance: 0.00723

Test Epoch: 95 
task: sign, mean loss: 0.89301, accuracy: 0.82840, avg. loss over tasks: 0.89301
Diversity Loss - Mean: -0.01228, Variance: 0.01088
Semantic Loss - Mean: 0.97480, Variance: 0.03653

Train Epoch: 96 
task: sign, mean loss: 0.03880, accuracy: 0.99457, avg. loss over tasks: 0.03880, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.03208, Variance: 0.00911
Semantic Loss - Mean: 0.06528, Variance: 0.00720

Test Epoch: 96 
task: sign, mean loss: 0.90497, accuracy: 0.82249, avg. loss over tasks: 0.90497
Diversity Loss - Mean: -0.01311, Variance: 0.01090
Semantic Loss - Mean: 0.98090, Variance: 0.03643

Train Epoch: 97 
task: sign, mean loss: 0.00268, accuracy: 1.00000, avg. loss over tasks: 0.00268, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.03685, Variance: 0.00913
Semantic Loss - Mean: 0.03544, Variance: 0.00716

Test Epoch: 97 
task: sign, mean loss: 1.19058, accuracy: 0.79882, avg. loss over tasks: 1.19058
Diversity Loss - Mean: -0.01772, Variance: 0.01091
Semantic Loss - Mean: 1.30086, Variance: 0.03629

Train Epoch: 98 
task: sign, mean loss: 0.01137, accuracy: 0.99457, avg. loss over tasks: 0.01137, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.03931, Variance: 0.00915
Semantic Loss - Mean: 0.02385, Variance: 0.00711

Test Epoch: 98 
task: sign, mean loss: 1.16029, accuracy: 0.79290, avg. loss over tasks: 1.16029
Diversity Loss - Mean: -0.01905, Variance: 0.01092
Semantic Loss - Mean: 1.26773, Variance: 0.03626

Train Epoch: 99 
task: sign, mean loss: 0.00156, accuracy: 1.00000, avg. loss over tasks: 0.00156, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.03925, Variance: 0.00917
Semantic Loss - Mean: 0.01399, Variance: 0.00704

Test Epoch: 99 
task: sign, mean loss: 1.03951, accuracy: 0.79882, avg. loss over tasks: 1.03951
Diversity Loss - Mean: -0.01554, Variance: 0.01094
Semantic Loss - Mean: 1.17871, Variance: 0.03624

Train Epoch: 100 
task: sign, mean loss: 0.00310, accuracy: 1.00000, avg. loss over tasks: 0.00310, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.04059, Variance: 0.00919
Semantic Loss - Mean: 0.03286, Variance: 0.00702

Test Epoch: 100 
task: sign, mean loss: 1.00075, accuracy: 0.80473, avg. loss over tasks: 1.00075
Diversity Loss - Mean: -0.02153, Variance: 0.01096
Semantic Loss - Mean: 1.13342, Variance: 0.03613

Train Epoch: 101 
task: sign, mean loss: 0.00215, accuracy: 1.00000, avg. loss over tasks: 0.00215, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.03999, Variance: 0.00921
Semantic Loss - Mean: 0.02868, Variance: 0.00700

Test Epoch: 101 
task: sign, mean loss: 1.05067, accuracy: 0.81657, avg. loss over tasks: 1.05067
Diversity Loss - Mean: -0.02369, Variance: 0.01097
Semantic Loss - Mean: 1.24750, Variance: 0.03607

Train Epoch: 102 
task: sign, mean loss: 0.00165, accuracy: 1.00000, avg. loss over tasks: 0.00165, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.03765, Variance: 0.00923
Semantic Loss - Mean: 0.02403, Variance: 0.00700

Test Epoch: 102 
task: sign, mean loss: 1.09709, accuracy: 0.80473, avg. loss over tasks: 1.09709
Diversity Loss - Mean: -0.02212, Variance: 0.01099
Semantic Loss - Mean: 1.26938, Variance: 0.03599

Train Epoch: 103 
task: sign, mean loss: 0.00175, accuracy: 1.00000, avg. loss over tasks: 0.00175, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.03780, Variance: 0.00925
Semantic Loss - Mean: 0.02179, Variance: 0.00694

Test Epoch: 103 
task: sign, mean loss: 1.09857, accuracy: 0.81065, avg. loss over tasks: 1.09857
Diversity Loss - Mean: -0.02556, Variance: 0.01100
Semantic Loss - Mean: 1.26797, Variance: 0.03588

Train Epoch: 104 
task: sign, mean loss: 0.00914, accuracy: 0.99457, avg. loss over tasks: 0.00914, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.03902, Variance: 0.00927
Semantic Loss - Mean: 0.03553, Variance: 0.00691

Test Epoch: 104 
task: sign, mean loss: 1.05962, accuracy: 0.81065, avg. loss over tasks: 1.05962
Diversity Loss - Mean: -0.02612, Variance: 0.01102
Semantic Loss - Mean: 1.19000, Variance: 0.03577

Train Epoch: 105 
task: sign, mean loss: 0.01516, accuracy: 0.99457, avg. loss over tasks: 0.01516, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.04403, Variance: 0.00929
Semantic Loss - Mean: 0.02780, Variance: 0.00686

Test Epoch: 105 
task: sign, mean loss: 1.03391, accuracy: 0.82840, avg. loss over tasks: 1.03391
Diversity Loss - Mean: -0.02597, Variance: 0.01103
Semantic Loss - Mean: 1.16696, Variance: 0.03565

Train Epoch: 106 
task: sign, mean loss: 0.00067, accuracy: 1.00000, avg. loss over tasks: 0.00067, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.04495, Variance: 0.00931
Semantic Loss - Mean: 0.02170, Variance: 0.00682

Test Epoch: 106 
task: sign, mean loss: 1.03599, accuracy: 0.81065, avg. loss over tasks: 1.03599
Diversity Loss - Mean: -0.01943, Variance: 0.01105
Semantic Loss - Mean: 1.18301, Variance: 0.03553

Train Epoch: 107 
task: sign, mean loss: 0.00405, accuracy: 1.00000, avg. loss over tasks: 0.00405, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.04108, Variance: 0.00933
Semantic Loss - Mean: 0.02102, Variance: 0.00677

Test Epoch: 107 
task: sign, mean loss: 1.14597, accuracy: 0.82249, avg. loss over tasks: 1.14597
Diversity Loss - Mean: -0.02756, Variance: 0.01106
Semantic Loss - Mean: 1.29372, Variance: 0.03537

Train Epoch: 108 
task: sign, mean loss: 0.01712, accuracy: 0.99457, avg. loss over tasks: 0.01712, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.04180, Variance: 0.00935
Semantic Loss - Mean: 0.04212, Variance: 0.00675

Test Epoch: 108 
task: sign, mean loss: 1.06144, accuracy: 0.82840, avg. loss over tasks: 1.06144
Diversity Loss - Mean: -0.03042, Variance: 0.01108
Semantic Loss - Mean: 1.13051, Variance: 0.03524

Train Epoch: 109 
task: sign, mean loss: 0.01198, accuracy: 0.99457, avg. loss over tasks: 0.01198, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.04629, Variance: 0.00937
Semantic Loss - Mean: 0.04132, Variance: 0.00671

Test Epoch: 109 
task: sign, mean loss: 1.01958, accuracy: 0.82249, avg. loss over tasks: 1.01958
Diversity Loss - Mean: -0.03206, Variance: 0.01109
Semantic Loss - Mean: 1.11347, Variance: 0.03509

Train Epoch: 110 
task: sign, mean loss: 0.00069, accuracy: 1.00000, avg. loss over tasks: 0.00069, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.04559, Variance: 0.00939
Semantic Loss - Mean: 0.01196, Variance: 0.00665

Test Epoch: 110 
task: sign, mean loss: 0.98812, accuracy: 0.81065, avg. loss over tasks: 0.98812
Diversity Loss - Mean: -0.03170, Variance: 0.01111
Semantic Loss - Mean: 1.08484, Variance: 0.03498

Train Epoch: 111 
task: sign, mean loss: 0.00134, accuracy: 1.00000, avg. loss over tasks: 0.00134, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.04825, Variance: 0.00941
Semantic Loss - Mean: 0.02553, Variance: 0.00663

Test Epoch: 111 
task: sign, mean loss: 0.99008, accuracy: 0.82840, avg. loss over tasks: 0.99008
Diversity Loss - Mean: -0.03067, Variance: 0.01112
Semantic Loss - Mean: 1.08761, Variance: 0.03492

Train Epoch: 112 
task: sign, mean loss: 0.00044, accuracy: 1.00000, avg. loss over tasks: 0.00044, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.04624, Variance: 0.00943
Semantic Loss - Mean: 0.01577, Variance: 0.00658

Test Epoch: 112 
task: sign, mean loss: 0.96453, accuracy: 0.82840, avg. loss over tasks: 0.96453
Diversity Loss - Mean: -0.03208, Variance: 0.01113
Semantic Loss - Mean: 1.09553, Variance: 0.03495

Train Epoch: 113 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.04862, Variance: 0.00945
Semantic Loss - Mean: 0.01509, Variance: 0.00654

Test Epoch: 113 
task: sign, mean loss: 0.99718, accuracy: 0.83432, avg. loss over tasks: 0.99718
Diversity Loss - Mean: -0.03319, Variance: 0.01114
Semantic Loss - Mean: 1.15876, Variance: 0.03503

Train Epoch: 114 
task: sign, mean loss: 0.00257, accuracy: 1.00000, avg. loss over tasks: 0.00257, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.04672, Variance: 0.00947
Semantic Loss - Mean: 0.03117, Variance: 0.00651

Test Epoch: 114 
task: sign, mean loss: 1.04914, accuracy: 0.82840, avg. loss over tasks: 1.04914
Diversity Loss - Mean: -0.03282, Variance: 0.01116
Semantic Loss - Mean: 1.21889, Variance: 0.03504

Train Epoch: 115 
task: sign, mean loss: 0.00136, accuracy: 1.00000, avg. loss over tasks: 0.00136, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.04784, Variance: 0.00949
Semantic Loss - Mean: 0.01868, Variance: 0.00646

Test Epoch: 115 
task: sign, mean loss: 1.06064, accuracy: 0.82840, avg. loss over tasks: 1.06064
Diversity Loss - Mean: -0.02891, Variance: 0.01117
Semantic Loss - Mean: 1.27715, Variance: 0.03515

Train Epoch: 116 
task: sign, mean loss: 0.00107, accuracy: 1.00000, avg. loss over tasks: 0.00107, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.04857, Variance: 0.00951
Semantic Loss - Mean: 0.02002, Variance: 0.00642

Test Epoch: 116 
task: sign, mean loss: 1.01857, accuracy: 0.82840, avg. loss over tasks: 1.01857
Diversity Loss - Mean: -0.03253, Variance: 0.01118
Semantic Loss - Mean: 1.20819, Variance: 0.03520

Train Epoch: 117 
task: sign, mean loss: 0.00076, accuracy: 1.00000, avg. loss over tasks: 0.00076, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.04728, Variance: 0.00953
Semantic Loss - Mean: 0.01176, Variance: 0.00637

Test Epoch: 117 
task: sign, mean loss: 1.04265, accuracy: 0.82840, avg. loss over tasks: 1.04265
Diversity Loss - Mean: -0.03172, Variance: 0.01119
Semantic Loss - Mean: 1.22966, Variance: 0.03528

Train Epoch: 118 
task: sign, mean loss: 0.00177, accuracy: 1.00000, avg. loss over tasks: 0.00177, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.04859, Variance: 0.00955
Semantic Loss - Mean: 0.01469, Variance: 0.00632

Test Epoch: 118 
task: sign, mean loss: 1.05010, accuracy: 0.83432, avg. loss over tasks: 1.05010
Diversity Loss - Mean: -0.03179, Variance: 0.01121
Semantic Loss - Mean: 1.22915, Variance: 0.03529

Train Epoch: 119 
task: sign, mean loss: 0.00076, accuracy: 1.00000, avg. loss over tasks: 0.00076, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.04716, Variance: 0.00956
Semantic Loss - Mean: 0.01868, Variance: 0.00628

Test Epoch: 119 
task: sign, mean loss: 1.07220, accuracy: 0.82840, avg. loss over tasks: 1.07220
Diversity Loss - Mean: -0.02944, Variance: 0.01122
Semantic Loss - Mean: 1.27193, Variance: 0.03529

Train Epoch: 120 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.04821, Variance: 0.00958
Semantic Loss - Mean: 0.01734, Variance: 0.00624

Test Epoch: 120 
task: sign, mean loss: 1.04467, accuracy: 0.82249, avg. loss over tasks: 1.04467
Diversity Loss - Mean: -0.03230, Variance: 0.01123
Semantic Loss - Mean: 1.23591, Variance: 0.03527

Train Epoch: 121 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.05233, Variance: 0.00960
Semantic Loss - Mean: 0.02131, Variance: 0.00620

Test Epoch: 121 
task: sign, mean loss: 1.08204, accuracy: 0.82840, avg. loss over tasks: 1.08204
Diversity Loss - Mean: -0.02775, Variance: 0.01124
Semantic Loss - Mean: 1.32695, Variance: 0.03524

Train Epoch: 122 
task: sign, mean loss: 0.00094, accuracy: 1.00000, avg. loss over tasks: 0.00094, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.05048, Variance: 0.00962
Semantic Loss - Mean: 0.01858, Variance: 0.00618

Test Epoch: 122 
task: sign, mean loss: 1.09193, accuracy: 0.82840, avg. loss over tasks: 1.09193
Diversity Loss - Mean: -0.03240, Variance: 0.01125
Semantic Loss - Mean: 1.29202, Variance: 0.03519

Train Epoch: 123 
task: sign, mean loss: 0.00077, accuracy: 1.00000, avg. loss over tasks: 0.00077, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.05269, Variance: 0.00964
Semantic Loss - Mean: 0.01379, Variance: 0.00613

Test Epoch: 123 
task: sign, mean loss: 1.07041, accuracy: 0.83432, avg. loss over tasks: 1.07041
Diversity Loss - Mean: -0.02882, Variance: 0.01126
Semantic Loss - Mean: 1.29075, Variance: 0.03512

Train Epoch: 124 
task: sign, mean loss: 0.00140, accuracy: 1.00000, avg. loss over tasks: 0.00140, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.05025, Variance: 0.00966
Semantic Loss - Mean: 0.01346, Variance: 0.00609

Test Epoch: 124 
task: sign, mean loss: 0.98649, accuracy: 0.83432, avg. loss over tasks: 0.98649
Diversity Loss - Mean: -0.03254, Variance: 0.01127
Semantic Loss - Mean: 1.14252, Variance: 0.03499

Train Epoch: 125 
task: sign, mean loss: 0.00215, accuracy: 1.00000, avg. loss over tasks: 0.00215, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.05164, Variance: 0.00967
Semantic Loss - Mean: 0.00983, Variance: 0.00604

Test Epoch: 125 
task: sign, mean loss: 1.03980, accuracy: 0.82840, avg. loss over tasks: 1.03980
Diversity Loss - Mean: -0.03015, Variance: 0.01128
Semantic Loss - Mean: 1.22438, Variance: 0.03487

Train Epoch: 126 
task: sign, mean loss: 0.00076, accuracy: 1.00000, avg. loss over tasks: 0.00076, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.04828, Variance: 0.00969
Semantic Loss - Mean: 0.01537, Variance: 0.00600

Test Epoch: 126 
task: sign, mean loss: 0.98349, accuracy: 0.83432, avg. loss over tasks: 0.98349
Diversity Loss - Mean: -0.03743, Variance: 0.01129
Semantic Loss - Mean: 1.09553, Variance: 0.03470

Train Epoch: 127 
task: sign, mean loss: 0.00131, accuracy: 1.00000, avg. loss over tasks: 0.00131, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.05159, Variance: 0.00971
Semantic Loss - Mean: 0.01079, Variance: 0.00596

Test Epoch: 127 
task: sign, mean loss: 1.00082, accuracy: 0.82840, avg. loss over tasks: 1.00082
Diversity Loss - Mean: -0.03778, Variance: 0.01130
Semantic Loss - Mean: 1.10445, Variance: 0.03455

Train Epoch: 128 
task: sign, mean loss: 0.00052, accuracy: 1.00000, avg. loss over tasks: 0.00052, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.05159, Variance: 0.00972
Semantic Loss - Mean: 0.01651, Variance: 0.00593

Test Epoch: 128 
task: sign, mean loss: 1.07254, accuracy: 0.83432, avg. loss over tasks: 1.07254
Diversity Loss - Mean: -0.03117, Variance: 0.01131
Semantic Loss - Mean: 1.21458, Variance: 0.03444

Train Epoch: 129 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.05170, Variance: 0.00974
Semantic Loss - Mean: 0.01350, Variance: 0.00590

Test Epoch: 129 
task: sign, mean loss: 1.04349, accuracy: 0.82249, avg. loss over tasks: 1.04349
Diversity Loss - Mean: -0.03648, Variance: 0.01132
Semantic Loss - Mean: 1.16796, Variance: 0.03431

Train Epoch: 130 
task: sign, mean loss: 0.00078, accuracy: 1.00000, avg. loss over tasks: 0.00078, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.05065, Variance: 0.00976
Semantic Loss - Mean: 0.01422, Variance: 0.00585

Test Epoch: 130 
task: sign, mean loss: 1.08158, accuracy: 0.82249, avg. loss over tasks: 1.08158
Diversity Loss - Mean: -0.03283, Variance: 0.01133
Semantic Loss - Mean: 1.21425, Variance: 0.03418

Train Epoch: 131 
task: sign, mean loss: 0.00053, accuracy: 1.00000, avg. loss over tasks: 0.00053, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.05253, Variance: 0.00978
Semantic Loss - Mean: 0.01292, Variance: 0.00582

Test Epoch: 131 
task: sign, mean loss: 1.18056, accuracy: 0.81065, avg. loss over tasks: 1.18056
Diversity Loss - Mean: -0.02994, Variance: 0.01134
Semantic Loss - Mean: 1.32468, Variance: 0.03407

Train Epoch: 132 
task: sign, mean loss: 0.00050, accuracy: 1.00000, avg. loss over tasks: 0.00050, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.05321, Variance: 0.00979
Semantic Loss - Mean: 0.00589, Variance: 0.00578

Test Epoch: 132 
task: sign, mean loss: 1.10766, accuracy: 0.82249, avg. loss over tasks: 1.10766
Diversity Loss - Mean: -0.03308, Variance: 0.01135
Semantic Loss - Mean: 1.24100, Variance: 0.03396

Train Epoch: 133 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.05330, Variance: 0.00981
Semantic Loss - Mean: 0.01377, Variance: 0.00574

Test Epoch: 133 
task: sign, mean loss: 1.13683, accuracy: 0.81657, avg. loss over tasks: 1.13683
Diversity Loss - Mean: -0.02968, Variance: 0.01136
Semantic Loss - Mean: 1.28958, Variance: 0.03386

Train Epoch: 134 
task: sign, mean loss: 0.00067, accuracy: 1.00000, avg. loss over tasks: 0.00067, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.05054, Variance: 0.00982
Semantic Loss - Mean: 0.01046, Variance: 0.00570

Test Epoch: 134 
task: sign, mean loss: 1.07552, accuracy: 0.82249, avg. loss over tasks: 1.07552
Diversity Loss - Mean: -0.03330, Variance: 0.01136
Semantic Loss - Mean: 1.20332, Variance: 0.03375

Train Epoch: 135 
task: sign, mean loss: 0.00175, accuracy: 1.00000, avg. loss over tasks: 0.00175, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.05045, Variance: 0.00984
Semantic Loss - Mean: 0.01524, Variance: 0.00567

Test Epoch: 135 
task: sign, mean loss: 1.11144, accuracy: 0.82249, avg. loss over tasks: 1.11144
Diversity Loss - Mean: -0.04001, Variance: 0.01138
Semantic Loss - Mean: 1.20657, Variance: 0.03364

Train Epoch: 136 
task: sign, mean loss: 0.00035, accuracy: 1.00000, avg. loss over tasks: 0.00035, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.05402, Variance: 0.00985
Semantic Loss - Mean: 0.01162, Variance: 0.00564

Test Epoch: 136 
task: sign, mean loss: 1.04113, accuracy: 0.82840, avg. loss over tasks: 1.04113
Diversity Loss - Mean: -0.03769, Variance: 0.01139
Semantic Loss - Mean: 1.14032, Variance: 0.03353

Train Epoch: 137 
task: sign, mean loss: 0.00052, accuracy: 1.00000, avg. loss over tasks: 0.00052, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.05188, Variance: 0.00987
Semantic Loss - Mean: 0.01984, Variance: 0.00561

Test Epoch: 137 
task: sign, mean loss: 1.06105, accuracy: 0.82840, avg. loss over tasks: 1.06105
Diversity Loss - Mean: -0.03372, Variance: 0.01140
Semantic Loss - Mean: 1.19757, Variance: 0.03342

Train Epoch: 138 
task: sign, mean loss: 0.00063, accuracy: 1.00000, avg. loss over tasks: 0.00063, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.05588, Variance: 0.00988
Semantic Loss - Mean: 0.01483, Variance: 0.00559

Test Epoch: 138 
task: sign, mean loss: 1.05074, accuracy: 0.82249, avg. loss over tasks: 1.05074
Diversity Loss - Mean: -0.03270, Variance: 0.01140
Semantic Loss - Mean: 1.22104, Variance: 0.03335

Train Epoch: 139 
task: sign, mean loss: 0.00083, accuracy: 1.00000, avg. loss over tasks: 0.00083, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.05455, Variance: 0.00990
Semantic Loss - Mean: 0.01186, Variance: 0.00555

Test Epoch: 139 
task: sign, mean loss: 1.08691, accuracy: 0.82840, avg. loss over tasks: 1.08691
Diversity Loss - Mean: -0.03295, Variance: 0.01141
Semantic Loss - Mean: 1.26202, Variance: 0.03328

Train Epoch: 140 
task: sign, mean loss: 0.00385, accuracy: 1.00000, avg. loss over tasks: 0.00385, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.05355, Variance: 0.00992
Semantic Loss - Mean: 0.01950, Variance: 0.00551

Test Epoch: 140 
task: sign, mean loss: 1.06101, accuracy: 0.83432, avg. loss over tasks: 1.06101
Diversity Loss - Mean: -0.03931, Variance: 0.01142
Semantic Loss - Mean: 1.18540, Variance: 0.03319

Train Epoch: 141 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.05378, Variance: 0.00993
Semantic Loss - Mean: 0.01413, Variance: 0.00550

Test Epoch: 141 
task: sign, mean loss: 1.04306, accuracy: 0.83432, avg. loss over tasks: 1.04306
Diversity Loss - Mean: -0.04108, Variance: 0.01144
Semantic Loss - Mean: 1.15084, Variance: 0.03308

Train Epoch: 142 
task: sign, mean loss: 0.00185, accuracy: 1.00000, avg. loss over tasks: 0.00185, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.05484, Variance: 0.00995
Semantic Loss - Mean: 0.02721, Variance: 0.00551

Test Epoch: 142 
task: sign, mean loss: 1.01899, accuracy: 0.82840, avg. loss over tasks: 1.01899
Diversity Loss - Mean: -0.04468, Variance: 0.01145
Semantic Loss - Mean: 1.09913, Variance: 0.03296

Train Epoch: 143 
task: sign, mean loss: 0.00057, accuracy: 1.00000, avg. loss over tasks: 0.00057, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.05498, Variance: 0.00997
Semantic Loss - Mean: 0.01266, Variance: 0.00549

Test Epoch: 143 
task: sign, mean loss: 1.10920, accuracy: 0.82249, avg. loss over tasks: 1.10920
Diversity Loss - Mean: -0.03686, Variance: 0.01146
Semantic Loss - Mean: 1.25549, Variance: 0.03287

Train Epoch: 144 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.05234, Variance: 0.00998
Semantic Loss - Mean: 0.00943, Variance: 0.00545

Test Epoch: 144 
task: sign, mean loss: 1.08190, accuracy: 0.82840, avg. loss over tasks: 1.08190
Diversity Loss - Mean: -0.03679, Variance: 0.01147
Semantic Loss - Mean: 1.22882, Variance: 0.03279

Train Epoch: 145 
task: sign, mean loss: 0.00040, accuracy: 1.00000, avg. loss over tasks: 0.00040, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.05335, Variance: 0.00999
Semantic Loss - Mean: 0.01877, Variance: 0.00545

Test Epoch: 145 
task: sign, mean loss: 1.11082, accuracy: 0.82249, avg. loss over tasks: 1.11082
Diversity Loss - Mean: -0.03893, Variance: 0.01148
Semantic Loss - Mean: 1.25397, Variance: 0.03271

Train Epoch: 146 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.05403, Variance: 0.01001
Semantic Loss - Mean: 0.02395, Variance: 0.00547

Test Epoch: 146 
task: sign, mean loss: 1.23129, accuracy: 0.81065, avg. loss over tasks: 1.23129
Diversity Loss - Mean: -0.03350, Variance: 0.01148
Semantic Loss - Mean: 1.40378, Variance: 0.03266

Train Epoch: 147 
task: sign, mean loss: 0.00127, accuracy: 1.00000, avg. loss over tasks: 0.00127, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.05393, Variance: 0.01002
Semantic Loss - Mean: 0.02159, Variance: 0.00544

Test Epoch: 147 
task: sign, mean loss: 1.18511, accuracy: 0.81065, avg. loss over tasks: 1.18511
Diversity Loss - Mean: -0.03160, Variance: 0.01149
Semantic Loss - Mean: 1.37675, Variance: 0.03262

Train Epoch: 148 
task: sign, mean loss: 0.00509, accuracy: 1.00000, avg. loss over tasks: 0.00509, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.05123, Variance: 0.01003
Semantic Loss - Mean: 0.01438, Variance: 0.00540

Test Epoch: 148 
task: sign, mean loss: 1.16962, accuracy: 0.80473, avg. loss over tasks: 1.16962
Diversity Loss - Mean: -0.03377, Variance: 0.01150
Semantic Loss - Mean: 1.32879, Variance: 0.03255

Train Epoch: 149 
task: sign, mean loss: 0.00071, accuracy: 1.00000, avg. loss over tasks: 0.00071, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.05343, Variance: 0.01005
Semantic Loss - Mean: 0.02350, Variance: 0.00539

Test Epoch: 149 
task: sign, mean loss: 1.10784, accuracy: 0.82249, avg. loss over tasks: 1.10784
Diversity Loss - Mean: -0.03932, Variance: 0.01151
Semantic Loss - Mean: 1.21534, Variance: 0.03245

Train Epoch: 150 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 3e-07
Diversity Loss - Mean: -0.05139, Variance: 0.01006
Semantic Loss - Mean: 0.00669, Variance: 0.00535

Test Epoch: 150 
task: sign, mean loss: 1.09763, accuracy: 0.82249, avg. loss over tasks: 1.09763
Diversity Loss - Mean: -0.03632, Variance: 0.01152
Semantic Loss - Mean: 1.22454, Variance: 0.03236

