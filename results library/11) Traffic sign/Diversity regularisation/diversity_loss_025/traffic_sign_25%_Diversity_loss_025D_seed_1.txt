Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09229, accuracy: 0.63587, avg. loss over tasks: 1.09229, lr: 3e-05
Diversity Loss - Mean: -0.00969, Variance: 0.01049
Semantic Loss - Mean: 1.43055, Variance: 0.07319

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17804, accuracy: 0.66272, avg. loss over tasks: 1.17804
Diversity Loss - Mean: -0.02896, Variance: 0.01240
Semantic Loss - Mean: 1.16179, Variance: 0.05434

Train Epoch: 2 
task: sign, mean loss: 0.96560, accuracy: 0.66848, avg. loss over tasks: 0.96560, lr: 6e-05
Diversity Loss - Mean: -0.01578, Variance: 0.01040
Semantic Loss - Mean: 0.98181, Variance: 0.03973

Test Epoch: 2 
task: sign, mean loss: 1.10730, accuracy: 0.66272, avg. loss over tasks: 1.10730
Diversity Loss - Mean: -0.02602, Variance: 0.01203
Semantic Loss - Mean: 1.14682, Variance: 0.03275

Train Epoch: 3 
task: sign, mean loss: 0.79104, accuracy: 0.69565, avg. loss over tasks: 0.79104, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.02820, Variance: 0.01015
Semantic Loss - Mean: 0.99422, Variance: 0.02753

Test Epoch: 3 
task: sign, mean loss: 1.31020, accuracy: 0.55621, avg. loss over tasks: 1.31020
Diversity Loss - Mean: -0.04793, Variance: 0.01134
Semantic Loss - Mean: 1.11338, Variance: 0.02977

Train Epoch: 4 
task: sign, mean loss: 0.72969, accuracy: 0.71196, avg. loss over tasks: 0.72969, lr: 0.00012
Diversity Loss - Mean: -0.04682, Variance: 0.00981
Semantic Loss - Mean: 0.88534, Variance: 0.02132

Test Epoch: 4 
task: sign, mean loss: 1.54074, accuracy: 0.43787, avg. loss over tasks: 1.54074
Diversity Loss - Mean: -0.05188, Variance: 0.01061
Semantic Loss - Mean: 1.06051, Variance: 0.02449

Train Epoch: 5 
task: sign, mean loss: 0.68471, accuracy: 0.72826, avg. loss over tasks: 0.68471, lr: 0.00015
Diversity Loss - Mean: -0.03466, Variance: 0.00941
Semantic Loss - Mean: 0.77355, Variance: 0.01753

Test Epoch: 5 
task: sign, mean loss: 2.52590, accuracy: 0.29586, avg. loss over tasks: 2.52590
Diversity Loss - Mean: -0.02972, Variance: 0.00996
Semantic Loss - Mean: 1.36632, Variance: 0.02297

Train Epoch: 6 
task: sign, mean loss: 0.78870, accuracy: 0.74457, avg. loss over tasks: 0.78870, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.03425, Variance: 0.00914
Semantic Loss - Mean: 0.75376, Variance: 0.01504

Test Epoch: 6 
task: sign, mean loss: 2.03474, accuracy: 0.55621, avg. loss over tasks: 2.03474
Diversity Loss - Mean: -0.02501, Variance: 0.01028
Semantic Loss - Mean: 1.39313, Variance: 0.02351

Train Epoch: 7 
task: sign, mean loss: 0.53259, accuracy: 0.77174, avg. loss over tasks: 0.53259, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.04343, Variance: 0.00912
Semantic Loss - Mean: 0.57401, Variance: 0.01319

Test Epoch: 7 
task: sign, mean loss: 1.66007, accuracy: 0.59172, avg. loss over tasks: 1.66007
Diversity Loss - Mean: -0.02422, Variance: 0.01053
Semantic Loss - Mean: 1.32421, Variance: 0.02401

Train Epoch: 8 
task: sign, mean loss: 0.50842, accuracy: 0.85326, avg. loss over tasks: 0.50842, lr: 0.00024
Diversity Loss - Mean: -0.02207, Variance: 0.00908
Semantic Loss - Mean: 0.57416, Variance: 0.01210

Test Epoch: 8 
task: sign, mean loss: 3.46025, accuracy: 0.36686, avg. loss over tasks: 3.46025
Diversity Loss - Mean: 0.03431, Variance: 0.01064
Semantic Loss - Mean: 2.09875, Variance: 0.02572

Train Epoch: 9 
task: sign, mean loss: 0.72678, accuracy: 0.76630, avg. loss over tasks: 0.72678, lr: 0.00027
Diversity Loss - Mean: -0.02859, Variance: 0.00910
Semantic Loss - Mean: 0.69942, Variance: 0.01124

Test Epoch: 9 
task: sign, mean loss: 2.38478, accuracy: 0.31361, avg. loss over tasks: 2.38478
Diversity Loss - Mean: -0.03120, Variance: 0.01076
Semantic Loss - Mean: 1.87617, Variance: 0.02701

Train Epoch: 10 
task: sign, mean loss: 0.85432, accuracy: 0.74457, avg. loss over tasks: 0.85432, lr: 0.0003
Diversity Loss - Mean: -0.03390, Variance: 0.00909
Semantic Loss - Mean: 0.69042, Variance: 0.01061

Test Epoch: 10 
task: sign, mean loss: 3.68940, accuracy: 0.25444, avg. loss over tasks: 3.68940
Diversity Loss - Mean: 0.06264, Variance: 0.01087
Semantic Loss - Mean: 2.60045, Variance: 0.02898

Train Epoch: 11 
task: sign, mean loss: 0.74564, accuracy: 0.73370, avg. loss over tasks: 0.74564, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.00935, Variance: 0.00903
Semantic Loss - Mean: 0.69813, Variance: 0.01012

Test Epoch: 11 
task: sign, mean loss: 2.47328, accuracy: 0.66272, avg. loss over tasks: 2.47328
Diversity Loss - Mean: 0.04906, Variance: 0.01119
Semantic Loss - Mean: 2.10230, Variance: 0.02956

Train Epoch: 12 
task: sign, mean loss: 0.52090, accuracy: 0.80435, avg. loss over tasks: 0.52090, lr: 0.000299849111021216
Diversity Loss - Mean: -0.00670, Variance: 0.00908
Semantic Loss - Mean: 0.53625, Variance: 0.00945

Test Epoch: 12 
task: sign, mean loss: 2.28611, accuracy: 0.44379, avg. loss over tasks: 2.28611
Diversity Loss - Mean: -0.02061, Variance: 0.01123
Semantic Loss - Mean: 1.80237, Variance: 0.03019

Train Epoch: 13 
task: sign, mean loss: 0.44703, accuracy: 0.80978, avg. loss over tasks: 0.44703, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.02510, Variance: 0.00919
Semantic Loss - Mean: 0.49727, Variance: 0.00912

Test Epoch: 13 
task: sign, mean loss: 2.26571, accuracy: 0.24852, avg. loss over tasks: 2.26571
Diversity Loss - Mean: -0.00774, Variance: 0.01137
Semantic Loss - Mean: 1.93125, Variance: 0.02973

Train Epoch: 14 
task: sign, mean loss: 0.53255, accuracy: 0.77717, avg. loss over tasks: 0.53255, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.01847, Variance: 0.00923
Semantic Loss - Mean: 0.54927, Variance: 0.00871

Test Epoch: 14 
task: sign, mean loss: 1.73273, accuracy: 0.70414, avg. loss over tasks: 1.73273
Diversity Loss - Mean: -0.01764, Variance: 0.01144
Semantic Loss - Mean: 1.53801, Variance: 0.02861

Train Epoch: 15 
task: sign, mean loss: 0.40389, accuracy: 0.83152, avg. loss over tasks: 0.40389, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.00839, Variance: 0.00920
Semantic Loss - Mean: 0.42705, Variance: 0.00836

Test Epoch: 15 
task: sign, mean loss: 2.10178, accuracy: 0.39053, avg. loss over tasks: 2.10178
Diversity Loss - Mean: 0.01830, Variance: 0.01144
Semantic Loss - Mean: 1.78970, Variance: 0.02741

Train Epoch: 16 
task: sign, mean loss: 0.32120, accuracy: 0.88043, avg. loss over tasks: 0.32120, lr: 0.000298643821800925
Diversity Loss - Mean: -0.01416, Variance: 0.00922
Semantic Loss - Mean: 0.37005, Variance: 0.00815

Test Epoch: 16 
task: sign, mean loss: 2.14801, accuracy: 0.50888, avg. loss over tasks: 2.14801
Diversity Loss - Mean: -0.03021, Variance: 0.01151
Semantic Loss - Mean: 1.83750, Variance: 0.02767

Train Epoch: 17 
task: sign, mean loss: 0.26246, accuracy: 0.92391, avg. loss over tasks: 0.26246, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.01396, Variance: 0.00928
Semantic Loss - Mean: 0.30788, Variance: 0.00795

Test Epoch: 17 
task: sign, mean loss: 1.76820, accuracy: 0.49704, avg. loss over tasks: 1.76820
Diversity Loss - Mean: -0.03553, Variance: 0.01170
Semantic Loss - Mean: 1.59348, Variance: 0.02760

Train Epoch: 18 
task: sign, mean loss: 0.37255, accuracy: 0.91848, avg. loss over tasks: 0.37255, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.01870, Variance: 0.00932
Semantic Loss - Mean: 0.36997, Variance: 0.00802

Test Epoch: 18 
task: sign, mean loss: 1.92284, accuracy: 0.46154, avg. loss over tasks: 1.92284
Diversity Loss - Mean: -0.04133, Variance: 0.01196
Semantic Loss - Mean: 1.68529, Variance: 0.02897

Train Epoch: 19 
task: sign, mean loss: 0.29845, accuracy: 0.91304, avg. loss over tasks: 0.29845, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.02923, Variance: 0.00944
Semantic Loss - Mean: 0.36064, Variance: 0.00805

Test Epoch: 19 
task: sign, mean loss: 2.09083, accuracy: 0.39645, avg. loss over tasks: 2.09083
Diversity Loss - Mean: -0.00042, Variance: 0.01216
Semantic Loss - Mean: 1.74839, Variance: 0.03062

Train Epoch: 20 
task: sign, mean loss: 0.29703, accuracy: 0.88587, avg. loss over tasks: 0.29703, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.02145, Variance: 0.00952
Semantic Loss - Mean: 0.37113, Variance: 0.00791

Test Epoch: 20 
task: sign, mean loss: 1.92015, accuracy: 0.59763, avg. loss over tasks: 1.92015
Diversity Loss - Mean: -0.00571, Variance: 0.01212
Semantic Loss - Mean: 1.77335, Variance: 0.03092

Train Epoch: 21 
task: sign, mean loss: 0.19656, accuracy: 0.93478, avg. loss over tasks: 0.19656, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.01247, Variance: 0.00953
Semantic Loss - Mean: 0.24132, Variance: 0.00767

Test Epoch: 21 
task: sign, mean loss: 1.99579, accuracy: 0.51479, avg. loss over tasks: 1.99579
Diversity Loss - Mean: -0.00941, Variance: 0.01213
Semantic Loss - Mean: 1.59711, Variance: 0.03264

Train Epoch: 22 
task: sign, mean loss: 0.16483, accuracy: 0.93478, avg. loss over tasks: 0.16483, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.00192, Variance: 0.00952
Semantic Loss - Mean: 0.21406, Variance: 0.00759

Test Epoch: 22 
task: sign, mean loss: 1.41444, accuracy: 0.70414, avg. loss over tasks: 1.41444
Diversity Loss - Mean: -0.03809, Variance: 0.01224
Semantic Loss - Mean: 1.29413, Variance: 0.03272

Train Epoch: 23 
task: sign, mean loss: 0.07724, accuracy: 0.96739, avg. loss over tasks: 0.07724, lr: 0.0002936689598802368
Diversity Loss - Mean: 0.00822, Variance: 0.00952
Semantic Loss - Mean: 0.11437, Variance: 0.00745

Test Epoch: 23 
task: sign, mean loss: 1.71193, accuracy: 0.72781, avg. loss over tasks: 1.71193
Diversity Loss - Mean: -0.03436, Variance: 0.01231
Semantic Loss - Mean: 1.42088, Variance: 0.03280

Train Epoch: 24 
task: sign, mean loss: 0.06725, accuracy: 0.98370, avg. loss over tasks: 0.06725, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.00845, Variance: 0.00953
Semantic Loss - Mean: 0.08346, Variance: 0.00718

Test Epoch: 24 
task: sign, mean loss: 2.03066, accuracy: 0.65089, avg. loss over tasks: 2.03066
Diversity Loss - Mean: -0.03368, Variance: 0.01232
Semantic Loss - Mean: 1.77493, Variance: 0.03224

Train Epoch: 25 
task: sign, mean loss: 0.04855, accuracy: 0.98370, avg. loss over tasks: 0.04855, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.01658, Variance: 0.00953
Semantic Loss - Mean: 0.09948, Variance: 0.00704

Test Epoch: 25 
task: sign, mean loss: 2.30551, accuracy: 0.49704, avg. loss over tasks: 2.30551
Diversity Loss - Mean: -0.02267, Variance: 0.01231
Semantic Loss - Mean: 1.90873, Variance: 0.03255

Train Epoch: 26 
task: sign, mean loss: 0.10639, accuracy: 0.95109, avg. loss over tasks: 0.10639, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.02215, Variance: 0.00954
Semantic Loss - Mean: 0.15935, Variance: 0.00702

Test Epoch: 26 
task: sign, mean loss: 2.45461, accuracy: 0.56805, avg. loss over tasks: 2.45461
Diversity Loss - Mean: -0.01524, Variance: 0.01227
Semantic Loss - Mean: 2.04797, Variance: 0.03301

Train Epoch: 27 
task: sign, mean loss: 0.35654, accuracy: 0.89674, avg. loss over tasks: 0.35654, lr: 0.000289228031029578
Diversity Loss - Mean: -0.02788, Variance: 0.00957
Semantic Loss - Mean: 0.37402, Variance: 0.00721

Test Epoch: 27 
task: sign, mean loss: 2.26807, accuracy: 0.58580, avg. loss over tasks: 2.26807
Diversity Loss - Mean: -0.00650, Variance: 0.01228
Semantic Loss - Mean: 1.97120, Variance: 0.03643

Train Epoch: 28 
task: sign, mean loss: 0.21711, accuracy: 0.90761, avg. loss over tasks: 0.21711, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.03971, Variance: 0.00964
Semantic Loss - Mean: 0.26242, Variance: 0.00709

Test Epoch: 28 
task: sign, mean loss: 2.86044, accuracy: 0.21893, avg. loss over tasks: 2.86044
Diversity Loss - Mean: 0.06182, Variance: 0.01217
Semantic Loss - Mean: 2.84879, Variance: 0.04182

Train Epoch: 29 
task: sign, mean loss: 0.21818, accuracy: 0.92391, avg. loss over tasks: 0.21818, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.04665, Variance: 0.00975
Semantic Loss - Mean: 0.31642, Variance: 0.00755

Test Epoch: 29 
task: sign, mean loss: 1.71392, accuracy: 0.58580, avg. loss over tasks: 1.71392
Diversity Loss - Mean: -0.00529, Variance: 0.01213
Semantic Loss - Mean: 1.42568, Variance: 0.04245

Train Epoch: 30 
task: sign, mean loss: 0.21575, accuracy: 0.94022, avg. loss over tasks: 0.21575, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.03724, Variance: 0.00980
Semantic Loss - Mean: 0.29671, Variance: 0.00764

Test Epoch: 30 
task: sign, mean loss: 1.58841, accuracy: 0.71006, avg. loss over tasks: 1.58841
Diversity Loss - Mean: -0.06036, Variance: 0.01220
Semantic Loss - Mean: 1.36178, Variance: 0.04286

Train Epoch: 31 
task: sign, mean loss: 0.31676, accuracy: 0.86957, avg. loss over tasks: 0.31676, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.04409, Variance: 0.00986
Semantic Loss - Mean: 0.34889, Variance: 0.00768

Test Epoch: 31 
task: sign, mean loss: 1.17907, accuracy: 0.75148, avg. loss over tasks: 1.17907
Diversity Loss - Mean: -0.05647, Variance: 0.01228
Semantic Loss - Mean: 1.17502, Variance: 0.04219

Train Epoch: 32 
task: sign, mean loss: 0.22455, accuracy: 0.90761, avg. loss over tasks: 0.22455, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.05036, Variance: 0.00993
Semantic Loss - Mean: 0.29887, Variance: 0.00761

Test Epoch: 32 
task: sign, mean loss: 0.99030, accuracy: 0.71598, avg. loss over tasks: 0.99030
Diversity Loss - Mean: -0.05781, Variance: 0.01235
Semantic Loss - Mean: 1.03243, Variance: 0.04171

Train Epoch: 33 
task: sign, mean loss: 0.07988, accuracy: 0.96196, avg. loss over tasks: 0.07988, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.03855, Variance: 0.00998
Semantic Loss - Mean: 0.11642, Variance: 0.00743

Test Epoch: 33 
task: sign, mean loss: 1.06647, accuracy: 0.64497, avg. loss over tasks: 1.06647
Diversity Loss - Mean: -0.02583, Variance: 0.01239
Semantic Loss - Mean: 1.01643, Variance: 0.04103

Train Epoch: 34 
task: sign, mean loss: 0.13365, accuracy: 0.96739, avg. loss over tasks: 0.13365, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.03553, Variance: 0.01001
Semantic Loss - Mean: 0.16282, Variance: 0.00735

Test Epoch: 34 
task: sign, mean loss: 1.40535, accuracy: 0.75740, avg. loss over tasks: 1.40535
Diversity Loss - Mean: -0.04326, Variance: 0.01244
Semantic Loss - Mean: 1.21645, Variance: 0.04039

Train Epoch: 35 
task: sign, mean loss: 0.10657, accuracy: 0.97826, avg. loss over tasks: 0.10657, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.03909, Variance: 0.01004
Semantic Loss - Mean: 0.15243, Variance: 0.00743

Test Epoch: 35 
task: sign, mean loss: 2.10457, accuracy: 0.69822, avg. loss over tasks: 2.10457
Diversity Loss - Mean: -0.01350, Variance: 0.01241
Semantic Loss - Mean: 1.93280, Variance: 0.04024

Train Epoch: 36 
task: sign, mean loss: 0.06711, accuracy: 0.97283, avg. loss over tasks: 0.06711, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.03038, Variance: 0.01004
Semantic Loss - Mean: 0.10305, Variance: 0.00729

Test Epoch: 36 
task: sign, mean loss: 1.64137, accuracy: 0.56213, avg. loss over tasks: 1.64137
Diversity Loss - Mean: -0.02186, Variance: 0.01242
Semantic Loss - Mean: 1.36499, Variance: 0.04056

Train Epoch: 37 
task: sign, mean loss: 0.05759, accuracy: 0.97826, avg. loss over tasks: 0.05759, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.03622, Variance: 0.01006
Semantic Loss - Mean: 0.09601, Variance: 0.00719

Test Epoch: 37 
task: sign, mean loss: 1.11543, accuracy: 0.66272, avg. loss over tasks: 1.11543
Diversity Loss - Mean: -0.04182, Variance: 0.01246
Semantic Loss - Mean: 0.97616, Variance: 0.03993

Train Epoch: 38 
task: sign, mean loss: 0.02799, accuracy: 0.99457, avg. loss over tasks: 0.02799, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.03608, Variance: 0.01010
Semantic Loss - Mean: 0.07684, Variance: 0.00708

Test Epoch: 38 
task: sign, mean loss: 1.49409, accuracy: 0.56213, avg. loss over tasks: 1.49409
Diversity Loss - Mean: -0.00766, Variance: 0.01246
Semantic Loss - Mean: 1.47661, Variance: 0.04080

Train Epoch: 39 
task: sign, mean loss: 0.01367, accuracy: 1.00000, avg. loss over tasks: 0.01367, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.03329, Variance: 0.01012
Semantic Loss - Mean: 0.03255, Variance: 0.00692

Test Epoch: 39 
task: sign, mean loss: 1.59206, accuracy: 0.62722, avg. loss over tasks: 1.59206
Diversity Loss - Mean: -0.02261, Variance: 0.01245
Semantic Loss - Mean: 1.35649, Variance: 0.04050

Train Epoch: 40 
task: sign, mean loss: 0.06550, accuracy: 0.97826, avg. loss over tasks: 0.06550, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.04126, Variance: 0.01017
Semantic Loss - Mean: 0.09885, Variance: 0.00693

Test Epoch: 40 
task: sign, mean loss: 2.41345, accuracy: 0.42604, avg. loss over tasks: 2.41345
Diversity Loss - Mean: -0.00454, Variance: 0.01240
Semantic Loss - Mean: 2.20890, Variance: 0.04198

Train Epoch: 41 
task: sign, mean loss: 0.05064, accuracy: 0.97826, avg. loss over tasks: 0.05064, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.04118, Variance: 0.01018
Semantic Loss - Mean: 0.08273, Variance: 0.00684

Test Epoch: 41 
task: sign, mean loss: 1.49340, accuracy: 0.70414, avg. loss over tasks: 1.49340
Diversity Loss - Mean: -0.04613, Variance: 0.01240
Semantic Loss - Mean: 1.31953, Variance: 0.04168

Train Epoch: 42 
task: sign, mean loss: 0.16941, accuracy: 0.95652, avg. loss over tasks: 0.16941, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.05718, Variance: 0.01022
Semantic Loss - Mean: 0.18193, Variance: 0.00698

Test Epoch: 42 
task: sign, mean loss: 1.58646, accuracy: 0.69822, avg. loss over tasks: 1.58646
Diversity Loss - Mean: -0.04743, Variance: 0.01242
Semantic Loss - Mean: 1.43253, Variance: 0.04114

Train Epoch: 43 
task: sign, mean loss: 0.06365, accuracy: 0.98370, avg. loss over tasks: 0.06365, lr: 0.000260757131773478
Diversity Loss - Mean: -0.05251, Variance: 0.01023
Semantic Loss - Mean: 0.09439, Variance: 0.00688

Test Epoch: 43 
task: sign, mean loss: 1.30034, accuracy: 0.65680, avg. loss over tasks: 1.30034
Diversity Loss - Mean: -0.04668, Variance: 0.01245
Semantic Loss - Mean: 1.07828, Variance: 0.04125

Train Epoch: 44 
task: sign, mean loss: 0.05001, accuracy: 0.98370, avg. loss over tasks: 0.05001, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.05676, Variance: 0.01026
Semantic Loss - Mean: 0.08827, Variance: 0.00688

Test Epoch: 44 
task: sign, mean loss: 1.78256, accuracy: 0.69231, avg. loss over tasks: 1.78256
Diversity Loss - Mean: -0.04717, Variance: 0.01246
Semantic Loss - Mean: 1.50390, Variance: 0.04149

Train Epoch: 45 
task: sign, mean loss: 0.03212, accuracy: 0.99457, avg. loss over tasks: 0.03212, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.05679, Variance: 0.01029
Semantic Loss - Mean: 0.07086, Variance: 0.00679

Test Epoch: 45 
task: sign, mean loss: 1.17946, accuracy: 0.73964, avg. loss over tasks: 1.17946
Diversity Loss - Mean: -0.05226, Variance: 0.01247
Semantic Loss - Mean: 1.21902, Variance: 0.04198

Train Epoch: 46 
task: sign, mean loss: 0.03400, accuracy: 0.98370, avg. loss over tasks: 0.03400, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.05704, Variance: 0.01031
Semantic Loss - Mean: 0.06555, Variance: 0.00669

Test Epoch: 46 
task: sign, mean loss: 1.55681, accuracy: 0.66272, avg. loss over tasks: 1.55681
Diversity Loss - Mean: -0.03260, Variance: 0.01246
Semantic Loss - Mean: 1.48359, Variance: 0.04271

Train Epoch: 47 
task: sign, mean loss: 0.03312, accuracy: 0.99457, avg. loss over tasks: 0.03312, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.05741, Variance: 0.01033
Semantic Loss - Mean: 0.06488, Variance: 0.00662

Test Epoch: 47 
task: sign, mean loss: 1.28801, accuracy: 0.70414, avg. loss over tasks: 1.28801
Diversity Loss - Mean: -0.06702, Variance: 0.01251
Semantic Loss - Mean: 1.24944, Variance: 0.04426

Train Epoch: 48 
task: sign, mean loss: 0.03859, accuracy: 0.98913, avg. loss over tasks: 0.03859, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.06174, Variance: 0.01036
Semantic Loss - Mean: 0.06191, Variance: 0.00657

Test Epoch: 48 
task: sign, mean loss: 1.30090, accuracy: 0.75148, avg. loss over tasks: 1.30090
Diversity Loss - Mean: -0.06055, Variance: 0.01255
Semantic Loss - Mean: 1.29828, Variance: 0.04474

Train Epoch: 49 
task: sign, mean loss: 0.04794, accuracy: 0.97826, avg. loss over tasks: 0.04794, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.06596, Variance: 0.01039
Semantic Loss - Mean: 0.04358, Variance: 0.00650

Test Epoch: 49 
task: sign, mean loss: 1.21263, accuracy: 0.74556, avg. loss over tasks: 1.21263
Diversity Loss - Mean: -0.05947, Variance: 0.01259
Semantic Loss - Mean: 1.33788, Variance: 0.04600

Train Epoch: 50 
task: sign, mean loss: 0.02421, accuracy: 0.99457, avg. loss over tasks: 0.02421, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.06583, Variance: 0.01043
Semantic Loss - Mean: 0.05441, Variance: 0.00641

Test Epoch: 50 
task: sign, mean loss: 1.59332, accuracy: 0.60947, avg. loss over tasks: 1.59332
Diversity Loss - Mean: -0.05322, Variance: 0.01261
Semantic Loss - Mean: 1.52983, Variance: 0.04812

Train Epoch: 51 
task: sign, mean loss: 0.01673, accuracy: 0.99457, avg. loss over tasks: 0.01673, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.06900, Variance: 0.01047
Semantic Loss - Mean: 0.05326, Variance: 0.00634

Test Epoch: 51 
task: sign, mean loss: 1.37765, accuracy: 0.73964, avg. loss over tasks: 1.37765
Diversity Loss - Mean: -0.06767, Variance: 0.01265
Semantic Loss - Mean: 1.08869, Variance: 0.04761

Train Epoch: 52 
task: sign, mean loss: 0.00670, accuracy: 0.99457, avg. loss over tasks: 0.00670, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.06888, Variance: 0.01052
Semantic Loss - Mean: 0.02936, Variance: 0.00628

Test Epoch: 52 
task: sign, mean loss: 1.44732, accuracy: 0.73964, avg. loss over tasks: 1.44732
Diversity Loss - Mean: -0.06045, Variance: 0.01267
Semantic Loss - Mean: 1.30209, Variance: 0.04852

Train Epoch: 53 
task: sign, mean loss: 0.05584, accuracy: 0.99457, avg. loss over tasks: 0.05584, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.07111, Variance: 0.01056
Semantic Loss - Mean: 0.07654, Variance: 0.00632

Test Epoch: 53 
task: sign, mean loss: 1.32999, accuracy: 0.77515, avg. loss over tasks: 1.32999
Diversity Loss - Mean: -0.06413, Variance: 0.01269
Semantic Loss - Mean: 1.33345, Variance: 0.04904

Train Epoch: 54 
task: sign, mean loss: 0.06659, accuracy: 0.98913, avg. loss over tasks: 0.06659, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.07516, Variance: 0.01061
Semantic Loss - Mean: 0.10362, Variance: 0.00627

Test Epoch: 54 
task: sign, mean loss: 1.92444, accuracy: 0.71006, avg. loss over tasks: 1.92444
Diversity Loss - Mean: -0.03699, Variance: 0.01268
Semantic Loss - Mean: 2.14144, Variance: 0.05016

Train Epoch: 55 
task: sign, mean loss: 0.02639, accuracy: 0.98913, avg. loss over tasks: 0.02639, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.07395, Variance: 0.01065
Semantic Loss - Mean: 0.04793, Variance: 0.00621

Test Epoch: 55 
task: sign, mean loss: 1.32722, accuracy: 0.76331, avg. loss over tasks: 1.32722
Diversity Loss - Mean: -0.07524, Variance: 0.01272
Semantic Loss - Mean: 1.16756, Variance: 0.05016

Train Epoch: 56 
task: sign, mean loss: 0.00635, accuracy: 1.00000, avg. loss over tasks: 0.00635, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.07396, Variance: 0.01070
Semantic Loss - Mean: 0.03711, Variance: 0.00616

Test Epoch: 56 
task: sign, mean loss: 1.06914, accuracy: 0.78698, avg. loss over tasks: 1.06914
Diversity Loss - Mean: -0.07976, Variance: 0.01276
Semantic Loss - Mean: 1.09153, Variance: 0.05013

Train Epoch: 57 
task: sign, mean loss: 0.00739, accuracy: 1.00000, avg. loss over tasks: 0.00739, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.07724, Variance: 0.01074
Semantic Loss - Mean: 0.02739, Variance: 0.00610

Test Epoch: 57 
task: sign, mean loss: 1.15345, accuracy: 0.74556, avg. loss over tasks: 1.15345
Diversity Loss - Mean: -0.07360, Variance: 0.01280
Semantic Loss - Mean: 1.25549, Variance: 0.04977

Train Epoch: 58 
task: sign, mean loss: 0.01537, accuracy: 0.98913, avg. loss over tasks: 0.01537, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.07859, Variance: 0.01079
Semantic Loss - Mean: 0.02741, Variance: 0.00602

Test Epoch: 58 
task: sign, mean loss: 2.17699, accuracy: 0.73373, avg. loss over tasks: 2.17699
Diversity Loss - Mean: -0.06010, Variance: 0.01280
Semantic Loss - Mean: 1.98521, Variance: 0.05021

Train Epoch: 59 
task: sign, mean loss: 0.06168, accuracy: 0.98370, avg. loss over tasks: 0.06168, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.08258, Variance: 0.01084
Semantic Loss - Mean: 0.07095, Variance: 0.00595

Test Epoch: 59 
task: sign, mean loss: 0.83056, accuracy: 0.82249, avg. loss over tasks: 0.83056
Diversity Loss - Mean: -0.09082, Variance: 0.01285
Semantic Loss - Mean: 0.81463, Variance: 0.05006

Train Epoch: 60 
task: sign, mean loss: 0.03982, accuracy: 0.98370, avg. loss over tasks: 0.03982, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.08243, Variance: 0.01090
Semantic Loss - Mean: 0.07510, Variance: 0.00594

Test Epoch: 60 
task: sign, mean loss: 1.52656, accuracy: 0.72781, avg. loss over tasks: 1.52656
Diversity Loss - Mean: -0.07778, Variance: 0.01288
Semantic Loss - Mean: 1.48104, Variance: 0.05005

Train Epoch: 61 
task: sign, mean loss: 0.05132, accuracy: 0.99457, avg. loss over tasks: 0.05132, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.08252, Variance: 0.01094
Semantic Loss - Mean: 0.08187, Variance: 0.00589

Test Epoch: 61 
task: sign, mean loss: 1.54764, accuracy: 0.70414, avg. loss over tasks: 1.54764
Diversity Loss - Mean: -0.08425, Variance: 0.01291
Semantic Loss - Mean: 1.48544, Variance: 0.05005

Train Epoch: 62 
task: sign, mean loss: 0.10289, accuracy: 0.95109, avg. loss over tasks: 0.10289, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.08517, Variance: 0.01099
Semantic Loss - Mean: 0.18662, Variance: 0.00593

Test Epoch: 62 
task: sign, mean loss: 1.69962, accuracy: 0.62130, avg. loss over tasks: 1.69962
Diversity Loss - Mean: -0.07441, Variance: 0.01293
Semantic Loss - Mean: 1.49933, Variance: 0.05014

Train Epoch: 63 
task: sign, mean loss: 0.11359, accuracy: 0.96196, avg. loss over tasks: 0.11359, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.08361, Variance: 0.01105
Semantic Loss - Mean: 0.14466, Variance: 0.00593

Test Epoch: 63 
task: sign, mean loss: 1.77648, accuracy: 0.60355, avg. loss over tasks: 1.77648
Diversity Loss - Mean: -0.05951, Variance: 0.01294
Semantic Loss - Mean: 1.70112, Variance: 0.05159

Train Epoch: 64 
task: sign, mean loss: 0.14695, accuracy: 0.94022, avg. loss over tasks: 0.14695, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.08075, Variance: 0.01110
Semantic Loss - Mean: 0.18897, Variance: 0.00613

Test Epoch: 64 
task: sign, mean loss: 0.78802, accuracy: 0.81657, avg. loss over tasks: 0.78802
Diversity Loss - Mean: -0.07610, Variance: 0.01297
Semantic Loss - Mean: 0.76406, Variance: 0.05143

Train Epoch: 65 
task: sign, mean loss: 0.08622, accuracy: 0.96196, avg. loss over tasks: 0.08622, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.08755, Variance: 0.01114
Semantic Loss - Mean: 0.13299, Variance: 0.00631

Test Epoch: 65 
task: sign, mean loss: 0.96123, accuracy: 0.75148, avg. loss over tasks: 0.96123
Diversity Loss - Mean: -0.08045, Variance: 0.01298
Semantic Loss - Mean: 0.83349, Variance: 0.05112

Train Epoch: 66 
task: sign, mean loss: 0.07693, accuracy: 0.96196, avg. loss over tasks: 0.07693, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.08672, Variance: 0.01118
Semantic Loss - Mean: 0.12817, Variance: 0.00630

Test Epoch: 66 
task: sign, mean loss: 0.53415, accuracy: 0.84024, avg. loss over tasks: 0.53415
Diversity Loss - Mean: -0.07954, Variance: 0.01301
Semantic Loss - Mean: 0.50478, Variance: 0.05055

Train Epoch: 67 
task: sign, mean loss: 0.05173, accuracy: 0.98370, avg. loss over tasks: 0.05173, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.09322, Variance: 0.01123
Semantic Loss - Mean: 0.13681, Variance: 0.00648

Test Epoch: 67 
task: sign, mean loss: 0.73353, accuracy: 0.79290, avg. loss over tasks: 0.73353
Diversity Loss - Mean: -0.07749, Variance: 0.01303
Semantic Loss - Mean: 0.85744, Variance: 0.05028

Train Epoch: 68 
task: sign, mean loss: 0.25417, accuracy: 0.95109, avg. loss over tasks: 0.25417, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.08925, Variance: 0.01126
Semantic Loss - Mean: 0.30319, Variance: 0.00679

Test Epoch: 68 
task: sign, mean loss: 1.26266, accuracy: 0.78698, avg. loss over tasks: 1.26266
Diversity Loss - Mean: -0.09409, Variance: 0.01306
Semantic Loss - Mean: 1.09153, Variance: 0.04992

Train Epoch: 69 
task: sign, mean loss: 0.27066, accuracy: 0.92935, avg. loss over tasks: 0.27066, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.09588, Variance: 0.01129
Semantic Loss - Mean: 0.28013, Variance: 0.00682

Test Epoch: 69 
task: sign, mean loss: 1.32331, accuracy: 0.67456, avg. loss over tasks: 1.32331
Diversity Loss - Mean: -0.07288, Variance: 0.01305
Semantic Loss - Mean: 1.28645, Variance: 0.04944

Train Epoch: 70 
task: sign, mean loss: 0.19073, accuracy: 0.92935, avg. loss over tasks: 0.19073, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.09643, Variance: 0.01133
Semantic Loss - Mean: 0.25478, Variance: 0.00689

Test Epoch: 70 
task: sign, mean loss: 2.24181, accuracy: 0.66272, avg. loss over tasks: 2.24181
Diversity Loss - Mean: -0.08367, Variance: 0.01304
Semantic Loss - Mean: 1.99367, Variance: 0.04938

Train Epoch: 71 
task: sign, mean loss: 0.27947, accuracy: 0.91304, avg. loss over tasks: 0.27947, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.09452, Variance: 0.01136
Semantic Loss - Mean: 0.32571, Variance: 0.00692

Test Epoch: 71 
task: sign, mean loss: 2.54939, accuracy: 0.42012, avg. loss over tasks: 2.54939
Diversity Loss - Mean: -0.07615, Variance: 0.01302
Semantic Loss - Mean: 2.25380, Variance: 0.04970

Train Epoch: 72 
task: sign, mean loss: 0.19856, accuracy: 0.93478, avg. loss over tasks: 0.19856, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.09455, Variance: 0.01139
Semantic Loss - Mean: 0.22972, Variance: 0.00687

Test Epoch: 72 
task: sign, mean loss: 2.32813, accuracy: 0.55621, avg. loss over tasks: 2.32813
Diversity Loss - Mean: -0.08698, Variance: 0.01303
Semantic Loss - Mean: 2.11381, Variance: 0.04964

Train Epoch: 73 
task: sign, mean loss: 0.22319, accuracy: 0.92935, avg. loss over tasks: 0.22319, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.09728, Variance: 0.01144
Semantic Loss - Mean: 0.26467, Variance: 0.00690

Test Epoch: 73 
task: sign, mean loss: 2.09994, accuracy: 0.46154, avg. loss over tasks: 2.09994
Diversity Loss - Mean: -0.07208, Variance: 0.01302
Semantic Loss - Mean: 2.15003, Variance: 0.04951

Train Epoch: 74 
task: sign, mean loss: 0.22926, accuracy: 0.92935, avg. loss over tasks: 0.22926, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.09346, Variance: 0.01148
Semantic Loss - Mean: 0.28222, Variance: 0.00692

Test Epoch: 74 
task: sign, mean loss: 1.93477, accuracy: 0.56805, avg. loss over tasks: 1.93477
Diversity Loss - Mean: -0.09213, Variance: 0.01305
Semantic Loss - Mean: 1.78687, Variance: 0.04924

Train Epoch: 75 
task: sign, mean loss: 0.12260, accuracy: 0.95652, avg. loss over tasks: 0.12260, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.09288, Variance: 0.01152
Semantic Loss - Mean: 0.16580, Variance: 0.00688

Test Epoch: 75 
task: sign, mean loss: 1.25026, accuracy: 0.73964, avg. loss over tasks: 1.25026
Diversity Loss - Mean: -0.10097, Variance: 0.01310
Semantic Loss - Mean: 1.17048, Variance: 0.04884

Train Epoch: 76 
task: sign, mean loss: 0.08227, accuracy: 0.95652, avg. loss over tasks: 0.08227, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.09109, Variance: 0.01156
Semantic Loss - Mean: 0.13285, Variance: 0.00688

Test Epoch: 76 
task: sign, mean loss: 0.97656, accuracy: 0.73964, avg. loss over tasks: 0.97656
Diversity Loss - Mean: -0.09367, Variance: 0.01314
Semantic Loss - Mean: 0.98428, Variance: 0.04838

Train Epoch: 77 
task: sign, mean loss: 0.12200, accuracy: 0.96196, avg. loss over tasks: 0.12200, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.08859, Variance: 0.01159
Semantic Loss - Mean: 0.16080, Variance: 0.00688

Test Epoch: 77 
task: sign, mean loss: 2.11299, accuracy: 0.33728, avg. loss over tasks: 2.11299
Diversity Loss - Mean: -0.05129, Variance: 0.01315
Semantic Loss - Mean: 2.27339, Variance: 0.04843

Train Epoch: 78 
task: sign, mean loss: 0.10039, accuracy: 0.96739, avg. loss over tasks: 0.10039, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.08910, Variance: 0.01161
Semantic Loss - Mean: 0.15104, Variance: 0.00689

Test Epoch: 78 
task: sign, mean loss: 1.41421, accuracy: 0.57988, avg. loss over tasks: 1.41421
Diversity Loss - Mean: -0.06530, Variance: 0.01317
Semantic Loss - Mean: 1.33178, Variance: 0.04812

Train Epoch: 79 
task: sign, mean loss: 0.06733, accuracy: 0.97826, avg. loss over tasks: 0.06733, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.08466, Variance: 0.01162
Semantic Loss - Mean: 0.12081, Variance: 0.00689

Test Epoch: 79 
task: sign, mean loss: 0.51388, accuracy: 0.82249, avg. loss over tasks: 0.51388
Diversity Loss - Mean: -0.08118, Variance: 0.01320
Semantic Loss - Mean: 0.54941, Variance: 0.04767

Train Epoch: 80 
task: sign, mean loss: 0.04251, accuracy: 0.98913, avg. loss over tasks: 0.04251, lr: 0.00015015
Diversity Loss - Mean: -0.07888, Variance: 0.01164
Semantic Loss - Mean: 0.09915, Variance: 0.00687

Test Epoch: 80 
task: sign, mean loss: 0.55460, accuracy: 0.85207, avg. loss over tasks: 0.55460
Diversity Loss - Mean: -0.07981, Variance: 0.01324
Semantic Loss - Mean: 0.56905, Variance: 0.04734

Train Epoch: 81 
task: sign, mean loss: 0.05323, accuracy: 0.99457, avg. loss over tasks: 0.05323, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.07855, Variance: 0.01164
Semantic Loss - Mean: 0.11798, Variance: 0.00696

Test Epoch: 81 
task: sign, mean loss: 0.71464, accuracy: 0.83432, avg. loss over tasks: 0.71464
Diversity Loss - Mean: -0.07465, Variance: 0.01325
Semantic Loss - Mean: 0.67622, Variance: 0.04707

Train Epoch: 82 
task: sign, mean loss: 0.29085, accuracy: 0.92935, avg. loss over tasks: 0.29085, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.08735, Variance: 0.01166
Semantic Loss - Mean: 0.29776, Variance: 0.00715

Test Epoch: 82 
task: sign, mean loss: 1.02839, accuracy: 0.72781, avg. loss over tasks: 1.02839
Diversity Loss - Mean: -0.05732, Variance: 0.01328
Semantic Loss - Mean: 0.97296, Variance: 0.04679

Train Epoch: 83 
task: sign, mean loss: 0.43315, accuracy: 0.86413, avg. loss over tasks: 0.43315, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.08052, Variance: 0.01168
Semantic Loss - Mean: 0.52675, Variance: 0.00731

Test Epoch: 83 
task: sign, mean loss: 1.16102, accuracy: 0.71598, avg. loss over tasks: 1.16102
Diversity Loss - Mean: -0.04291, Variance: 0.01330
Semantic Loss - Mean: 0.88840, Variance: 0.04637

Train Epoch: 84 
task: sign, mean loss: 0.41036, accuracy: 0.82609, avg. loss over tasks: 0.41036, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.08983, Variance: 0.01171
Semantic Loss - Mean: 0.52018, Variance: 0.00778

Test Epoch: 84 
task: sign, mean loss: 1.08729, accuracy: 0.72781, avg. loss over tasks: 1.08729
Diversity Loss - Mean: -0.05515, Variance: 0.01331
Semantic Loss - Mean: 0.88066, Variance: 0.04602

Train Epoch: 85 
task: sign, mean loss: 0.28938, accuracy: 0.88587, avg. loss over tasks: 0.28938, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.08742, Variance: 0.01175
Semantic Loss - Mean: 0.33458, Variance: 0.00782

Test Epoch: 85 
task: sign, mean loss: 0.63125, accuracy: 0.81065, avg. loss over tasks: 0.63125
Diversity Loss - Mean: -0.08040, Variance: 0.01334
Semantic Loss - Mean: 0.57965, Variance: 0.04552

Train Epoch: 86 
task: sign, mean loss: 0.27500, accuracy: 0.92391, avg. loss over tasks: 0.27500, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.09484, Variance: 0.01179
Semantic Loss - Mean: 0.33439, Variance: 0.00790

Test Epoch: 86 
task: sign, mean loss: 0.60235, accuracy: 0.84024, avg. loss over tasks: 0.60235
Diversity Loss - Mean: -0.08093, Variance: 0.01338
Semantic Loss - Mean: 0.53081, Variance: 0.04502

Train Epoch: 87 
task: sign, mean loss: 0.27189, accuracy: 0.91304, avg. loss over tasks: 0.27189, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.09622, Variance: 0.01182
Semantic Loss - Mean: 0.32683, Variance: 0.00793

Test Epoch: 87 
task: sign, mean loss: 0.93371, accuracy: 0.80473, avg. loss over tasks: 0.93371
Diversity Loss - Mean: -0.08750, Variance: 0.01338
Semantic Loss - Mean: 0.82966, Variance: 0.04461

Train Epoch: 88 
task: sign, mean loss: 0.26065, accuracy: 0.89674, avg. loss over tasks: 0.26065, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.09578, Variance: 0.01185
Semantic Loss - Mean: 0.32523, Variance: 0.00792

Test Epoch: 88 
task: sign, mean loss: 2.11628, accuracy: 0.40237, avg. loss over tasks: 2.11628
Diversity Loss - Mean: -0.05842, Variance: 0.01336
Semantic Loss - Mean: 1.58708, Variance: 0.04464

Train Epoch: 89 
task: sign, mean loss: 0.15970, accuracy: 0.95652, avg. loss over tasks: 0.15970, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.09386, Variance: 0.01189
Semantic Loss - Mean: 0.22202, Variance: 0.00796

Test Epoch: 89 
task: sign, mean loss: 1.23968, accuracy: 0.60947, avg. loss over tasks: 1.23968
Diversity Loss - Mean: -0.07588, Variance: 0.01334
Semantic Loss - Mean: 0.89308, Variance: 0.04444

Train Epoch: 90 
task: sign, mean loss: 0.14222, accuracy: 0.94022, avg. loss over tasks: 0.14222, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.09803, Variance: 0.01192
Semantic Loss - Mean: 0.22862, Variance: 0.00807

Test Epoch: 90 
task: sign, mean loss: 1.01678, accuracy: 0.82840, avg. loss over tasks: 1.01678
Diversity Loss - Mean: -0.07534, Variance: 0.01335
Semantic Loss - Mean: 0.87763, Variance: 0.04410

Train Epoch: 91 
task: sign, mean loss: 0.16586, accuracy: 0.93478, avg. loss over tasks: 0.16586, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.09468, Variance: 0.01194
Semantic Loss - Mean: 0.21619, Variance: 0.00810

Test Epoch: 91 
task: sign, mean loss: 0.56442, accuracy: 0.85799, avg. loss over tasks: 0.56442
Diversity Loss - Mean: -0.09195, Variance: 0.01338
Semantic Loss - Mean: 0.49923, Variance: 0.04370

Train Epoch: 92 
task: sign, mean loss: 0.08309, accuracy: 0.97283, avg. loss over tasks: 0.08309, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.09023, Variance: 0.01196
Semantic Loss - Mean: 0.15856, Variance: 0.00816

Test Epoch: 92 
task: sign, mean loss: 0.79453, accuracy: 0.81657, avg. loss over tasks: 0.79453
Diversity Loss - Mean: -0.09105, Variance: 0.01341
Semantic Loss - Mean: 0.71489, Variance: 0.04342

Train Epoch: 93 
task: sign, mean loss: 0.07034, accuracy: 0.97826, avg. loss over tasks: 0.07034, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.09145, Variance: 0.01198
Semantic Loss - Mean: 0.12163, Variance: 0.00822

Test Epoch: 93 
task: sign, mean loss: 1.01782, accuracy: 0.77515, avg. loss over tasks: 1.01782
Diversity Loss - Mean: -0.06815, Variance: 0.01342
Semantic Loss - Mean: 0.99530, Variance: 0.04321

Train Epoch: 94 
task: sign, mean loss: 0.02891, accuracy: 0.98913, avg. loss over tasks: 0.02891, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.09030, Variance: 0.01199
Semantic Loss - Mean: 0.07469, Variance: 0.00822

Test Epoch: 94 
task: sign, mean loss: 0.94488, accuracy: 0.73373, avg. loss over tasks: 0.94488
Diversity Loss - Mean: -0.06040, Variance: 0.01343
Semantic Loss - Mean: 0.96484, Variance: 0.04308

Train Epoch: 95 
task: sign, mean loss: 0.05950, accuracy: 0.98913, avg. loss over tasks: 0.05950, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.08769, Variance: 0.01200
Semantic Loss - Mean: 0.11703, Variance: 0.00821

Test Epoch: 95 
task: sign, mean loss: 0.54432, accuracy: 0.88757, avg. loss over tasks: 0.54432
Diversity Loss - Mean: -0.07596, Variance: 0.01345
Semantic Loss - Mean: 0.55072, Variance: 0.04269

Train Epoch: 96 
task: sign, mean loss: 0.04370, accuracy: 0.97826, avg. loss over tasks: 0.04370, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.08676, Variance: 0.01201
Semantic Loss - Mean: 0.10957, Variance: 0.00828

Test Epoch: 96 
task: sign, mean loss: 0.44756, accuracy: 0.89349, avg. loss over tasks: 0.44756
Diversity Loss - Mean: -0.09148, Variance: 0.01348
Semantic Loss - Mean: 0.41612, Variance: 0.04231

Train Epoch: 97 
task: sign, mean loss: 0.06212, accuracy: 0.97826, avg. loss over tasks: 0.06212, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.09286, Variance: 0.01202
Semantic Loss - Mean: 0.14026, Variance: 0.00843

Test Epoch: 97 
task: sign, mean loss: 0.63897, accuracy: 0.86391, avg. loss over tasks: 0.63897
Diversity Loss - Mean: -0.08751, Variance: 0.01350
Semantic Loss - Mean: 0.60724, Variance: 0.04195

Train Epoch: 98 
task: sign, mean loss: 0.02359, accuracy: 0.99457, avg. loss over tasks: 0.02359, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.09522, Variance: 0.01204
Semantic Loss - Mean: 0.07914, Variance: 0.00846

Test Epoch: 98 
task: sign, mean loss: 0.65818, accuracy: 0.86391, avg. loss over tasks: 0.65818
Diversity Loss - Mean: -0.08584, Variance: 0.01352
Semantic Loss - Mean: 0.62846, Variance: 0.04161

Train Epoch: 99 
task: sign, mean loss: 0.03401, accuracy: 0.99457, avg. loss over tasks: 0.03401, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.09224, Variance: 0.01205
Semantic Loss - Mean: 0.08416, Variance: 0.00845

Test Epoch: 99 
task: sign, mean loss: 0.74660, accuracy: 0.79882, avg. loss over tasks: 0.74660
Diversity Loss - Mean: -0.06864, Variance: 0.01354
Semantic Loss - Mean: 0.78789, Variance: 0.04131

Train Epoch: 100 
task: sign, mean loss: 0.02489, accuracy: 1.00000, avg. loss over tasks: 0.02489, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.09090, Variance: 0.01206
Semantic Loss - Mean: 0.10060, Variance: 0.00851

Test Epoch: 100 
task: sign, mean loss: 0.77571, accuracy: 0.77515, avg. loss over tasks: 0.77571
Diversity Loss - Mean: -0.06118, Variance: 0.01355
Semantic Loss - Mean: 0.79738, Variance: 0.04100

Train Epoch: 101 
task: sign, mean loss: 0.02962, accuracy: 0.98913, avg. loss over tasks: 0.02962, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.08956, Variance: 0.01207
Semantic Loss - Mean: 0.07901, Variance: 0.00851

Test Epoch: 101 
task: sign, mean loss: 0.78492, accuracy: 0.86982, avg. loss over tasks: 0.78492
Diversity Loss - Mean: -0.08014, Variance: 0.01356
Semantic Loss - Mean: 0.76167, Variance: 0.04067

Train Epoch: 102 
task: sign, mean loss: 0.01596, accuracy: 0.99457, avg. loss over tasks: 0.01596, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.09052, Variance: 0.01207
Semantic Loss - Mean: 0.06132, Variance: 0.00852

Test Epoch: 102 
task: sign, mean loss: 0.80492, accuracy: 0.81657, avg. loss over tasks: 0.80492
Diversity Loss - Mean: -0.06776, Variance: 0.01358
Semantic Loss - Mean: 0.82446, Variance: 0.04039

Train Epoch: 103 
task: sign, mean loss: 0.02792, accuracy: 0.99457, avg. loss over tasks: 0.02792, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.08841, Variance: 0.01208
Semantic Loss - Mean: 0.08568, Variance: 0.00851

Test Epoch: 103 
task: sign, mean loss: 1.06649, accuracy: 0.74556, avg. loss over tasks: 1.06649
Diversity Loss - Mean: -0.06216, Variance: 0.01359
Semantic Loss - Mean: 1.06503, Variance: 0.04022

Train Epoch: 104 
task: sign, mean loss: 0.01317, accuracy: 0.99457, avg. loss over tasks: 0.01317, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.09098, Variance: 0.01209
Semantic Loss - Mean: 0.05883, Variance: 0.00850

Test Epoch: 104 
task: sign, mean loss: 0.92084, accuracy: 0.76923, avg. loss over tasks: 0.92084
Diversity Loss - Mean: -0.06876, Variance: 0.01361
Semantic Loss - Mean: 0.93818, Variance: 0.04014

Train Epoch: 105 
task: sign, mean loss: 0.01619, accuracy: 0.99457, avg. loss over tasks: 0.01619, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.09131, Variance: 0.01210
Semantic Loss - Mean: 0.05165, Variance: 0.00847

Test Epoch: 105 
task: sign, mean loss: 0.84191, accuracy: 0.78698, avg. loss over tasks: 0.84191
Diversity Loss - Mean: -0.07163, Variance: 0.01362
Semantic Loss - Mean: 0.85998, Variance: 0.03992

Train Epoch: 106 
task: sign, mean loss: 0.01641, accuracy: 0.98913, avg. loss over tasks: 0.01641, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.08995, Variance: 0.01211
Semantic Loss - Mean: 0.05677, Variance: 0.00845

Test Epoch: 106 
task: sign, mean loss: 1.14298, accuracy: 0.69231, avg. loss over tasks: 1.14298
Diversity Loss - Mean: -0.06294, Variance: 0.01364
Semantic Loss - Mean: 1.11626, Variance: 0.03970

Train Epoch: 107 
task: sign, mean loss: 0.01650, accuracy: 0.99457, avg. loss over tasks: 0.01650, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.09062, Variance: 0.01211
Semantic Loss - Mean: 0.05765, Variance: 0.00844

Test Epoch: 107 
task: sign, mean loss: 1.06348, accuracy: 0.74556, avg. loss over tasks: 1.06348
Diversity Loss - Mean: -0.06973, Variance: 0.01365
Semantic Loss - Mean: 1.04356, Variance: 0.03950

Train Epoch: 108 
task: sign, mean loss: 0.01215, accuracy: 1.00000, avg. loss over tasks: 0.01215, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.09170, Variance: 0.01212
Semantic Loss - Mean: 0.06357, Variance: 0.00844

Test Epoch: 108 
task: sign, mean loss: 1.04443, accuracy: 0.75148, avg. loss over tasks: 1.04443
Diversity Loss - Mean: -0.06807, Variance: 0.01366
Semantic Loss - Mean: 1.03977, Variance: 0.03942

Train Epoch: 109 
task: sign, mean loss: 0.01573, accuracy: 0.98913, avg. loss over tasks: 0.01573, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.09263, Variance: 0.01212
Semantic Loss - Mean: 0.07557, Variance: 0.00845

Test Epoch: 109 
task: sign, mean loss: 1.00595, accuracy: 0.77515, avg. loss over tasks: 1.00595
Diversity Loss - Mean: -0.06821, Variance: 0.01368
Semantic Loss - Mean: 1.06262, Variance: 0.03944

Train Epoch: 110 
task: sign, mean loss: 0.00962, accuracy: 1.00000, avg. loss over tasks: 0.00962, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.09080, Variance: 0.01213
Semantic Loss - Mean: 0.03924, Variance: 0.00841

Test Epoch: 110 
task: sign, mean loss: 1.05986, accuracy: 0.74556, avg. loss over tasks: 1.05986
Diversity Loss - Mean: -0.06903, Variance: 0.01369
Semantic Loss - Mean: 1.11900, Variance: 0.03940

Train Epoch: 111 
task: sign, mean loss: 0.00963, accuracy: 1.00000, avg. loss over tasks: 0.00963, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.09158, Variance: 0.01213
Semantic Loss - Mean: 0.06623, Variance: 0.00844

Test Epoch: 111 
task: sign, mean loss: 1.07111, accuracy: 0.72189, avg. loss over tasks: 1.07111
Diversity Loss - Mean: -0.06801, Variance: 0.01371
Semantic Loss - Mean: 1.10866, Variance: 0.03934

Train Epoch: 112 
task: sign, mean loss: 0.00474, accuracy: 1.00000, avg. loss over tasks: 0.00474, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.09172, Variance: 0.01213
Semantic Loss - Mean: 0.04892, Variance: 0.00843

Test Epoch: 112 
task: sign, mean loss: 1.09117, accuracy: 0.75148, avg. loss over tasks: 1.09117
Diversity Loss - Mean: -0.06903, Variance: 0.01372
Semantic Loss - Mean: 1.13567, Variance: 0.03928

Train Epoch: 113 
task: sign, mean loss: 0.00651, accuracy: 1.00000, avg. loss over tasks: 0.00651, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.09222, Variance: 0.01214
Semantic Loss - Mean: 0.04036, Variance: 0.00839

Test Epoch: 113 
task: sign, mean loss: 0.92420, accuracy: 0.78698, avg. loss over tasks: 0.92420
Diversity Loss - Mean: -0.07550, Variance: 0.01373
Semantic Loss - Mean: 0.94441, Variance: 0.03910

Train Epoch: 114 
task: sign, mean loss: 0.00615, accuracy: 1.00000, avg. loss over tasks: 0.00615, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.09262, Variance: 0.01215
Semantic Loss - Mean: 0.06689, Variance: 0.00845

Test Epoch: 114 
task: sign, mean loss: 0.91182, accuracy: 0.78698, avg. loss over tasks: 0.91182
Diversity Loss - Mean: -0.07934, Variance: 0.01375
Semantic Loss - Mean: 0.92297, Variance: 0.03894

Train Epoch: 115 
task: sign, mean loss: 0.00753, accuracy: 1.00000, avg. loss over tasks: 0.00753, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.09311, Variance: 0.01215
Semantic Loss - Mean: 0.05961, Variance: 0.00847

Test Epoch: 115 
task: sign, mean loss: 1.09389, accuracy: 0.73373, avg. loss over tasks: 1.09389
Diversity Loss - Mean: -0.07448, Variance: 0.01376
Semantic Loss - Mean: 1.09718, Variance: 0.03883

Train Epoch: 116 
task: sign, mean loss: 0.01084, accuracy: 1.00000, avg. loss over tasks: 0.01084, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.09420, Variance: 0.01216
Semantic Loss - Mean: 0.05728, Variance: 0.00845

Test Epoch: 116 
task: sign, mean loss: 0.86802, accuracy: 0.76923, avg. loss over tasks: 0.86802
Diversity Loss - Mean: -0.07836, Variance: 0.01377
Semantic Loss - Mean: 0.88443, Variance: 0.03868

Train Epoch: 117 
task: sign, mean loss: 0.00357, accuracy: 1.00000, avg. loss over tasks: 0.00357, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.09274, Variance: 0.01216
Semantic Loss - Mean: 0.04624, Variance: 0.00844

Test Epoch: 117 
task: sign, mean loss: 0.90612, accuracy: 0.76331, avg. loss over tasks: 0.90612
Diversity Loss - Mean: -0.07560, Variance: 0.01379
Semantic Loss - Mean: 0.92697, Variance: 0.03865

Train Epoch: 118 
task: sign, mean loss: 0.00548, accuracy: 1.00000, avg. loss over tasks: 0.00548, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.09283, Variance: 0.01216
Semantic Loss - Mean: 0.05068, Variance: 0.00842

Test Epoch: 118 
task: sign, mean loss: 0.98978, accuracy: 0.73964, avg. loss over tasks: 0.98978
Diversity Loss - Mean: -0.07224, Variance: 0.01380
Semantic Loss - Mean: 1.00378, Variance: 0.03858

Train Epoch: 119 
task: sign, mean loss: 0.00258, accuracy: 1.00000, avg. loss over tasks: 0.00258, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.09290, Variance: 0.01217
Semantic Loss - Mean: 0.05023, Variance: 0.00843

Test Epoch: 119 
task: sign, mean loss: 0.91124, accuracy: 0.75740, avg. loss over tasks: 0.91124
Diversity Loss - Mean: -0.07399, Variance: 0.01381
Semantic Loss - Mean: 0.94511, Variance: 0.03839

Train Epoch: 120 
task: sign, mean loss: 0.00302, accuracy: 1.00000, avg. loss over tasks: 0.00302, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.09416, Variance: 0.01217
Semantic Loss - Mean: 0.04812, Variance: 0.00838

Test Epoch: 120 
task: sign, mean loss: 0.79559, accuracy: 0.80473, avg. loss over tasks: 0.79559
Diversity Loss - Mean: -0.07908, Variance: 0.01382
Semantic Loss - Mean: 0.82719, Variance: 0.03816

Train Epoch: 121 
task: sign, mean loss: 0.00716, accuracy: 1.00000, avg. loss over tasks: 0.00716, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.09557, Variance: 0.01217
Semantic Loss - Mean: 0.05653, Variance: 0.00836

Test Epoch: 121 
task: sign, mean loss: 0.92598, accuracy: 0.75740, avg. loss over tasks: 0.92598
Diversity Loss - Mean: -0.07437, Variance: 0.01383
Semantic Loss - Mean: 0.98063, Variance: 0.03799

Train Epoch: 122 
task: sign, mean loss: 0.00498, accuracy: 1.00000, avg. loss over tasks: 0.00498, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.09415, Variance: 0.01218
Semantic Loss - Mean: 0.05284, Variance: 0.00837

Test Epoch: 122 
task: sign, mean loss: 0.90341, accuracy: 0.76923, avg. loss over tasks: 0.90341
Diversity Loss - Mean: -0.07633, Variance: 0.01384
Semantic Loss - Mean: 0.96742, Variance: 0.03782

Train Epoch: 123 
task: sign, mean loss: 0.00181, accuracy: 1.00000, avg. loss over tasks: 0.00181, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.09580, Variance: 0.01219
Semantic Loss - Mean: 0.03128, Variance: 0.00833

Test Epoch: 123 
task: sign, mean loss: 0.97668, accuracy: 0.78107, avg. loss over tasks: 0.97668
Diversity Loss - Mean: -0.07430, Variance: 0.01385
Semantic Loss - Mean: 1.04545, Variance: 0.03770

Train Epoch: 124 
task: sign, mean loss: 0.00662, accuracy: 1.00000, avg. loss over tasks: 0.00662, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.09436, Variance: 0.01219
Semantic Loss - Mean: 0.04337, Variance: 0.00831

Test Epoch: 124 
task: sign, mean loss: 0.98299, accuracy: 0.75740, avg. loss over tasks: 0.98299
Diversity Loss - Mean: -0.07394, Variance: 0.01386
Semantic Loss - Mean: 1.04466, Variance: 0.03759

Train Epoch: 125 
task: sign, mean loss: 0.00851, accuracy: 1.00000, avg. loss over tasks: 0.00851, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.09616, Variance: 0.01219
Semantic Loss - Mean: 0.03387, Variance: 0.00826

Test Epoch: 125 
task: sign, mean loss: 0.95864, accuracy: 0.75740, avg. loss over tasks: 0.95864
Diversity Loss - Mean: -0.07464, Variance: 0.01387
Semantic Loss - Mean: 1.00178, Variance: 0.03750

Train Epoch: 126 
task: sign, mean loss: 0.00954, accuracy: 0.99457, avg. loss over tasks: 0.00954, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.09402, Variance: 0.01219
Semantic Loss - Mean: 0.04030, Variance: 0.00822

Test Epoch: 126 
task: sign, mean loss: 0.87631, accuracy: 0.79290, avg. loss over tasks: 0.87631
Diversity Loss - Mean: -0.07979, Variance: 0.01388
Semantic Loss - Mean: 0.87916, Variance: 0.03736

Train Epoch: 127 
task: sign, mean loss: 0.00617, accuracy: 1.00000, avg. loss over tasks: 0.00617, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.09394, Variance: 0.01220
Semantic Loss - Mean: 0.03013, Variance: 0.00818

Test Epoch: 127 
task: sign, mean loss: 0.86785, accuracy: 0.79882, avg. loss over tasks: 0.86785
Diversity Loss - Mean: -0.08039, Variance: 0.01389
Semantic Loss - Mean: 0.84638, Variance: 0.03719

Train Epoch: 128 
task: sign, mean loss: 0.00318, accuracy: 1.00000, avg. loss over tasks: 0.00318, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.09562, Variance: 0.01221
Semantic Loss - Mean: 0.03426, Variance: 0.00814

Test Epoch: 128 
task: sign, mean loss: 0.89292, accuracy: 0.79882, avg. loss over tasks: 0.89292
Diversity Loss - Mean: -0.08083, Variance: 0.01390
Semantic Loss - Mean: 0.86745, Variance: 0.03701

Train Epoch: 129 
task: sign, mean loss: 0.00160, accuracy: 1.00000, avg. loss over tasks: 0.00160, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.09526, Variance: 0.01221
Semantic Loss - Mean: 0.04034, Variance: 0.00814

Test Epoch: 129 
task: sign, mean loss: 0.86185, accuracy: 0.78698, avg. loss over tasks: 0.86185
Diversity Loss - Mean: -0.08011, Variance: 0.01391
Semantic Loss - Mean: 0.85676, Variance: 0.03683

Train Epoch: 130 
task: sign, mean loss: 0.00641, accuracy: 1.00000, avg. loss over tasks: 0.00641, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.09646, Variance: 0.01222
Semantic Loss - Mean: 0.02831, Variance: 0.00809

Test Epoch: 130 
task: sign, mean loss: 0.94655, accuracy: 0.77515, avg. loss over tasks: 0.94655
Diversity Loss - Mean: -0.07800, Variance: 0.01392
Semantic Loss - Mean: 0.92964, Variance: 0.03669

Train Epoch: 131 
task: sign, mean loss: 0.00246, accuracy: 1.00000, avg. loss over tasks: 0.00246, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.09549, Variance: 0.01222
Semantic Loss - Mean: 0.04197, Variance: 0.00808

Test Epoch: 131 
task: sign, mean loss: 1.04031, accuracy: 0.73964, avg. loss over tasks: 1.04031
Diversity Loss - Mean: -0.07347, Variance: 0.01393
Semantic Loss - Mean: 1.02458, Variance: 0.03663

Train Epoch: 132 
task: sign, mean loss: 0.00162, accuracy: 1.00000, avg. loss over tasks: 0.00162, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.09690, Variance: 0.01222
Semantic Loss - Mean: 0.02787, Variance: 0.00806

Test Epoch: 132 
task: sign, mean loss: 0.92128, accuracy: 0.76923, avg. loss over tasks: 0.92128
Diversity Loss - Mean: -0.07810, Variance: 0.01394
Semantic Loss - Mean: 0.91830, Variance: 0.03650

Train Epoch: 133 
task: sign, mean loss: 0.00262, accuracy: 1.00000, avg. loss over tasks: 0.00262, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.09674, Variance: 0.01223
Semantic Loss - Mean: 0.03723, Variance: 0.00804

Test Epoch: 133 
task: sign, mean loss: 0.96755, accuracy: 0.76923, avg. loss over tasks: 0.96755
Diversity Loss - Mean: -0.07701, Variance: 0.01395
Semantic Loss - Mean: 0.95918, Variance: 0.03636

Train Epoch: 134 
task: sign, mean loss: 0.00429, accuracy: 1.00000, avg. loss over tasks: 0.00429, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.09555, Variance: 0.01223
Semantic Loss - Mean: 0.02320, Variance: 0.00800

Test Epoch: 134 
task: sign, mean loss: 0.97456, accuracy: 0.76331, avg. loss over tasks: 0.97456
Diversity Loss - Mean: -0.07797, Variance: 0.01396
Semantic Loss - Mean: 0.95976, Variance: 0.03622

Train Epoch: 135 
task: sign, mean loss: 0.00392, accuracy: 1.00000, avg. loss over tasks: 0.00392, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.09620, Variance: 0.01223
Semantic Loss - Mean: 0.02970, Variance: 0.00796

Test Epoch: 135 
task: sign, mean loss: 0.93723, accuracy: 0.76923, avg. loss over tasks: 0.93723
Diversity Loss - Mean: -0.08020, Variance: 0.01397
Semantic Loss - Mean: 0.92072, Variance: 0.03608

Train Epoch: 136 
task: sign, mean loss: 0.00192, accuracy: 1.00000, avg. loss over tasks: 0.00192, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.09602, Variance: 0.01223
Semantic Loss - Mean: 0.03693, Variance: 0.00794

Test Epoch: 136 
task: sign, mean loss: 0.99230, accuracy: 0.74556, avg. loss over tasks: 0.99230
Diversity Loss - Mean: -0.07672, Variance: 0.01398
Semantic Loss - Mean: 0.98587, Variance: 0.03596

Train Epoch: 137 
task: sign, mean loss: 0.00502, accuracy: 1.00000, avg. loss over tasks: 0.00502, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.09733, Variance: 0.01224
Semantic Loss - Mean: 0.05080, Variance: 0.00795

Test Epoch: 137 
task: sign, mean loss: 1.05091, accuracy: 0.72781, avg. loss over tasks: 1.05091
Diversity Loss - Mean: -0.07510, Variance: 0.01399
Semantic Loss - Mean: 1.05294, Variance: 0.03588

Train Epoch: 138 
task: sign, mean loss: 0.00301, accuracy: 1.00000, avg. loss over tasks: 0.00301, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.09680, Variance: 0.01224
Semantic Loss - Mean: 0.05121, Variance: 0.00795

Test Epoch: 138 
task: sign, mean loss: 1.10136, accuracy: 0.71598, avg. loss over tasks: 1.10136
Diversity Loss - Mean: -0.07280, Variance: 0.01400
Semantic Loss - Mean: 1.10250, Variance: 0.03583

Train Epoch: 139 
task: sign, mean loss: 0.00139, accuracy: 1.00000, avg. loss over tasks: 0.00139, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.09611, Variance: 0.01225
Semantic Loss - Mean: 0.04752, Variance: 0.00794

Test Epoch: 139 
task: sign, mean loss: 1.13733, accuracy: 0.71598, avg. loss over tasks: 1.13733
Diversity Loss - Mean: -0.07350, Variance: 0.01401
Semantic Loss - Mean: 1.13018, Variance: 0.03576

Train Epoch: 140 
task: sign, mean loss: 0.03019, accuracy: 0.99457, avg. loss over tasks: 0.03019, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.09623, Variance: 0.01225
Semantic Loss - Mean: 0.06342, Variance: 0.00795

Test Epoch: 140 
task: sign, mean loss: 0.96434, accuracy: 0.75148, avg. loss over tasks: 0.96434
Diversity Loss - Mean: -0.07872, Variance: 0.01402
Semantic Loss - Mean: 0.98279, Variance: 0.03564

Train Epoch: 141 
task: sign, mean loss: 0.00245, accuracy: 1.00000, avg. loss over tasks: 0.00245, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.09782, Variance: 0.01225
Semantic Loss - Mean: 0.04123, Variance: 0.00793

Test Epoch: 141 
task: sign, mean loss: 1.02807, accuracy: 0.73373, avg. loss over tasks: 1.02807
Diversity Loss - Mean: -0.07516, Variance: 0.01403
Semantic Loss - Mean: 1.04392, Variance: 0.03555

Train Epoch: 142 
task: sign, mean loss: 0.00412, accuracy: 1.00000, avg. loss over tasks: 0.00412, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.09585, Variance: 0.01226
Semantic Loss - Mean: 0.08025, Variance: 0.00801

Test Epoch: 142 
task: sign, mean loss: 0.92926, accuracy: 0.76331, avg. loss over tasks: 0.92926
Diversity Loss - Mean: -0.07835, Variance: 0.01403
Semantic Loss - Mean: 0.94937, Variance: 0.03541

Train Epoch: 143 
task: sign, mean loss: 0.00986, accuracy: 1.00000, avg. loss over tasks: 0.00986, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.09677, Variance: 0.01226
Semantic Loss - Mean: 0.05212, Variance: 0.00797

Test Epoch: 143 
task: sign, mean loss: 0.96109, accuracy: 0.75148, avg. loss over tasks: 0.96109
Diversity Loss - Mean: -0.07673, Variance: 0.01404
Semantic Loss - Mean: 0.99387, Variance: 0.03528

Train Epoch: 144 
task: sign, mean loss: 0.00450, accuracy: 1.00000, avg. loss over tasks: 0.00450, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.09783, Variance: 0.01227
Semantic Loss - Mean: 0.03341, Variance: 0.00793

Test Epoch: 144 
task: sign, mean loss: 1.00619, accuracy: 0.75148, avg. loss over tasks: 1.00619
Diversity Loss - Mean: -0.07490, Variance: 0.01405
Semantic Loss - Mean: 1.03397, Variance: 0.03517

Train Epoch: 145 
task: sign, mean loss: 0.00180, accuracy: 1.00000, avg. loss over tasks: 0.00180, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.09689, Variance: 0.01227
Semantic Loss - Mean: 0.03780, Variance: 0.00793

Test Epoch: 145 
task: sign, mean loss: 0.90313, accuracy: 0.77515, avg. loss over tasks: 0.90313
Diversity Loss - Mean: -0.07871, Variance: 0.01406
Semantic Loss - Mean: 0.94748, Variance: 0.03502

Train Epoch: 146 
task: sign, mean loss: 0.00968, accuracy: 0.99457, avg. loss over tasks: 0.00968, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.09687, Variance: 0.01227
Semantic Loss - Mean: 0.04483, Variance: 0.00791

Test Epoch: 146 
task: sign, mean loss: 1.14691, accuracy: 0.73373, avg. loss over tasks: 1.14691
Diversity Loss - Mean: -0.07163, Variance: 0.01406
Semantic Loss - Mean: 1.14882, Variance: 0.03492

Train Epoch: 147 
task: sign, mean loss: 0.00147, accuracy: 1.00000, avg. loss over tasks: 0.00147, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.09591, Variance: 0.01227
Semantic Loss - Mean: 0.03961, Variance: 0.00792

Test Epoch: 147 
task: sign, mean loss: 1.05941, accuracy: 0.75148, avg. loss over tasks: 1.05941
Diversity Loss - Mean: -0.07389, Variance: 0.01407
Semantic Loss - Mean: 1.07206, Variance: 0.03480

Train Epoch: 148 
task: sign, mean loss: 0.00515, accuracy: 1.00000, avg. loss over tasks: 0.00515, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.09529, Variance: 0.01228
Semantic Loss - Mean: 0.03710, Variance: 0.00790

Test Epoch: 148 
task: sign, mean loss: 1.04545, accuracy: 0.73964, avg. loss over tasks: 1.04545
Diversity Loss - Mean: -0.07641, Variance: 0.01408
Semantic Loss - Mean: 1.04775, Variance: 0.03467

Train Epoch: 149 
task: sign, mean loss: 0.00429, accuracy: 1.00000, avg. loss over tasks: 0.00429, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.09646, Variance: 0.01228
Semantic Loss - Mean: 0.05948, Variance: 0.00793

Test Epoch: 149 
task: sign, mean loss: 0.97384, accuracy: 0.75148, avg. loss over tasks: 0.97384
Diversity Loss - Mean: -0.07979, Variance: 0.01408
Semantic Loss - Mean: 0.97948, Variance: 0.03454

Train Epoch: 150 
task: sign, mean loss: 0.00196, accuracy: 1.00000, avg. loss over tasks: 0.00196, lr: 3e-07
Diversity Loss - Mean: -0.09639, Variance: 0.01228
Semantic Loss - Mean: 0.03373, Variance: 0.00791

Test Epoch: 150 
task: sign, mean loss: 1.00955, accuracy: 0.74556, avg. loss over tasks: 1.00955
Diversity Loss - Mean: -0.07660, Variance: 0.01409
Semantic Loss - Mean: 1.02745, Variance: 0.03443

