Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09215, accuracy: 0.63587, avg. loss over tasks: 1.09215, lr: 3e-05
Diversity Loss - Mean: -0.00990, Variance: 0.01051
Semantic Loss - Mean: 1.43149, Variance: 0.07273

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17718, accuracy: 0.66272, avg. loss over tasks: 1.17718
Diversity Loss - Mean: -0.02881, Variance: 0.01241
Semantic Loss - Mean: 1.16128, Variance: 0.05390

Train Epoch: 2 
task: sign, mean loss: 0.96174, accuracy: 0.67391, avg. loss over tasks: 0.96174, lr: 6e-05
Diversity Loss - Mean: -0.01546, Variance: 0.01043
Semantic Loss - Mean: 0.98277, Variance: 0.03932

Test Epoch: 2 
task: sign, mean loss: 1.10242, accuracy: 0.66272, avg. loss over tasks: 1.10242
Diversity Loss - Mean: -0.02554, Variance: 0.01195
Semantic Loss - Mean: 1.14902, Variance: 0.03279

Train Epoch: 3 
task: sign, mean loss: 0.78397, accuracy: 0.69022, avg. loss over tasks: 0.78397, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.02799, Variance: 0.01020
Semantic Loss - Mean: 0.99292, Variance: 0.02722

Test Epoch: 3 
task: sign, mean loss: 1.26488, accuracy: 0.62130, avg. loss over tasks: 1.26488
Diversity Loss - Mean: -0.04533, Variance: 0.01126
Semantic Loss - Mean: 1.11265, Variance: 0.02903

Train Epoch: 4 
task: sign, mean loss: 0.75032, accuracy: 0.66304, avg. loss over tasks: 0.75032, lr: 0.00012
Diversity Loss - Mean: -0.04667, Variance: 0.00984
Semantic Loss - Mean: 0.88665, Variance: 0.02109

Test Epoch: 4 
task: sign, mean loss: 1.47561, accuracy: 0.60947, avg. loss over tasks: 1.47561
Diversity Loss - Mean: -0.04507, Variance: 0.01067
Semantic Loss - Mean: 1.09562, Variance: 0.02378

Train Epoch: 5 
task: sign, mean loss: 0.74150, accuracy: 0.66304, avg. loss over tasks: 0.74150, lr: 0.00015
Diversity Loss - Mean: -0.03824, Variance: 0.00949
Semantic Loss - Mean: 0.78210, Variance: 0.01729

Test Epoch: 5 
task: sign, mean loss: 1.99976, accuracy: 0.42604, avg. loss over tasks: 1.99976
Diversity Loss - Mean: -0.03864, Variance: 0.01011
Semantic Loss - Mean: 1.30664, Variance: 0.02338

Train Epoch: 6 
task: sign, mean loss: 0.64183, accuracy: 0.78261, avg. loss over tasks: 0.64183, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.03928, Variance: 0.00931
Semantic Loss - Mean: 0.70906, Variance: 0.01471

Test Epoch: 6 
task: sign, mean loss: 2.42959, accuracy: 0.39053, avg. loss over tasks: 2.42959
Diversity Loss - Mean: -0.00588, Variance: 0.01010
Semantic Loss - Mean: 1.58674, Variance: 0.02524

Train Epoch: 7 
task: sign, mean loss: 0.63173, accuracy: 0.76630, avg. loss over tasks: 0.63173, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.05281, Variance: 0.00940
Semantic Loss - Mean: 0.66625, Variance: 0.01305

Test Epoch: 7 
task: sign, mean loss: 2.46076, accuracy: 0.33728, avg. loss over tasks: 2.46076
Diversity Loss - Mean: 0.00915, Variance: 0.01004
Semantic Loss - Mean: 2.09225, Variance: 0.02882

Train Epoch: 8 
task: sign, mean loss: 0.66246, accuracy: 0.80435, avg. loss over tasks: 0.66246, lr: 0.00024
Diversity Loss - Mean: -0.01536, Variance: 0.00932
Semantic Loss - Mean: 0.68164, Variance: 0.01213

Test Epoch: 8 
task: sign, mean loss: 2.30023, accuracy: 0.50888, avg. loss over tasks: 2.30023
Diversity Loss - Mean: 0.03497, Variance: 0.01016
Semantic Loss - Mean: 1.73059, Variance: 0.03038

Train Epoch: 9 
task: sign, mean loss: 0.75677, accuracy: 0.75000, avg. loss over tasks: 0.75677, lr: 0.00027
Diversity Loss - Mean: -0.02432, Variance: 0.00925
Semantic Loss - Mean: 0.67388, Variance: 0.01129

Test Epoch: 9 
task: sign, mean loss: 1.78499, accuracy: 0.50888, avg. loss over tasks: 1.78499
Diversity Loss - Mean: 0.00237, Variance: 0.01038
Semantic Loss - Mean: 1.53770, Variance: 0.03054

Train Epoch: 10 
task: sign, mean loss: 0.83036, accuracy: 0.66848, avg. loss over tasks: 0.83036, lr: 0.0003
Diversity Loss - Mean: -0.03123, Variance: 0.00928
Semantic Loss - Mean: 0.75697, Variance: 0.01051

Test Epoch: 10 
task: sign, mean loss: 3.62628, accuracy: 0.20118, avg. loss over tasks: 3.62628
Diversity Loss - Mean: 0.04711, Variance: 0.01058
Semantic Loss - Mean: 2.45774, Variance: 0.03395

Train Epoch: 11 
task: sign, mean loss: 0.59121, accuracy: 0.73370, avg. loss over tasks: 0.59121, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.01735, Variance: 0.00932
Semantic Loss - Mean: 0.53021, Variance: 0.00980

Test Epoch: 11 
task: sign, mean loss: 1.66860, accuracy: 0.66864, avg. loss over tasks: 1.66860
Diversity Loss - Mean: -0.02440, Variance: 0.01092
Semantic Loss - Mean: 1.40518, Variance: 0.03254

Train Epoch: 12 
task: sign, mean loss: 0.42477, accuracy: 0.83152, avg. loss over tasks: 0.42477, lr: 0.000299849111021216
Diversity Loss - Mean: 0.00582, Variance: 0.00932
Semantic Loss - Mean: 0.41515, Variance: 0.00920

Test Epoch: 12 
task: sign, mean loss: 3.99754, accuracy: 0.29586, avg. loss over tasks: 3.99754
Diversity Loss - Mean: 0.07388, Variance: 0.01113
Semantic Loss - Mean: 3.05568, Variance: 0.03818

Train Epoch: 13 
task: sign, mean loss: 0.52944, accuracy: 0.84783, avg. loss over tasks: 0.52944, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.01148, Variance: 0.00940
Semantic Loss - Mean: 0.55904, Variance: 0.00888

Test Epoch: 13 
task: sign, mean loss: 2.01569, accuracy: 0.49704, avg. loss over tasks: 2.01569
Diversity Loss - Mean: -0.01113, Variance: 0.01114
Semantic Loss - Mean: 1.78210, Variance: 0.04028

Train Epoch: 14 
task: sign, mean loss: 0.43298, accuracy: 0.84783, avg. loss over tasks: 0.43298, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.02119, Variance: 0.00950
Semantic Loss - Mean: 0.46971, Variance: 0.00869

Test Epoch: 14 
task: sign, mean loss: 1.73695, accuracy: 0.68047, avg. loss over tasks: 1.73695
Diversity Loss - Mean: -0.03616, Variance: 0.01142
Semantic Loss - Mean: 1.72301, Variance: 0.03818

Train Epoch: 15 
task: sign, mean loss: 0.32450, accuracy: 0.85326, avg. loss over tasks: 0.32450, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.02563, Variance: 0.00957
Semantic Loss - Mean: 0.38431, Variance: 0.00846

Test Epoch: 15 
task: sign, mean loss: 1.77943, accuracy: 0.63314, avg. loss over tasks: 1.77943
Diversity Loss - Mean: -0.02584, Variance: 0.01146
Semantic Loss - Mean: 1.57412, Variance: 0.03660

Train Epoch: 16 
task: sign, mean loss: 0.31906, accuracy: 0.88587, avg. loss over tasks: 0.31906, lr: 0.000298643821800925
Diversity Loss - Mean: -0.01845, Variance: 0.00959
Semantic Loss - Mean: 0.37519, Variance: 0.00809

Test Epoch: 16 
task: sign, mean loss: 2.53503, accuracy: 0.46154, avg. loss over tasks: 2.53503
Diversity Loss - Mean: -0.00126, Variance: 0.01137
Semantic Loss - Mean: 2.28024, Variance: 0.03519

Train Epoch: 17 
task: sign, mean loss: 0.21605, accuracy: 0.92935, avg. loss over tasks: 0.21605, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.00464, Variance: 0.00961
Semantic Loss - Mean: 0.24553, Variance: 0.00780

Test Epoch: 17 
task: sign, mean loss: 2.31954, accuracy: 0.67456, avg. loss over tasks: 2.31954
Diversity Loss - Mean: -0.01905, Variance: 0.01158
Semantic Loss - Mean: 2.06731, Variance: 0.03429

Train Epoch: 18 
task: sign, mean loss: 0.27656, accuracy: 0.90761, avg. loss over tasks: 0.27656, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.02318, Variance: 0.00971
Semantic Loss - Mean: 0.29336, Variance: 0.00761

Test Epoch: 18 
task: sign, mean loss: 1.82465, accuracy: 0.68639, avg. loss over tasks: 1.82465
Diversity Loss - Mean: -0.05552, Variance: 0.01172
Semantic Loss - Mean: 1.64466, Variance: 0.03401

Train Epoch: 19 
task: sign, mean loss: 0.19748, accuracy: 0.92391, avg. loss over tasks: 0.19748, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.02156, Variance: 0.00977
Semantic Loss - Mean: 0.23718, Variance: 0.00746

Test Epoch: 19 
task: sign, mean loss: 2.72494, accuracy: 0.37278, avg. loss over tasks: 2.72494
Diversity Loss - Mean: 0.02580, Variance: 0.01179
Semantic Loss - Mean: 2.15546, Variance: 0.03582

Train Epoch: 20 
task: sign, mean loss: 0.32648, accuracy: 0.90761, avg. loss over tasks: 0.32648, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.01706, Variance: 0.00983
Semantic Loss - Mean: 0.35985, Variance: 0.00743

Test Epoch: 20 
task: sign, mean loss: 1.92331, accuracy: 0.60947, avg. loss over tasks: 1.92331
Diversity Loss - Mean: -0.04018, Variance: 0.01189
Semantic Loss - Mean: 1.76825, Variance: 0.03615

Train Epoch: 21 
task: sign, mean loss: 0.28205, accuracy: 0.89130, avg. loss over tasks: 0.28205, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.01994, Variance: 0.00985
Semantic Loss - Mean: 0.32027, Variance: 0.00744

Test Epoch: 21 
task: sign, mean loss: 1.82324, accuracy: 0.63905, avg. loss over tasks: 1.82324
Diversity Loss - Mean: -0.04834, Variance: 0.01208
Semantic Loss - Mean: 1.64293, Variance: 0.03505

Train Epoch: 22 
task: sign, mean loss: 0.19832, accuracy: 0.92391, avg. loss over tasks: 0.19832, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.02469, Variance: 0.00989
Semantic Loss - Mean: 0.22433, Variance: 0.00729

Test Epoch: 22 
task: sign, mean loss: 2.85277, accuracy: 0.42012, avg. loss over tasks: 2.85277
Diversity Loss - Mean: 0.01489, Variance: 0.01204
Semantic Loss - Mean: 2.49511, Variance: 0.03489

Train Epoch: 23 
task: sign, mean loss: 0.06851, accuracy: 0.98370, avg. loss over tasks: 0.06851, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.02078, Variance: 0.00992
Semantic Loss - Mean: 0.10304, Variance: 0.00705

Test Epoch: 23 
task: sign, mean loss: 2.89187, accuracy: 0.39645, avg. loss over tasks: 2.89187
Diversity Loss - Mean: 0.01009, Variance: 0.01201
Semantic Loss - Mean: 2.40854, Variance: 0.03584

Train Epoch: 24 
task: sign, mean loss: 0.12337, accuracy: 0.95652, avg. loss over tasks: 0.12337, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.01817, Variance: 0.00993
Semantic Loss - Mean: 0.15632, Variance: 0.00691

Test Epoch: 24 
task: sign, mean loss: 1.52942, accuracy: 0.63905, avg. loss over tasks: 1.52942
Diversity Loss - Mean: -0.05806, Variance: 0.01212
Semantic Loss - Mean: 1.37186, Variance: 0.03566

Train Epoch: 25 
task: sign, mean loss: 0.07437, accuracy: 0.97283, avg. loss over tasks: 0.07437, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.02467, Variance: 0.00994
Semantic Loss - Mean: 0.09966, Variance: 0.00674

Test Epoch: 25 
task: sign, mean loss: 1.87372, accuracy: 0.65089, avg. loss over tasks: 1.87372
Diversity Loss - Mean: -0.04894, Variance: 0.01219
Semantic Loss - Mean: 1.61013, Variance: 0.03510

Train Epoch: 26 
task: sign, mean loss: 0.08948, accuracy: 0.96739, avg. loss over tasks: 0.08948, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.03306, Variance: 0.00999
Semantic Loss - Mean: 0.13028, Variance: 0.00664

Test Epoch: 26 
task: sign, mean loss: 1.95335, accuracy: 0.60355, avg. loss over tasks: 1.95335
Diversity Loss - Mean: -0.06447, Variance: 0.01228
Semantic Loss - Mean: 1.70818, Variance: 0.03444

Train Epoch: 27 
task: sign, mean loss: 0.13053, accuracy: 0.97826, avg. loss over tasks: 0.13053, lr: 0.000289228031029578
Diversity Loss - Mean: -0.03213, Variance: 0.01001
Semantic Loss - Mean: 0.14683, Variance: 0.00679

Test Epoch: 27 
task: sign, mean loss: 2.19379, accuracy: 0.52663, avg. loss over tasks: 2.19379
Diversity Loss - Mean: -0.04765, Variance: 0.01237
Semantic Loss - Mean: 1.87083, Variance: 0.03464

Train Epoch: 28 
task: sign, mean loss: 0.10868, accuracy: 0.97283, avg. loss over tasks: 0.10868, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.03010, Variance: 0.01002
Semantic Loss - Mean: 0.13954, Variance: 0.00667

Test Epoch: 28 
task: sign, mean loss: 2.00560, accuracy: 0.45562, avg. loss over tasks: 2.00560
Diversity Loss - Mean: -0.02764, Variance: 0.01240
Semantic Loss - Mean: 1.57753, Variance: 0.03493

Train Epoch: 29 
task: sign, mean loss: 0.19171, accuracy: 0.93478, avg. loss over tasks: 0.19171, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.03789, Variance: 0.01009
Semantic Loss - Mean: 0.24985, Variance: 0.00685

Test Epoch: 29 
task: sign, mean loss: 1.29901, accuracy: 0.64497, avg. loss over tasks: 1.29901
Diversity Loss - Mean: -0.01814, Variance: 0.01244
Semantic Loss - Mean: 1.19787, Variance: 0.03645

Train Epoch: 30 
task: sign, mean loss: 0.23839, accuracy: 0.92391, avg. loss over tasks: 0.23839, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.02950, Variance: 0.01015
Semantic Loss - Mean: 0.30891, Variance: 0.00697

Test Epoch: 30 
task: sign, mean loss: 1.97817, accuracy: 0.56213, avg. loss over tasks: 1.97817
Diversity Loss - Mean: -0.02456, Variance: 0.01246
Semantic Loss - Mean: 1.71670, Variance: 0.03848

Train Epoch: 31 
task: sign, mean loss: 0.14596, accuracy: 0.94022, avg. loss over tasks: 0.14596, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.04188, Variance: 0.01018
Semantic Loss - Mean: 0.21939, Variance: 0.00697

Test Epoch: 31 
task: sign, mean loss: 3.07801, accuracy: 0.34320, avg. loss over tasks: 3.07801
Diversity Loss - Mean: 0.00285, Variance: 0.01236
Semantic Loss - Mean: 2.58366, Variance: 0.04293

Train Epoch: 32 
task: sign, mean loss: 0.03416, accuracy: 0.99457, avg. loss over tasks: 0.03416, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.03993, Variance: 0.01021
Semantic Loss - Mean: 0.08428, Variance: 0.00685

Test Epoch: 32 
task: sign, mean loss: 1.80532, accuracy: 0.62722, avg. loss over tasks: 1.80532
Diversity Loss - Mean: -0.04234, Variance: 0.01238
Semantic Loss - Mean: 1.62470, Variance: 0.04347

Train Epoch: 33 
task: sign, mean loss: 0.04379, accuracy: 0.99457, avg. loss over tasks: 0.04379, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.03140, Variance: 0.01023
Semantic Loss - Mean: 0.05004, Variance: 0.00669

Test Epoch: 33 
task: sign, mean loss: 1.36928, accuracy: 0.73964, avg. loss over tasks: 1.36928
Diversity Loss - Mean: -0.06364, Variance: 0.01251
Semantic Loss - Mean: 1.24872, Variance: 0.04306

Train Epoch: 34 
task: sign, mean loss: 0.06877, accuracy: 0.96739, avg. loss over tasks: 0.06877, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.03372, Variance: 0.01026
Semantic Loss - Mean: 0.10002, Variance: 0.00660

Test Epoch: 34 
task: sign, mean loss: 2.84750, accuracy: 0.68047, avg. loss over tasks: 2.84750
Diversity Loss - Mean: -0.07992, Variance: 0.01274
Semantic Loss - Mean: 2.59321, Variance: 0.04339

Train Epoch: 35 
task: sign, mean loss: 0.08661, accuracy: 0.96196, avg. loss over tasks: 0.08661, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.03776, Variance: 0.01030
Semantic Loss - Mean: 0.11038, Variance: 0.00657

Test Epoch: 35 
task: sign, mean loss: 2.69898, accuracy: 0.67456, avg. loss over tasks: 2.69898
Diversity Loss - Mean: -0.09112, Variance: 0.01295
Semantic Loss - Mean: 2.27108, Variance: 0.04316

Train Epoch: 36 
task: sign, mean loss: 0.06834, accuracy: 0.97283, avg. loss over tasks: 0.06834, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.03751, Variance: 0.01031
Semantic Loss - Mean: 0.08425, Variance: 0.00643

Test Epoch: 36 
task: sign, mean loss: 2.30253, accuracy: 0.62130, avg. loss over tasks: 2.30253
Diversity Loss - Mean: -0.05488, Variance: 0.01299
Semantic Loss - Mean: 2.17203, Variance: 0.04542

Train Epoch: 37 
task: sign, mean loss: 0.03243, accuracy: 0.99457, avg. loss over tasks: 0.03243, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.04260, Variance: 0.01033
Semantic Loss - Mean: 0.08039, Variance: 0.00633

Test Epoch: 37 
task: sign, mean loss: 1.51095, accuracy: 0.68047, avg. loss over tasks: 1.51095
Diversity Loss - Mean: -0.05182, Variance: 0.01301
Semantic Loss - Mean: 1.31953, Variance: 0.04665

Train Epoch: 38 
task: sign, mean loss: 0.01930, accuracy: 0.99457, avg. loss over tasks: 0.01930, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.04532, Variance: 0.01036
Semantic Loss - Mean: 0.06053, Variance: 0.00633

Test Epoch: 38 
task: sign, mean loss: 1.72667, accuracy: 0.55621, avg. loss over tasks: 1.72667
Diversity Loss - Mean: -0.00660, Variance: 0.01297
Semantic Loss - Mean: 1.77864, Variance: 0.04748

Train Epoch: 39 
task: sign, mean loss: 0.02295, accuracy: 0.98913, avg. loss over tasks: 0.02295, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.04366, Variance: 0.01037
Semantic Loss - Mean: 0.03826, Variance: 0.00621

Test Epoch: 39 
task: sign, mean loss: 1.46779, accuracy: 0.66864, avg. loss over tasks: 1.46779
Diversity Loss - Mean: -0.04368, Variance: 0.01295
Semantic Loss - Mean: 1.18339, Variance: 0.04740

Train Epoch: 40 
task: sign, mean loss: 0.01999, accuracy: 0.99457, avg. loss over tasks: 0.01999, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.04942, Variance: 0.01039
Semantic Loss - Mean: 0.05217, Variance: 0.00615

Test Epoch: 40 
task: sign, mean loss: 1.62119, accuracy: 0.69231, avg. loss over tasks: 1.62119
Diversity Loss - Mean: -0.04810, Variance: 0.01294
Semantic Loss - Mean: 1.39140, Variance: 0.04666

Train Epoch: 41 
task: sign, mean loss: 0.03905, accuracy: 0.98913, avg. loss over tasks: 0.03905, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.05009, Variance: 0.01040
Semantic Loss - Mean: 0.06753, Variance: 0.00611

Test Epoch: 41 
task: sign, mean loss: 1.94717, accuracy: 0.65089, avg. loss over tasks: 1.94717
Diversity Loss - Mean: -0.05016, Variance: 0.01294
Semantic Loss - Mean: 1.74931, Variance: 0.04680

Train Epoch: 42 
task: sign, mean loss: 0.12062, accuracy: 0.94565, avg. loss over tasks: 0.12062, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.06240, Variance: 0.01043
Semantic Loss - Mean: 0.16919, Variance: 0.00611

Test Epoch: 42 
task: sign, mean loss: 1.03729, accuracy: 0.69822, avg. loss over tasks: 1.03729
Diversity Loss - Mean: -0.03734, Variance: 0.01297
Semantic Loss - Mean: 0.96897, Variance: 0.04625

Train Epoch: 43 
task: sign, mean loss: 0.59167, accuracy: 0.83696, avg. loss over tasks: 0.59167, lr: 0.000260757131773478
Diversity Loss - Mean: -0.07400, Variance: 0.01051
Semantic Loss - Mean: 0.60474, Variance: 0.00644

Test Epoch: 43 
task: sign, mean loss: 3.60190, accuracy: 0.28402, avg. loss over tasks: 3.60190
Diversity Loss - Mean: 0.00225, Variance: 0.01296
Semantic Loss - Mean: 2.50018, Variance: 0.04944

Train Epoch: 44 
task: sign, mean loss: 0.63413, accuracy: 0.79348, avg. loss over tasks: 0.63413, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.08659, Variance: 0.01061
Semantic Loss - Mean: 0.68114, Variance: 0.00680

Test Epoch: 44 
task: sign, mean loss: 3.61011, accuracy: 0.20118, avg. loss over tasks: 3.61011
Diversity Loss - Mean: -0.03455, Variance: 0.01293
Semantic Loss - Mean: 2.55982, Variance: 0.05119

Train Epoch: 45 
task: sign, mean loss: 0.71847, accuracy: 0.76630, avg. loss over tasks: 0.71847, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.09392, Variance: 0.01075
Semantic Loss - Mean: 0.76124, Variance: 0.00692

Test Epoch: 45 
task: sign, mean loss: 2.21673, accuracy: 0.38462, avg. loss over tasks: 2.21673
Diversity Loss - Mean: -0.04579, Variance: 0.01292
Semantic Loss - Mean: 1.91722, Variance: 0.05111

Train Epoch: 46 
task: sign, mean loss: 0.39604, accuracy: 0.84783, avg. loss over tasks: 0.39604, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.08797, Variance: 0.01084
Semantic Loss - Mean: 0.45315, Variance: 0.00689

Test Epoch: 46 
task: sign, mean loss: 1.82014, accuracy: 0.67456, avg. loss over tasks: 1.82014
Diversity Loss - Mean: -0.08449, Variance: 0.01294
Semantic Loss - Mean: 1.71223, Variance: 0.05376

Train Epoch: 47 
task: sign, mean loss: 0.38216, accuracy: 0.85326, avg. loss over tasks: 0.38216, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.08623, Variance: 0.01090
Semantic Loss - Mean: 0.42919, Variance: 0.00687

Test Epoch: 47 
task: sign, mean loss: 1.98394, accuracy: 0.60947, avg. loss over tasks: 1.98394
Diversity Loss - Mean: -0.08200, Variance: 0.01302
Semantic Loss - Mean: 1.73611, Variance: 0.05324

Train Epoch: 48 
task: sign, mean loss: 0.20680, accuracy: 0.91848, avg. loss over tasks: 0.20680, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.07484, Variance: 0.01095
Semantic Loss - Mean: 0.24621, Variance: 0.00679

Test Epoch: 48 
task: sign, mean loss: 2.58307, accuracy: 0.50296, avg. loss over tasks: 2.58307
Diversity Loss - Mean: -0.07661, Variance: 0.01307
Semantic Loss - Mean: 2.31892, Variance: 0.05279

Train Epoch: 49 
task: sign, mean loss: 0.17189, accuracy: 0.94022, avg. loss over tasks: 0.17189, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.06817, Variance: 0.01098
Semantic Loss - Mean: 0.19424, Variance: 0.00685

Test Epoch: 49 
task: sign, mean loss: 3.83636, accuracy: 0.28402, avg. loss over tasks: 3.83636
Diversity Loss - Mean: -0.02676, Variance: 0.01306
Semantic Loss - Mean: 3.32890, Variance: 0.05306

Train Epoch: 50 
task: sign, mean loss: 0.22966, accuracy: 0.91304, avg. loss over tasks: 0.22966, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.06782, Variance: 0.01101
Semantic Loss - Mean: 0.26353, Variance: 0.00682

Test Epoch: 50 
task: sign, mean loss: 3.84154, accuracy: 0.20710, avg. loss over tasks: 3.84154
Diversity Loss - Mean: -0.03419, Variance: 0.01307
Semantic Loss - Mean: 3.14149, Variance: 0.05410

Train Epoch: 51 
task: sign, mean loss: 0.22185, accuracy: 0.89674, avg. loss over tasks: 0.22185, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.07832, Variance: 0.01105
Semantic Loss - Mean: 0.25755, Variance: 0.00679

Test Epoch: 51 
task: sign, mean loss: 1.95286, accuracy: 0.58580, avg. loss over tasks: 1.95286
Diversity Loss - Mean: -0.09060, Variance: 0.01315
Semantic Loss - Mean: 1.83369, Variance: 0.05344

Train Epoch: 52 
task: sign, mean loss: 0.11793, accuracy: 0.95652, avg. loss over tasks: 0.11793, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.07606, Variance: 0.01110
Semantic Loss - Mean: 0.14589, Variance: 0.00670

Test Epoch: 52 
task: sign, mean loss: 2.96377, accuracy: 0.31953, avg. loss over tasks: 2.96377
Diversity Loss - Mean: -0.06087, Variance: 0.01319
Semantic Loss - Mean: 2.68814, Variance: 0.05336

Train Epoch: 53 
task: sign, mean loss: 0.13175, accuracy: 0.93478, avg. loss over tasks: 0.13175, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.07454, Variance: 0.01114
Semantic Loss - Mean: 0.18308, Variance: 0.00664

Test Epoch: 53 
task: sign, mean loss: 1.29314, accuracy: 0.71006, avg. loss over tasks: 1.29314
Diversity Loss - Mean: -0.09799, Variance: 0.01328
Semantic Loss - Mean: 1.29029, Variance: 0.05298

Train Epoch: 54 
task: sign, mean loss: 0.09548, accuracy: 0.96196, avg. loss over tasks: 0.09548, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.06615, Variance: 0.01115
Semantic Loss - Mean: 0.12130, Variance: 0.00655

Test Epoch: 54 
task: sign, mean loss: 2.45963, accuracy: 0.57396, avg. loss over tasks: 2.45963
Diversity Loss - Mean: -0.09198, Variance: 0.01335
Semantic Loss - Mean: 2.34363, Variance: 0.05273

Train Epoch: 55 
task: sign, mean loss: 0.16073, accuracy: 0.93478, avg. loss over tasks: 0.16073, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.07102, Variance: 0.01118
Semantic Loss - Mean: 0.19687, Variance: 0.00651

Test Epoch: 55 
task: sign, mean loss: 2.67936, accuracy: 0.61538, avg. loss over tasks: 2.67936
Diversity Loss - Mean: -0.11042, Variance: 0.01345
Semantic Loss - Mean: 2.42867, Variance: 0.05214

Train Epoch: 56 
task: sign, mean loss: 0.16422, accuracy: 0.92391, avg. loss over tasks: 0.16422, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.07777, Variance: 0.01121
Semantic Loss - Mean: 0.18865, Variance: 0.00647

Test Epoch: 56 
task: sign, mean loss: 1.46842, accuracy: 0.59763, avg. loss over tasks: 1.46842
Diversity Loss - Mean: -0.08323, Variance: 0.01351
Semantic Loss - Mean: 1.49596, Variance: 0.05243

Train Epoch: 57 
task: sign, mean loss: 0.05169, accuracy: 0.98370, avg. loss over tasks: 0.05169, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.07686, Variance: 0.01125
Semantic Loss - Mean: 0.08729, Variance: 0.00641

Test Epoch: 57 
task: sign, mean loss: 1.58505, accuracy: 0.61538, avg. loss over tasks: 1.58505
Diversity Loss - Mean: -0.08051, Variance: 0.01358
Semantic Loss - Mean: 1.42735, Variance: 0.05185

Train Epoch: 58 
task: sign, mean loss: 0.03596, accuracy: 0.98370, avg. loss over tasks: 0.03596, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.07515, Variance: 0.01128
Semantic Loss - Mean: 0.05008, Variance: 0.00632

Test Epoch: 58 
task: sign, mean loss: 1.44313, accuracy: 0.69822, avg. loss over tasks: 1.44313
Diversity Loss - Mean: -0.08222, Variance: 0.01364
Semantic Loss - Mean: 1.37794, Variance: 0.05135

Train Epoch: 59 
task: sign, mean loss: 0.01730, accuracy: 1.00000, avg. loss over tasks: 0.01730, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.07375, Variance: 0.01130
Semantic Loss - Mean: 0.03048, Variance: 0.00622

Test Epoch: 59 
task: sign, mean loss: 1.45119, accuracy: 0.70414, avg. loss over tasks: 1.45119
Diversity Loss - Mean: -0.08330, Variance: 0.01370
Semantic Loss - Mean: 1.41486, Variance: 0.05081

Train Epoch: 60 
task: sign, mean loss: 0.02966, accuracy: 0.98913, avg. loss over tasks: 0.02966, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.07335, Variance: 0.01133
Semantic Loss - Mean: 0.04637, Variance: 0.00614

Test Epoch: 60 
task: sign, mean loss: 1.79585, accuracy: 0.66272, avg. loss over tasks: 1.79585
Diversity Loss - Mean: -0.07464, Variance: 0.01373
Semantic Loss - Mean: 1.71922, Variance: 0.05040

Train Epoch: 61 
task: sign, mean loss: 0.11216, accuracy: 0.97826, avg. loss over tasks: 0.11216, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.07524, Variance: 0.01135
Semantic Loss - Mean: 0.12476, Variance: 0.00611

Test Epoch: 61 
task: sign, mean loss: 1.67415, accuracy: 0.63314, avg. loss over tasks: 1.67415
Diversity Loss - Mean: -0.09362, Variance: 0.01378
Semantic Loss - Mean: 1.60070, Variance: 0.04986

Train Epoch: 62 
task: sign, mean loss: 0.10520, accuracy: 0.96196, avg. loss over tasks: 0.10520, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.07602, Variance: 0.01137
Semantic Loss - Mean: 0.14889, Variance: 0.00619

Test Epoch: 62 
task: sign, mean loss: 1.48355, accuracy: 0.69822, avg. loss over tasks: 1.48355
Diversity Loss - Mean: -0.09042, Variance: 0.01382
Semantic Loss - Mean: 1.47041, Variance: 0.04968

Train Epoch: 63 
task: sign, mean loss: 0.08501, accuracy: 0.97283, avg. loss over tasks: 0.08501, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.08360, Variance: 0.01140
Semantic Loss - Mean: 0.09590, Variance: 0.00614

Test Epoch: 63 
task: sign, mean loss: 1.65273, accuracy: 0.64497, avg. loss over tasks: 1.65273
Diversity Loss - Mean: -0.06854, Variance: 0.01385
Semantic Loss - Mean: 1.69084, Variance: 0.05016

Train Epoch: 64 
task: sign, mean loss: 0.02000, accuracy: 0.99457, avg. loss over tasks: 0.02000, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.08333, Variance: 0.01142
Semantic Loss - Mean: 0.03609, Variance: 0.00606

Test Epoch: 64 
task: sign, mean loss: 2.19357, accuracy: 0.52071, avg. loss over tasks: 2.19357
Diversity Loss - Mean: -0.07236, Variance: 0.01387
Semantic Loss - Mean: 2.12882, Variance: 0.05038

Train Epoch: 65 
task: sign, mean loss: 0.02190, accuracy: 0.99457, avg. loss over tasks: 0.02190, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.08330, Variance: 0.01145
Semantic Loss - Mean: 0.04329, Variance: 0.00599

Test Epoch: 65 
task: sign, mean loss: 1.95830, accuracy: 0.57988, avg. loss over tasks: 1.95830
Diversity Loss - Mean: -0.08178, Variance: 0.01391
Semantic Loss - Mean: 1.79947, Variance: 0.05029

Train Epoch: 66 
task: sign, mean loss: 0.01329, accuracy: 1.00000, avg. loss over tasks: 0.01329, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.08319, Variance: 0.01147
Semantic Loss - Mean: 0.03367, Variance: 0.00592

Test Epoch: 66 
task: sign, mean loss: 1.78967, accuracy: 0.66272, avg. loss over tasks: 1.78967
Diversity Loss - Mean: -0.08522, Variance: 0.01394
Semantic Loss - Mean: 1.64105, Variance: 0.05008

Train Epoch: 67 
task: sign, mean loss: 0.00869, accuracy: 0.99457, avg. loss over tasks: 0.00869, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.08199, Variance: 0.01150
Semantic Loss - Mean: 0.01371, Variance: 0.00583

Test Epoch: 67 
task: sign, mean loss: 1.72429, accuracy: 0.67456, avg. loss over tasks: 1.72429
Diversity Loss - Mean: -0.08687, Variance: 0.01398
Semantic Loss - Mean: 1.63494, Variance: 0.05020

Train Epoch: 68 
task: sign, mean loss: 0.01493, accuracy: 1.00000, avg. loss over tasks: 0.01493, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.08134, Variance: 0.01152
Semantic Loss - Mean: 0.03557, Variance: 0.00577

Test Epoch: 68 
task: sign, mean loss: 1.67880, accuracy: 0.71006, avg. loss over tasks: 1.67880
Diversity Loss - Mean: -0.09352, Variance: 0.01403
Semantic Loss - Mean: 1.63169, Variance: 0.05015

Train Epoch: 69 
task: sign, mean loss: 0.01099, accuracy: 1.00000, avg. loss over tasks: 0.01099, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.08529, Variance: 0.01155
Semantic Loss - Mean: 0.02306, Variance: 0.00569

Test Epoch: 69 
task: sign, mean loss: 1.47200, accuracy: 0.72781, avg. loss over tasks: 1.47200
Diversity Loss - Mean: -0.09372, Variance: 0.01408
Semantic Loss - Mean: 1.41023, Variance: 0.04996

Train Epoch: 70 
task: sign, mean loss: 0.00754, accuracy: 1.00000, avg. loss over tasks: 0.00754, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.08526, Variance: 0.01157
Semantic Loss - Mean: 0.01678, Variance: 0.00561

Test Epoch: 70 
task: sign, mean loss: 1.68696, accuracy: 0.65089, avg. loss over tasks: 1.68696
Diversity Loss - Mean: -0.09019, Variance: 0.01412
Semantic Loss - Mean: 1.64253, Variance: 0.04975

Train Epoch: 71 
task: sign, mean loss: 0.01028, accuracy: 0.99457, avg. loss over tasks: 0.01028, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.08417, Variance: 0.01159
Semantic Loss - Mean: 0.02003, Variance: 0.00557

Test Epoch: 71 
task: sign, mean loss: 1.73712, accuracy: 0.64497, avg. loss over tasks: 1.73712
Diversity Loss - Mean: -0.09303, Variance: 0.01416
Semantic Loss - Mean: 1.67106, Variance: 0.04958

Train Epoch: 72 
task: sign, mean loss: 0.01629, accuracy: 0.98913, avg. loss over tasks: 0.01629, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.08558, Variance: 0.01161
Semantic Loss - Mean: 0.04407, Variance: 0.00554

Test Epoch: 72 
task: sign, mean loss: 2.66234, accuracy: 0.44970, avg. loss over tasks: 2.66234
Diversity Loss - Mean: -0.07200, Variance: 0.01418
Semantic Loss - Mean: 2.47386, Variance: 0.04997

Train Epoch: 73 
task: sign, mean loss: 0.10548, accuracy: 0.97283, avg. loss over tasks: 0.10548, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.08908, Variance: 0.01163
Semantic Loss - Mean: 0.12559, Variance: 0.00556

Test Epoch: 73 
task: sign, mean loss: 1.11373, accuracy: 0.72781, avg. loss over tasks: 1.11373
Diversity Loss - Mean: -0.09381, Variance: 0.01423
Semantic Loss - Mean: 1.02952, Variance: 0.04962

Train Epoch: 74 
task: sign, mean loss: 0.03176, accuracy: 0.99457, avg. loss over tasks: 0.03176, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.08960, Variance: 0.01165
Semantic Loss - Mean: 0.04403, Variance: 0.00551

Test Epoch: 74 
task: sign, mean loss: 1.75470, accuracy: 0.60947, avg. loss over tasks: 1.75470
Diversity Loss - Mean: -0.07713, Variance: 0.01424
Semantic Loss - Mean: 1.75220, Variance: 0.05053

Train Epoch: 75 
task: sign, mean loss: 0.04899, accuracy: 0.98370, avg. loss over tasks: 0.04899, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.08798, Variance: 0.01167
Semantic Loss - Mean: 0.07657, Variance: 0.00547

Test Epoch: 75 
task: sign, mean loss: 1.40233, accuracy: 0.66272, avg. loss over tasks: 1.40233
Diversity Loss - Mean: -0.08860, Variance: 0.01426
Semantic Loss - Mean: 1.44240, Variance: 0.05101

Train Epoch: 76 
task: sign, mean loss: 0.00704, accuracy: 1.00000, avg. loss over tasks: 0.00704, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.08908, Variance: 0.01168
Semantic Loss - Mean: 0.02379, Variance: 0.00541

Test Epoch: 76 
task: sign, mean loss: 1.27931, accuracy: 0.71598, avg. loss over tasks: 1.27931
Diversity Loss - Mean: -0.09393, Variance: 0.01429
Semantic Loss - Mean: 1.26113, Variance: 0.05093

Train Epoch: 77 
task: sign, mean loss: 0.00891, accuracy: 1.00000, avg. loss over tasks: 0.00891, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.09255, Variance: 0.01170
Semantic Loss - Mean: 0.03931, Variance: 0.00539

Test Epoch: 77 
task: sign, mean loss: 1.19955, accuracy: 0.72189, avg. loss over tasks: 1.19955
Diversity Loss - Mean: -0.09833, Variance: 0.01433
Semantic Loss - Mean: 1.16972, Variance: 0.05084

Train Epoch: 78 
task: sign, mean loss: 0.00634, accuracy: 1.00000, avg. loss over tasks: 0.00634, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.09082, Variance: 0.01171
Semantic Loss - Mean: 0.02487, Variance: 0.00534

Test Epoch: 78 
task: sign, mean loss: 1.44277, accuracy: 0.66864, avg. loss over tasks: 1.44277
Diversity Loss - Mean: -0.09542, Variance: 0.01436
Semantic Loss - Mean: 1.41758, Variance: 0.05131

Train Epoch: 79 
task: sign, mean loss: 0.00639, accuracy: 0.99457, avg. loss over tasks: 0.00639, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.09184, Variance: 0.01173
Semantic Loss - Mean: 0.02154, Variance: 0.00529

Test Epoch: 79 
task: sign, mean loss: 1.55781, accuracy: 0.66272, avg. loss over tasks: 1.55781
Diversity Loss - Mean: -0.09442, Variance: 0.01438
Semantic Loss - Mean: 1.43596, Variance: 0.05221

Train Epoch: 80 
task: sign, mean loss: 0.00445, accuracy: 1.00000, avg. loss over tasks: 0.00445, lr: 0.00015015
Diversity Loss - Mean: -0.09289, Variance: 0.01175
Semantic Loss - Mean: 0.01413, Variance: 0.00524

Test Epoch: 80 
task: sign, mean loss: 1.50869, accuracy: 0.68047, avg. loss over tasks: 1.50869
Diversity Loss - Mean: -0.09970, Variance: 0.01441
Semantic Loss - Mean: 1.35038, Variance: 0.05263

Train Epoch: 81 
task: sign, mean loss: 0.00349, accuracy: 1.00000, avg. loss over tasks: 0.00349, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.09448, Variance: 0.01176
Semantic Loss - Mean: 0.02229, Variance: 0.00524

Test Epoch: 81 
task: sign, mean loss: 1.46577, accuracy: 0.68639, avg. loss over tasks: 1.46577
Diversity Loss - Mean: -0.10066, Variance: 0.01444
Semantic Loss - Mean: 1.33451, Variance: 0.05263

Train Epoch: 82 
task: sign, mean loss: 0.00718, accuracy: 0.99457, avg. loss over tasks: 0.00718, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.09559, Variance: 0.01178
Semantic Loss - Mean: 0.01704, Variance: 0.00519

Test Epoch: 82 
task: sign, mean loss: 1.37906, accuracy: 0.71006, avg. loss over tasks: 1.37906
Diversity Loss - Mean: -0.10456, Variance: 0.01447
Semantic Loss - Mean: 1.33539, Variance: 0.05313

Train Epoch: 83 
task: sign, mean loss: 0.00788, accuracy: 1.00000, avg. loss over tasks: 0.00788, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.09770, Variance: 0.01180
Semantic Loss - Mean: 0.03596, Variance: 0.00517

Test Epoch: 83 
task: sign, mean loss: 1.63985, accuracy: 0.66864, avg. loss over tasks: 1.63985
Diversity Loss - Mean: -0.09806, Variance: 0.01449
Semantic Loss - Mean: 1.55721, Variance: 0.05349

Train Epoch: 84 
task: sign, mean loss: 0.00627, accuracy: 1.00000, avg. loss over tasks: 0.00627, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.09498, Variance: 0.01181
Semantic Loss - Mean: 0.01961, Variance: 0.00512

Test Epoch: 84 
task: sign, mean loss: 1.66049, accuracy: 0.66272, avg. loss over tasks: 1.66049
Diversity Loss - Mean: -0.10137, Variance: 0.01452
Semantic Loss - Mean: 1.58763, Variance: 0.05372

Train Epoch: 85 
task: sign, mean loss: 0.00410, accuracy: 1.00000, avg. loss over tasks: 0.00410, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.09668, Variance: 0.01182
Semantic Loss - Mean: 0.00697, Variance: 0.00506

Test Epoch: 85 
task: sign, mean loss: 1.66132, accuracy: 0.65680, avg. loss over tasks: 1.66132
Diversity Loss - Mean: -0.10332, Variance: 0.01454
Semantic Loss - Mean: 1.60394, Variance: 0.05384

Train Epoch: 86 
task: sign, mean loss: 0.01654, accuracy: 0.98913, avg. loss over tasks: 0.01654, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.09606, Variance: 0.01184
Semantic Loss - Mean: 0.02455, Variance: 0.00500

Test Epoch: 86 
task: sign, mean loss: 1.66302, accuracy: 0.64497, avg. loss over tasks: 1.66302
Diversity Loss - Mean: -0.09965, Variance: 0.01457
Semantic Loss - Mean: 1.66663, Variance: 0.05430

Train Epoch: 87 
task: sign, mean loss: 0.00277, accuracy: 1.00000, avg. loss over tasks: 0.00277, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.09735, Variance: 0.01185
Semantic Loss - Mean: 0.01816, Variance: 0.00495

Test Epoch: 87 
task: sign, mean loss: 1.68131, accuracy: 0.65680, avg. loss over tasks: 1.68131
Diversity Loss - Mean: -0.10073, Variance: 0.01459
Semantic Loss - Mean: 1.66410, Variance: 0.05454

Train Epoch: 88 
task: sign, mean loss: 0.00414, accuracy: 1.00000, avg. loss over tasks: 0.00414, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.10036, Variance: 0.01187
Semantic Loss - Mean: 0.01146, Variance: 0.00490

Test Epoch: 88 
task: sign, mean loss: 1.67789, accuracy: 0.68047, avg. loss over tasks: 1.67789
Diversity Loss - Mean: -0.10366, Variance: 0.01461
Semantic Loss - Mean: 1.58260, Variance: 0.05477

Train Epoch: 89 
task: sign, mean loss: 0.00934, accuracy: 0.99457, avg. loss over tasks: 0.00934, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.10005, Variance: 0.01189
Semantic Loss - Mean: 0.01408, Variance: 0.00486

Test Epoch: 89 
task: sign, mean loss: 1.87161, accuracy: 0.61538, avg. loss over tasks: 1.87161
Diversity Loss - Mean: -0.10142, Variance: 0.01463
Semantic Loss - Mean: 1.71775, Variance: 0.05506

Train Epoch: 90 
task: sign, mean loss: 0.01171, accuracy: 0.99457, avg. loss over tasks: 0.01171, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.09978, Variance: 0.01190
Semantic Loss - Mean: 0.02282, Variance: 0.00482

Test Epoch: 90 
task: sign, mean loss: 2.24965, accuracy: 0.53254, avg. loss over tasks: 2.24965
Diversity Loss - Mean: -0.09406, Variance: 0.01464
Semantic Loss - Mean: 2.06670, Variance: 0.05539

Train Epoch: 91 
task: sign, mean loss: 0.03915, accuracy: 0.98913, avg. loss over tasks: 0.03915, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.10230, Variance: 0.01192
Semantic Loss - Mean: 0.05932, Variance: 0.00483

Test Epoch: 91 
task: sign, mean loss: 2.05203, accuracy: 0.52071, avg. loss over tasks: 2.05203
Diversity Loss - Mean: -0.09628, Variance: 0.01466
Semantic Loss - Mean: 1.83412, Variance: 0.05562

Train Epoch: 92 
task: sign, mean loss: 0.07148, accuracy: 0.96196, avg. loss over tasks: 0.07148, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.10479, Variance: 0.01194
Semantic Loss - Mean: 0.10160, Variance: 0.00482

Test Epoch: 92 
task: sign, mean loss: 1.41428, accuracy: 0.68639, avg. loss over tasks: 1.41428
Diversity Loss - Mean: -0.10766, Variance: 0.01468
Semantic Loss - Mean: 1.25308, Variance: 0.05552

Train Epoch: 93 
task: sign, mean loss: 0.03051, accuracy: 0.99457, avg. loss over tasks: 0.03051, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.10452, Variance: 0.01196
Semantic Loss - Mean: 0.06171, Variance: 0.00497

Test Epoch: 93 
task: sign, mean loss: 1.38574, accuracy: 0.75740, avg. loss over tasks: 1.38574
Diversity Loss - Mean: -0.11546, Variance: 0.01473
Semantic Loss - Mean: 1.21988, Variance: 0.05530

Train Epoch: 94 
task: sign, mean loss: 0.00559, accuracy: 1.00000, avg. loss over tasks: 0.00559, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.10270, Variance: 0.01197
Semantic Loss - Mean: 0.02683, Variance: 0.00495

Test Epoch: 94 
task: sign, mean loss: 1.42769, accuracy: 0.75740, avg. loss over tasks: 1.42769
Diversity Loss - Mean: -0.11162, Variance: 0.01477
Semantic Loss - Mean: 1.25448, Variance: 0.05516

Train Epoch: 95 
task: sign, mean loss: 0.00395, accuracy: 1.00000, avg. loss over tasks: 0.00395, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.10310, Variance: 0.01199
Semantic Loss - Mean: 0.01368, Variance: 0.00491

Test Epoch: 95 
task: sign, mean loss: 1.54105, accuracy: 0.75740, avg. loss over tasks: 1.54105
Diversity Loss - Mean: -0.10778, Variance: 0.01480
Semantic Loss - Mean: 1.42159, Variance: 0.05515

Train Epoch: 96 
task: sign, mean loss: 0.00412, accuracy: 1.00000, avg. loss over tasks: 0.00412, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.10180, Variance: 0.01200
Semantic Loss - Mean: 0.01974, Variance: 0.00487

Test Epoch: 96 
task: sign, mean loss: 1.38817, accuracy: 0.76331, avg. loss over tasks: 1.38817
Diversity Loss - Mean: -0.10902, Variance: 0.01483
Semantic Loss - Mean: 1.29995, Variance: 0.05479

Train Epoch: 97 
task: sign, mean loss: 0.00695, accuracy: 1.00000, avg. loss over tasks: 0.00695, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.10292, Variance: 0.01202
Semantic Loss - Mean: 0.01842, Variance: 0.00483

Test Epoch: 97 
task: sign, mean loss: 1.51921, accuracy: 0.72781, avg. loss over tasks: 1.51921
Diversity Loss - Mean: -0.10905, Variance: 0.01487
Semantic Loss - Mean: 1.45424, Variance: 0.05448

Train Epoch: 98 
task: sign, mean loss: 0.00250, accuracy: 1.00000, avg. loss over tasks: 0.00250, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.10348, Variance: 0.01203
Semantic Loss - Mean: 0.00898, Variance: 0.00479

Test Epoch: 98 
task: sign, mean loss: 1.58662, accuracy: 0.71006, avg. loss over tasks: 1.58662
Diversity Loss - Mean: -0.10734, Variance: 0.01489
Semantic Loss - Mean: 1.49932, Variance: 0.05427

Train Epoch: 99 
task: sign, mean loss: 0.00771, accuracy: 0.99457, avg. loss over tasks: 0.00771, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.10438, Variance: 0.01204
Semantic Loss - Mean: 0.01095, Variance: 0.00474

Test Epoch: 99 
task: sign, mean loss: 1.59594, accuracy: 0.69822, avg. loss over tasks: 1.59594
Diversity Loss - Mean: -0.10693, Variance: 0.01492
Semantic Loss - Mean: 1.46776, Variance: 0.05401

Train Epoch: 100 
task: sign, mean loss: 0.00808, accuracy: 1.00000, avg. loss over tasks: 0.00808, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.10425, Variance: 0.01205
Semantic Loss - Mean: 0.01923, Variance: 0.00471

Test Epoch: 100 
task: sign, mean loss: 1.39059, accuracy: 0.71598, avg. loss over tasks: 1.39059
Diversity Loss - Mean: -0.11143, Variance: 0.01495
Semantic Loss - Mean: 1.26218, Variance: 0.05382

Train Epoch: 101 
task: sign, mean loss: 0.00189, accuracy: 1.00000, avg. loss over tasks: 0.00189, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.10510, Variance: 0.01207
Semantic Loss - Mean: 0.01138, Variance: 0.00467

Test Epoch: 101 
task: sign, mean loss: 1.59840, accuracy: 0.69822, avg. loss over tasks: 1.59840
Diversity Loss - Mean: -0.11254, Variance: 0.01498
Semantic Loss - Mean: 1.43847, Variance: 0.05406

Train Epoch: 102 
task: sign, mean loss: 0.00057, accuracy: 1.00000, avg. loss over tasks: 0.00057, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.10667, Variance: 0.01208
Semantic Loss - Mean: 0.01002, Variance: 0.00463

Test Epoch: 102 
task: sign, mean loss: 1.65734, accuracy: 0.70414, avg. loss over tasks: 1.65734
Diversity Loss - Mean: -0.11259, Variance: 0.01500
Semantic Loss - Mean: 1.47818, Variance: 0.05441

Train Epoch: 103 
task: sign, mean loss: 0.00314, accuracy: 1.00000, avg. loss over tasks: 0.00314, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.10480, Variance: 0.01209
Semantic Loss - Mean: 0.01168, Variance: 0.00459

Test Epoch: 103 
task: sign, mean loss: 1.64163, accuracy: 0.67456, avg. loss over tasks: 1.64163
Diversity Loss - Mean: -0.11183, Variance: 0.01502
Semantic Loss - Mean: 1.47956, Variance: 0.05472

Train Epoch: 104 
task: sign, mean loss: 0.00380, accuracy: 1.00000, avg. loss over tasks: 0.00380, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.10668, Variance: 0.01211
Semantic Loss - Mean: 0.01414, Variance: 0.00455

Test Epoch: 104 
task: sign, mean loss: 1.64175, accuracy: 0.69822, avg. loss over tasks: 1.64175
Diversity Loss - Mean: -0.11317, Variance: 0.01504
Semantic Loss - Mean: 1.45872, Variance: 0.05485

Train Epoch: 105 
task: sign, mean loss: 0.00080, accuracy: 1.00000, avg. loss over tasks: 0.00080, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.10672, Variance: 0.01212
Semantic Loss - Mean: 0.00640, Variance: 0.00451

Test Epoch: 105 
task: sign, mean loss: 1.59818, accuracy: 0.68047, avg. loss over tasks: 1.59818
Diversity Loss - Mean: -0.11267, Variance: 0.01507
Semantic Loss - Mean: 1.45365, Variance: 0.05500

Train Epoch: 106 
task: sign, mean loss: 0.00249, accuracy: 1.00000, avg. loss over tasks: 0.00249, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.10766, Variance: 0.01213
Semantic Loss - Mean: 0.00765, Variance: 0.00447

Test Epoch: 106 
task: sign, mean loss: 1.46559, accuracy: 0.68047, avg. loss over tasks: 1.46559
Diversity Loss - Mean: -0.11167, Variance: 0.01509
Semantic Loss - Mean: 1.36152, Variance: 0.05504

Train Epoch: 107 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.10734, Variance: 0.01215
Semantic Loss - Mean: 0.00536, Variance: 0.00443

Test Epoch: 107 
task: sign, mean loss: 1.55498, accuracy: 0.69231, avg. loss over tasks: 1.55498
Diversity Loss - Mean: -0.11324, Variance: 0.01511
Semantic Loss - Mean: 1.45342, Variance: 0.05504

Train Epoch: 108 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.10771, Variance: 0.01216
Semantic Loss - Mean: 0.00678, Variance: 0.00440

Test Epoch: 108 
task: sign, mean loss: 1.54053, accuracy: 0.70414, avg. loss over tasks: 1.54053
Diversity Loss - Mean: -0.11439, Variance: 0.01514
Semantic Loss - Mean: 1.43436, Variance: 0.05496

Train Epoch: 109 
task: sign, mean loss: 0.00372, accuracy: 1.00000, avg. loss over tasks: 0.00372, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.10869, Variance: 0.01217
Semantic Loss - Mean: 0.02342, Variance: 0.00437

Test Epoch: 109 
task: sign, mean loss: 1.74734, accuracy: 0.66272, avg. loss over tasks: 1.74734
Diversity Loss - Mean: -0.11680, Variance: 0.01516
Semantic Loss - Mean: 1.57444, Variance: 0.05469

Train Epoch: 110 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.10923, Variance: 0.01218
Semantic Loss - Mean: 0.00501, Variance: 0.00433

Test Epoch: 110 
task: sign, mean loss: 1.73037, accuracy: 0.66864, avg. loss over tasks: 1.73037
Diversity Loss - Mean: -0.11610, Variance: 0.01519
Semantic Loss - Mean: 1.57909, Variance: 0.05444

Train Epoch: 111 
task: sign, mean loss: 0.00114, accuracy: 1.00000, avg. loss over tasks: 0.00114, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.10958, Variance: 0.01220
Semantic Loss - Mean: 0.00946, Variance: 0.00431

Test Epoch: 111 
task: sign, mean loss: 1.70046, accuracy: 0.67456, avg. loss over tasks: 1.70046
Diversity Loss - Mean: -0.11724, Variance: 0.01521
Semantic Loss - Mean: 1.54101, Variance: 0.05419

Train Epoch: 112 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.10874, Variance: 0.01221
Semantic Loss - Mean: 0.00522, Variance: 0.00427

Test Epoch: 112 
task: sign, mean loss: 1.62500, accuracy: 0.68047, avg. loss over tasks: 1.62500
Diversity Loss - Mean: -0.11702, Variance: 0.01523
Semantic Loss - Mean: 1.49533, Variance: 0.05393

Train Epoch: 113 
task: sign, mean loss: 0.00215, accuracy: 1.00000, avg. loss over tasks: 0.00215, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.11036, Variance: 0.01222
Semantic Loss - Mean: 0.00891, Variance: 0.00424

Test Epoch: 113 
task: sign, mean loss: 1.61286, accuracy: 0.67456, avg. loss over tasks: 1.61286
Diversity Loss - Mean: -0.11718, Variance: 0.01526
Semantic Loss - Mean: 1.48860, Variance: 0.05374

Train Epoch: 114 
task: sign, mean loss: 0.02121, accuracy: 0.98913, avg. loss over tasks: 0.02121, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.11003, Variance: 0.01223
Semantic Loss - Mean: 0.03120, Variance: 0.00422

Test Epoch: 114 
task: sign, mean loss: 1.57265, accuracy: 0.62722, avg. loss over tasks: 1.57265
Diversity Loss - Mean: -0.11242, Variance: 0.01528
Semantic Loss - Mean: 1.52291, Variance: 0.05381

Train Epoch: 115 
task: sign, mean loss: 0.00308, accuracy: 1.00000, avg. loss over tasks: 0.00308, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.10913, Variance: 0.01224
Semantic Loss - Mean: 0.01305, Variance: 0.00419

Test Epoch: 115 
task: sign, mean loss: 1.77309, accuracy: 0.59172, avg. loss over tasks: 1.77309
Diversity Loss - Mean: -0.11201, Variance: 0.01530
Semantic Loss - Mean: 1.73487, Variance: 0.05406

Train Epoch: 116 
task: sign, mean loss: 0.00868, accuracy: 0.99457, avg. loss over tasks: 0.00868, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.10970, Variance: 0.01225
Semantic Loss - Mean: 0.02557, Variance: 0.00417

Test Epoch: 116 
task: sign, mean loss: 1.62148, accuracy: 0.62130, avg. loss over tasks: 1.62148
Diversity Loss - Mean: -0.11546, Variance: 0.01532
Semantic Loss - Mean: 1.53201, Variance: 0.05394

Train Epoch: 117 
task: sign, mean loss: 0.00155, accuracy: 1.00000, avg. loss over tasks: 0.00155, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.11061, Variance: 0.01226
Semantic Loss - Mean: 0.01725, Variance: 0.00415

Test Epoch: 117 
task: sign, mean loss: 1.59328, accuracy: 0.65089, avg. loss over tasks: 1.59328
Diversity Loss - Mean: -0.11656, Variance: 0.01535
Semantic Loss - Mean: 1.47225, Variance: 0.05373

Train Epoch: 118 
task: sign, mean loss: 0.01062, accuracy: 0.98913, avg. loss over tasks: 0.01062, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.11082, Variance: 0.01227
Semantic Loss - Mean: 0.01728, Variance: 0.00413

Test Epoch: 118 
task: sign, mean loss: 1.51522, accuracy: 0.66864, avg. loss over tasks: 1.51522
Diversity Loss - Mean: -0.11768, Variance: 0.01537
Semantic Loss - Mean: 1.39485, Variance: 0.05344

Train Epoch: 119 
task: sign, mean loss: 0.00094, accuracy: 1.00000, avg. loss over tasks: 0.00094, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.11101, Variance: 0.01228
Semantic Loss - Mean: 0.00718, Variance: 0.00410

Test Epoch: 119 
task: sign, mean loss: 1.40080, accuracy: 0.68639, avg. loss over tasks: 1.40080
Diversity Loss - Mean: -0.11859, Variance: 0.01539
Semantic Loss - Mean: 1.30090, Variance: 0.05312

Train Epoch: 120 
task: sign, mean loss: 0.00709, accuracy: 0.99457, avg. loss over tasks: 0.00709, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.11113, Variance: 0.01229
Semantic Loss - Mean: 0.01370, Variance: 0.00407

Test Epoch: 120 
task: sign, mean loss: 1.54314, accuracy: 0.67456, avg. loss over tasks: 1.54314
Diversity Loss - Mean: -0.11872, Variance: 0.01542
Semantic Loss - Mean: 1.44817, Variance: 0.05285

Train Epoch: 121 
task: sign, mean loss: 0.00104, accuracy: 1.00000, avg. loss over tasks: 0.00104, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.11314, Variance: 0.01231
Semantic Loss - Mean: 0.00811, Variance: 0.00404

Test Epoch: 121 
task: sign, mean loss: 1.57039, accuracy: 0.65089, avg. loss over tasks: 1.57039
Diversity Loss - Mean: -0.11679, Variance: 0.01544
Semantic Loss - Mean: 1.47708, Variance: 0.05263

Train Epoch: 122 
task: sign, mean loss: 0.00118, accuracy: 1.00000, avg. loss over tasks: 0.00118, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.11279, Variance: 0.01232
Semantic Loss - Mean: 0.01136, Variance: 0.00402

Test Epoch: 122 
task: sign, mean loss: 1.70791, accuracy: 0.62130, avg. loss over tasks: 1.70791
Diversity Loss - Mean: -0.11589, Variance: 0.01545
Semantic Loss - Mean: 1.60902, Variance: 0.05246

Train Epoch: 123 
task: sign, mean loss: 0.00068, accuracy: 1.00000, avg. loss over tasks: 0.00068, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.11392, Variance: 0.01233
Semantic Loss - Mean: 0.01014, Variance: 0.00400

Test Epoch: 123 
task: sign, mean loss: 1.70712, accuracy: 0.63905, avg. loss over tasks: 1.70712
Diversity Loss - Mean: -0.11604, Variance: 0.01547
Semantic Loss - Mean: 1.64534, Variance: 0.05232

Train Epoch: 124 
task: sign, mean loss: 0.00161, accuracy: 1.00000, avg. loss over tasks: 0.00161, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.11315, Variance: 0.01235
Semantic Loss - Mean: 0.01280, Variance: 0.00397

Test Epoch: 124 
task: sign, mean loss: 1.59442, accuracy: 0.63905, avg. loss over tasks: 1.59442
Diversity Loss - Mean: -0.11563, Variance: 0.01549
Semantic Loss - Mean: 1.52792, Variance: 0.05213

Train Epoch: 125 
task: sign, mean loss: 0.00055, accuracy: 1.00000, avg. loss over tasks: 0.00055, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.11316, Variance: 0.01236
Semantic Loss - Mean: 0.00450, Variance: 0.00394

Test Epoch: 125 
task: sign, mean loss: 1.64095, accuracy: 0.63314, avg. loss over tasks: 1.64095
Diversity Loss - Mean: -0.11576, Variance: 0.01551
Semantic Loss - Mean: 1.58261, Variance: 0.05199

Train Epoch: 126 
task: sign, mean loss: 0.00406, accuracy: 1.00000, avg. loss over tasks: 0.00406, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.11245, Variance: 0.01237
Semantic Loss - Mean: 0.01195, Variance: 0.00391

Test Epoch: 126 
task: sign, mean loss: 1.57302, accuracy: 0.65680, avg. loss over tasks: 1.57302
Diversity Loss - Mean: -0.11738, Variance: 0.01553
Semantic Loss - Mean: 1.50885, Variance: 0.05183

Train Epoch: 127 
task: sign, mean loss: 0.00223, accuracy: 1.00000, avg. loss over tasks: 0.00223, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.11268, Variance: 0.01238
Semantic Loss - Mean: 0.00845, Variance: 0.00389

Test Epoch: 127 
task: sign, mean loss: 1.52599, accuracy: 0.66864, avg. loss over tasks: 1.52599
Diversity Loss - Mean: -0.11711, Variance: 0.01555
Semantic Loss - Mean: 1.47678, Variance: 0.05167

Train Epoch: 128 
task: sign, mean loss: 0.00226, accuracy: 1.00000, avg. loss over tasks: 0.00226, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.11273, Variance: 0.01239
Semantic Loss - Mean: 0.00738, Variance: 0.00386

Test Epoch: 128 
task: sign, mean loss: 1.69143, accuracy: 0.62130, avg. loss over tasks: 1.69143
Diversity Loss - Mean: -0.11454, Variance: 0.01557
Semantic Loss - Mean: 1.63705, Variance: 0.05150

Train Epoch: 129 
task: sign, mean loss: 0.01915, accuracy: 0.99457, avg. loss over tasks: 0.01915, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.11260, Variance: 0.01240
Semantic Loss - Mean: 0.01506, Variance: 0.00383

Test Epoch: 129 
task: sign, mean loss: 1.59124, accuracy: 0.66272, avg. loss over tasks: 1.59124
Diversity Loss - Mean: -0.11819, Variance: 0.01558
Semantic Loss - Mean: 1.50734, Variance: 0.05131

Train Epoch: 130 
task: sign, mean loss: 0.00170, accuracy: 1.00000, avg. loss over tasks: 0.00170, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.11272, Variance: 0.01241
Semantic Loss - Mean: 0.00954, Variance: 0.00380

Test Epoch: 130 
task: sign, mean loss: 1.60197, accuracy: 0.66864, avg. loss over tasks: 1.60197
Diversity Loss - Mean: -0.11804, Variance: 0.01560
Semantic Loss - Mean: 1.48916, Variance: 0.05110

Train Epoch: 131 
task: sign, mean loss: 0.00266, accuracy: 1.00000, avg. loss over tasks: 0.00266, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.11354, Variance: 0.01242
Semantic Loss - Mean: 0.00790, Variance: 0.00377

Test Epoch: 131 
task: sign, mean loss: 1.79018, accuracy: 0.62130, avg. loss over tasks: 1.79018
Diversity Loss - Mean: -0.11495, Variance: 0.01562
Semantic Loss - Mean: 1.64418, Variance: 0.05090

Train Epoch: 132 
task: sign, mean loss: 0.00078, accuracy: 1.00000, avg. loss over tasks: 0.00078, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.11395, Variance: 0.01243
Semantic Loss - Mean: 0.00445, Variance: 0.00375

Test Epoch: 132 
task: sign, mean loss: 1.69439, accuracy: 0.65680, avg. loss over tasks: 1.69439
Diversity Loss - Mean: -0.11737, Variance: 0.01564
Semantic Loss - Mean: 1.56506, Variance: 0.05071

Train Epoch: 133 
task: sign, mean loss: 0.00196, accuracy: 1.00000, avg. loss over tasks: 0.00196, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.11356, Variance: 0.01244
Semantic Loss - Mean: 0.00712, Variance: 0.00372

Test Epoch: 133 
task: sign, mean loss: 1.75111, accuracy: 0.60947, avg. loss over tasks: 1.75111
Diversity Loss - Mean: -0.11612, Variance: 0.01565
Semantic Loss - Mean: 1.62222, Variance: 0.05054

Train Epoch: 134 
task: sign, mean loss: 0.00890, accuracy: 0.99457, avg. loss over tasks: 0.00890, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.11348, Variance: 0.01245
Semantic Loss - Mean: 0.01106, Variance: 0.00370

Test Epoch: 134 
task: sign, mean loss: 1.68522, accuracy: 0.64497, avg. loss over tasks: 1.68522
Diversity Loss - Mean: -0.11789, Variance: 0.01567
Semantic Loss - Mean: 1.57475, Variance: 0.05038

Train Epoch: 135 
task: sign, mean loss: 0.00295, accuracy: 1.00000, avg. loss over tasks: 0.00295, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.11363, Variance: 0.01246
Semantic Loss - Mean: 0.00626, Variance: 0.00367

Test Epoch: 135 
task: sign, mean loss: 1.71201, accuracy: 0.66272, avg. loss over tasks: 1.71201
Diversity Loss - Mean: -0.11876, Variance: 0.01568
Semantic Loss - Mean: 1.60269, Variance: 0.05023

Train Epoch: 136 
task: sign, mean loss: 0.00069, accuracy: 1.00000, avg. loss over tasks: 0.00069, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.11441, Variance: 0.01247
Semantic Loss - Mean: 0.00510, Variance: 0.00365

Test Epoch: 136 
task: sign, mean loss: 1.71605, accuracy: 0.66272, avg. loss over tasks: 1.71605
Diversity Loss - Mean: -0.11816, Variance: 0.01570
Semantic Loss - Mean: 1.61284, Variance: 0.05008

Train Epoch: 137 
task: sign, mean loss: 0.00184, accuracy: 1.00000, avg. loss over tasks: 0.00184, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.11459, Variance: 0.01248
Semantic Loss - Mean: 0.00785, Variance: 0.00362

Test Epoch: 137 
task: sign, mean loss: 1.61881, accuracy: 0.66864, avg. loss over tasks: 1.61881
Diversity Loss - Mean: -0.11782, Variance: 0.01572
Semantic Loss - Mean: 1.51314, Variance: 0.04990

Train Epoch: 138 
task: sign, mean loss: 0.00217, accuracy: 1.00000, avg. loss over tasks: 0.00217, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.11456, Variance: 0.01250
Semantic Loss - Mean: 0.00997, Variance: 0.00360

Test Epoch: 138 
task: sign, mean loss: 1.62766, accuracy: 0.66272, avg. loss over tasks: 1.62766
Diversity Loss - Mean: -0.11808, Variance: 0.01573
Semantic Loss - Mean: 1.52539, Variance: 0.04973

Train Epoch: 139 
task: sign, mean loss: 0.00120, accuracy: 1.00000, avg. loss over tasks: 0.00120, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.11449, Variance: 0.01251
Semantic Loss - Mean: 0.00808, Variance: 0.00357

Test Epoch: 139 
task: sign, mean loss: 1.70059, accuracy: 0.63905, avg. loss over tasks: 1.70059
Diversity Loss - Mean: -0.11695, Variance: 0.01575
Semantic Loss - Mean: 1.58615, Variance: 0.04956

Train Epoch: 140 
task: sign, mean loss: 0.01488, accuracy: 0.98913, avg. loss over tasks: 0.01488, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.11360, Variance: 0.01252
Semantic Loss - Mean: 0.01863, Variance: 0.00356

Test Epoch: 140 
task: sign, mean loss: 1.60489, accuracy: 0.67456, avg. loss over tasks: 1.60489
Diversity Loss - Mean: -0.11951, Variance: 0.01576
Semantic Loss - Mean: 1.49555, Variance: 0.04938

Train Epoch: 141 
task: sign, mean loss: 0.00409, accuracy: 1.00000, avg. loss over tasks: 0.00409, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.11402, Variance: 0.01253
Semantic Loss - Mean: 0.01290, Variance: 0.00354

Test Epoch: 141 
task: sign, mean loss: 1.57061, accuracy: 0.66864, avg. loss over tasks: 1.57061
Diversity Loss - Mean: -0.11973, Variance: 0.01578
Semantic Loss - Mean: 1.46969, Variance: 0.04920

Train Epoch: 142 
task: sign, mean loss: 0.00464, accuracy: 1.00000, avg. loss over tasks: 0.00464, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.11478, Variance: 0.01254
Semantic Loss - Mean: 0.03014, Variance: 0.00355

Test Epoch: 142 
task: sign, mean loss: 1.51600, accuracy: 0.69231, avg. loss over tasks: 1.51600
Diversity Loss - Mean: -0.12053, Variance: 0.01580
Semantic Loss - Mean: 1.42281, Variance: 0.04902

Train Epoch: 143 
task: sign, mean loss: 0.00794, accuracy: 1.00000, avg. loss over tasks: 0.00794, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.11358, Variance: 0.01255
Semantic Loss - Mean: 0.01852, Variance: 0.00354

Test Epoch: 143 
task: sign, mean loss: 1.59551, accuracy: 0.67456, avg. loss over tasks: 1.59551
Diversity Loss - Mean: -0.11921, Variance: 0.01581
Semantic Loss - Mean: 1.50963, Variance: 0.04886

Train Epoch: 144 
task: sign, mean loss: 0.00187, accuracy: 1.00000, avg. loss over tasks: 0.00187, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.11326, Variance: 0.01256
Semantic Loss - Mean: 0.01177, Variance: 0.00352

Test Epoch: 144 
task: sign, mean loss: 1.57289, accuracy: 0.66272, avg. loss over tasks: 1.57289
Diversity Loss - Mean: -0.11856, Variance: 0.01583
Semantic Loss - Mean: 1.49135, Variance: 0.04869

Train Epoch: 145 
task: sign, mean loss: 0.00053, accuracy: 1.00000, avg. loss over tasks: 0.00053, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.11379, Variance: 0.01257
Semantic Loss - Mean: 0.00637, Variance: 0.00350

Test Epoch: 145 
task: sign, mean loss: 1.67072, accuracy: 0.66272, avg. loss over tasks: 1.67072
Diversity Loss - Mean: -0.11893, Variance: 0.01584
Semantic Loss - Mean: 1.57357, Variance: 0.04853

Train Epoch: 146 
task: sign, mean loss: 0.03448, accuracy: 0.99457, avg. loss over tasks: 0.03448, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.11481, Variance: 0.01258
Semantic Loss - Mean: 0.04429, Variance: 0.00349

Test Epoch: 146 
task: sign, mean loss: 1.80501, accuracy: 0.59172, avg. loss over tasks: 1.80501
Diversity Loss - Mean: -0.11578, Variance: 0.01586
Semantic Loss - Mean: 1.68307, Variance: 0.04838

Train Epoch: 147 
task: sign, mean loss: 0.00298, accuracy: 1.00000, avg. loss over tasks: 0.00298, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.11332, Variance: 0.01259
Semantic Loss - Mean: 0.00745, Variance: 0.00346

Test Epoch: 147 
task: sign, mean loss: 1.79216, accuracy: 0.60947, avg. loss over tasks: 1.79216
Diversity Loss - Mean: -0.11700, Variance: 0.01587
Semantic Loss - Mean: 1.69013, Variance: 0.04823

Train Epoch: 148 
task: sign, mean loss: 0.00086, accuracy: 1.00000, avg. loss over tasks: 0.00086, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.11295, Variance: 0.01260
Semantic Loss - Mean: 0.00626, Variance: 0.00344

Test Epoch: 148 
task: sign, mean loss: 1.72561, accuracy: 0.66864, avg. loss over tasks: 1.72561
Diversity Loss - Mean: -0.11732, Variance: 0.01588
Semantic Loss - Mean: 1.62162, Variance: 0.04806

Train Epoch: 149 
task: sign, mean loss: 0.00120, accuracy: 1.00000, avg. loss over tasks: 0.00120, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.11428, Variance: 0.01260
Semantic Loss - Mean: 0.00777, Variance: 0.00342

Test Epoch: 149 
task: sign, mean loss: 1.63259, accuracy: 0.65089, avg. loss over tasks: 1.63259
Diversity Loss - Mean: -0.11841, Variance: 0.01589
Semantic Loss - Mean: 1.53171, Variance: 0.04790

Train Epoch: 150 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 3e-07
Diversity Loss - Mean: -0.11312, Variance: 0.01261
Semantic Loss - Mean: 0.00459, Variance: 0.00340

Test Epoch: 150 
task: sign, mean loss: 1.69342, accuracy: 0.66272, avg. loss over tasks: 1.69342
Diversity Loss - Mean: -0.11870, Variance: 0.01591
Semantic Loss - Mean: 1.57238, Variance: 0.04775

