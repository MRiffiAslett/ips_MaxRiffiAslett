Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09280, accuracy: 0.63587, avg. loss over tasks: 1.09280, lr: 3e-05
Diversity Loss - Mean: -0.00987, Variance: 0.01053
Semantic Loss - Mean: 1.43125, Variance: 0.07277

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17686, accuracy: 0.66272, avg. loss over tasks: 1.17686
Diversity Loss - Mean: -0.02882, Variance: 0.01240
Semantic Loss - Mean: 1.16119, Variance: 0.05365

Train Epoch: 2 
task: sign, mean loss: 0.97238, accuracy: 0.66304, avg. loss over tasks: 0.97238, lr: 6e-05
Diversity Loss - Mean: -0.01575, Variance: 0.01045
Semantic Loss - Mean: 0.98263, Variance: 0.03941

Test Epoch: 2 
task: sign, mean loss: 1.10591, accuracy: 0.66272, avg. loss over tasks: 1.10591
Diversity Loss - Mean: -0.02544, Variance: 0.01194
Semantic Loss - Mean: 1.14979, Variance: 0.03231

Train Epoch: 3 
task: sign, mean loss: 0.79416, accuracy: 0.70109, avg. loss over tasks: 0.79416, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.02759, Variance: 0.01021
Semantic Loss - Mean: 0.99288, Variance: 0.02720

Test Epoch: 3 
task: sign, mean loss: 1.27480, accuracy: 0.63314, avg. loss over tasks: 1.27480
Diversity Loss - Mean: -0.04790, Variance: 0.01137
Semantic Loss - Mean: 1.11270, Variance: 0.02904

Train Epoch: 4 
task: sign, mean loss: 0.77726, accuracy: 0.67935, avg. loss over tasks: 0.77726, lr: 0.00012
Diversity Loss - Mean: -0.04706, Variance: 0.00995
Semantic Loss - Mean: 0.89335, Variance: 0.02108

Test Epoch: 4 
task: sign, mean loss: 1.56959, accuracy: 0.44970, avg. loss over tasks: 1.56959
Diversity Loss - Mean: -0.04416, Variance: 0.01070
Semantic Loss - Mean: 1.08965, Variance: 0.02371

Train Epoch: 5 
task: sign, mean loss: 0.72571, accuracy: 0.72283, avg. loss over tasks: 0.72571, lr: 0.00015
Diversity Loss - Mean: -0.03766, Variance: 0.00962
Semantic Loss - Mean: 0.78143, Variance: 0.01726

Test Epoch: 5 
task: sign, mean loss: 2.33982, accuracy: 0.31953, avg. loss over tasks: 2.33982
Diversity Loss - Mean: -0.03620, Variance: 0.01012
Semantic Loss - Mean: 1.37918, Variance: 0.02216

Train Epoch: 6 
task: sign, mean loss: 0.74903, accuracy: 0.73370, avg. loss over tasks: 0.74903, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.03578, Variance: 0.00942
Semantic Loss - Mean: 0.76340, Variance: 0.01483

Test Epoch: 6 
task: sign, mean loss: 1.99990, accuracy: 0.65680, avg. loss over tasks: 1.99990
Diversity Loss - Mean: -0.00128, Variance: 0.01058
Semantic Loss - Mean: 1.50791, Variance: 0.02216

Train Epoch: 7 
task: sign, mean loss: 0.62965, accuracy: 0.75000, avg. loss over tasks: 0.62965, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.04397, Variance: 0.00941
Semantic Loss - Mean: 0.64077, Variance: 0.01299

Test Epoch: 7 
task: sign, mean loss: 1.72977, accuracy: 0.53846, avg. loss over tasks: 1.72977
Diversity Loss - Mean: -0.03960, Variance: 0.01054
Semantic Loss - Mean: 1.44419, Variance: 0.02263

Train Epoch: 8 
task: sign, mean loss: 0.59962, accuracy: 0.76087, avg. loss over tasks: 0.59962, lr: 0.00024
Diversity Loss - Mean: -0.03068, Variance: 0.00923
Semantic Loss - Mean: 0.62694, Variance: 0.01184

Test Epoch: 8 
task: sign, mean loss: 2.27508, accuracy: 0.49112, avg. loss over tasks: 2.27508
Diversity Loss - Mean: 0.01934, Variance: 0.01040
Semantic Loss - Mean: 1.67625, Variance: 0.02396

Train Epoch: 9 
task: sign, mean loss: 0.56600, accuracy: 0.79891, avg. loss over tasks: 0.56600, lr: 0.00027
Diversity Loss - Mean: -0.02699, Variance: 0.00909
Semantic Loss - Mean: 0.56014, Variance: 0.01094

Test Epoch: 9 
task: sign, mean loss: 2.25610, accuracy: 0.36095, avg. loss over tasks: 2.25610
Diversity Loss - Mean: 0.01888, Variance: 0.01033
Semantic Loss - Mean: 1.90311, Variance: 0.02797

Train Epoch: 10 
task: sign, mean loss: 0.75728, accuracy: 0.70652, avg. loss over tasks: 0.75728, lr: 0.0003
Diversity Loss - Mean: -0.01837, Variance: 0.00897
Semantic Loss - Mean: 0.65649, Variance: 0.01024

Test Epoch: 10 
task: sign, mean loss: 3.26038, accuracy: 0.31361, avg. loss over tasks: 3.26038
Diversity Loss - Mean: 0.03198, Variance: 0.01052
Semantic Loss - Mean: 2.28115, Variance: 0.03097

Train Epoch: 11 
task: sign, mean loss: 0.72403, accuracy: 0.75543, avg. loss over tasks: 0.72403, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.00536, Variance: 0.00884
Semantic Loss - Mean: 0.68252, Variance: 0.01008

Test Epoch: 11 
task: sign, mean loss: 2.15454, accuracy: 0.49704, avg. loss over tasks: 2.15454
Diversity Loss - Mean: 0.01251, Variance: 0.01056
Semantic Loss - Mean: 1.70593, Variance: 0.03167

Train Epoch: 12 
task: sign, mean loss: 0.70274, accuracy: 0.73913, avg. loss over tasks: 0.70274, lr: 0.000299849111021216
Diversity Loss - Mean: -0.00299, Variance: 0.00899
Semantic Loss - Mean: 0.70183, Variance: 0.00957

Test Epoch: 12 
task: sign, mean loss: 2.02658, accuracy: 0.37870, avg. loss over tasks: 2.02658
Diversity Loss - Mean: 0.01318, Variance: 0.01078
Semantic Loss - Mean: 1.90351, Variance: 0.03125

Train Epoch: 13 
task: sign, mean loss: 0.84288, accuracy: 0.68478, avg. loss over tasks: 0.84288, lr: 0.0002996605710257114
Diversity Loss - Mean: -0.03905, Variance: 0.00925
Semantic Loss - Mean: 0.85237, Variance: 0.00906

Test Epoch: 13 
task: sign, mean loss: 0.96223, accuracy: 0.56805, avg. loss over tasks: 0.96223
Diversity Loss - Mean: -0.00010, Variance: 0.01130
Semantic Loss - Mean: 0.98331, Variance: 0.03011

Train Epoch: 14 
task: sign, mean loss: 0.54300, accuracy: 0.80978, avg. loss over tasks: 0.54300, lr: 0.00029939674795518656
Diversity Loss - Mean: -0.03353, Variance: 0.00945
Semantic Loss - Mean: 0.55716, Variance: 0.00869

Test Epoch: 14 
task: sign, mean loss: 0.95157, accuracy: 0.70414, avg. loss over tasks: 0.95157
Diversity Loss - Mean: -0.03902, Variance: 0.01151
Semantic Loss - Mean: 0.91214, Variance: 0.02906

Train Epoch: 15 
task: sign, mean loss: 0.42926, accuracy: 0.84239, avg. loss over tasks: 0.42926, lr: 0.0002990577746525024
Diversity Loss - Mean: -0.02219, Variance: 0.00949
Semantic Loss - Mean: 0.47023, Variance: 0.00871

Test Epoch: 15 
task: sign, mean loss: 1.31217, accuracy: 0.68047, avg. loss over tasks: 1.31217
Diversity Loss - Mean: -0.06191, Variance: 0.01161
Semantic Loss - Mean: 1.17000, Variance: 0.02823

Train Epoch: 16 
task: sign, mean loss: 0.35608, accuracy: 0.86957, avg. loss over tasks: 0.35608, lr: 0.000298643821800925
Diversity Loss - Mean: -0.01121, Variance: 0.00948
Semantic Loss - Mean: 0.40472, Variance: 0.00865

Test Epoch: 16 
task: sign, mean loss: 0.94105, accuracy: 0.74556, avg. loss over tasks: 0.94105
Diversity Loss - Mean: -0.01430, Variance: 0.01164
Semantic Loss - Mean: 0.88316, Variance: 0.02811

Train Epoch: 17 
task: sign, mean loss: 0.26287, accuracy: 0.89130, avg. loss over tasks: 0.26287, lr: 0.0002981550978381814
Diversity Loss - Mean: -0.00917, Variance: 0.00942
Semantic Loss - Mean: 0.31222, Variance: 0.00847

Test Epoch: 17 
task: sign, mean loss: 1.37742, accuracy: 0.65089, avg. loss over tasks: 1.37742
Diversity Loss - Mean: -0.00112, Variance: 0.01169
Semantic Loss - Mean: 1.06625, Variance: 0.02840

Train Epoch: 18 
task: sign, mean loss: 0.39526, accuracy: 0.88043, avg. loss over tasks: 0.39526, lr: 0.00029759184885150465
Diversity Loss - Mean: -0.02312, Variance: 0.00939
Semantic Loss - Mean: 0.41374, Variance: 0.00865

Test Epoch: 18 
task: sign, mean loss: 0.87513, accuracy: 0.72189, avg. loss over tasks: 0.87513
Diversity Loss - Mean: -0.01970, Variance: 0.01175
Semantic Loss - Mean: 0.78461, Variance: 0.02804

Train Epoch: 19 
task: sign, mean loss: 0.26972, accuracy: 0.88587, avg. loss over tasks: 0.26972, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.03228, Variance: 0.00941
Semantic Loss - Mean: 0.31754, Variance: 0.00851

Test Epoch: 19 
task: sign, mean loss: 1.07554, accuracy: 0.71006, avg. loss over tasks: 1.07554
Diversity Loss - Mean: -0.02001, Variance: 0.01173
Semantic Loss - Mean: 1.15330, Variance: 0.02732

Train Epoch: 20 
task: sign, mean loss: 0.24548, accuracy: 0.88043, avg. loss over tasks: 0.24548, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.02372, Variance: 0.00946
Semantic Loss - Mean: 0.30190, Variance: 0.00886

Test Epoch: 20 
task: sign, mean loss: 1.89039, accuracy: 0.72189, avg. loss over tasks: 1.89039
Diversity Loss - Mean: 0.00269, Variance: 0.01170
Semantic Loss - Mean: 1.70241, Variance: 0.02798

Train Epoch: 21 
task: sign, mean loss: 0.36274, accuracy: 0.84783, avg. loss over tasks: 0.36274, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.01928, Variance: 0.00947
Semantic Loss - Mean: 0.40725, Variance: 0.00902

Test Epoch: 21 
task: sign, mean loss: 1.40514, accuracy: 0.69822, avg. loss over tasks: 1.40514
Diversity Loss - Mean: 0.02424, Variance: 0.01173
Semantic Loss - Mean: 1.13154, Variance: 0.02932

Train Epoch: 22 
task: sign, mean loss: 0.31920, accuracy: 0.88587, avg. loss over tasks: 0.31920, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.01687, Variance: 0.00947
Semantic Loss - Mean: 0.42370, Variance: 0.00944

Test Epoch: 22 
task: sign, mean loss: 0.68166, accuracy: 0.80473, avg. loss over tasks: 0.68166
Diversity Loss - Mean: 0.00635, Variance: 0.01185
Semantic Loss - Mean: 0.74491, Variance: 0.02916

Train Epoch: 23 
task: sign, mean loss: 0.34090, accuracy: 0.84783, avg. loss over tasks: 0.34090, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.02566, Variance: 0.00949
Semantic Loss - Mean: 0.36468, Variance: 0.00931

Test Epoch: 23 
task: sign, mean loss: 1.00089, accuracy: 0.72781, avg. loss over tasks: 1.00089
Diversity Loss - Mean: -0.02027, Variance: 0.01199
Semantic Loss - Mean: 0.85149, Variance: 0.02887

Train Epoch: 24 
task: sign, mean loss: 0.22202, accuracy: 0.90761, avg. loss over tasks: 0.22202, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.03258, Variance: 0.00953
Semantic Loss - Mean: 0.28592, Variance: 0.00934

Test Epoch: 24 
task: sign, mean loss: 0.87722, accuracy: 0.71598, avg. loss over tasks: 0.87722
Diversity Loss - Mean: -0.01039, Variance: 0.01212
Semantic Loss - Mean: 0.89325, Variance: 0.02968

Train Epoch: 25 
task: sign, mean loss: 0.16891, accuracy: 0.93478, avg. loss over tasks: 0.16891, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.03105, Variance: 0.00957
Semantic Loss - Mean: 0.20756, Variance: 0.00914

Test Epoch: 25 
task: sign, mean loss: 1.10745, accuracy: 0.62722, avg. loss over tasks: 1.10745
Diversity Loss - Mean: -0.00063, Variance: 0.01218
Semantic Loss - Mean: 1.18557, Variance: 0.03045

Train Epoch: 26 
task: sign, mean loss: 0.17048, accuracy: 0.90761, avg. loss over tasks: 0.17048, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.02870, Variance: 0.00959
Semantic Loss - Mean: 0.23863, Variance: 0.00902

Test Epoch: 26 
task: sign, mean loss: 1.46199, accuracy: 0.73373, avg. loss over tasks: 1.46199
Diversity Loss - Mean: -0.01772, Variance: 0.01220
Semantic Loss - Mean: 1.34141, Variance: 0.03252

Train Epoch: 27 
task: sign, mean loss: 0.19967, accuracy: 0.92391, avg. loss over tasks: 0.19967, lr: 0.000289228031029578
Diversity Loss - Mean: -0.03247, Variance: 0.00959
Semantic Loss - Mean: 0.26145, Variance: 0.00920

Test Epoch: 27 
task: sign, mean loss: 0.66036, accuracy: 0.76923, avg. loss over tasks: 0.66036
Diversity Loss - Mean: -0.03466, Variance: 0.01227
Semantic Loss - Mean: 0.70031, Variance: 0.03247

Train Epoch: 28 
task: sign, mean loss: 0.15864, accuracy: 0.91304, avg. loss over tasks: 0.15864, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.03946, Variance: 0.00960
Semantic Loss - Mean: 0.22004, Variance: 0.00924

Test Epoch: 28 
task: sign, mean loss: 0.84172, accuracy: 0.79290, avg. loss over tasks: 0.84172
Diversity Loss - Mean: 0.00258, Variance: 0.01223
Semantic Loss - Mean: 0.88394, Variance: 0.03258

Train Epoch: 29 
task: sign, mean loss: 0.10250, accuracy: 0.95652, avg. loss over tasks: 0.10250, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.03534, Variance: 0.00959
Semantic Loss - Mean: 0.18071, Variance: 0.00922

Test Epoch: 29 
task: sign, mean loss: 0.88784, accuracy: 0.76923, avg. loss over tasks: 0.88784
Diversity Loss - Mean: 0.00330, Variance: 0.01221
Semantic Loss - Mean: 0.85489, Variance: 0.03245

Train Epoch: 30 
task: sign, mean loss: 0.22078, accuracy: 0.93478, avg. loss over tasks: 0.22078, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.03662, Variance: 0.00958
Semantic Loss - Mean: 0.29023, Variance: 0.00936

Test Epoch: 30 
task: sign, mean loss: 1.30828, accuracy: 0.68047, avg. loss over tasks: 1.30828
Diversity Loss - Mean: 0.00853, Variance: 0.01213
Semantic Loss - Mean: 1.08494, Variance: 0.03244

Train Epoch: 31 
task: sign, mean loss: 0.70824, accuracy: 0.83152, avg. loss over tasks: 0.70824, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.04397, Variance: 0.00960
Semantic Loss - Mean: 0.65940, Variance: 0.00954

Test Epoch: 31 
task: sign, mean loss: 2.28618, accuracy: 0.70414, avg. loss over tasks: 2.28618
Diversity Loss - Mean: -0.09722, Variance: 0.01239
Semantic Loss - Mean: 1.96901, Variance: 0.03799

Train Epoch: 32 
task: sign, mean loss: 0.33768, accuracy: 0.87500, avg. loss over tasks: 0.33768, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.06745, Variance: 0.00966
Semantic Loss - Mean: 0.46747, Variance: 0.01002

Test Epoch: 32 
task: sign, mean loss: 1.53460, accuracy: 0.71598, avg. loss over tasks: 1.53460
Diversity Loss - Mean: -0.05352, Variance: 0.01249
Semantic Loss - Mean: 1.32909, Variance: 0.03868

Train Epoch: 33 
task: sign, mean loss: 0.24018, accuracy: 0.89674, avg. loss over tasks: 0.24018, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.05929, Variance: 0.00972
Semantic Loss - Mean: 0.28607, Variance: 0.00986

Test Epoch: 33 
task: sign, mean loss: 0.98228, accuracy: 0.66864, avg. loss over tasks: 0.98228
Diversity Loss - Mean: -0.03600, Variance: 0.01256
Semantic Loss - Mean: 1.19861, Variance: 0.03926

Train Epoch: 34 
task: sign, mean loss: 0.25825, accuracy: 0.91848, avg. loss over tasks: 0.25825, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.05474, Variance: 0.00977
Semantic Loss - Mean: 0.29965, Variance: 0.00995

Test Epoch: 34 
task: sign, mean loss: 0.79436, accuracy: 0.80473, avg. loss over tasks: 0.79436
Diversity Loss - Mean: -0.08178, Variance: 0.01266
Semantic Loss - Mean: 0.79889, Variance: 0.03877

Train Epoch: 35 
task: sign, mean loss: 0.17417, accuracy: 0.95109, avg. loss over tasks: 0.17417, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.05489, Variance: 0.00982
Semantic Loss - Mean: 0.23248, Variance: 0.01003

Test Epoch: 35 
task: sign, mean loss: 0.32277, accuracy: 0.88757, avg. loss over tasks: 0.32277
Diversity Loss - Mean: -0.05206, Variance: 0.01275
Semantic Loss - Mean: 0.32586, Variance: 0.03777

Train Epoch: 36 
task: sign, mean loss: 0.07669, accuracy: 0.96739, avg. loss over tasks: 0.07669, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.04117, Variance: 0.00983
Semantic Loss - Mean: 0.10867, Variance: 0.00981

Test Epoch: 36 
task: sign, mean loss: 0.51270, accuracy: 0.83432, avg. loss over tasks: 0.51270
Diversity Loss - Mean: -0.02857, Variance: 0.01276
Semantic Loss - Mean: 0.49701, Variance: 0.03718

Train Epoch: 37 
task: sign, mean loss: 0.11798, accuracy: 0.95652, avg. loss over tasks: 0.11798, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.04127, Variance: 0.00984
Semantic Loss - Mean: 0.18372, Variance: 0.00984

Test Epoch: 37 
task: sign, mean loss: 0.32647, accuracy: 0.88166, avg. loss over tasks: 0.32647
Diversity Loss - Mean: -0.04210, Variance: 0.01281
Semantic Loss - Mean: 0.33839, Variance: 0.03628

Train Epoch: 38 
task: sign, mean loss: 0.07115, accuracy: 0.97826, avg. loss over tasks: 0.07115, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.04438, Variance: 0.00986
Semantic Loss - Mean: 0.12522, Variance: 0.00980

Test Epoch: 38 
task: sign, mean loss: 0.39480, accuracy: 0.88166, avg. loss over tasks: 0.39480
Diversity Loss - Mean: -0.02813, Variance: 0.01282
Semantic Loss - Mean: 0.45195, Variance: 0.03556

Train Epoch: 39 
task: sign, mean loss: 0.05172, accuracy: 0.98370, avg. loss over tasks: 0.05172, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.04120, Variance: 0.00987
Semantic Loss - Mean: 0.11478, Variance: 0.00975

Test Epoch: 39 
task: sign, mean loss: 1.10445, accuracy: 0.82249, avg. loss over tasks: 1.10445
Diversity Loss - Mean: -0.04816, Variance: 0.01283
Semantic Loss - Mean: 0.98565, Variance: 0.03602

Train Epoch: 40 
task: sign, mean loss: 0.11725, accuracy: 0.96196, avg. loss over tasks: 0.11725, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.04818, Variance: 0.00991
Semantic Loss - Mean: 0.15976, Variance: 0.00973

Test Epoch: 40 
task: sign, mean loss: 1.08173, accuracy: 0.83432, avg. loss over tasks: 1.08173
Diversity Loss - Mean: -0.04429, Variance: 0.01284
Semantic Loss - Mean: 1.00072, Variance: 0.03546

Train Epoch: 41 
task: sign, mean loss: 0.06213, accuracy: 0.97826, avg. loss over tasks: 0.06213, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.04767, Variance: 0.00993
Semantic Loss - Mean: 0.10094, Variance: 0.00960

Test Epoch: 41 
task: sign, mean loss: 1.37234, accuracy: 0.79290, avg. loss over tasks: 1.37234
Diversity Loss - Mean: -0.03156, Variance: 0.01283
Semantic Loss - Mean: 1.16152, Variance: 0.03539

Train Epoch: 42 
task: sign, mean loss: 0.09513, accuracy: 0.95652, avg. loss over tasks: 0.09513, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.05365, Variance: 0.00996
Semantic Loss - Mean: 0.14917, Variance: 0.00956

Test Epoch: 42 
task: sign, mean loss: 0.92852, accuracy: 0.82249, avg. loss over tasks: 0.92852
Diversity Loss - Mean: -0.05598, Variance: 0.01283
Semantic Loss - Mean: 0.87583, Variance: 0.03506

Train Epoch: 43 
task: sign, mean loss: 0.09351, accuracy: 0.96196, avg. loss over tasks: 0.09351, lr: 0.000260757131773478
Diversity Loss - Mean: -0.05502, Variance: 0.00999
Semantic Loss - Mean: 0.13100, Variance: 0.00942

Test Epoch: 43 
task: sign, mean loss: 1.19597, accuracy: 0.79290, avg. loss over tasks: 1.19597
Diversity Loss - Mean: -0.03760, Variance: 0.01280
Semantic Loss - Mean: 1.15971, Variance: 0.03506

Train Epoch: 44 
task: sign, mean loss: 0.13070, accuracy: 0.95652, avg. loss over tasks: 0.13070, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.05532, Variance: 0.01001
Semantic Loss - Mean: 0.18156, Variance: 0.00943

Test Epoch: 44 
task: sign, mean loss: 0.86187, accuracy: 0.81065, avg. loss over tasks: 0.86187
Diversity Loss - Mean: -0.03256, Variance: 0.01278
Semantic Loss - Mean: 0.69686, Variance: 0.03467

Train Epoch: 45 
task: sign, mean loss: 0.04559, accuracy: 0.98370, avg. loss over tasks: 0.04559, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.05656, Variance: 0.01003
Semantic Loss - Mean: 0.09998, Variance: 0.00945

Test Epoch: 45 
task: sign, mean loss: 0.45354, accuracy: 0.88166, avg. loss over tasks: 0.45354
Diversity Loss - Mean: -0.03603, Variance: 0.01280
Semantic Loss - Mean: 0.44273, Variance: 0.03436

Train Epoch: 46 
task: sign, mean loss: 0.04226, accuracy: 0.98913, avg. loss over tasks: 0.04226, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.05753, Variance: 0.01005
Semantic Loss - Mean: 0.08968, Variance: 0.00939

Test Epoch: 46 
task: sign, mean loss: 0.53150, accuracy: 0.85207, avg. loss over tasks: 0.53150
Diversity Loss - Mean: -0.04270, Variance: 0.01280
Semantic Loss - Mean: 0.54416, Variance: 0.03414

Train Epoch: 47 
task: sign, mean loss: 0.03370, accuracy: 0.98913, avg. loss over tasks: 0.03370, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.06204, Variance: 0.01007
Semantic Loss - Mean: 0.09209, Variance: 0.00937

Test Epoch: 47 
task: sign, mean loss: 1.11552, accuracy: 0.75740, avg. loss over tasks: 1.11552
Diversity Loss - Mean: -0.04666, Variance: 0.01281
Semantic Loss - Mean: 0.93037, Variance: 0.03444

Train Epoch: 48 
task: sign, mean loss: 0.08096, accuracy: 0.96196, avg. loss over tasks: 0.08096, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.06931, Variance: 0.01012
Semantic Loss - Mean: 0.11660, Variance: 0.00939

Test Epoch: 48 
task: sign, mean loss: 1.30742, accuracy: 0.77515, avg. loss over tasks: 1.30742
Diversity Loss - Mean: -0.03903, Variance: 0.01285
Semantic Loss - Mean: 1.15811, Variance: 0.03453

Train Epoch: 49 
task: sign, mean loss: 0.08401, accuracy: 0.98370, avg. loss over tasks: 0.08401, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.06770, Variance: 0.01017
Semantic Loss - Mean: 0.11379, Variance: 0.00937

Test Epoch: 49 
task: sign, mean loss: 0.77289, accuracy: 0.81657, avg. loss over tasks: 0.77289
Diversity Loss - Mean: -0.05484, Variance: 0.01292
Semantic Loss - Mean: 0.69823, Variance: 0.03444

Train Epoch: 50 
task: sign, mean loss: 0.08186, accuracy: 0.98370, avg. loss over tasks: 0.08186, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.06481, Variance: 0.01021
Semantic Loss - Mean: 0.12356, Variance: 0.00930

Test Epoch: 50 
task: sign, mean loss: 0.58229, accuracy: 0.82840, avg. loss over tasks: 0.58229
Diversity Loss - Mean: -0.06931, Variance: 0.01298
Semantic Loss - Mean: 0.56126, Variance: 0.03472

Train Epoch: 51 
task: sign, mean loss: 0.02114, accuracy: 1.00000, avg. loss over tasks: 0.02114, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.06421, Variance: 0.01026
Semantic Loss - Mean: 0.06894, Variance: 0.00922

Test Epoch: 51 
task: sign, mean loss: 0.69041, accuracy: 0.82249, avg. loss over tasks: 0.69041
Diversity Loss - Mean: -0.05803, Variance: 0.01302
Semantic Loss - Mean: 0.61103, Variance: 0.03444

Train Epoch: 52 
task: sign, mean loss: 0.05946, accuracy: 0.97826, avg. loss over tasks: 0.05946, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.06616, Variance: 0.01031
Semantic Loss - Mean: 0.09000, Variance: 0.00917

Test Epoch: 52 
task: sign, mean loss: 0.49975, accuracy: 0.88166, avg. loss over tasks: 0.49975
Diversity Loss - Mean: -0.04101, Variance: 0.01303
Semantic Loss - Mean: 0.59276, Variance: 0.03420

Train Epoch: 53 
task: sign, mean loss: 0.08955, accuracy: 0.97283, avg. loss over tasks: 0.08955, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.06494, Variance: 0.01035
Semantic Loss - Mean: 0.13369, Variance: 0.00916

Test Epoch: 53 
task: sign, mean loss: 0.45755, accuracy: 0.90533, avg. loss over tasks: 0.45755
Diversity Loss - Mean: -0.05650, Variance: 0.01303
Semantic Loss - Mean: 0.43238, Variance: 0.03363

Train Epoch: 54 
task: sign, mean loss: 0.08925, accuracy: 0.96196, avg. loss over tasks: 0.08925, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.07228, Variance: 0.01038
Semantic Loss - Mean: 0.17024, Variance: 0.00913

Test Epoch: 54 
task: sign, mean loss: 1.32773, accuracy: 0.78107, avg. loss over tasks: 1.32773
Diversity Loss - Mean: -0.04103, Variance: 0.01299
Semantic Loss - Mean: 1.12368, Variance: 0.03335

Train Epoch: 55 
task: sign, mean loss: 0.13770, accuracy: 0.96196, avg. loss over tasks: 0.13770, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.07697, Variance: 0.01043
Semantic Loss - Mean: 0.17997, Variance: 0.00912

Test Epoch: 55 
task: sign, mean loss: 1.44837, accuracy: 0.72781, avg. loss over tasks: 1.44837
Diversity Loss - Mean: -0.07351, Variance: 0.01296
Semantic Loss - Mean: 1.16545, Variance: 0.03359

Train Epoch: 56 
task: sign, mean loss: 0.13340, accuracy: 0.96739, avg. loss over tasks: 0.13340, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.08516, Variance: 0.01048
Semantic Loss - Mean: 0.17195, Variance: 0.00904

Test Epoch: 56 
task: sign, mean loss: 2.01632, accuracy: 0.58580, avg. loss over tasks: 2.01632
Diversity Loss - Mean: -0.06441, Variance: 0.01295
Semantic Loss - Mean: 1.79645, Variance: 0.03416

Train Epoch: 57 
task: sign, mean loss: 0.06166, accuracy: 0.97826, avg. loss over tasks: 0.06166, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.08436, Variance: 0.01054
Semantic Loss - Mean: 0.11666, Variance: 0.00903

Test Epoch: 57 
task: sign, mean loss: 3.57283, accuracy: 0.30178, avg. loss over tasks: 3.57283
Diversity Loss - Mean: -0.04981, Variance: 0.01295
Semantic Loss - Mean: 2.72026, Variance: 0.03545

Train Epoch: 58 
task: sign, mean loss: 0.02247, accuracy: 0.99457, avg. loss over tasks: 0.02247, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.07829, Variance: 0.01058
Semantic Loss - Mean: 0.07136, Variance: 0.00898

Test Epoch: 58 
task: sign, mean loss: 1.41195, accuracy: 0.75740, avg. loss over tasks: 1.41195
Diversity Loss - Mean: -0.07634, Variance: 0.01297
Semantic Loss - Mean: 1.13464, Variance: 0.03630

Train Epoch: 59 
task: sign, mean loss: 0.02943, accuracy: 0.99457, avg. loss over tasks: 0.02943, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.07791, Variance: 0.01061
Semantic Loss - Mean: 0.06831, Variance: 0.00902

Test Epoch: 59 
task: sign, mean loss: 1.11127, accuracy: 0.80473, avg. loss over tasks: 1.11127
Diversity Loss - Mean: -0.07623, Variance: 0.01298
Semantic Loss - Mean: 1.10803, Variance: 0.03626

Train Epoch: 60 
task: sign, mean loss: 0.01502, accuracy: 0.98913, avg. loss over tasks: 0.01502, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.07682, Variance: 0.01064
Semantic Loss - Mean: 0.05232, Variance: 0.00894

Test Epoch: 60 
task: sign, mean loss: 1.01361, accuracy: 0.80473, avg. loss over tasks: 1.01361
Diversity Loss - Mean: -0.06886, Variance: 0.01299
Semantic Loss - Mean: 1.12096, Variance: 0.03646

Train Epoch: 61 
task: sign, mean loss: 0.01257, accuracy: 1.00000, avg. loss over tasks: 0.01257, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.07892, Variance: 0.01067
Semantic Loss - Mean: 0.04609, Variance: 0.00885

Test Epoch: 61 
task: sign, mean loss: 0.98935, accuracy: 0.75148, avg. loss over tasks: 0.98935
Diversity Loss - Mean: -0.07603, Variance: 0.01300
Semantic Loss - Mean: 1.06756, Variance: 0.03678

Train Epoch: 62 
task: sign, mean loss: 0.05364, accuracy: 0.98913, avg. loss over tasks: 0.05364, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.07932, Variance: 0.01070
Semantic Loss - Mean: 0.10254, Variance: 0.00882

Test Epoch: 62 
task: sign, mean loss: 0.78545, accuracy: 0.84615, avg. loss over tasks: 0.78545
Diversity Loss - Mean: -0.08674, Variance: 0.01304
Semantic Loss - Mean: 0.70815, Variance: 0.03655

Train Epoch: 63 
task: sign, mean loss: 0.04310, accuracy: 0.98913, avg. loss over tasks: 0.04310, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.08064, Variance: 0.01073
Semantic Loss - Mean: 0.06525, Variance: 0.00872

Test Epoch: 63 
task: sign, mean loss: 1.14093, accuracy: 0.78107, avg. loss over tasks: 1.14093
Diversity Loss - Mean: -0.08947, Variance: 0.01304
Semantic Loss - Mean: 1.06321, Variance: 0.03717

Train Epoch: 64 
task: sign, mean loss: 0.00781, accuracy: 1.00000, avg. loss over tasks: 0.00781, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.08573, Variance: 0.01077
Semantic Loss - Mean: 0.04230, Variance: 0.00866

Test Epoch: 64 
task: sign, mean loss: 1.21676, accuracy: 0.75740, avg. loss over tasks: 1.21676
Diversity Loss - Mean: -0.08077, Variance: 0.01303
Semantic Loss - Mean: 1.32684, Variance: 0.03793

Train Epoch: 65 
task: sign, mean loss: 0.01109, accuracy: 1.00000, avg. loss over tasks: 0.01109, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.08545, Variance: 0.01081
Semantic Loss - Mean: 0.04970, Variance: 0.00857

Test Epoch: 65 
task: sign, mean loss: 1.18996, accuracy: 0.75740, avg. loss over tasks: 1.18996
Diversity Loss - Mean: -0.07823, Variance: 0.01304
Semantic Loss - Mean: 1.13356, Variance: 0.03823

Train Epoch: 66 
task: sign, mean loss: 0.00870, accuracy: 1.00000, avg. loss over tasks: 0.00870, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.08503, Variance: 0.01085
Semantic Loss - Mean: 0.03874, Variance: 0.00848

Test Epoch: 66 
task: sign, mean loss: 1.01892, accuracy: 0.79290, avg. loss over tasks: 1.01892
Diversity Loss - Mean: -0.08065, Variance: 0.01305
Semantic Loss - Mean: 0.87681, Variance: 0.03813

Train Epoch: 67 
task: sign, mean loss: 0.00434, accuracy: 1.00000, avg. loss over tasks: 0.00434, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.08599, Variance: 0.01089
Semantic Loss - Mean: 0.02641, Variance: 0.00839

Test Epoch: 67 
task: sign, mean loss: 1.08149, accuracy: 0.79290, avg. loss over tasks: 1.08149
Diversity Loss - Mean: -0.08054, Variance: 0.01305
Semantic Loss - Mean: 1.04700, Variance: 0.03933

Train Epoch: 68 
task: sign, mean loss: 0.06241, accuracy: 0.97826, avg. loss over tasks: 0.06241, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.08808, Variance: 0.01093
Semantic Loss - Mean: 0.10136, Variance: 0.00839

Test Epoch: 68 
task: sign, mean loss: 0.82934, accuracy: 0.83432, avg. loss over tasks: 0.82934
Diversity Loss - Mean: -0.08628, Variance: 0.01305
Semantic Loss - Mean: 0.77495, Variance: 0.03940

Train Epoch: 69 
task: sign, mean loss: 0.00712, accuracy: 1.00000, avg. loss over tasks: 0.00712, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.09335, Variance: 0.01097
Semantic Loss - Mean: 0.06081, Variance: 0.00835

Test Epoch: 69 
task: sign, mean loss: 0.83437, accuracy: 0.84615, avg. loss over tasks: 0.83437
Diversity Loss - Mean: -0.08277, Variance: 0.01305
Semantic Loss - Mean: 0.89442, Variance: 0.03967

Train Epoch: 70 
task: sign, mean loss: 0.00618, accuracy: 1.00000, avg. loss over tasks: 0.00618, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.09398, Variance: 0.01101
Semantic Loss - Mean: 0.03681, Variance: 0.00825

Test Epoch: 70 
task: sign, mean loss: 0.85077, accuracy: 0.85799, avg. loss over tasks: 0.85077
Diversity Loss - Mean: -0.08474, Variance: 0.01305
Semantic Loss - Mean: 0.97013, Variance: 0.03982

Train Epoch: 71 
task: sign, mean loss: 0.00728, accuracy: 1.00000, avg. loss over tasks: 0.00728, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.09500, Variance: 0.01106
Semantic Loss - Mean: 0.04937, Variance: 0.00825

Test Epoch: 71 
task: sign, mean loss: 0.68870, accuracy: 0.86982, avg. loss over tasks: 0.68870
Diversity Loss - Mean: -0.08360, Variance: 0.01305
Semantic Loss - Mean: 0.73924, Variance: 0.03975

Train Epoch: 72 
task: sign, mean loss: 0.00286, accuracy: 1.00000, avg. loss over tasks: 0.00286, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.09580, Variance: 0.01111
Semantic Loss - Mean: 0.02906, Variance: 0.00817

Test Epoch: 72 
task: sign, mean loss: 0.81694, accuracy: 0.84615, avg. loss over tasks: 0.81694
Diversity Loss - Mean: -0.07869, Variance: 0.01305
Semantic Loss - Mean: 0.82498, Variance: 0.03981

Train Epoch: 73 
task: sign, mean loss: 0.00622, accuracy: 1.00000, avg. loss over tasks: 0.00622, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.09393, Variance: 0.01115
Semantic Loss - Mean: 0.03349, Variance: 0.00811

Test Epoch: 73 
task: sign, mean loss: 0.91102, accuracy: 0.84024, avg. loss over tasks: 0.91102
Diversity Loss - Mean: -0.08110, Variance: 0.01306
Semantic Loss - Mean: 0.85051, Variance: 0.03961

Train Epoch: 74 
task: sign, mean loss: 0.00308, accuracy: 1.00000, avg. loss over tasks: 0.00308, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.09448, Variance: 0.01119
Semantic Loss - Mean: 0.03840, Variance: 0.00804

Test Epoch: 74 
task: sign, mean loss: 1.09582, accuracy: 0.81657, avg. loss over tasks: 1.09582
Diversity Loss - Mean: -0.08092, Variance: 0.01306
Semantic Loss - Mean: 1.04915, Variance: 0.03958

Train Epoch: 75 
task: sign, mean loss: 0.00306, accuracy: 1.00000, avg. loss over tasks: 0.00306, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.09593, Variance: 0.01123
Semantic Loss - Mean: 0.03504, Variance: 0.00800

Test Epoch: 75 
task: sign, mean loss: 0.93639, accuracy: 0.84615, avg. loss over tasks: 0.93639
Diversity Loss - Mean: -0.08450, Variance: 0.01307
Semantic Loss - Mean: 0.89123, Variance: 0.03949

Train Epoch: 76 
task: sign, mean loss: 0.00381, accuracy: 1.00000, avg. loss over tasks: 0.00381, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.09587, Variance: 0.01126
Semantic Loss - Mean: 0.02919, Variance: 0.00794

Test Epoch: 76 
task: sign, mean loss: 0.90259, accuracy: 0.85207, avg. loss over tasks: 0.90259
Diversity Loss - Mean: -0.08485, Variance: 0.01308
Semantic Loss - Mean: 0.84027, Variance: 0.03926

Train Epoch: 77 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.09777, Variance: 0.01130
Semantic Loss - Mean: 0.03412, Variance: 0.00791

Test Epoch: 77 
task: sign, mean loss: 0.94970, accuracy: 0.84615, avg. loss over tasks: 0.94970
Diversity Loss - Mean: -0.08494, Variance: 0.01309
Semantic Loss - Mean: 0.94094, Variance: 0.03936

Train Epoch: 78 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.09775, Variance: 0.01134
Semantic Loss - Mean: 0.01403, Variance: 0.00782

Test Epoch: 78 
task: sign, mean loss: 1.07724, accuracy: 0.82249, avg. loss over tasks: 1.07724
Diversity Loss - Mean: -0.08869, Variance: 0.01310
Semantic Loss - Mean: 1.09594, Variance: 0.03963

Train Epoch: 79 
task: sign, mean loss: 0.00221, accuracy: 1.00000, avg. loss over tasks: 0.00221, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.09943, Variance: 0.01139
Semantic Loss - Mean: 0.01504, Variance: 0.00774

Test Epoch: 79 
task: sign, mean loss: 1.08568, accuracy: 0.82249, avg. loss over tasks: 1.08568
Diversity Loss - Mean: -0.08885, Variance: 0.01311
Semantic Loss - Mean: 1.07547, Variance: 0.03973

Train Epoch: 80 
task: sign, mean loss: 0.00101, accuracy: 1.00000, avg. loss over tasks: 0.00101, lr: 0.00015015
Diversity Loss - Mean: -0.09946, Variance: 0.01143
Semantic Loss - Mean: 0.01016, Variance: 0.00764

Test Epoch: 80 
task: sign, mean loss: 1.04486, accuracy: 0.82249, avg. loss over tasks: 1.04486
Diversity Loss - Mean: -0.09242, Variance: 0.01313
Semantic Loss - Mean: 1.00666, Variance: 0.03978

Train Epoch: 81 
task: sign, mean loss: 0.00306, accuracy: 1.00000, avg. loss over tasks: 0.00306, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.10273, Variance: 0.01148
Semantic Loss - Mean: 0.02030, Variance: 0.00756

Test Epoch: 81 
task: sign, mean loss: 0.98832, accuracy: 0.82840, avg. loss over tasks: 0.98832
Diversity Loss - Mean: -0.09368, Variance: 0.01314
Semantic Loss - Mean: 0.94239, Variance: 0.03967

Train Epoch: 82 
task: sign, mean loss: 0.00120, accuracy: 1.00000, avg. loss over tasks: 0.00120, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.10327, Variance: 0.01152
Semantic Loss - Mean: 0.01953, Variance: 0.00749

Test Epoch: 82 
task: sign, mean loss: 0.93483, accuracy: 0.86391, avg. loss over tasks: 0.93483
Diversity Loss - Mean: -0.09907, Variance: 0.01316
Semantic Loss - Mean: 0.89201, Variance: 0.03972

Train Epoch: 83 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.10439, Variance: 0.01157
Semantic Loss - Mean: 0.03970, Variance: 0.00750

Test Epoch: 83 
task: sign, mean loss: 1.02919, accuracy: 0.85207, avg. loss over tasks: 1.02919
Diversity Loss - Mean: -0.09670, Variance: 0.01318
Semantic Loss - Mean: 1.02335, Variance: 0.04014

Train Epoch: 84 
task: sign, mean loss: 0.03641, accuracy: 0.98913, avg. loss over tasks: 0.03641, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.10288, Variance: 0.01162
Semantic Loss - Mean: 0.06688, Variance: 0.00752

Test Epoch: 84 
task: sign, mean loss: 0.79366, accuracy: 0.86982, avg. loss over tasks: 0.79366
Diversity Loss - Mean: -0.09495, Variance: 0.01319
Semantic Loss - Mean: 0.78592, Variance: 0.04005

Train Epoch: 85 
task: sign, mean loss: 0.00602, accuracy: 1.00000, avg. loss over tasks: 0.00602, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.10344, Variance: 0.01166
Semantic Loss - Mean: 0.01585, Variance: 0.00743

Test Epoch: 85 
task: sign, mean loss: 0.84677, accuracy: 0.86982, avg. loss over tasks: 0.84677
Diversity Loss - Mean: -0.09331, Variance: 0.01321
Semantic Loss - Mean: 0.79679, Variance: 0.03988

Train Epoch: 86 
task: sign, mean loss: 0.00444, accuracy: 1.00000, avg. loss over tasks: 0.00444, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.10423, Variance: 0.01171
Semantic Loss - Mean: 0.03234, Variance: 0.00738

Test Epoch: 86 
task: sign, mean loss: 0.95201, accuracy: 0.88166, avg. loss over tasks: 0.95201
Diversity Loss - Mean: -0.08996, Variance: 0.01322
Semantic Loss - Mean: 0.94447, Variance: 0.03966

Train Epoch: 87 
task: sign, mean loss: 0.00273, accuracy: 1.00000, avg. loss over tasks: 0.00273, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.10325, Variance: 0.01175
Semantic Loss - Mean: 0.02840, Variance: 0.00732

Test Epoch: 87 
task: sign, mean loss: 0.94114, accuracy: 0.85799, avg. loss over tasks: 0.94114
Diversity Loss - Mean: -0.09314, Variance: 0.01323
Semantic Loss - Mean: 0.87169, Variance: 0.03958

Train Epoch: 88 
task: sign, mean loss: 0.00111, accuracy: 1.00000, avg. loss over tasks: 0.00111, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.10547, Variance: 0.01180
Semantic Loss - Mean: 0.01814, Variance: 0.00726

Test Epoch: 88 
task: sign, mean loss: 0.84934, accuracy: 0.85207, avg. loss over tasks: 0.84934
Diversity Loss - Mean: -0.09674, Variance: 0.01325
Semantic Loss - Mean: 0.72569, Variance: 0.03974

Train Epoch: 89 
task: sign, mean loss: 0.01242, accuracy: 0.99457, avg. loss over tasks: 0.01242, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.10593, Variance: 0.01185
Semantic Loss - Mean: 0.03140, Variance: 0.00721

Test Epoch: 89 
task: sign, mean loss: 0.87263, accuracy: 0.85799, avg. loss over tasks: 0.87263
Diversity Loss - Mean: -0.09486, Variance: 0.01327
Semantic Loss - Mean: 0.80163, Variance: 0.03997

Train Epoch: 90 
task: sign, mean loss: 0.00233, accuracy: 1.00000, avg. loss over tasks: 0.00233, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.10702, Variance: 0.01189
Semantic Loss - Mean: 0.02345, Variance: 0.00714

Test Epoch: 90 
task: sign, mean loss: 1.04817, accuracy: 0.84024, avg. loss over tasks: 1.04817
Diversity Loss - Mean: -0.09514, Variance: 0.01328
Semantic Loss - Mean: 0.96481, Variance: 0.04035

Train Epoch: 91 
task: sign, mean loss: 0.00444, accuracy: 1.00000, avg. loss over tasks: 0.00444, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.10716, Variance: 0.01194
Semantic Loss - Mean: 0.01028, Variance: 0.00707

Test Epoch: 91 
task: sign, mean loss: 0.98698, accuracy: 0.85207, avg. loss over tasks: 0.98698
Diversity Loss - Mean: -0.09786, Variance: 0.01329
Semantic Loss - Mean: 0.87424, Variance: 0.04049

Train Epoch: 92 
task: sign, mean loss: 0.00307, accuracy: 1.00000, avg. loss over tasks: 0.00307, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.10698, Variance: 0.01198
Semantic Loss - Mean: 0.01613, Variance: 0.00702

Test Epoch: 92 
task: sign, mean loss: 1.02621, accuracy: 0.84615, avg. loss over tasks: 1.02621
Diversity Loss - Mean: -0.09770, Variance: 0.01330
Semantic Loss - Mean: 0.88733, Variance: 0.04052

Train Epoch: 93 
task: sign, mean loss: 0.00085, accuracy: 1.00000, avg. loss over tasks: 0.00085, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.10739, Variance: 0.01202
Semantic Loss - Mean: 0.00525, Variance: 0.00695

Test Epoch: 93 
task: sign, mean loss: 0.96123, accuracy: 0.85207, avg. loss over tasks: 0.96123
Diversity Loss - Mean: -0.10248, Variance: 0.01332
Semantic Loss - Mean: 0.82877, Variance: 0.04060

Train Epoch: 94 
task: sign, mean loss: 0.00052, accuracy: 1.00000, avg. loss over tasks: 0.00052, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.10828, Variance: 0.01206
Semantic Loss - Mean: 0.00607, Variance: 0.00688

Test Epoch: 94 
task: sign, mean loss: 0.98408, accuracy: 0.85207, avg. loss over tasks: 0.98408
Diversity Loss - Mean: -0.10200, Variance: 0.01334
Semantic Loss - Mean: 0.85647, Variance: 0.04064

Train Epoch: 95 
task: sign, mean loss: 0.00046, accuracy: 1.00000, avg. loss over tasks: 0.00046, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.10922, Variance: 0.01210
Semantic Loss - Mean: 0.00729, Variance: 0.00681

Test Epoch: 95 
task: sign, mean loss: 1.01718, accuracy: 0.85207, avg. loss over tasks: 1.01718
Diversity Loss - Mean: -0.10274, Variance: 0.01336
Semantic Loss - Mean: 0.89694, Variance: 0.04064

Train Epoch: 96 
task: sign, mean loss: 0.00065, accuracy: 1.00000, avg. loss over tasks: 0.00065, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.10919, Variance: 0.01213
Semantic Loss - Mean: 0.01585, Variance: 0.00675

Test Epoch: 96 
task: sign, mean loss: 0.94222, accuracy: 0.85207, avg. loss over tasks: 0.94222
Diversity Loss - Mean: -0.10177, Variance: 0.01338
Semantic Loss - Mean: 0.83053, Variance: 0.04043

Train Epoch: 97 
task: sign, mean loss: 0.00102, accuracy: 1.00000, avg. loss over tasks: 0.00102, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.11028, Variance: 0.01217
Semantic Loss - Mean: 0.01247, Variance: 0.00670

Test Epoch: 97 
task: sign, mean loss: 1.10468, accuracy: 0.83432, avg. loss over tasks: 1.10468
Diversity Loss - Mean: -0.10186, Variance: 0.01340
Semantic Loss - Mean: 0.98258, Variance: 0.04027

Train Epoch: 98 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.11104, Variance: 0.01222
Semantic Loss - Mean: 0.00882, Variance: 0.00663

Test Epoch: 98 
task: sign, mean loss: 1.09278, accuracy: 0.83432, avg. loss over tasks: 1.09278
Diversity Loss - Mean: -0.10351, Variance: 0.01341
Semantic Loss - Mean: 0.95407, Variance: 0.04010

Train Epoch: 99 
task: sign, mean loss: 0.00895, accuracy: 0.99457, avg. loss over tasks: 0.00895, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.11234, Variance: 0.01226
Semantic Loss - Mean: 0.01253, Variance: 0.00657

Test Epoch: 99 
task: sign, mean loss: 1.09747, accuracy: 0.83432, avg. loss over tasks: 1.09747
Diversity Loss - Mean: -0.10220, Variance: 0.01343
Semantic Loss - Mean: 0.99483, Variance: 0.03990

Train Epoch: 100 
task: sign, mean loss: 0.00155, accuracy: 1.00000, avg. loss over tasks: 0.00155, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.11141, Variance: 0.01230
Semantic Loss - Mean: 0.02727, Variance: 0.00655

Test Epoch: 100 
task: sign, mean loss: 1.05747, accuracy: 0.84024, avg. loss over tasks: 1.05747
Diversity Loss - Mean: -0.10217, Variance: 0.01344
Semantic Loss - Mean: 0.97690, Variance: 0.03971

Train Epoch: 101 
task: sign, mean loss: 0.00149, accuracy: 1.00000, avg. loss over tasks: 0.00149, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.11301, Variance: 0.01234
Semantic Loss - Mean: 0.01710, Variance: 0.00651

Test Epoch: 101 
task: sign, mean loss: 1.10358, accuracy: 0.83432, avg. loss over tasks: 1.10358
Diversity Loss - Mean: -0.10396, Variance: 0.01345
Semantic Loss - Mean: 1.05031, Variance: 0.03976

Train Epoch: 102 
task: sign, mean loss: 0.00048, accuracy: 1.00000, avg. loss over tasks: 0.00048, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.11279, Variance: 0.01237
Semantic Loss - Mean: 0.01376, Variance: 0.00647

Test Epoch: 102 
task: sign, mean loss: 1.17478, accuracy: 0.83432, avg. loss over tasks: 1.17478
Diversity Loss - Mean: -0.10460, Variance: 0.01347
Semantic Loss - Mean: 1.08474, Variance: 0.03990

Train Epoch: 103 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.11293, Variance: 0.01241
Semantic Loss - Mean: 0.01542, Variance: 0.00642

Test Epoch: 103 
task: sign, mean loss: 1.14679, accuracy: 0.83432, avg. loss over tasks: 1.14679
Diversity Loss - Mean: -0.10585, Variance: 0.01349
Semantic Loss - Mean: 0.99857, Variance: 0.03978

Train Epoch: 104 
task: sign, mean loss: 0.00071, accuracy: 1.00000, avg. loss over tasks: 0.00071, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.11423, Variance: 0.01245
Semantic Loss - Mean: 0.00968, Variance: 0.00637

Test Epoch: 104 
task: sign, mean loss: 1.05153, accuracy: 0.85207, avg. loss over tasks: 1.05153
Diversity Loss - Mean: -0.10652, Variance: 0.01350
Semantic Loss - Mean: 0.88917, Variance: 0.03957

Train Epoch: 105 
task: sign, mean loss: 0.00037, accuracy: 1.00000, avg. loss over tasks: 0.00037, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.11428, Variance: 0.01248
Semantic Loss - Mean: 0.00560, Variance: 0.00631

Test Epoch: 105 
task: sign, mean loss: 1.08039, accuracy: 0.84024, avg. loss over tasks: 1.08039
Diversity Loss - Mean: -0.10691, Variance: 0.01352
Semantic Loss - Mean: 0.91310, Variance: 0.03936

Train Epoch: 106 
task: sign, mean loss: 0.00041, accuracy: 1.00000, avg. loss over tasks: 0.00041, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.11493, Variance: 0.01252
Semantic Loss - Mean: 0.00778, Variance: 0.00625

Test Epoch: 106 
task: sign, mean loss: 1.02500, accuracy: 0.84615, avg. loss over tasks: 1.02500
Diversity Loss - Mean: -0.10587, Variance: 0.01353
Semantic Loss - Mean: 0.87375, Variance: 0.03913

Train Epoch: 107 
task: sign, mean loss: 0.00051, accuracy: 1.00000, avg. loss over tasks: 0.00051, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.11471, Variance: 0.01256
Semantic Loss - Mean: 0.00539, Variance: 0.00619

Test Epoch: 107 
task: sign, mean loss: 1.08438, accuracy: 0.84024, avg. loss over tasks: 1.08438
Diversity Loss - Mean: -0.10785, Variance: 0.01355
Semantic Loss - Mean: 0.91576, Variance: 0.03892

Train Epoch: 108 
task: sign, mean loss: 0.00064, accuracy: 1.00000, avg. loss over tasks: 0.00064, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.11533, Variance: 0.01259
Semantic Loss - Mean: 0.00554, Variance: 0.00614

Test Epoch: 108 
task: sign, mean loss: 1.07937, accuracy: 0.84024, avg. loss over tasks: 1.07937
Diversity Loss - Mean: -0.10841, Variance: 0.01357
Semantic Loss - Mean: 0.90686, Variance: 0.03877

Train Epoch: 109 
task: sign, mean loss: 0.00035, accuracy: 1.00000, avg. loss over tasks: 0.00035, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.11488, Variance: 0.01263
Semantic Loss - Mean: 0.00470, Variance: 0.00608

Test Epoch: 109 
task: sign, mean loss: 1.03425, accuracy: 0.84024, avg. loss over tasks: 1.03425
Diversity Loss - Mean: -0.10824, Variance: 0.01358
Semantic Loss - Mean: 0.87436, Variance: 0.03863

Train Epoch: 110 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.11600, Variance: 0.01266
Semantic Loss - Mean: 0.00904, Variance: 0.00603

Test Epoch: 110 
task: sign, mean loss: 1.09720, accuracy: 0.82840, avg. loss over tasks: 1.09720
Diversity Loss - Mean: -0.10716, Variance: 0.01360
Semantic Loss - Mean: 0.94173, Variance: 0.03847

Train Epoch: 111 
task: sign, mean loss: 0.00107, accuracy: 1.00000, avg. loss over tasks: 0.00107, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.11590, Variance: 0.01270
Semantic Loss - Mean: 0.01541, Variance: 0.00599

Test Epoch: 111 
task: sign, mean loss: 1.07896, accuracy: 0.82840, avg. loss over tasks: 1.07896
Diversity Loss - Mean: -0.10732, Variance: 0.01362
Semantic Loss - Mean: 0.94004, Variance: 0.03829

Train Epoch: 112 
task: sign, mean loss: 0.00026, accuracy: 1.00000, avg. loss over tasks: 0.00026, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.11610, Variance: 0.01273
Semantic Loss - Mean: 0.00488, Variance: 0.00594

Test Epoch: 112 
task: sign, mean loss: 1.01009, accuracy: 0.84024, avg. loss over tasks: 1.01009
Diversity Loss - Mean: -0.10822, Variance: 0.01364
Semantic Loss - Mean: 0.89131, Variance: 0.03813

Train Epoch: 113 
task: sign, mean loss: 0.00161, accuracy: 1.00000, avg. loss over tasks: 0.00161, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.11667, Variance: 0.01276
Semantic Loss - Mean: 0.00623, Variance: 0.00589

Test Epoch: 113 
task: sign, mean loss: 0.91255, accuracy: 0.85799, avg. loss over tasks: 0.91255
Diversity Loss - Mean: -0.10912, Variance: 0.01365
Semantic Loss - Mean: 0.79537, Variance: 0.03795

Train Epoch: 114 
task: sign, mean loss: 0.00358, accuracy: 1.00000, avg. loss over tasks: 0.00358, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.11611, Variance: 0.01280
Semantic Loss - Mean: 0.00918, Variance: 0.00584

Test Epoch: 114 
task: sign, mean loss: 0.96088, accuracy: 0.86391, avg. loss over tasks: 0.96088
Diversity Loss - Mean: -0.10922, Variance: 0.01367
Semantic Loss - Mean: 0.81970, Variance: 0.03779

Train Epoch: 115 
task: sign, mean loss: 0.00042, accuracy: 1.00000, avg. loss over tasks: 0.00042, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.11584, Variance: 0.01283
Semantic Loss - Mean: 0.00796, Variance: 0.00579

Test Epoch: 115 
task: sign, mean loss: 0.98434, accuracy: 0.85799, avg. loss over tasks: 0.98434
Diversity Loss - Mean: -0.10893, Variance: 0.01369
Semantic Loss - Mean: 0.83275, Variance: 0.03759

Train Epoch: 116 
task: sign, mean loss: 0.00162, accuracy: 1.00000, avg. loss over tasks: 0.00162, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.11634, Variance: 0.01286
Semantic Loss - Mean: 0.00605, Variance: 0.00574

Test Epoch: 116 
task: sign, mean loss: 0.90682, accuracy: 0.86982, avg. loss over tasks: 0.90682
Diversity Loss - Mean: -0.11010, Variance: 0.01370
Semantic Loss - Mean: 0.75870, Variance: 0.03737

Train Epoch: 117 
task: sign, mean loss: 0.00037, accuracy: 1.00000, avg. loss over tasks: 0.00037, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.11620, Variance: 0.01289
Semantic Loss - Mean: 0.00449, Variance: 0.00570

Test Epoch: 117 
task: sign, mean loss: 0.91336, accuracy: 0.86982, avg. loss over tasks: 0.91336
Diversity Loss - Mean: -0.11041, Variance: 0.01372
Semantic Loss - Mean: 0.77012, Variance: 0.03718

Train Epoch: 118 
task: sign, mean loss: 0.00090, accuracy: 1.00000, avg. loss over tasks: 0.00090, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.11640, Variance: 0.01292
Semantic Loss - Mean: 0.00973, Variance: 0.00565

Test Epoch: 118 
task: sign, mean loss: 0.94791, accuracy: 0.86982, avg. loss over tasks: 0.94791
Diversity Loss - Mean: -0.10975, Variance: 0.01373
Semantic Loss - Mean: 0.79854, Variance: 0.03698

Train Epoch: 119 
task: sign, mean loss: 0.00024, accuracy: 1.00000, avg. loss over tasks: 0.00024, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.11739, Variance: 0.01295
Semantic Loss - Mean: 0.00885, Variance: 0.00561

Test Epoch: 119 
task: sign, mean loss: 0.90677, accuracy: 0.87574, avg. loss over tasks: 0.90677
Diversity Loss - Mean: -0.10981, Variance: 0.01375
Semantic Loss - Mean: 0.76899, Variance: 0.03680

Train Epoch: 120 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.11678, Variance: 0.01298
Semantic Loss - Mean: 0.01160, Variance: 0.00558

Test Epoch: 120 
task: sign, mean loss: 0.92728, accuracy: 0.86391, avg. loss over tasks: 0.92728
Diversity Loss - Mean: -0.11122, Variance: 0.01376
Semantic Loss - Mean: 0.78596, Variance: 0.03662

Train Epoch: 121 
task: sign, mean loss: 0.00095, accuracy: 1.00000, avg. loss over tasks: 0.00095, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.11891, Variance: 0.01301
Semantic Loss - Mean: 0.01250, Variance: 0.00554

Test Epoch: 121 
task: sign, mean loss: 0.94613, accuracy: 0.86982, avg. loss over tasks: 0.94613
Diversity Loss - Mean: -0.11001, Variance: 0.01378
Semantic Loss - Mean: 0.81155, Variance: 0.03646

Train Epoch: 122 
task: sign, mean loss: 0.00030, accuracy: 1.00000, avg. loss over tasks: 0.00030, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.11847, Variance: 0.01305
Semantic Loss - Mean: 0.00784, Variance: 0.00550

Test Epoch: 122 
task: sign, mean loss: 0.98290, accuracy: 0.86391, avg. loss over tasks: 0.98290
Diversity Loss - Mean: -0.11152, Variance: 0.01380
Semantic Loss - Mean: 0.84006, Variance: 0.03631

Train Epoch: 123 
task: sign, mean loss: 0.00061, accuracy: 1.00000, avg. loss over tasks: 0.00061, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.11924, Variance: 0.01308
Semantic Loss - Mean: 0.00624, Variance: 0.00546

Test Epoch: 123 
task: sign, mean loss: 1.01207, accuracy: 0.85207, avg. loss over tasks: 1.01207
Diversity Loss - Mean: -0.11098, Variance: 0.01381
Semantic Loss - Mean: 0.87067, Variance: 0.03620

Train Epoch: 124 
task: sign, mean loss: 0.00126, accuracy: 1.00000, avg. loss over tasks: 0.00126, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.11872, Variance: 0.01310
Semantic Loss - Mean: 0.00833, Variance: 0.00542

Test Epoch: 124 
task: sign, mean loss: 0.91669, accuracy: 0.86982, avg. loss over tasks: 0.91669
Diversity Loss - Mean: -0.11100, Variance: 0.01383
Semantic Loss - Mean: 0.77278, Variance: 0.03603

Train Epoch: 125 
task: sign, mean loss: 0.00038, accuracy: 1.00000, avg. loss over tasks: 0.00038, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.11916, Variance: 0.01313
Semantic Loss - Mean: 0.00623, Variance: 0.00538

Test Epoch: 125 
task: sign, mean loss: 0.97098, accuracy: 0.86391, avg. loss over tasks: 0.97098
Diversity Loss - Mean: -0.11055, Variance: 0.01384
Semantic Loss - Mean: 0.83145, Variance: 0.03589

Train Epoch: 126 
task: sign, mean loss: 0.00073, accuracy: 1.00000, avg. loss over tasks: 0.00073, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.11860, Variance: 0.01316
Semantic Loss - Mean: 0.01226, Variance: 0.00536

Test Epoch: 126 
task: sign, mean loss: 0.87262, accuracy: 0.87574, avg. loss over tasks: 0.87262
Diversity Loss - Mean: -0.11172, Variance: 0.01386
Semantic Loss - Mean: 0.74072, Variance: 0.03572

Train Epoch: 127 
task: sign, mean loss: 0.00176, accuracy: 1.00000, avg. loss over tasks: 0.00176, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.11853, Variance: 0.01319
Semantic Loss - Mean: 0.00489, Variance: 0.00531

Test Epoch: 127 
task: sign, mean loss: 0.95678, accuracy: 0.86391, avg. loss over tasks: 0.95678
Diversity Loss - Mean: -0.11254, Variance: 0.01388
Semantic Loss - Mean: 0.80697, Variance: 0.03558

Train Epoch: 128 
task: sign, mean loss: 0.00023, accuracy: 1.00000, avg. loss over tasks: 0.00023, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.11914, Variance: 0.01322
Semantic Loss - Mean: 0.00458, Variance: 0.00527

Test Epoch: 128 
task: sign, mean loss: 1.02638, accuracy: 0.84024, avg. loss over tasks: 1.02638
Diversity Loss - Mean: -0.11192, Variance: 0.01389
Semantic Loss - Mean: 0.86949, Variance: 0.03544

Train Epoch: 129 
task: sign, mean loss: 0.00054, accuracy: 1.00000, avg. loss over tasks: 0.00054, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.11912, Variance: 0.01325
Semantic Loss - Mean: 0.00366, Variance: 0.00523

Test Epoch: 129 
task: sign, mean loss: 0.93417, accuracy: 0.85799, avg. loss over tasks: 0.93417
Diversity Loss - Mean: -0.11177, Variance: 0.01391
Semantic Loss - Mean: 0.79834, Variance: 0.03527

Train Epoch: 130 
task: sign, mean loss: 0.00081, accuracy: 1.00000, avg. loss over tasks: 0.00081, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.11955, Variance: 0.01328
Semantic Loss - Mean: 0.00563, Variance: 0.00519

Test Epoch: 130 
task: sign, mean loss: 0.97828, accuracy: 0.84615, avg. loss over tasks: 0.97828
Diversity Loss - Mean: -0.11151, Variance: 0.01392
Semantic Loss - Mean: 0.83137, Variance: 0.03512

Train Epoch: 131 
task: sign, mean loss: 0.00040, accuracy: 1.00000, avg. loss over tasks: 0.00040, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.11938, Variance: 0.01331
Semantic Loss - Mean: 0.00388, Variance: 0.00516

Test Epoch: 131 
task: sign, mean loss: 1.06194, accuracy: 0.85207, avg. loss over tasks: 1.06194
Diversity Loss - Mean: -0.11130, Variance: 0.01393
Semantic Loss - Mean: 0.88964, Variance: 0.03495

Train Epoch: 132 
task: sign, mean loss: 0.00049, accuracy: 1.00000, avg. loss over tasks: 0.00049, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.11987, Variance: 0.01333
Semantic Loss - Mean: 0.01104, Variance: 0.00514

Test Epoch: 132 
task: sign, mean loss: 1.03711, accuracy: 0.84615, avg. loss over tasks: 1.03711
Diversity Loss - Mean: -0.11178, Variance: 0.01395
Semantic Loss - Mean: 0.87708, Variance: 0.03481

Train Epoch: 133 
task: sign, mean loss: 0.00045, accuracy: 1.00000, avg. loss over tasks: 0.00045, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.12049, Variance: 0.01336
Semantic Loss - Mean: 0.00511, Variance: 0.00510

Test Epoch: 133 
task: sign, mean loss: 1.03243, accuracy: 0.84024, avg. loss over tasks: 1.03243
Diversity Loss - Mean: -0.11193, Variance: 0.01396
Semantic Loss - Mean: 0.87092, Variance: 0.03464

Train Epoch: 134 
task: sign, mean loss: 0.00023, accuracy: 1.00000, avg. loss over tasks: 0.00023, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.12000, Variance: 0.01339
Semantic Loss - Mean: 0.00239, Variance: 0.00506

Test Epoch: 134 
task: sign, mean loss: 1.00540, accuracy: 0.84615, avg. loss over tasks: 1.00540
Diversity Loss - Mean: -0.11253, Variance: 0.01398
Semantic Loss - Mean: 0.84906, Variance: 0.03446

Train Epoch: 135 
task: sign, mean loss: 0.00033, accuracy: 1.00000, avg. loss over tasks: 0.00033, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.11879, Variance: 0.01341
Semantic Loss - Mean: 0.00384, Variance: 0.00502

Test Epoch: 135 
task: sign, mean loss: 1.00585, accuracy: 0.85207, avg. loss over tasks: 1.00585
Diversity Loss - Mean: -0.11372, Variance: 0.01399
Semantic Loss - Mean: 0.85228, Variance: 0.03428

Train Epoch: 136 
task: sign, mean loss: 0.00016, accuracy: 1.00000, avg. loss over tasks: 0.00016, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.11970, Variance: 0.01344
Semantic Loss - Mean: 0.00721, Variance: 0.00499

Test Epoch: 136 
task: sign, mean loss: 0.94398, accuracy: 0.86391, avg. loss over tasks: 0.94398
Diversity Loss - Mean: -0.11261, Variance: 0.01401
Semantic Loss - Mean: 0.79714, Variance: 0.03410

Train Epoch: 137 
task: sign, mean loss: 0.00037, accuracy: 1.00000, avg. loss over tasks: 0.00037, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.11948, Variance: 0.01346
Semantic Loss - Mean: 0.00747, Variance: 0.00496

Test Epoch: 137 
task: sign, mean loss: 0.95592, accuracy: 0.86391, avg. loss over tasks: 0.95592
Diversity Loss - Mean: -0.11249, Variance: 0.01402
Semantic Loss - Mean: 0.80460, Variance: 0.03392

Train Epoch: 138 
task: sign, mean loss: 0.00296, accuracy: 1.00000, avg. loss over tasks: 0.00296, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.11935, Variance: 0.01349
Semantic Loss - Mean: 0.01009, Variance: 0.00493

Test Epoch: 138 
task: sign, mean loss: 0.94932, accuracy: 0.86391, avg. loss over tasks: 0.94932
Diversity Loss - Mean: -0.11251, Variance: 0.01403
Semantic Loss - Mean: 0.80898, Variance: 0.03375

Train Epoch: 139 
task: sign, mean loss: 0.00018, accuracy: 1.00000, avg. loss over tasks: 0.00018, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.11989, Variance: 0.01351
Semantic Loss - Mean: 0.00601, Variance: 0.00490

Test Epoch: 139 
task: sign, mean loss: 1.00419, accuracy: 0.85207, avg. loss over tasks: 1.00419
Diversity Loss - Mean: -0.11244, Variance: 0.01405
Semantic Loss - Mean: 0.85135, Variance: 0.03358

Train Epoch: 140 
task: sign, mean loss: 0.00052, accuracy: 1.00000, avg. loss over tasks: 0.00052, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.11986, Variance: 0.01354
Semantic Loss - Mean: 0.00906, Variance: 0.00487

Test Epoch: 140 
task: sign, mean loss: 0.96122, accuracy: 0.85799, avg. loss over tasks: 0.96122
Diversity Loss - Mean: -0.11351, Variance: 0.01406
Semantic Loss - Mean: 0.81776, Variance: 0.03341

Train Epoch: 141 
task: sign, mean loss: 0.00021, accuracy: 1.00000, avg. loss over tasks: 0.00021, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.11931, Variance: 0.01356
Semantic Loss - Mean: 0.00414, Variance: 0.00484

Test Epoch: 141 
task: sign, mean loss: 0.94309, accuracy: 0.86391, avg. loss over tasks: 0.94309
Diversity Loss - Mean: -0.11327, Variance: 0.01408
Semantic Loss - Mean: 0.79590, Variance: 0.03324

Train Epoch: 142 
task: sign, mean loss: 0.00030, accuracy: 1.00000, avg. loss over tasks: 0.00030, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.11910, Variance: 0.01359
Semantic Loss - Mean: 0.00742, Variance: 0.00481

Test Epoch: 142 
task: sign, mean loss: 0.90765, accuracy: 0.87574, avg. loss over tasks: 0.90765
Diversity Loss - Mean: -0.11394, Variance: 0.01409
Semantic Loss - Mean: 0.76244, Variance: 0.03307

Train Epoch: 143 
task: sign, mean loss: 0.00035, accuracy: 1.00000, avg. loss over tasks: 0.00035, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.12098, Variance: 0.01361
Semantic Loss - Mean: 0.00537, Variance: 0.00477

Test Epoch: 143 
task: sign, mean loss: 0.95258, accuracy: 0.85799, avg. loss over tasks: 0.95258
Diversity Loss - Mean: -0.11308, Variance: 0.01410
Semantic Loss - Mean: 0.81377, Variance: 0.03291

Train Epoch: 144 
task: sign, mean loss: 0.00289, accuracy: 1.00000, avg. loss over tasks: 0.00289, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.12015, Variance: 0.01363
Semantic Loss - Mean: 0.00476, Variance: 0.00474

Test Epoch: 144 
task: sign, mean loss: 0.95629, accuracy: 0.85799, avg. loss over tasks: 0.95629
Diversity Loss - Mean: -0.11318, Variance: 0.01412
Semantic Loss - Mean: 0.81436, Variance: 0.03276

Train Epoch: 145 
task: sign, mean loss: 0.00020, accuracy: 1.00000, avg. loss over tasks: 0.00020, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.11941, Variance: 0.01365
Semantic Loss - Mean: 0.00294, Variance: 0.00471

Test Epoch: 145 
task: sign, mean loss: 0.94769, accuracy: 0.85799, avg. loss over tasks: 0.94769
Diversity Loss - Mean: -0.11339, Variance: 0.01413
Semantic Loss - Mean: 0.80690, Variance: 0.03260

Train Epoch: 146 
task: sign, mean loss: 0.00022, accuracy: 1.00000, avg. loss over tasks: 0.00022, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.11988, Variance: 0.01368
Semantic Loss - Mean: 0.01214, Variance: 0.00469

Test Epoch: 146 
task: sign, mean loss: 1.07551, accuracy: 0.84024, avg. loss over tasks: 1.07551
Diversity Loss - Mean: -0.11217, Variance: 0.01414
Semantic Loss - Mean: 0.91511, Variance: 0.03246

Train Epoch: 147 
task: sign, mean loss: 0.00021, accuracy: 1.00000, avg. loss over tasks: 0.00021, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.11965, Variance: 0.01370
Semantic Loss - Mean: 0.00414, Variance: 0.00466

Test Epoch: 147 
task: sign, mean loss: 1.02855, accuracy: 0.84024, avg. loss over tasks: 1.02855
Diversity Loss - Mean: -0.11214, Variance: 0.01415
Semantic Loss - Mean: 0.88158, Variance: 0.03231

Train Epoch: 148 
task: sign, mean loss: 0.00110, accuracy: 1.00000, avg. loss over tasks: 0.00110, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.11856, Variance: 0.01372
Semantic Loss - Mean: 0.00448, Variance: 0.00463

Test Epoch: 148 
task: sign, mean loss: 1.01959, accuracy: 0.84615, avg. loss over tasks: 1.01959
Diversity Loss - Mean: -0.11286, Variance: 0.01416
Semantic Loss - Mean: 0.86856, Variance: 0.03217

Train Epoch: 149 
task: sign, mean loss: 0.00049, accuracy: 1.00000, avg. loss over tasks: 0.00049, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.11912, Variance: 0.01374
Semantic Loss - Mean: 0.01058, Variance: 0.00460

Test Epoch: 149 
task: sign, mean loss: 0.98026, accuracy: 0.85207, avg. loss over tasks: 0.98026
Diversity Loss - Mean: -0.11333, Variance: 0.01418
Semantic Loss - Mean: 0.82496, Variance: 0.03203

Train Epoch: 150 
task: sign, mean loss: 0.00032, accuracy: 1.00000, avg. loss over tasks: 0.00032, lr: 3e-07
Diversity Loss - Mean: -0.11988, Variance: 0.01376
Semantic Loss - Mean: 0.00285, Variance: 0.00457

Test Epoch: 150 
task: sign, mean loss: 0.99200, accuracy: 0.86391, avg. loss over tasks: 0.99200
Diversity Loss - Mean: -0.11344, Variance: 0.01419
Semantic Loss - Mean: 0.83780, Variance: 0.03188

