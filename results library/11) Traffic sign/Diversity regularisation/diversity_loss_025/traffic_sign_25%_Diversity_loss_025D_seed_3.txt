Used config:
{'B': 16,
 'B_seq': 16,
 'D': 512,
 'D_inner': 2048,
 'D_k': 64,
 'D_v': 64,
 'H': 8,
 'I': 32,
 'M': 10,
 'N': 192,
 'attention_map': False,
 'attn_dropout': 0.1,
 'data_dir': 'data/traffic/dsets',
 'dropout': 0.1,
 'eager': True,
 'enc_type': 'resnet18',
 'eps': 1e-06,
 'is_image': True,
 'lr': 0.0003,
 'mask_K': 0,
 'mask_p': 0,
 'n_chan_in': 3,
 'n_class': 4,
 'n_epoch': 150,
 'n_epoch_warmup': 10,
 'n_res_blocks': 4,
 'n_token': 1,
 'n_worker': 8,
 'patch_size': [100, 100],
 'patch_stride': [100, 100],
 'pin_memory': True,
 'pretrained': True,
 'seed': 0,
 'semantic_diversity_loss': True,
 'shuffle': True,
 'shuffle_style': 'batch',
 'tasks': {'task0': {'act_fn': 'softmax',
                     'id': 0,
                     'metric': 'accuracy',
                     'name': 'sign'}},
 'track_efficiency': False,
 'track_epoch': 0,
 'use_pos': False,
 'wd': 0.1}
Original dataset size: 1970
Target counts: {0: 121, 1: 19, 2: 30, 3: 14}
Actual counts after filtering: {0: 121, 1: 19, 2: 30, 3: 14}
Filtered dataset size after stratification: 184
Original dataset size: 1807
Target counts: {0: 112, 1: 21, 2: 20, 3: 16}
Actual counts after filtering: {0: 112, 1: 21, 2: 20, 3: 16}
Filtered dataset size after stratification: 169
/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
Train Epoch: 1 
task: sign, mean loss: 1.09058, accuracy: 0.63587, avg. loss over tasks: 1.09058, lr: 3e-05
Diversity Loss - Mean: -0.00977, Variance: 0.01050
Semantic Loss - Mean: 1.43142, Variance: 0.07245

/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test Epoch: 1 
task: sign, mean loss: 1.17764, accuracy: 0.66272, avg. loss over tasks: 1.17764
Diversity Loss - Mean: -0.02886, Variance: 0.01239
Semantic Loss - Mean: 1.16115, Variance: 0.05360

Train Epoch: 2 
task: sign, mean loss: 0.96582, accuracy: 0.67391, avg. loss over tasks: 0.96582, lr: 6e-05
Diversity Loss - Mean: -0.01540, Variance: 0.01046
Semantic Loss - Mean: 0.98039, Variance: 0.03920

Test Epoch: 2 
task: sign, mean loss: 1.11164, accuracy: 0.66272, avg. loss over tasks: 1.11164
Diversity Loss - Mean: -0.02578, Variance: 0.01201
Semantic Loss - Mean: 1.14989, Variance: 0.03274

Train Epoch: 3 
task: sign, mean loss: 0.78943, accuracy: 0.70109, avg. loss over tasks: 0.78943, lr: 8.999999999999999e-05
Diversity Loss - Mean: -0.02795, Variance: 0.01023
Semantic Loss - Mean: 0.99144, Variance: 0.02722

Test Epoch: 3 
task: sign, mean loss: 1.23254, accuracy: 0.57988, avg. loss over tasks: 1.23254
Diversity Loss - Mean: -0.04740, Variance: 0.01147
Semantic Loss - Mean: 1.10964, Variance: 0.03023

Train Epoch: 4 
task: sign, mean loss: 0.73481, accuracy: 0.72826, avg. loss over tasks: 0.73481, lr: 0.00012
Diversity Loss - Mean: -0.04607, Variance: 0.00986
Semantic Loss - Mean: 0.88402, Variance: 0.02098

Test Epoch: 4 
task: sign, mean loss: 1.50218, accuracy: 0.56213, avg. loss over tasks: 1.50218
Diversity Loss - Mean: -0.04191, Variance: 0.01095
Semantic Loss - Mean: 1.08325, Variance: 0.02482

Train Epoch: 5 
task: sign, mean loss: 0.81733, accuracy: 0.70109, avg. loss over tasks: 0.81733, lr: 0.00015
Diversity Loss - Mean: -0.04040, Variance: 0.00951
Semantic Loss - Mean: 0.82904, Variance: 0.01730

Test Epoch: 5 
task: sign, mean loss: 2.54280, accuracy: 0.27811, avg. loss over tasks: 2.54280
Diversity Loss - Mean: -0.01883, Variance: 0.01059
Semantic Loss - Mean: 1.39000, Variance: 0.02271

Train Epoch: 6 
task: sign, mean loss: 0.88285, accuracy: 0.70109, avg. loss over tasks: 0.88285, lr: 0.00017999999999999998
Diversity Loss - Mean: -0.06680, Variance: 0.00952
Semantic Loss - Mean: 0.88307, Variance: 0.01477

Test Epoch: 6 
task: sign, mean loss: 1.86798, accuracy: 0.32544, avg. loss over tasks: 1.86798
Diversity Loss - Mean: -0.04067, Variance: 0.01044
Semantic Loss - Mean: 1.38722, Variance: 0.02156

Train Epoch: 7 
task: sign, mean loss: 0.69211, accuracy: 0.75543, avg. loss over tasks: 0.69211, lr: 0.00020999999999999998
Diversity Loss - Mean: -0.05695, Variance: 0.00968
Semantic Loss - Mean: 0.67159, Variance: 0.01298

Test Epoch: 7 
task: sign, mean loss: 3.17410, accuracy: 0.15385, avg. loss over tasks: 3.17410
Diversity Loss - Mean: 0.01436, Variance: 0.01037
Semantic Loss - Mean: 2.24718, Variance: 0.02213

Train Epoch: 8 
task: sign, mean loss: 0.55069, accuracy: 0.77174, avg. loss over tasks: 0.55069, lr: 0.00024
Diversity Loss - Mean: -0.02827, Variance: 0.00957
Semantic Loss - Mean: 0.59597, Variance: 0.01186

Test Epoch: 8 
task: sign, mean loss: 2.55113, accuracy: 0.49704, avg. loss over tasks: 2.55113
Diversity Loss - Mean: 0.01205, Variance: 0.01034
Semantic Loss - Mean: 1.87188, Variance: 0.02595

Train Epoch: 9 
task: sign, mean loss: 0.85395, accuracy: 0.76630, avg. loss over tasks: 0.85395, lr: 0.00027
Diversity Loss - Mean: -0.02962, Variance: 0.00949
Semantic Loss - Mean: 0.74007, Variance: 0.01084

Test Epoch: 9 
task: sign, mean loss: 2.04415, accuracy: 0.34911, avg. loss over tasks: 2.04415
Diversity Loss - Mean: -0.02001, Variance: 0.01032
Semantic Loss - Mean: 1.81127, Variance: 0.02640

Train Epoch: 10 
task: sign, mean loss: 0.65388, accuracy: 0.78261, avg. loss over tasks: 0.65388, lr: 0.0003
Diversity Loss - Mean: -0.03576, Variance: 0.00939
Semantic Loss - Mean: 0.64893, Variance: 0.01016

Test Epoch: 10 
task: sign, mean loss: 4.11580, accuracy: 0.21302, avg. loss over tasks: 4.11580
Diversity Loss - Mean: 0.07066, Variance: 0.01046
Semantic Loss - Mean: 3.01745, Variance: 0.03057

Train Epoch: 11 
task: sign, mean loss: 0.54673, accuracy: 0.79348, avg. loss over tasks: 0.54673, lr: 0.0002999622730061346
Diversity Loss - Mean: -0.01603, Variance: 0.00928
Semantic Loss - Mean: 0.57466, Variance: 0.00964

Test Epoch: 11 
task: sign, mean loss: 1.82948, accuracy: 0.43195, avg. loss over tasks: 1.82948
Diversity Loss - Mean: 0.02053, Variance: 0.01091
Semantic Loss - Mean: 1.46164, Variance: 0.03595

Train Epoch: 12 
task: sign, mean loss: 0.30969, accuracy: 0.90761, avg. loss over tasks: 0.30969, lr: 0.000299849111021216
Diversity Loss - Mean: -0.00843, Variance: 0.00933
Semantic Loss - Mean: 0.39009, Variance: 0.00928

Test Epoch: 12 
task: sign, mean loss: 3.91708, accuracy: 0.30769, avg. loss over tasks: 3.91708
Diversity Loss - Mean: 0.12644, Variance: 0.01108
Semantic Loss - Mean: 2.58650, Variance: 0.04031

Train Epoch: 13 
task: sign, mean loss: 0.30858, accuracy: 0.90761, avg. loss over tasks: 0.30858, lr: 0.0002996605710257114
Diversity Loss - Mean: 0.02346, Variance: 0.00926
Semantic Loss - Mean: 0.39586, Variance: 0.00952

Test Epoch: 13 
task: sign, mean loss: 1.22761, accuracy: 0.71598, avg. loss over tasks: 1.22761
Diversity Loss - Mean: -0.00750, Variance: 0.01109
Semantic Loss - Mean: 1.00646, Variance: 0.03975

Train Epoch: 14 
task: sign, mean loss: 0.25908, accuracy: 0.88587, avg. loss over tasks: 0.25908, lr: 0.00029939674795518656
Diversity Loss - Mean: 0.02535, Variance: 0.00924
Semantic Loss - Mean: 0.31455, Variance: 0.00942

Test Epoch: 14 
task: sign, mean loss: 1.06874, accuracy: 0.61538, avg. loss over tasks: 1.06874
Diversity Loss - Mean: 0.05111, Variance: 0.01100
Semantic Loss - Mean: 1.13804, Variance: 0.04439

Train Epoch: 15 
task: sign, mean loss: 0.24524, accuracy: 0.90217, avg. loss over tasks: 0.24524, lr: 0.0002990577746525024
Diversity Loss - Mean: 0.00698, Variance: 0.00929
Semantic Loss - Mean: 0.30860, Variance: 0.01031

Test Epoch: 15 
task: sign, mean loss: 1.02398, accuracy: 0.73964, avg. loss over tasks: 1.02398
Diversity Loss - Mean: 0.01225, Variance: 0.01093
Semantic Loss - Mean: 0.95781, Variance: 0.04285

Train Epoch: 16 
task: sign, mean loss: 0.15608, accuracy: 0.94565, avg. loss over tasks: 0.15608, lr: 0.000298643821800925
Diversity Loss - Mean: 0.01416, Variance: 0.00930
Semantic Loss - Mean: 0.22000, Variance: 0.01039

Test Epoch: 16 
task: sign, mean loss: 1.43383, accuracy: 0.73373, avg. loss over tasks: 1.43383
Diversity Loss - Mean: 0.00010, Variance: 0.01110
Semantic Loss - Mean: 1.23712, Variance: 0.04182

Train Epoch: 17 
task: sign, mean loss: 0.11638, accuracy: 0.96196, avg. loss over tasks: 0.11638, lr: 0.0002981550978381814
Diversity Loss - Mean: 0.01036, Variance: 0.00932
Semantic Loss - Mean: 0.20593, Variance: 0.01073

Test Epoch: 17 
task: sign, mean loss: 0.92670, accuracy: 0.68047, avg. loss over tasks: 0.92670
Diversity Loss - Mean: 0.04222, Variance: 0.01099
Semantic Loss - Mean: 0.93493, Variance: 0.04261

Train Epoch: 18 
task: sign, mean loss: 0.20152, accuracy: 0.91848, avg. loss over tasks: 0.20152, lr: 0.00029759184885150465
Diversity Loss - Mean: 0.00648, Variance: 0.00938
Semantic Loss - Mean: 0.25063, Variance: 0.01072

Test Epoch: 18 
task: sign, mean loss: 1.03082, accuracy: 0.79290, avg. loss over tasks: 1.03082
Diversity Loss - Mean: 0.00203, Variance: 0.01102
Semantic Loss - Mean: 0.89998, Variance: 0.04220

Train Epoch: 19 
task: sign, mean loss: 0.15918, accuracy: 0.92935, avg. loss over tasks: 0.15918, lr: 0.0002969543584537218
Diversity Loss - Mean: -0.00492, Variance: 0.00944
Semantic Loss - Mean: 0.20311, Variance: 0.01064

Test Epoch: 19 
task: sign, mean loss: 0.77066, accuracy: 0.79882, avg. loss over tasks: 0.77066
Diversity Loss - Mean: 0.00155, Variance: 0.01099
Semantic Loss - Mean: 0.77459, Variance: 0.04155

Train Epoch: 20 
task: sign, mean loss: 0.21009, accuracy: 0.93478, avg. loss over tasks: 0.21009, lr: 0.0002962429476404462
Diversity Loss - Mean: -0.01775, Variance: 0.00949
Semantic Loss - Mean: 0.26201, Variance: 0.01065

Test Epoch: 20 
task: sign, mean loss: 2.56716, accuracy: 0.40237, avg. loss over tasks: 2.56716
Diversity Loss - Mean: 0.07775, Variance: 0.01108
Semantic Loss - Mean: 1.97023, Variance: 0.04406

Train Epoch: 21 
task: sign, mean loss: 0.20724, accuracy: 0.91848, avg. loss over tasks: 0.20724, lr: 0.00029545797462844647
Diversity Loss - Mean: -0.02127, Variance: 0.00952
Semantic Loss - Mean: 0.27242, Variance: 0.01059

Test Epoch: 21 
task: sign, mean loss: 1.68604, accuracy: 0.68639, avg. loss over tasks: 1.68604
Diversity Loss - Mean: 0.03207, Variance: 0.01119
Semantic Loss - Mean: 1.42988, Variance: 0.04354

Train Epoch: 22 
task: sign, mean loss: 0.25422, accuracy: 0.90761, avg. loss over tasks: 0.25422, lr: 0.0002945998346752736
Diversity Loss - Mean: -0.02928, Variance: 0.00953
Semantic Loss - Mean: 0.33336, Variance: 0.01084

Test Epoch: 22 
task: sign, mean loss: 1.88849, accuracy: 0.54438, avg. loss over tasks: 1.88849
Diversity Loss - Mean: 0.00652, Variance: 0.01133
Semantic Loss - Mean: 1.64381, Variance: 0.04380

Train Epoch: 23 
task: sign, mean loss: 0.24625, accuracy: 0.92391, avg. loss over tasks: 0.24625, lr: 0.0002936689598802368
Diversity Loss - Mean: -0.04132, Variance: 0.00958
Semantic Loss - Mean: 0.30431, Variance: 0.01073

Test Epoch: 23 
task: sign, mean loss: 1.22762, accuracy: 0.63314, avg. loss over tasks: 1.22762
Diversity Loss - Mean: -0.03561, Variance: 0.01143
Semantic Loss - Mean: 1.17964, Variance: 0.04363

Train Epoch: 24 
task: sign, mean loss: 0.14891, accuracy: 0.94565, avg. loss over tasks: 0.14891, lr: 0.00029266581896682876
Diversity Loss - Mean: -0.03812, Variance: 0.00961
Semantic Loss - Mean: 0.21736, Variance: 0.01065

Test Epoch: 24 
task: sign, mean loss: 1.63607, accuracy: 0.53254, avg. loss over tasks: 1.63607
Diversity Loss - Mean: 0.02517, Variance: 0.01145
Semantic Loss - Mean: 1.32748, Variance: 0.04437

Train Epoch: 25 
task: sign, mean loss: 0.07682, accuracy: 0.98370, avg. loss over tasks: 0.07682, lr: 0.00029159091704670885
Diversity Loss - Mean: -0.03461, Variance: 0.00964
Semantic Loss - Mean: 0.16216, Variance: 0.01076

Test Epoch: 25 
task: sign, mean loss: 1.71460, accuracy: 0.49704, avg. loss over tasks: 1.71460
Diversity Loss - Mean: 0.01708, Variance: 0.01148
Semantic Loss - Mean: 1.56552, Variance: 0.04879

Train Epoch: 26 
task: sign, mean loss: 0.18407, accuracy: 0.94022, avg. loss over tasks: 0.18407, lr: 0.00029044479536536455
Diversity Loss - Mean: -0.03654, Variance: 0.00965
Semantic Loss - Mean: 0.22759, Variance: 0.01074

Test Epoch: 26 
task: sign, mean loss: 1.82350, accuracy: 0.52071, avg. loss over tasks: 1.82350
Diversity Loss - Mean: 0.00379, Variance: 0.01151
Semantic Loss - Mean: 1.51617, Variance: 0.04909

Train Epoch: 27 
task: sign, mean loss: 0.26082, accuracy: 0.89130, avg. loss over tasks: 0.26082, lr: 0.000289228031029578
Diversity Loss - Mean: -0.04233, Variance: 0.00968
Semantic Loss - Mean: 0.34428, Variance: 0.01144

Test Epoch: 27 
task: sign, mean loss: 1.03150, accuracy: 0.63314, avg. loss over tasks: 1.03150
Diversity Loss - Mean: -0.00123, Variance: 0.01149
Semantic Loss - Mean: 1.06354, Variance: 0.04861

Train Epoch: 28 
task: sign, mean loss: 0.12840, accuracy: 0.94022, avg. loss over tasks: 0.12840, lr: 0.0002879412367168349
Diversity Loss - Mean: -0.04366, Variance: 0.00972
Semantic Loss - Mean: 0.19874, Variance: 0.01144

Test Epoch: 28 
task: sign, mean loss: 1.42359, accuracy: 0.52071, avg. loss over tasks: 1.42359
Diversity Loss - Mean: 0.00348, Variance: 0.01150
Semantic Loss - Mean: 1.30376, Variance: 0.04986

Train Epoch: 29 
task: sign, mean loss: 0.08288, accuracy: 0.97283, avg. loss over tasks: 0.08288, lr: 0.00028658506036682353
Diversity Loss - Mean: -0.04635, Variance: 0.00978
Semantic Loss - Mean: 0.15215, Variance: 0.01145

Test Epoch: 29 
task: sign, mean loss: 1.13775, accuracy: 0.68639, avg. loss over tasks: 1.13775
Diversity Loss - Mean: -0.02327, Variance: 0.01155
Semantic Loss - Mean: 1.04897, Variance: 0.05152

Train Epoch: 30 
task: sign, mean loss: 0.08892, accuracy: 0.96196, avg. loss over tasks: 0.08892, lr: 0.00028516018485517746
Diversity Loss - Mean: -0.04501, Variance: 0.00982
Semantic Loss - Mean: 0.16014, Variance: 0.01137

Test Epoch: 30 
task: sign, mean loss: 1.06105, accuracy: 0.79882, avg. loss over tasks: 1.06105
Diversity Loss - Mean: -0.02074, Variance: 0.01156
Semantic Loss - Mean: 1.15115, Variance: 0.05141

Train Epoch: 31 
task: sign, mean loss: 0.07145, accuracy: 0.97283, avg. loss over tasks: 0.07145, lr: 0.00028366732764962686
Diversity Loss - Mean: -0.04373, Variance: 0.00985
Semantic Loss - Mean: 0.16942, Variance: 0.01134

Test Epoch: 31 
task: sign, mean loss: 0.78449, accuracy: 0.85207, avg. loss over tasks: 0.78449
Diversity Loss - Mean: -0.04827, Variance: 0.01164
Semantic Loss - Mean: 0.70708, Variance: 0.05022

Train Epoch: 32 
task: sign, mean loss: 0.13462, accuracy: 0.93478, avg. loss over tasks: 0.13462, lr: 0.00028210724044873213
Diversity Loss - Mean: -0.04249, Variance: 0.00990
Semantic Loss - Mean: 0.22301, Variance: 0.01155

Test Epoch: 32 
task: sign, mean loss: 1.85395, accuracy: 0.49112, avg. loss over tasks: 1.85395
Diversity Loss - Mean: -0.00677, Variance: 0.01168
Semantic Loss - Mean: 1.48909, Variance: 0.05629

Train Epoch: 33 
task: sign, mean loss: 0.05700, accuracy: 0.97283, avg. loss over tasks: 0.05700, lr: 0.00028048070880338095
Diversity Loss - Mean: -0.04314, Variance: 0.00993
Semantic Loss - Mean: 0.14116, Variance: 0.01157

Test Epoch: 33 
task: sign, mean loss: 1.45393, accuracy: 0.58580, avg. loss over tasks: 1.45393
Diversity Loss - Mean: -0.01060, Variance: 0.01167
Semantic Loss - Mean: 0.95443, Variance: 0.05877

Train Epoch: 34 
task: sign, mean loss: 0.27670, accuracy: 0.94022, avg. loss over tasks: 0.27670, lr: 0.00027878855172123963
Diversity Loss - Mean: -0.05036, Variance: 0.00998
Semantic Loss - Mean: 0.29085, Variance: 0.01171

Test Epoch: 34 
task: sign, mean loss: 2.30192, accuracy: 0.69822, avg. loss over tasks: 2.30192
Diversity Loss - Mean: -0.09201, Variance: 0.01181
Semantic Loss - Mean: 1.98655, Variance: 0.06116

Train Epoch: 35 
task: sign, mean loss: 0.23404, accuracy: 0.92935, avg. loss over tasks: 0.23404, lr: 0.00027703162125435835
Diversity Loss - Mean: -0.06849, Variance: 0.01006
Semantic Loss - Mean: 0.30698, Variance: 0.01179

Test Epoch: 35 
task: sign, mean loss: 0.90440, accuracy: 0.74556, avg. loss over tasks: 0.90440
Diversity Loss - Mean: -0.07297, Variance: 0.01194
Semantic Loss - Mean: 0.95570, Variance: 0.06044

Train Epoch: 36 
task: sign, mean loss: 0.08878, accuracy: 0.96739, avg. loss over tasks: 0.08878, lr: 0.00027521080207013716
Diversity Loss - Mean: -0.06327, Variance: 0.01013
Semantic Loss - Mean: 0.13847, Variance: 0.01152

Test Epoch: 36 
task: sign, mean loss: 1.38474, accuracy: 0.56805, avg. loss over tasks: 1.38474
Diversity Loss - Mean: -0.01155, Variance: 0.01196
Semantic Loss - Mean: 1.07832, Variance: 0.06020

Train Epoch: 37 
task: sign, mean loss: 0.09296, accuracy: 0.95652, avg. loss over tasks: 0.09296, lr: 0.0002733270110058693
Diversity Loss - Mean: -0.06030, Variance: 0.01021
Semantic Loss - Mean: 0.16979, Variance: 0.01151

Test Epoch: 37 
task: sign, mean loss: 1.20201, accuracy: 0.54438, avg. loss over tasks: 1.20201
Diversity Loss - Mean: 0.02940, Variance: 0.01195
Semantic Loss - Mean: 1.16098, Variance: 0.06218

Train Epoch: 38 
task: sign, mean loss: 0.06257, accuracy: 0.98370, avg. loss over tasks: 0.06257, lr: 0.00027138119660708587
Diversity Loss - Mean: -0.05355, Variance: 0.01029
Semantic Loss - Mean: 0.10917, Variance: 0.01136

Test Epoch: 38 
task: sign, mean loss: 1.94581, accuracy: 0.46154, avg. loss over tasks: 1.94581
Diversity Loss - Mean: 0.00600, Variance: 0.01193
Semantic Loss - Mean: 1.75358, Variance: 0.06339

Train Epoch: 39 
task: sign, mean loss: 0.01041, accuracy: 1.00000, avg. loss over tasks: 0.01041, lr: 0.0002693743386499349
Diversity Loss - Mean: -0.04124, Variance: 0.01033
Semantic Loss - Mean: 0.05354, Variance: 0.01124

Test Epoch: 39 
task: sign, mean loss: 1.20105, accuracy: 0.68047, avg. loss over tasks: 1.20105
Diversity Loss - Mean: -0.03221, Variance: 0.01197
Semantic Loss - Mean: 0.94576, Variance: 0.06360

Train Epoch: 40 
task: sign, mean loss: 0.02882, accuracy: 0.98913, avg. loss over tasks: 0.02882, lr: 0.00026730744764783427
Diversity Loss - Mean: -0.04508, Variance: 0.01038
Semantic Loss - Mean: 0.08187, Variance: 0.01114

Test Epoch: 40 
task: sign, mean loss: 1.12867, accuracy: 0.72189, avg. loss over tasks: 1.12867
Diversity Loss - Mean: -0.02868, Variance: 0.01201
Semantic Loss - Mean: 1.09938, Variance: 0.06359

Train Epoch: 41 
task: sign, mean loss: 0.02384, accuracy: 0.98913, avg. loss over tasks: 0.02384, lr: 0.00026518156434264794
Diversity Loss - Mean: -0.04144, Variance: 0.01041
Semantic Loss - Mean: 0.08713, Variance: 0.01102

Test Epoch: 41 
task: sign, mean loss: 1.18715, accuracy: 0.67456, avg. loss over tasks: 1.18715
Diversity Loss - Mean: -0.02341, Variance: 0.01205
Semantic Loss - Mean: 1.19258, Variance: 0.06403

Train Epoch: 42 
task: sign, mean loss: 0.09657, accuracy: 0.98370, avg. loss over tasks: 0.09657, lr: 0.0002629977591806411
Diversity Loss - Mean: -0.04928, Variance: 0.01048
Semantic Loss - Mean: 0.14844, Variance: 0.01098

Test Epoch: 42 
task: sign, mean loss: 0.96982, accuracy: 0.79290, avg. loss over tasks: 0.96982
Diversity Loss - Mean: -0.03712, Variance: 0.01210
Semantic Loss - Mean: 0.98640, Variance: 0.06449

Train Epoch: 43 
task: sign, mean loss: 0.03276, accuracy: 0.98913, avg. loss over tasks: 0.03276, lr: 0.000260757131773478
Diversity Loss - Mean: -0.05653, Variance: 0.01053
Semantic Loss - Mean: 0.08477, Variance: 0.01098

Test Epoch: 43 
task: sign, mean loss: 1.02455, accuracy: 0.68639, avg. loss over tasks: 1.02455
Diversity Loss - Mean: -0.03324, Variance: 0.01214
Semantic Loss - Mean: 0.93506, Variance: 0.06598

Train Epoch: 44 
task: sign, mean loss: 0.02581, accuracy: 0.99457, avg. loss over tasks: 0.02581, lr: 0.0002584608103445346
Diversity Loss - Mean: -0.05496, Variance: 0.01057
Semantic Loss - Mean: 0.05740, Variance: 0.01082

Test Epoch: 44 
task: sign, mean loss: 1.11775, accuracy: 0.73964, avg. loss over tasks: 1.11775
Diversity Loss - Mean: -0.03786, Variance: 0.01216
Semantic Loss - Mean: 1.01862, Variance: 0.06545

Train Epoch: 45 
task: sign, mean loss: 0.06356, accuracy: 0.98370, avg. loss over tasks: 0.06356, lr: 0.0002561099511608041
Diversity Loss - Mean: -0.05768, Variance: 0.01061
Semantic Loss - Mean: 0.09883, Variance: 0.01081

Test Epoch: 45 
task: sign, mean loss: 0.82247, accuracy: 0.79882, avg. loss over tasks: 0.82247
Diversity Loss - Mean: -0.02297, Variance: 0.01216
Semantic Loss - Mean: 0.94201, Variance: 0.06677

Train Epoch: 46 
task: sign, mean loss: 0.03098, accuracy: 0.98913, avg. loss over tasks: 0.03098, lr: 0.00025370573795068164
Diversity Loss - Mean: -0.05527, Variance: 0.01064
Semantic Loss - Mean: 0.07963, Variance: 0.01076

Test Epoch: 46 
task: sign, mean loss: 0.80591, accuracy: 0.80473, avg. loss over tasks: 0.80591
Diversity Loss - Mean: -0.01994, Variance: 0.01217
Semantic Loss - Mean: 0.97451, Variance: 0.06669

Train Epoch: 47 
task: sign, mean loss: 0.06323, accuracy: 0.97826, avg. loss over tasks: 0.06323, lr: 0.0002512493813079214
Diversity Loss - Mean: -0.05943, Variance: 0.01068
Semantic Loss - Mean: 0.13327, Variance: 0.01084

Test Epoch: 47 
task: sign, mean loss: 0.50534, accuracy: 0.89349, avg. loss over tasks: 0.50534
Diversity Loss - Mean: -0.04829, Variance: 0.01220
Semantic Loss - Mean: 0.45612, Variance: 0.06600

Train Epoch: 48 
task: sign, mean loss: 0.05336, accuracy: 0.97826, avg. loss over tasks: 0.05336, lr: 0.0002487421180820659
Diversity Loss - Mean: -0.06687, Variance: 0.01073
Semantic Loss - Mean: 0.09577, Variance: 0.01078

Test Epoch: 48 
task: sign, mean loss: 0.70288, accuracy: 0.86982, avg. loss over tasks: 0.70288
Diversity Loss - Mean: -0.05893, Variance: 0.01222
Semantic Loss - Mean: 0.61290, Variance: 0.06532

Train Epoch: 49 
task: sign, mean loss: 0.05361, accuracy: 0.97826, avg. loss over tasks: 0.05361, lr: 0.0002461852107556558
Diversity Loss - Mean: -0.06797, Variance: 0.01079
Semantic Loss - Mean: 0.10271, Variance: 0.01082

Test Epoch: 49 
task: sign, mean loss: 0.63193, accuracy: 0.85799, avg. loss over tasks: 0.63193
Diversity Loss - Mean: -0.03508, Variance: 0.01227
Semantic Loss - Mean: 0.69604, Variance: 0.06484

Train Epoch: 50 
task: sign, mean loss: 0.05519, accuracy: 0.97826, avg. loss over tasks: 0.05519, lr: 0.00024357994680853121
Diversity Loss - Mean: -0.06584, Variance: 0.01083
Semantic Loss - Mean: 0.10439, Variance: 0.01073

Test Epoch: 50 
task: sign, mean loss: 0.60047, accuracy: 0.86982, avg. loss over tasks: 0.60047
Diversity Loss - Mean: -0.06065, Variance: 0.01234
Semantic Loss - Mean: 0.63204, Variance: 0.06400

Train Epoch: 51 
task: sign, mean loss: 0.09875, accuracy: 0.95109, avg. loss over tasks: 0.09875, lr: 0.00024092763806954684
Diversity Loss - Mean: -0.06970, Variance: 0.01086
Semantic Loss - Mean: 0.19938, Variance: 0.01088

Test Epoch: 51 
task: sign, mean loss: 0.58313, accuracy: 0.89941, avg. loss over tasks: 0.58313
Diversity Loss - Mean: -0.06459, Variance: 0.01238
Semantic Loss - Mean: 0.59410, Variance: 0.06315

Train Epoch: 52 
task: sign, mean loss: 0.05098, accuracy: 0.98370, avg. loss over tasks: 0.05098, lr: 0.00023822962005602707
Diversity Loss - Mean: -0.07164, Variance: 0.01090
Semantic Loss - Mean: 0.11652, Variance: 0.01086

Test Epoch: 52 
task: sign, mean loss: 0.34060, accuracy: 0.88757, avg. loss over tasks: 0.34060
Diversity Loss - Mean: -0.06397, Variance: 0.01243
Semantic Loss - Mean: 0.33903, Variance: 0.06205

Train Epoch: 53 
task: sign, mean loss: 0.09222, accuracy: 0.98370, avg. loss over tasks: 0.09222, lr: 0.00023548725130129248
Diversity Loss - Mean: -0.07130, Variance: 0.01092
Semantic Loss - Mean: 0.14192, Variance: 0.01086

Test Epoch: 53 
task: sign, mean loss: 0.67806, accuracy: 0.86391, avg. loss over tasks: 0.67806
Diversity Loss - Mean: -0.07133, Variance: 0.01249
Semantic Loss - Mean: 0.66425, Variance: 0.06113

Train Epoch: 54 
task: sign, mean loss: 0.02624, accuracy: 0.99457, avg. loss over tasks: 0.02624, lr: 0.00023270191267059755
Diversity Loss - Mean: -0.07660, Variance: 0.01095
Semantic Loss - Mean: 0.08535, Variance: 0.01097

Test Epoch: 54 
task: sign, mean loss: 0.97827, accuracy: 0.78698, avg. loss over tasks: 0.97827
Diversity Loss - Mean: -0.05998, Variance: 0.01252
Semantic Loss - Mean: 0.83041, Variance: 0.06063

Train Epoch: 55 
task: sign, mean loss: 0.02710, accuracy: 0.98913, avg. loss over tasks: 0.02710, lr: 0.00022987500666582316
Diversity Loss - Mean: -0.07488, Variance: 0.01099
Semantic Loss - Mean: 0.05972, Variance: 0.01083

Test Epoch: 55 
task: sign, mean loss: 0.63349, accuracy: 0.88757, avg. loss over tasks: 0.63349
Diversity Loss - Mean: -0.08761, Variance: 0.01259
Semantic Loss - Mean: 0.51400, Variance: 0.06009

Train Epoch: 56 
task: sign, mean loss: 0.03098, accuracy: 0.99457, avg. loss over tasks: 0.03098, lr: 0.00022700795671927503
Diversity Loss - Mean: -0.07816, Variance: 0.01102
Semantic Loss - Mean: 0.06444, Variance: 0.01077

Test Epoch: 56 
task: sign, mean loss: 0.97066, accuracy: 0.82840, avg. loss over tasks: 0.97066
Diversity Loss - Mean: -0.08223, Variance: 0.01267
Semantic Loss - Mean: 0.95446, Variance: 0.05960

Train Epoch: 57 
task: sign, mean loss: 0.07002, accuracy: 0.97826, avg. loss over tasks: 0.07002, lr: 0.00022410220647694235
Diversity Loss - Mean: -0.08382, Variance: 0.01106
Semantic Loss - Mean: 0.11168, Variance: 0.01070

Test Epoch: 57 
task: sign, mean loss: 1.38592, accuracy: 0.79290, avg. loss over tasks: 1.38592
Diversity Loss - Mean: -0.08062, Variance: 0.01270
Semantic Loss - Mean: 1.19910, Variance: 0.05902

Train Epoch: 58 
task: sign, mean loss: 0.03956, accuracy: 0.99457, avg. loss over tasks: 0.03956, lr: 0.00022115921907157884
Diversity Loss - Mean: -0.08219, Variance: 0.01108
Semantic Loss - Mean: 0.08252, Variance: 0.01064

Test Epoch: 58 
task: sign, mean loss: 1.36483, accuracy: 0.79290, avg. loss over tasks: 1.36483
Diversity Loss - Mean: -0.08187, Variance: 0.01273
Semantic Loss - Mean: 1.08798, Variance: 0.05876

Train Epoch: 59 
task: sign, mean loss: 0.06130, accuracy: 0.98370, avg. loss over tasks: 0.06130, lr: 0.00021818047638597106
Diversity Loss - Mean: -0.07998, Variance: 0.01110
Semantic Loss - Mean: 0.09983, Variance: 0.01062

Test Epoch: 59 
task: sign, mean loss: 1.02428, accuracy: 0.77515, avg. loss over tasks: 1.02428
Diversity Loss - Mean: -0.06764, Variance: 0.01276
Semantic Loss - Mean: 0.87087, Variance: 0.05837

Train Epoch: 60 
task: sign, mean loss: 0.11982, accuracy: 0.96739, avg. loss over tasks: 0.11982, lr: 0.00021516747830676604
Diversity Loss - Mean: -0.08494, Variance: 0.01112
Semantic Loss - Mean: 0.13548, Variance: 0.01058

Test Epoch: 60 
task: sign, mean loss: 0.48493, accuracy: 0.83432, avg. loss over tasks: 0.48493
Diversity Loss - Mean: -0.07051, Variance: 0.01278
Semantic Loss - Mean: 0.49293, Variance: 0.05811

Train Epoch: 61 
task: sign, mean loss: 0.04363, accuracy: 0.97826, avg. loss over tasks: 0.04363, lr: 0.0002121217419692331
Diversity Loss - Mean: -0.09175, Variance: 0.01116
Semantic Loss - Mean: 0.12015, Variance: 0.01070

Test Epoch: 61 
task: sign, mean loss: 0.61901, accuracy: 0.83432, avg. loss over tasks: 0.61901
Diversity Loss - Mean: -0.07140, Variance: 0.01281
Semantic Loss - Mean: 0.63243, Variance: 0.05753

Train Epoch: 62 
task: sign, mean loss: 0.06591, accuracy: 0.98370, avg. loss over tasks: 0.06591, lr: 0.00020904480099334042
Diversity Loss - Mean: -0.08930, Variance: 0.01119
Semantic Loss - Mean: 0.11869, Variance: 0.01080

Test Epoch: 62 
task: sign, mean loss: 0.56324, accuracy: 0.84615, avg. loss over tasks: 0.56324
Diversity Loss - Mean: -0.08303, Variance: 0.01286
Semantic Loss - Mean: 0.49506, Variance: 0.05692

Train Epoch: 63 
task: sign, mean loss: 0.03904, accuracy: 0.98913, avg. loss over tasks: 0.03904, lr: 0.00020593820471153146
Diversity Loss - Mean: -0.08980, Variance: 0.01122
Semantic Loss - Mean: 0.07355, Variance: 0.01067

Test Epoch: 63 
task: sign, mean loss: 0.57878, accuracy: 0.85207, avg. loss over tasks: 0.57878
Diversity Loss - Mean: -0.08899, Variance: 0.01290
Semantic Loss - Mean: 0.55867, Variance: 0.05626

Train Epoch: 64 
task: sign, mean loss: 0.07424, accuracy: 0.97283, avg. loss over tasks: 0.07424, lr: 0.0002028035173885892
Diversity Loss - Mean: -0.08868, Variance: 0.01125
Semantic Loss - Mean: 0.10903, Variance: 0.01063

Test Epoch: 64 
task: sign, mean loss: 0.78416, accuracy: 0.82840, avg. loss over tasks: 0.78416
Diversity Loss - Mean: -0.08414, Variance: 0.01294
Semantic Loss - Mean: 0.74900, Variance: 0.05574

Train Epoch: 65 
task: sign, mean loss: 0.05129, accuracy: 0.97283, avg. loss over tasks: 0.05129, lr: 0.00019964231743398178
Diversity Loss - Mean: -0.09269, Variance: 0.01129
Semantic Loss - Mean: 0.09841, Variance: 0.01060

Test Epoch: 65 
task: sign, mean loss: 0.51012, accuracy: 0.85799, avg. loss over tasks: 0.51012
Diversity Loss - Mean: -0.08601, Variance: 0.01296
Semantic Loss - Mean: 0.60036, Variance: 0.05528

Train Epoch: 66 
task: sign, mean loss: 0.06101, accuracy: 0.98370, avg. loss over tasks: 0.06101, lr: 0.00019645619660708585
Diversity Loss - Mean: -0.08996, Variance: 0.01132
Semantic Loss - Mean: 0.10426, Variance: 0.01059

Test Epoch: 66 
task: sign, mean loss: 1.44443, accuracy: 0.82249, avg. loss over tasks: 1.44443
Diversity Loss - Mean: -0.06628, Variance: 0.01297
Semantic Loss - Mean: 1.57437, Variance: 0.05612

Train Epoch: 67 
task: sign, mean loss: 0.12370, accuracy: 0.97826, avg. loss over tasks: 0.12370, lr: 0.00019324675921568777
Diversity Loss - Mean: -0.09146, Variance: 0.01136
Semantic Loss - Mean: 0.15633, Variance: 0.01057

Test Epoch: 67 
task: sign, mean loss: 1.50552, accuracy: 0.59172, avg. loss over tasks: 1.50552
Diversity Loss - Mean: -0.06369, Variance: 0.01297
Semantic Loss - Mean: 1.32101, Variance: 0.05664

Train Epoch: 68 
task: sign, mean loss: 0.15004, accuracy: 0.95109, avg. loss over tasks: 0.15004, lr: 0.00019001562130816624
Diversity Loss - Mean: -0.08837, Variance: 0.01138
Semantic Loss - Mean: 0.20740, Variance: 0.01066

Test Epoch: 68 
task: sign, mean loss: 0.95078, accuracy: 0.79882, avg. loss over tasks: 0.95078
Diversity Loss - Mean: -0.07951, Variance: 0.01299
Semantic Loss - Mean: 0.92380, Variance: 0.05698

Train Epoch: 69 
task: sign, mean loss: 0.02132, accuracy: 0.98913, avg. loss over tasks: 0.02132, lr: 0.0001867644098597634
Diversity Loss - Mean: -0.09330, Variance: 0.01142
Semantic Loss - Mean: 0.09338, Variance: 0.01067

Test Epoch: 69 
task: sign, mean loss: 0.77408, accuracy: 0.89349, avg. loss over tasks: 0.77408
Diversity Loss - Mean: -0.07468, Variance: 0.01301
Semantic Loss - Mean: 1.06266, Variance: 0.05825

Train Epoch: 70 
task: sign, mean loss: 0.05721, accuracy: 0.99457, avg. loss over tasks: 0.05721, lr: 0.00018349476195335369
Diversity Loss - Mean: -0.09166, Variance: 0.01145
Semantic Loss - Mean: 0.08983, Variance: 0.01055

Test Epoch: 70 
task: sign, mean loss: 0.60695, accuracy: 0.88757, avg. loss over tasks: 0.60695
Diversity Loss - Mean: -0.08728, Variance: 0.01303
Semantic Loss - Mean: 0.63605, Variance: 0.05791

Train Epoch: 71 
task: sign, mean loss: 0.01105, accuracy: 1.00000, avg. loss over tasks: 0.01105, lr: 0.00018020832395512342
Diversity Loss - Mean: -0.09378, Variance: 0.01148
Semantic Loss - Mean: 0.06306, Variance: 0.01057

Test Epoch: 71 
task: sign, mean loss: 0.46460, accuracy: 0.92308, avg. loss over tasks: 0.46460
Diversity Loss - Mean: -0.09094, Variance: 0.01305
Semantic Loss - Mean: 0.47408, Variance: 0.05741

Train Epoch: 72 
task: sign, mean loss: 0.00319, accuracy: 1.00000, avg. loss over tasks: 0.00319, lr: 0.00017690675068557572
Diversity Loss - Mean: -0.09324, Variance: 0.01151
Semantic Loss - Mean: 0.04816, Variance: 0.01048

Test Epoch: 72 
task: sign, mean loss: 0.52523, accuracy: 0.91716, avg. loss over tasks: 0.52523
Diversity Loss - Mean: -0.08844, Variance: 0.01308
Semantic Loss - Mean: 0.61366, Variance: 0.05705

Train Epoch: 73 
task: sign, mean loss: 0.01415, accuracy: 0.99457, avg. loss over tasks: 0.01415, lr: 0.00017359170458627858
Diversity Loss - Mean: -0.09530, Variance: 0.01155
Semantic Loss - Mean: 0.05308, Variance: 0.01043

Test Epoch: 73 
task: sign, mean loss: 0.51848, accuracy: 0.91716, avg. loss over tasks: 0.51848
Diversity Loss - Mean: -0.09151, Variance: 0.01312
Semantic Loss - Mean: 0.52006, Variance: 0.05639

Train Epoch: 74 
task: sign, mean loss: 0.01756, accuracy: 1.00000, avg. loss over tasks: 0.01756, lr: 0.00017026485488277568
Diversity Loss - Mean: -0.09241, Variance: 0.01157
Semantic Loss - Mean: 0.05413, Variance: 0.01032

Test Epoch: 74 
task: sign, mean loss: 0.54629, accuracy: 0.89349, avg. loss over tasks: 0.54629
Diversity Loss - Mean: -0.08898, Variance: 0.01315
Semantic Loss - Mean: 0.54891, Variance: 0.05569

Train Epoch: 75 
task: sign, mean loss: 0.01746, accuracy: 0.99457, avg. loss over tasks: 0.01746, lr: 0.00016692787674408067
Diversity Loss - Mean: -0.09188, Variance: 0.01159
Semantic Loss - Mean: 0.05230, Variance: 0.01027

Test Epoch: 75 
task: sign, mean loss: 0.48030, accuracy: 0.91716, avg. loss over tasks: 0.48030
Diversity Loss - Mean: -0.09757, Variance: 0.01321
Semantic Loss - Mean: 0.44890, Variance: 0.05519

Train Epoch: 76 
task: sign, mean loss: 0.00571, accuracy: 1.00000, avg. loss over tasks: 0.00571, lr: 0.00016358245043917945
Diversity Loss - Mean: -0.09213, Variance: 0.01161
Semantic Loss - Mean: 0.04126, Variance: 0.01021

Test Epoch: 76 
task: sign, mean loss: 0.43625, accuracy: 0.91716, avg. loss over tasks: 0.43625
Diversity Loss - Mean: -0.09985, Variance: 0.01327
Semantic Loss - Mean: 0.38021, Variance: 0.05461

Train Epoch: 77 
task: sign, mean loss: 0.00188, accuracy: 1.00000, avg. loss over tasks: 0.00188, lr: 0.00016023026049096414
Diversity Loss - Mean: -0.09291, Variance: 0.01164
Semantic Loss - Mean: 0.03801, Variance: 0.01017

Test Epoch: 77 
task: sign, mean loss: 0.42314, accuracy: 0.93491, avg. loss over tasks: 0.42314
Diversity Loss - Mean: -0.09771, Variance: 0.01333
Semantic Loss - Mean: 0.37594, Variance: 0.05403

Train Epoch: 78 
task: sign, mean loss: 0.00503, accuracy: 1.00000, avg. loss over tasks: 0.00503, lr: 0.00015687299482802466
Diversity Loss - Mean: -0.09435, Variance: 0.01167
Semantic Loss - Mean: 0.03158, Variance: 0.01007

Test Epoch: 78 
task: sign, mean loss: 0.42918, accuracy: 0.93491, avg. loss over tasks: 0.42918
Diversity Loss - Mean: -0.09777, Variance: 0.01338
Semantic Loss - Mean: 0.39888, Variance: 0.05350

Train Epoch: 79 
task: sign, mean loss: 0.00174, accuracy: 1.00000, avg. loss over tasks: 0.00174, lr: 0.0001535123439347264
Diversity Loss - Mean: -0.09598, Variance: 0.01170
Semantic Loss - Mean: 0.02291, Variance: 0.00999

Test Epoch: 79 
task: sign, mean loss: 0.47229, accuracy: 0.90533, avg. loss over tasks: 0.47229
Diversity Loss - Mean: -0.09857, Variance: 0.01344
Semantic Loss - Mean: 0.45494, Variance: 0.05296

Train Epoch: 80 
task: sign, mean loss: 0.00137, accuracy: 1.00000, avg. loss over tasks: 0.00137, lr: 0.00015015
Diversity Loss - Mean: -0.09653, Variance: 0.01172
Semantic Loss - Mean: 0.02489, Variance: 0.00989

Test Epoch: 80 
task: sign, mean loss: 0.45852, accuracy: 0.91716, avg. loss over tasks: 0.45852
Diversity Loss - Mean: -0.09984, Variance: 0.01350
Semantic Loss - Mean: 0.43203, Variance: 0.05237

Train Epoch: 81 
task: sign, mean loss: 0.01267, accuracy: 0.99457, avg. loss over tasks: 0.01267, lr: 0.00014678765606527362
Diversity Loss - Mean: -0.09798, Variance: 0.01175
Semantic Loss - Mean: 0.03385, Variance: 0.00981

Test Epoch: 81 
task: sign, mean loss: 0.37189, accuracy: 0.94083, avg. loss over tasks: 0.37189
Diversity Loss - Mean: -0.10078, Variance: 0.01356
Semantic Loss - Mean: 0.31792, Variance: 0.05178

Train Epoch: 82 
task: sign, mean loss: 0.03933, accuracy: 0.98370, avg. loss over tasks: 0.03933, lr: 0.00014342700517197535
Diversity Loss - Mean: -0.09881, Variance: 0.01177
Semantic Loss - Mean: 0.07254, Variance: 0.00981

Test Epoch: 82 
task: sign, mean loss: 0.50730, accuracy: 0.91716, avg. loss over tasks: 0.50730
Diversity Loss - Mean: -0.10280, Variance: 0.01361
Semantic Loss - Mean: 0.45713, Variance: 0.05127

Train Epoch: 83 
task: sign, mean loss: 0.11370, accuracy: 0.97283, avg. loss over tasks: 0.11370, lr: 0.0001400697395090358
Diversity Loss - Mean: -0.09845, Variance: 0.01180
Semantic Loss - Mean: 0.19992, Variance: 0.00994

Test Epoch: 83 
task: sign, mean loss: 0.76834, accuracy: 0.87574, avg. loss over tasks: 0.76834
Diversity Loss - Mean: -0.08475, Variance: 0.01365
Semantic Loss - Mean: 0.80174, Variance: 0.05090

Train Epoch: 84 
task: sign, mean loss: 0.15538, accuracy: 0.96196, avg. loss over tasks: 0.15538, lr: 0.0001367175495608205
Diversity Loss - Mean: -0.09636, Variance: 0.01183
Semantic Loss - Mean: 0.22639, Variance: 0.01013

Test Epoch: 84 
task: sign, mean loss: 0.74022, accuracy: 0.81657, avg. loss over tasks: 0.74022
Diversity Loss - Mean: -0.08159, Variance: 0.01369
Semantic Loss - Mean: 0.64578, Variance: 0.05055

Train Epoch: 85 
task: sign, mean loss: 0.04145, accuracy: 0.98913, avg. loss over tasks: 0.04145, lr: 0.0001333721232559193
Diversity Loss - Mean: -0.09839, Variance: 0.01185
Semantic Loss - Mean: 0.08485, Variance: 0.01015

Test Epoch: 85 
task: sign, mean loss: 0.58282, accuracy: 0.85207, avg. loss over tasks: 0.58282
Diversity Loss - Mean: -0.09645, Variance: 0.01371
Semantic Loss - Mean: 0.57930, Variance: 0.05015

Train Epoch: 86 
task: sign, mean loss: 0.02645, accuracy: 0.98913, avg. loss over tasks: 0.02645, lr: 0.00013003514511722433
Diversity Loss - Mean: -0.09834, Variance: 0.01188
Semantic Loss - Mean: 0.08970, Variance: 0.01018

Test Epoch: 86 
task: sign, mean loss: 0.70525, accuracy: 0.86982, avg. loss over tasks: 0.70525
Diversity Loss - Mean: -0.09859, Variance: 0.01374
Semantic Loss - Mean: 0.61700, Variance: 0.04977

Train Epoch: 87 
task: sign, mean loss: 0.01229, accuracy: 0.99457, avg. loss over tasks: 0.01229, lr: 0.00012670829541372138
Diversity Loss - Mean: -0.09792, Variance: 0.01191
Semantic Loss - Mean: 0.09641, Variance: 0.01030

Test Epoch: 87 
task: sign, mean loss: 0.75624, accuracy: 0.87574, avg. loss over tasks: 0.75624
Diversity Loss - Mean: -0.10195, Variance: 0.01378
Semantic Loss - Mean: 0.60345, Variance: 0.04935

Train Epoch: 88 
task: sign, mean loss: 0.04623, accuracy: 0.98913, avg. loss over tasks: 0.04623, lr: 0.0001233932493144243
Diversity Loss - Mean: -0.10387, Variance: 0.01195
Semantic Loss - Mean: 0.08297, Variance: 0.01030

Test Epoch: 88 
task: sign, mean loss: 0.37557, accuracy: 0.92899, avg. loss over tasks: 0.37557
Diversity Loss - Mean: -0.10081, Variance: 0.01385
Semantic Loss - Mean: 0.34437, Variance: 0.04891

Train Epoch: 89 
task: sign, mean loss: 0.08811, accuracy: 0.97826, avg. loss over tasks: 0.08811, lr: 0.00012009167604487657
Diversity Loss - Mean: -0.10173, Variance: 0.01199
Semantic Loss - Mean: 0.13589, Variance: 0.01027

Test Epoch: 89 
task: sign, mean loss: 0.28912, accuracy: 0.88166, avg. loss over tasks: 0.28912
Diversity Loss - Mean: -0.07503, Variance: 0.01389
Semantic Loss - Mean: 0.37226, Variance: 0.04850

Train Epoch: 90 
task: sign, mean loss: 0.03390, accuracy: 0.98913, avg. loss over tasks: 0.03390, lr: 0.00011680523804664632
Diversity Loss - Mean: -0.10625, Variance: 0.01204
Semantic Loss - Mean: 0.07954, Variance: 0.01028

Test Epoch: 90 
task: sign, mean loss: 0.30029, accuracy: 0.89941, avg. loss over tasks: 0.30029
Diversity Loss - Mean: -0.06809, Variance: 0.01392
Semantic Loss - Mean: 0.32528, Variance: 0.04802

Train Epoch: 91 
task: sign, mean loss: 0.02625, accuracy: 0.98913, avg. loss over tasks: 0.02625, lr: 0.00011353559014023658
Diversity Loss - Mean: -0.10675, Variance: 0.01208
Semantic Loss - Mean: 0.07425, Variance: 0.01028

Test Epoch: 91 
task: sign, mean loss: 0.49804, accuracy: 0.91716, avg. loss over tasks: 0.49804
Diversity Loss - Mean: -0.08715, Variance: 0.01390
Semantic Loss - Mean: 0.46101, Variance: 0.04767

Train Epoch: 92 
task: sign, mean loss: 0.01652, accuracy: 1.00000, avg. loss over tasks: 0.01652, lr: 0.00011028437869183373
Diversity Loss - Mean: -0.10671, Variance: 0.01212
Semantic Loss - Mean: 0.08685, Variance: 0.01039

Test Epoch: 92 
task: sign, mean loss: 0.36887, accuracy: 0.91716, avg. loss over tasks: 0.36887
Diversity Loss - Mean: -0.09877, Variance: 0.01391
Semantic Loss - Mean: 0.39489, Variance: 0.04735

Train Epoch: 93 
task: sign, mean loss: 0.00911, accuracy: 1.00000, avg. loss over tasks: 0.00911, lr: 0.0001070532407843122
Diversity Loss - Mean: -0.10533, Variance: 0.01215
Semantic Loss - Mean: 0.03564, Variance: 0.01034

Test Epoch: 93 
task: sign, mean loss: 0.55104, accuracy: 0.89941, avg. loss over tasks: 0.55104
Diversity Loss - Mean: -0.10227, Variance: 0.01394
Semantic Loss - Mean: 0.53266, Variance: 0.04704

Train Epoch: 94 
task: sign, mean loss: 0.01666, accuracy: 0.99457, avg. loss over tasks: 0.01666, lr: 0.00010384380339291414
Diversity Loss - Mean: -0.10337, Variance: 0.01218
Semantic Loss - Mean: 0.03965, Variance: 0.01026

Test Epoch: 94 
task: sign, mean loss: 0.51468, accuracy: 0.89349, avg. loss over tasks: 0.51468
Diversity Loss - Mean: -0.10439, Variance: 0.01398
Semantic Loss - Mean: 0.47649, Variance: 0.04679

Train Epoch: 95 
task: sign, mean loss: 0.00650, accuracy: 1.00000, avg. loss over tasks: 0.00650, lr: 0.00010065768256601821
Diversity Loss - Mean: -0.10309, Variance: 0.01221
Semantic Loss - Mean: 0.04001, Variance: 0.01021

Test Epoch: 95 
task: sign, mean loss: 0.44977, accuracy: 0.89349, avg. loss over tasks: 0.44977
Diversity Loss - Mean: -0.10466, Variance: 0.01402
Semantic Loss - Mean: 0.41881, Variance: 0.04650

Train Epoch: 96 
task: sign, mean loss: 0.04275, accuracy: 0.99457, avg. loss over tasks: 0.04275, lr: 9.749648261141081e-05
Diversity Loss - Mean: -0.10330, Variance: 0.01223
Semantic Loss - Mean: 0.06912, Variance: 0.01015

Test Epoch: 96 
task: sign, mean loss: 0.31208, accuracy: 0.91124, avg. loss over tasks: 0.31208
Diversity Loss - Mean: -0.10449, Variance: 0.01406
Semantic Loss - Mean: 0.29495, Variance: 0.04612

Train Epoch: 97 
task: sign, mean loss: 0.01490, accuracy: 0.98913, avg. loss over tasks: 0.01490, lr: 9.436179528846854e-05
Diversity Loss - Mean: -0.10404, Variance: 0.01226
Semantic Loss - Mean: 0.05052, Variance: 0.01013

Test Epoch: 97 
task: sign, mean loss: 0.28926, accuracy: 0.93491, avg. loss over tasks: 0.28926
Diversity Loss - Mean: -0.10505, Variance: 0.01409
Semantic Loss - Mean: 0.26744, Variance: 0.04569

Train Epoch: 98 
task: sign, mean loss: 0.00239, accuracy: 1.00000, avg. loss over tasks: 0.00239, lr: 9.125519900665955e-05
Diversity Loss - Mean: -0.10612, Variance: 0.01228
Semantic Loss - Mean: 0.02413, Variance: 0.01005

Test Epoch: 98 
task: sign, mean loss: 0.31517, accuracy: 0.92308, avg. loss over tasks: 0.31517
Diversity Loss - Mean: -0.10435, Variance: 0.01412
Semantic Loss - Mean: 0.29113, Variance: 0.04528

Train Epoch: 99 
task: sign, mean loss: 0.03415, accuracy: 0.99457, avg. loss over tasks: 0.03415, lr: 8.817825803076689e-05
Diversity Loss - Mean: -0.10580, Variance: 0.01230
Semantic Loss - Mean: 0.04640, Variance: 0.00998

Test Epoch: 99 
task: sign, mean loss: 0.40131, accuracy: 0.92308, avg. loss over tasks: 0.40131
Diversity Loss - Mean: -0.10525, Variance: 0.01415
Semantic Loss - Mean: 0.38557, Variance: 0.04487

Train Epoch: 100 
task: sign, mean loss: 0.01530, accuracy: 0.99457, avg. loss over tasks: 0.01530, lr: 8.513252169323391e-05
Diversity Loss - Mean: -0.10670, Variance: 0.01233
Semantic Loss - Mean: 0.05182, Variance: 0.00997

Test Epoch: 100 
task: sign, mean loss: 0.39186, accuracy: 0.92308, avg. loss over tasks: 0.39186
Diversity Loss - Mean: -0.10720, Variance: 0.01418
Semantic Loss - Mean: 0.37677, Variance: 0.04447

Train Epoch: 101 
task: sign, mean loss: 0.00449, accuracy: 1.00000, avg. loss over tasks: 0.00449, lr: 8.21195236140289e-05
Diversity Loss - Mean: -0.10641, Variance: 0.01236
Semantic Loss - Mean: 0.03019, Variance: 0.00993

Test Epoch: 101 
task: sign, mean loss: 0.56162, accuracy: 0.92308, avg. loss over tasks: 0.56162
Diversity Loss - Mean: -0.10728, Variance: 0.01420
Semantic Loss - Mean: 0.49696, Variance: 0.04411

Train Epoch: 102 
task: sign, mean loss: 0.00352, accuracy: 1.00000, avg. loss over tasks: 0.00352, lr: 7.914078092842115e-05
Diversity Loss - Mean: -0.10717, Variance: 0.01238
Semantic Loss - Mean: 0.03356, Variance: 0.00993

Test Epoch: 102 
task: sign, mean loss: 0.55191, accuracy: 0.92899, avg. loss over tasks: 0.55191
Diversity Loss - Mean: -0.10799, Variance: 0.01423
Semantic Loss - Mean: 0.47022, Variance: 0.04380

Train Epoch: 103 
task: sign, mean loss: 0.00711, accuracy: 0.99457, avg. loss over tasks: 0.00711, lr: 7.619779352305762e-05
Diversity Loss - Mean: -0.10726, Variance: 0.01241
Semantic Loss - Mean: 0.02936, Variance: 0.00987

Test Epoch: 103 
task: sign, mean loss: 0.50615, accuracy: 0.92308, avg. loss over tasks: 0.50615
Diversity Loss - Mean: -0.10675, Variance: 0.01425
Semantic Loss - Mean: 0.42797, Variance: 0.04348

Train Epoch: 104 
task: sign, mean loss: 0.00158, accuracy: 1.00000, avg. loss over tasks: 0.00158, lr: 7.329204328072498e-05
Diversity Loss - Mean: -0.10710, Variance: 0.01243
Semantic Loss - Mean: 0.01836, Variance: 0.00981

Test Epoch: 104 
task: sign, mean loss: 0.46502, accuracy: 0.93491, avg. loss over tasks: 0.46502
Diversity Loss - Mean: -0.10742, Variance: 0.01427
Semantic Loss - Mean: 0.37619, Variance: 0.04314

Train Epoch: 105 
task: sign, mean loss: 0.00307, accuracy: 1.00000, avg. loss over tasks: 0.00307, lr: 7.042499333417682e-05
Diversity Loss - Mean: -0.10769, Variance: 0.01245
Semantic Loss - Mean: 0.02335, Variance: 0.00974

Test Epoch: 105 
task: sign, mean loss: 0.53672, accuracy: 0.92308, avg. loss over tasks: 0.53672
Diversity Loss - Mean: -0.10793, Variance: 0.01428
Semantic Loss - Mean: 0.42619, Variance: 0.04281

Train Epoch: 106 
task: sign, mean loss: 0.00181, accuracy: 1.00000, avg. loss over tasks: 0.00181, lr: 6.759808732940244e-05
Diversity Loss - Mean: -0.10839, Variance: 0.01248
Semantic Loss - Mean: 0.02360, Variance: 0.00967

Test Epoch: 106 
task: sign, mean loss: 0.43385, accuracy: 0.93491, avg. loss over tasks: 0.43385
Diversity Loss - Mean: -0.10841, Variance: 0.01430
Semantic Loss - Mean: 0.35856, Variance: 0.04245

Train Epoch: 107 
task: sign, mean loss: 0.00362, accuracy: 1.00000, avg. loss over tasks: 0.00362, lr: 6.481274869870747e-05
Diversity Loss - Mean: -0.10735, Variance: 0.01250
Semantic Loss - Mean: 0.02504, Variance: 0.00961

Test Epoch: 107 
task: sign, mean loss: 0.48005, accuracy: 0.94083, avg. loss over tasks: 0.48005
Diversity Loss - Mean: -0.10825, Variance: 0.01432
Semantic Loss - Mean: 0.40249, Variance: 0.04209

Train Epoch: 108 
task: sign, mean loss: 0.00106, accuracy: 1.00000, avg. loss over tasks: 0.00106, lr: 6.207037994397291e-05
Diversity Loss - Mean: -0.10924, Variance: 0.01253
Semantic Loss - Mean: 0.02867, Variance: 0.00957

Test Epoch: 108 
task: sign, mean loss: 0.40949, accuracy: 0.94083, avg. loss over tasks: 0.40949
Diversity Loss - Mean: -0.10922, Variance: 0.01435
Semantic Loss - Mean: 0.34047, Variance: 0.04172

Train Epoch: 109 
task: sign, mean loss: 0.00160, accuracy: 1.00000, avg. loss over tasks: 0.00160, lr: 5.937236193045315e-05
Diversity Loss - Mean: -0.10966, Variance: 0.01255
Semantic Loss - Mean: 0.04181, Variance: 0.00956

Test Epoch: 109 
task: sign, mean loss: 0.41290, accuracy: 0.94083, avg. loss over tasks: 0.41290
Diversity Loss - Mean: -0.10868, Variance: 0.01438
Semantic Loss - Mean: 0.34587, Variance: 0.04137

Train Epoch: 110 
task: sign, mean loss: 0.00185, accuracy: 1.00000, avg. loss over tasks: 0.00185, lr: 5.6720053191468786e-05
Diversity Loss - Mean: -0.11035, Variance: 0.01258
Semantic Loss - Mean: 0.01396, Variance: 0.00948

Test Epoch: 110 
task: sign, mean loss: 0.43517, accuracy: 0.94675, avg. loss over tasks: 0.43517
Diversity Loss - Mean: -0.10591, Variance: 0.01439
Semantic Loss - Mean: 0.36036, Variance: 0.04104

Train Epoch: 111 
task: sign, mean loss: 0.00251, accuracy: 1.00000, avg. loss over tasks: 0.00251, lr: 5.4114789244344125e-05
Diversity Loss - Mean: -0.11109, Variance: 0.01260
Semantic Loss - Mean: 0.02975, Variance: 0.00943

Test Epoch: 111 
task: sign, mean loss: 0.37021, accuracy: 0.94675, avg. loss over tasks: 0.37021
Diversity Loss - Mean: -0.10657, Variance: 0.01442
Semantic Loss - Mean: 0.31902, Variance: 0.04070

Train Epoch: 112 
task: sign, mean loss: 0.00071, accuracy: 1.00000, avg. loss over tasks: 0.00071, lr: 5.155788191793406e-05
Diversity Loss - Mean: -0.11092, Variance: 0.01263
Semantic Loss - Mean: 0.01973, Variance: 0.00936

Test Epoch: 112 
task: sign, mean loss: 0.39060, accuracy: 0.94083, avg. loss over tasks: 0.39060
Diversity Loss - Mean: -0.10800, Variance: 0.01444
Semantic Loss - Mean: 0.32234, Variance: 0.04037

Train Epoch: 113 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 4.905061869207866e-05
Diversity Loss - Mean: -0.11069, Variance: 0.01265
Semantic Loss - Mean: 0.01788, Variance: 0.00929

Test Epoch: 113 
task: sign, mean loss: 0.38078, accuracy: 0.94675, avg. loss over tasks: 0.38078
Diversity Loss - Mean: -0.10872, Variance: 0.01447
Semantic Loss - Mean: 0.32097, Variance: 0.04004

Train Epoch: 114 
task: sign, mean loss: 0.00135, accuracy: 1.00000, avg. loss over tasks: 0.00135, lr: 4.659426204931833e-05
Diversity Loss - Mean: -0.11008, Variance: 0.01268
Semantic Loss - Mean: 0.02591, Variance: 0.00925

Test Epoch: 114 
task: sign, mean loss: 0.40175, accuracy: 0.94083, avg. loss over tasks: 0.40175
Diversity Loss - Mean: -0.10954, Variance: 0.01449
Semantic Loss - Mean: 0.33059, Variance: 0.03973

Train Epoch: 115 
task: sign, mean loss: 0.00159, accuracy: 1.00000, avg. loss over tasks: 0.00159, lr: 4.419004883919586e-05
Diversity Loss - Mean: -0.11006, Variance: 0.01270
Semantic Loss - Mean: 0.02743, Variance: 0.00921

Test Epoch: 115 
task: sign, mean loss: 0.42257, accuracy: 0.92899, avg. loss over tasks: 0.42257
Diversity Loss - Mean: -0.10818, Variance: 0.01451
Semantic Loss - Mean: 0.36222, Variance: 0.03944

Train Epoch: 116 
task: sign, mean loss: 0.00315, accuracy: 1.00000, avg. loss over tasks: 0.00315, lr: 4.183918965546539e-05
Diversity Loss - Mean: -0.11009, Variance: 0.01272
Semantic Loss - Mean: 0.01448, Variance: 0.00913

Test Epoch: 116 
task: sign, mean loss: 0.37842, accuracy: 0.92899, avg. loss over tasks: 0.37842
Diversity Loss - Mean: -0.10774, Variance: 0.01454
Semantic Loss - Mean: 0.33767, Variance: 0.03915

Train Epoch: 117 
task: sign, mean loss: 0.00171, accuracy: 1.00000, avg. loss over tasks: 0.00171, lr: 3.954286822652202e-05
Diversity Loss - Mean: -0.11122, Variance: 0.01274
Semantic Loss - Mean: 0.02335, Variance: 0.00908

Test Epoch: 117 
task: sign, mean loss: 0.40956, accuracy: 0.92899, avg. loss over tasks: 0.40956
Diversity Loss - Mean: -0.10960, Variance: 0.01457
Semantic Loss - Mean: 0.34819, Variance: 0.03886

Train Epoch: 118 
task: sign, mean loss: 0.01161, accuracy: 0.99457, avg. loss over tasks: 0.01161, lr: 3.730224081935891e-05
Diversity Loss - Mean: -0.11132, Variance: 0.01276
Semantic Loss - Mean: 0.02831, Variance: 0.00902

Test Epoch: 118 
task: sign, mean loss: 0.34872, accuracy: 0.92899, avg. loss over tasks: 0.34872
Diversity Loss - Mean: -0.10730, Variance: 0.01458
Semantic Loss - Mean: 0.30018, Variance: 0.03857

Train Epoch: 119 
task: sign, mean loss: 0.00411, accuracy: 1.00000, avg. loss over tasks: 0.00411, lr: 3.5118435657352036e-05
Diversity Loss - Mean: -0.11181, Variance: 0.01278
Semantic Loss - Mean: 0.02261, Variance: 0.00896

Test Epoch: 119 
task: sign, mean loss: 0.30828, accuracy: 0.95266, avg. loss over tasks: 0.30828
Diversity Loss - Mean: -0.10959, Variance: 0.01460
Semantic Loss - Mean: 0.25886, Variance: 0.03826

Train Epoch: 120 
task: sign, mean loss: 0.00069, accuracy: 1.00000, avg. loss over tasks: 0.00069, lr: 3.299255235216578e-05
Diversity Loss - Mean: -0.11065, Variance: 0.01280
Semantic Loss - Mean: 0.02556, Variance: 0.00891

Test Epoch: 120 
task: sign, mean loss: 0.31200, accuracy: 0.94675, avg. loss over tasks: 0.31200
Diversity Loss - Mean: -0.11042, Variance: 0.01462
Semantic Loss - Mean: 0.26680, Variance: 0.03796

Train Epoch: 121 
task: sign, mean loss: 0.00489, accuracy: 1.00000, avg. loss over tasks: 0.00489, lr: 3.092566135006513e-05
Diversity Loss - Mean: -0.11241, Variance: 0.01282
Semantic Loss - Mean: 0.03902, Variance: 0.00886

Test Epoch: 121 
task: sign, mean loss: 0.33245, accuracy: 0.94675, avg. loss over tasks: 0.33245
Diversity Loss - Mean: -0.10936, Variance: 0.01464
Semantic Loss - Mean: 0.29132, Variance: 0.03767

Train Epoch: 122 
task: sign, mean loss: 0.00243, accuracy: 1.00000, avg. loss over tasks: 0.00243, lr: 2.891880339291414e-05
Diversity Loss - Mean: -0.11155, Variance: 0.01285
Semantic Loss - Mean: 0.02580, Variance: 0.00881

Test Epoch: 122 
task: sign, mean loss: 0.37651, accuracy: 0.94675, avg. loss over tasks: 0.37651
Diversity Loss - Mean: -0.11087, Variance: 0.01466
Semantic Loss - Mean: 0.32396, Variance: 0.03738

Train Epoch: 123 
task: sign, mean loss: 0.00039, accuracy: 1.00000, avg. loss over tasks: 0.00039, lr: 2.6972988994130713e-05
Diversity Loss - Mean: -0.11293, Variance: 0.01287
Semantic Loss - Mean: 0.01182, Variance: 0.00875

Test Epoch: 123 
task: sign, mean loss: 0.39482, accuracy: 0.94675, avg. loss over tasks: 0.39482
Diversity Loss - Mean: -0.11022, Variance: 0.01467
Semantic Loss - Mean: 0.33454, Variance: 0.03710

Train Epoch: 124 
task: sign, mean loss: 0.00261, accuracy: 1.00000, avg. loss over tasks: 0.00261, lr: 2.5089197929862797e-05
Diversity Loss - Mean: -0.11279, Variance: 0.01289
Semantic Loss - Mean: 0.01958, Variance: 0.00870

Test Epoch: 124 
task: sign, mean loss: 0.37582, accuracy: 0.93491, avg. loss over tasks: 0.37582
Diversity Loss - Mean: -0.11054, Variance: 0.01469
Semantic Loss - Mean: 0.32153, Variance: 0.03683

Train Epoch: 125 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 2.326837874564162e-05
Diversity Loss - Mean: -0.11260, Variance: 0.01291
Semantic Loss - Mean: 0.01023, Variance: 0.00863

Test Epoch: 125 
task: sign, mean loss: 0.39553, accuracy: 0.94083, avg. loss over tasks: 0.39553
Diversity Loss - Mean: -0.11148, Variance: 0.01471
Semantic Loss - Mean: 0.33449, Variance: 0.03655

Train Epoch: 126 
task: sign, mean loss: 0.00104, accuracy: 1.00000, avg. loss over tasks: 0.00104, lr: 2.1511448278760362e-05
Diversity Loss - Mean: -0.11273, Variance: 0.01293
Semantic Loss - Mean: 0.01420, Variance: 0.00857

Test Epoch: 126 
task: sign, mean loss: 0.36153, accuracy: 0.94083, avg. loss over tasks: 0.36153
Diversity Loss - Mean: -0.11147, Variance: 0.01474
Semantic Loss - Mean: 0.31668, Variance: 0.03628

Train Epoch: 127 
task: sign, mean loss: 0.00253, accuracy: 1.00000, avg. loss over tasks: 0.00253, lr: 1.981929119661905e-05
Diversity Loss - Mean: -0.11181, Variance: 0.01295
Semantic Loss - Mean: 0.01871, Variance: 0.00852

Test Epoch: 127 
task: sign, mean loss: 0.39493, accuracy: 0.93491, avg. loss over tasks: 0.39493
Diversity Loss - Mean: -0.11114, Variance: 0.01476
Semantic Loss - Mean: 0.34274, Variance: 0.03602

Train Epoch: 128 
task: sign, mean loss: 0.00252, accuracy: 1.00000, avg. loss over tasks: 0.00252, lr: 1.819275955126781e-05
Diversity Loss - Mean: -0.11304, Variance: 0.01297
Semantic Loss - Mean: 0.01028, Variance: 0.00846

Test Epoch: 128 
task: sign, mean loss: 0.38221, accuracy: 0.94083, avg. loss over tasks: 0.38221
Diversity Loss - Mean: -0.11031, Variance: 0.01478
Semantic Loss - Mean: 0.34386, Variance: 0.03576

Train Epoch: 129 
task: sign, mean loss: 0.00526, accuracy: 1.00000, avg. loss over tasks: 0.00526, lr: 1.6632672350373086e-05
Diversity Loss - Mean: -0.11285, Variance: 0.01299
Semantic Loss - Mean: 0.01637, Variance: 0.00840

Test Epoch: 129 
task: sign, mean loss: 0.40136, accuracy: 0.94675, avg. loss over tasks: 0.40136
Diversity Loss - Mean: -0.11114, Variance: 0.01479
Semantic Loss - Mean: 0.35615, Variance: 0.03550

Train Epoch: 130 
task: sign, mean loss: 0.00091, accuracy: 1.00000, avg. loss over tasks: 0.00091, lr: 1.5139815144822505e-05
Diversity Loss - Mean: -0.11261, Variance: 0.01301
Semantic Loss - Mean: 0.01934, Variance: 0.00836

Test Epoch: 130 
task: sign, mean loss: 0.39410, accuracy: 0.94675, avg. loss over tasks: 0.39410
Diversity Loss - Mean: -0.11062, Variance: 0.01481
Semantic Loss - Mean: 0.35110, Variance: 0.03525

Train Epoch: 131 
task: sign, mean loss: 0.00119, accuracy: 1.00000, avg. loss over tasks: 0.00119, lr: 1.371493963317641e-05
Diversity Loss - Mean: -0.11198, Variance: 0.01302
Semantic Loss - Mean: 0.01147, Variance: 0.00830

Test Epoch: 131 
task: sign, mean loss: 0.44737, accuracy: 0.94675, avg. loss over tasks: 0.44737
Diversity Loss - Mean: -0.11069, Variance: 0.01483
Semantic Loss - Mean: 0.39229, Variance: 0.03500

Train Epoch: 132 
task: sign, mean loss: 0.00124, accuracy: 1.00000, avg. loss over tasks: 0.00124, lr: 1.235876328316513e-05
Diversity Loss - Mean: -0.11366, Variance: 0.01304
Semantic Loss - Mean: 0.01423, Variance: 0.00824

Test Epoch: 132 
task: sign, mean loss: 0.42385, accuracy: 0.94675, avg. loss over tasks: 0.42385
Diversity Loss - Mean: -0.11096, Variance: 0.01484
Semantic Loss - Mean: 0.36672, Variance: 0.03476

Train Epoch: 133 
task: sign, mean loss: 0.00114, accuracy: 1.00000, avg. loss over tasks: 0.00114, lr: 1.1071968970422028e-05
Diversity Loss - Mean: -0.11437, Variance: 0.01306
Semantic Loss - Mean: 0.02346, Variance: 0.00821

Test Epoch: 133 
task: sign, mean loss: 0.45417, accuracy: 0.94675, avg. loss over tasks: 0.45417
Diversity Loss - Mean: -0.10954, Variance: 0.01486
Semantic Loss - Mean: 0.39409, Variance: 0.03451

Train Epoch: 134 
task: sign, mean loss: 0.00068, accuracy: 1.00000, avg. loss over tasks: 0.00068, lr: 9.855204634635412e-06
Diversity Loss - Mean: -0.11385, Variance: 0.01308
Semantic Loss - Mean: 0.00781, Variance: 0.00815

Test Epoch: 134 
task: sign, mean loss: 0.42500, accuracy: 0.94675, avg. loss over tasks: 0.42500
Diversity Loss - Mean: -0.11137, Variance: 0.01487
Semantic Loss - Mean: 0.36628, Variance: 0.03428

Train Epoch: 135 
task: sign, mean loss: 0.00206, accuracy: 1.00000, avg. loss over tasks: 0.00206, lr: 8.70908295329112e-06
Diversity Loss - Mean: -0.11223, Variance: 0.01310
Semantic Loss - Mean: 0.00946, Variance: 0.00810

Test Epoch: 135 
task: sign, mean loss: 0.45427, accuracy: 0.94675, avg. loss over tasks: 0.45427
Diversity Loss - Mean: -0.11153, Variance: 0.01489
Semantic Loss - Mean: 0.38258, Variance: 0.03404

Train Epoch: 136 
task: sign, mean loss: 0.00070, accuracy: 1.00000, avg. loss over tasks: 0.00070, lr: 7.634181033171244e-06
Diversity Loss - Mean: -0.11393, Variance: 0.01312
Semantic Loss - Mean: 0.01373, Variance: 0.00805

Test Epoch: 136 
task: sign, mean loss: 0.41288, accuracy: 0.94675, avg. loss over tasks: 0.41288
Diversity Loss - Mean: -0.11233, Variance: 0.01491
Semantic Loss - Mean: 0.35292, Variance: 0.03381

Train Epoch: 137 
task: sign, mean loss: 0.00057, accuracy: 1.00000, avg. loss over tasks: 0.00057, lr: 6.631040119763172e-06
Diversity Loss - Mean: -0.11359, Variance: 0.01313
Semantic Loss - Mean: 0.01570, Variance: 0.00800

Test Epoch: 137 
task: sign, mean loss: 0.38023, accuracy: 0.95266, avg. loss over tasks: 0.38023
Diversity Loss - Mean: -0.11162, Variance: 0.01493
Semantic Loss - Mean: 0.33056, Variance: 0.03358

Train Epoch: 138 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 5.700165324726391e-06
Diversity Loss - Mean: -0.11357, Variance: 0.01315
Semantic Loss - Mean: 0.01899, Variance: 0.00797

Test Epoch: 138 
task: sign, mean loss: 0.38495, accuracy: 0.95266, avg. loss over tasks: 0.38495
Diversity Loss - Mean: -0.11164, Variance: 0.01495
Semantic Loss - Mean: 0.33579, Variance: 0.03335

Train Epoch: 139 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 4.842025371553471e-06
Diversity Loss - Mean: -0.11335, Variance: 0.01317
Semantic Loss - Mean: 0.01847, Variance: 0.00792

Test Epoch: 139 
task: sign, mean loss: 0.40194, accuracy: 0.94675, avg. loss over tasks: 0.40194
Diversity Loss - Mean: -0.11183, Variance: 0.01497
Semantic Loss - Mean: 0.35084, Variance: 0.03312

Train Epoch: 140 
task: sign, mean loss: 0.00099, accuracy: 1.00000, avg. loss over tasks: 0.00099, lr: 4.05705235955373e-06
Diversity Loss - Mean: -0.11377, Variance: 0.01319
Semantic Loss - Mean: 0.02921, Variance: 0.00789

Test Epoch: 140 
task: sign, mean loss: 0.39688, accuracy: 0.94675, avg. loss over tasks: 0.39688
Diversity Loss - Mean: -0.11219, Variance: 0.01499
Semantic Loss - Mean: 0.34200, Variance: 0.03290

Train Epoch: 141 
task: sign, mean loss: 0.00075, accuracy: 1.00000, avg. loss over tasks: 0.00075, lr: 3.3456415462781634e-06
Diversity Loss - Mean: -0.11384, Variance: 0.01320
Semantic Loss - Mean: 0.00766, Variance: 0.00783

Test Epoch: 141 
task: sign, mean loss: 0.35808, accuracy: 0.95266, avg. loss over tasks: 0.35808
Diversity Loss - Mean: -0.11194, Variance: 0.01501
Semantic Loss - Mean: 0.31716, Variance: 0.03268

Train Epoch: 142 
task: sign, mean loss: 0.00390, accuracy: 1.00000, avg. loss over tasks: 0.00390, lr: 2.708151148495344e-06
Diversity Loss - Mean: -0.11275, Variance: 0.01322
Semantic Loss - Mean: 0.02064, Variance: 0.00779

Test Epoch: 142 
task: sign, mean loss: 0.37389, accuracy: 0.94675, avg. loss over tasks: 0.37389
Diversity Loss - Mean: -0.11288, Variance: 0.01503
Semantic Loss - Mean: 0.32350, Variance: 0.03246

Train Epoch: 143 
task: sign, mean loss: 0.00105, accuracy: 1.00000, avg. loss over tasks: 0.00105, lr: 2.1449021618186215e-06
Diversity Loss - Mean: -0.11470, Variance: 0.01324
Semantic Loss - Mean: 0.01808, Variance: 0.00775

Test Epoch: 143 
task: sign, mean loss: 0.39042, accuracy: 0.94675, avg. loss over tasks: 0.39042
Diversity Loss - Mean: -0.11200, Variance: 0.01505
Semantic Loss - Mean: 0.33794, Variance: 0.03225

Train Epoch: 144 
task: sign, mean loss: 0.00073, accuracy: 1.00000, avg. loss over tasks: 0.00073, lr: 1.656178199074985e-06
Diversity Loss - Mean: -0.11362, Variance: 0.01325
Semantic Loss - Mean: 0.01267, Variance: 0.00770

Test Epoch: 144 
task: sign, mean loss: 0.39087, accuracy: 0.95266, avg. loss over tasks: 0.39087
Diversity Loss - Mean: -0.11290, Variance: 0.01507
Semantic Loss - Mean: 0.33970, Variance: 0.03204

Train Epoch: 145 
task: sign, mean loss: 0.00056, accuracy: 1.00000, avg. loss over tasks: 0.00056, lr: 1.2422253474976123e-06
Diversity Loss - Mean: -0.11437, Variance: 0.01326
Semantic Loss - Mean: 0.01555, Variance: 0.00767

Test Epoch: 145 
task: sign, mean loss: 0.42655, accuracy: 0.94675, avg. loss over tasks: 0.42655
Diversity Loss - Mean: -0.11193, Variance: 0.01509
Semantic Loss - Mean: 0.36492, Variance: 0.03183

Train Epoch: 146 
task: sign, mean loss: 0.00129, accuracy: 1.00000, avg. loss over tasks: 0.00129, lr: 9.032520448134255e-07
Diversity Loss - Mean: -0.11375, Variance: 0.01328
Semantic Loss - Mean: 0.02481, Variance: 0.00765

Test Epoch: 146 
task: sign, mean loss: 0.47052, accuracy: 0.94675, avg. loss over tasks: 0.47052
Diversity Loss - Mean: -0.10996, Variance: 0.01509
Semantic Loss - Mean: 0.40297, Variance: 0.03163

Train Epoch: 147 
task: sign, mean loss: 0.00063, accuracy: 1.00000, avg. loss over tasks: 0.00063, lr: 6.39428974288556e-07
Diversity Loss - Mean: -0.11402, Variance: 0.01330
Semantic Loss - Mean: 0.02116, Variance: 0.00761

Test Epoch: 147 
task: sign, mean loss: 0.42986, accuracy: 0.94675, avg. loss over tasks: 0.42986
Diversity Loss - Mean: -0.10979, Variance: 0.01510
Semantic Loss - Mean: 0.37387, Variance: 0.03143

Train Epoch: 148 
task: sign, mean loss: 0.00407, accuracy: 1.00000, avg. loss over tasks: 0.00407, lr: 4.5088897878400875e-07
Diversity Loss - Mean: -0.11232, Variance: 0.01331
Semantic Loss - Mean: 0.01771, Variance: 0.00757

Test Epoch: 148 
task: sign, mean loss: 0.44932, accuracy: 0.94675, avg. loss over tasks: 0.44932
Diversity Loss - Mean: -0.11143, Variance: 0.01511
Semantic Loss - Mean: 0.38565, Variance: 0.03123

Train Epoch: 149 
task: sign, mean loss: 0.00203, accuracy: 1.00000, avg. loss over tasks: 0.00203, lr: 3.3772699386539635e-07
Diversity Loss - Mean: -0.11253, Variance: 0.01332
Semantic Loss - Mean: 0.03998, Variance: 0.00758

Test Epoch: 149 
task: sign, mean loss: 0.38482, accuracy: 0.94675, avg. loss over tasks: 0.38482
Diversity Loss - Mean: -0.11225, Variance: 0.01513
Semantic Loss - Mean: 0.33462, Variance: 0.03103

Train Epoch: 150 
task: sign, mean loss: 0.00066, accuracy: 1.00000, avg. loss over tasks: 0.00066, lr: 3e-07
Diversity Loss - Mean: -0.11452, Variance: 0.01334
Semantic Loss - Mean: 0.01477, Variance: 0.00753

Test Epoch: 150 
task: sign, mean loss: 0.40090, accuracy: 0.95266, avg. loss over tasks: 0.40090
Diversity Loss - Mean: -0.11226, Variance: 0.01515
Semantic Loss - Mean: 0.34597, Variance: 0.03084

